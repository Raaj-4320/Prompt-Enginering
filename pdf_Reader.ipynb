{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raaj-4320/Prompt-Enginering/blob/main/pdf_Reader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkqSwythjtSL",
        "outputId": "ca9da607-3908-4d1f-b05f-7e4312e75bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxW-f9RCj9uo"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES2rwCDOkAGv"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_pages, extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cZoAgQcxkLpk",
        "outputId": "f8c56c55-8c3d-4b2f-9b35-9213583db1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "<LTTextBoxHorizontal(8) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.986,40.500,432.002,49.500 '541\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,485.150,432.375,607.500>\n",
            "<LTLine 72.000,485.275,432.500,485.275>\n",
            "<LTLine 72.125,485.150,72.125,607.500>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 126.250,491.130,378.250,602.250 matrix=[252.00,0.00,0.00,111.12, (126.25,491.13)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'need to wait until you have seen the whole sentence before translating it. We will go\\nthrough the implementation of an encoder–decoder in Chapter 16 (as you will see, it\\nis a bit more complex than what Figure 15-4 suggests).\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.004,301.806,419.282,324.906 'Figure 15-4. Sequence-to-sequence (top left), sequence-to-vector (top right), vector-to-\\nsequence (bottom left), and encoder–decoder (bottom right) networks\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.004,277.206,425.308,287.706 'This versatility sounds promising, but how do you train a recurrent neural network?\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,202.005,432.004,264.866 'Training RNNs\\nTo  train  an  RNN,  the  trick  is  to  unroll  it  through  time  (like  we  just  did)  and  then\\nuse regular backpropagation (see Figure 15-5). This strategy is called backpropagation\\nthrough time (BPTT).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,70.005,432.003,193.905 'Just like in regular backpropagation, there is a first forward pass through the unrolled\\nnetwork (represented by the dashed arrows). Then the output sequence is evaluated\\nusing  a  loss  function  ℒ(Y(0),  Y(1),  …,  Y(T);  Ŷ(0),  Ŷ(1),  …,  Ŷ(T))  (where  Y(i)  is  the  ith\\ntarget,  Ŷ(i)  is  the  ith  prediction,  and  T  is  the  max  time  step).  Note  that  this  loss\\nfunction  may  ignore  some  outputs.  For  example,  in  a  sequence-to-vector  RNN,  all\\noutputs are ignored except for the very last one. In Figure 15-5, the loss function is\\ncomputed based on the last three outputs only. The gradients of that loss function are\\nthen  propagated  backward  through  the  unrolled  network  (represented  by  the  solid\\narrows).  In  this  example,  since  the  outputs  Ŷ(0)  and  Ŷ(1)  are  not  used  to  compute\\nthe loss, the gradients do not flow backward through them; they only flow through\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.004,40.500,84.442,49.500 '542 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.910,40.500,266.935,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,563.575,432.500,563.575>\n",
            "<LTLine 432.375,330.469,432.375,563.700>\n",
            "<LTLine 72.000,330.594,432.500,330.594>\n",
            "<LTLine 72.125,330.469,72.125,563.700>\n",
            "<LTFigure(I1) 81.249,336.449,423.251,558.450 matrix=[342.00,0.00,0.00,222.00, (81.25,336.45)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,544.636,432.004,605.537 'Ŷ(2),  Ŷ(3),  and  Ŷ(4).  Moreover,  since  the  same  parameters  W  and  b  are  used  at  each\\ntime step, their gradients will be tweaked multiple times during backprop. Once the\\nbackward  phase  is  complete  and  all  the  gradients  have  been  computed,  BPTT  can\\nperform  a  gradient  descent  step  to  update  the  parameters  (this  is  no  different  from\\nregular backprop).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.998,338.223,247.926,348.723 'Figure 15-5. Backpropagation through time\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,288.423,431.998,324.123 'Fortunately,  Keras  takes  care  of  all  of  this  complexity  for  you,  as  you  will  see.  But\\nbefore we get there, let’s load a time series and start analyzing it using classical tools\\nto better understand what we’re dealing with, and to get some baseline metrics.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.992,188.023,432.005,276.084 'Forecasting a Time Series\\nAll right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s Transit\\nAuthority.  Your  first  task  is  to  build  a  model  capable  of  forecasting  the  number\\nof  passengers  that  will  ride  on  bus  and  rail  the  next  day.  You  have  access  to  daily\\nridership  data  since  2001.  Let’s  walk  through  together  how  you  would  handle  this.\\nWe’ll start by loading and cleaning up the data:3\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,162.336,190.997,181.036 'import pandas as pd\\nfrom pathlib import Path\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,111.336,416.247,150.436 'path = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\\ndf = pd.read_csv(path, parse_dates=[\"service_date\"])\\ndf.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]  # shorter names\\ndf = df.sort_values(\"date\").set_index(\"date\")\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,68.954,366.304,76.954 '3 The latest data from the Chicago Transit Authority is available at the Chicago Data Portal.\\n'>\n",
            "<LTTextBoxHorizontal(7) 326.723,40.500,402.512,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.980,40.500,431.996,49.500 '543\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,538.375,432.500,538.375>\n",
            "<LTLine 432.375,354.287,432.375,538.500>\n",
            "<LTLine 72.000,354.412,432.500,354.412>\n",
            "<LTLine 72.125,354.287,72.125,538.500>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 144.250,360.267,360.250,533.250 matrix=[216.00,0.00,0.00,172.98, (144.25,360.27)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,587.950,407.750,606.650 'df = df.drop(\"total\", axis=1)  # no need for total, it\\'s just bus + rail\\ndf = df.drop_duplicates()  # remove duplicated months (2011-10 and 2014-07)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,542.843,432.004,579.137 'We  load  the  CSV  file,  set  short  column  names,  sort  the  rows  by  date,  remove  the\\nredundant total column, and drop duplicate rows. Now let’s check what the first few\\nrows look like:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,455.957,237.750,535.857 '>>> df.head()\\n           day_type     bus    rail\\ndate\\n2001-01-01        U  297192  126455\\n2001-01-02        W  780827  501952\\n2001-01-03        W  824923  536432\\n2001-01-04        W  870021  550011\\n2001-01-05        W  890426  557917\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,410.850,432.004,447.143 'On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded\\na  train.  The  day_type  column  contains  W  for  Weekdays,  A  for  Saturdays,  and  U  for\\nSundays or holidays.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,379.650,432.001,402.750 'Now  let’s  plot  the  bus  and  rail  ridership  figures  over  a  few  months  in  2019,  to  see\\nwhat it looks like (see Figure 15-6):\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,364.163,220.747,372.663 'import matplotlib.pyplot as plt\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.997,333.563,382.247,352.263 'df[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,161.182,231.111,171.682 'Figure 15-6. Daily ridership in Chicago\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.000,40.500,84.438,49.500 '544 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.906,40.500,266.931,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,326.588,432.500,326.588>\n",
            "<LTLine 432.375,177.245,432.375,326.713>\n",
            "<LTLine 72.000,177.370,432.500,177.370>\n",
            "<LTLine 72.125,177.245,72.125,326.713>\n",
            "<LTFigure(I1) 79.452,183.225,425.048,321.463 matrix=[345.60,0.00,0.00,138.24, (79.45,183.23)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,493.643,432.005,605.537 'Note that Pandas includes both the start and end month in the range, so this plots the\\ndata from the 1st of March all the way up to the 31st of May. This is a time series: data\\nwith values at different time steps, usually at regular intervals. More specifically, since\\nthere are multiple values per time step, this is called a multivariate time series. If we\\nonly looked at the bus column, it would be a univariate time series, with a single value\\nper time step. Predicting future values (i.e., forecasting) is the most typical task when\\ndealing with time series, and this is what we will focus on in this chapter. Other tasks\\ninclude imputation (filling in missing past values), classification, anomaly detection,\\nand more.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,412.043,432.005,485.543 'Looking  at  Figure  15-6,  we  can  see  that  a  similar  pattern  is  clearly  repeated  every\\nweek.  This  is  called  a  weekly  seasonality.  In  fact,  it’s  so  strong  in  this  case  that\\nforecasting tomorrow’s ridership by just copying the values from a week earlier will\\nyield  reasonably  good  results.  This  is  called  naive  forecasting:  simply  copying  a  past\\nvalue to make our forecast. Naive forecasting is often a great baseline, and it can even\\nbe tricky to beat in some cases.\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.794,350.952,396.004,395.112 'In general, naive forecasting means copying the latest known value\\n(e.g.,  forecasting  that  tomorrow  will  be  the  same  as  today).  How‐\\never, in our case, copying the value from the previous week works\\nbetter, due to the strong weekly seasonality.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,281.243,432.003,329.543 'To visualize these naive forecasts, let’s overlay the two time series (for bus and rail) as\\nwell as the same time series lagged by one week (i.e., shifted toward the right) using\\ndotted  lines.  We’ll  also  plot  the  difference  between  the  two  (i.e.,  the  value  at  time  t\\nminus the value at time t – 7); this is called differencing (see Figure 15-7):\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,265.757,331.250,274.257 'diff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,204.557,416.250,253.857 'fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\\ndf.plot(ax=axs[0], legend=False, marker=\".\")  # original time series\\ndf.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")  # lagged\\ndiff_7.plot(ax=axs[1], grid=True, marker=\".\")  # 7-day difference time series\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,134.250,432.005,195.743 'Not  too  bad!  Notice  how  closely  the  lagged  time  series  track  the  actual  time  series.\\nWhen a time series is correlated with a lagged version of itself, we say that the time\\nseries is autocorrelated. As you can see, most of the differences are fairly small, except\\nat the end of May. Maybe there was a holiday at that time? Let’s check the day_type\\ncolumn:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.996,108.563,322.746,127.263 '>>> list(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])\\n[\\'A\\', \\'U\\', \\'U\\']\\n'>\n",
            "<LTTextBoxHorizontal(8) 326.725,40.500,402.514,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.982,40.500,431.998,49.500 '545\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,347.406,126.944,396.907 matrix=[37.94,0.00,0.00,49.50, (89.00,347.41)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,360.733,409.764,383.833 'Figure 15-7. Time series overlaid with 7-day lagged time series (top), and difference\\nbetween t and t – 7 (bottom)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,298.333,432.004,346.633 'Indeed,  there  was  a  long  weekend  back  then:  the  Monday  was  the  Memorial  Day\\nholiday.  We  could  use  this  column  to  improve  our  forecasts,  but  for  now  let’s  just\\nmeasure the mean absolute error over the three-month period we’re arbitrarily focus‐\\ning on—March, April, and May 2019—to get a rough idea:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,252.247,186.751,291.347 '>>> diff_7.abs().mean()\\nbus     43915.608696\\nrail    42143.271739\\ndtype: float64\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,207.733,432.005,243.433 'Our  naive  forecasts  get  an  MAE  of  about  43,916  bus  riders,  and  about  42,143  rail\\nriders.  It’s  hard  to  tell  at  a  glance  how  good  or  bad  this  is,  so  let’s  put  the  forecast\\nerrors into perspective by dividing them by the target values:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.998,151.447,318.498,200.747 '>>> targets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\\n>>> (diff_7 / targets).abs().mean()\\nbus     0.082938\\nrail    0.089948\\ndtype: float64\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,81.733,432.005,142.633 'What we just computed is called the mean absolute percentage error (MAPE): it looks\\nlike our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0% for rail. It’s\\ninteresting  to  note  that  the  MAE  for  the  rail  forecasts  looks  slightly  better  than  the\\nMAE  for  the  bus  forecasts,  while  the  opposite  is  true  for  the  MAPE.  That’s  because\\nthe bus ridership is larger than the rail ridership, so naturally the forecast errors are\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.004,40.500,84.442,49.500 '546 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.910,40.500,266.935,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,389.397,432.375,607.500>\n",
            "<LTLine 72.000,389.522,432.500,389.522>\n",
            "<LTLine 72.125,389.397,72.125,607.500>\n",
            "<LTFigure(I1) 79.450,395.377,425.050,602.250 matrix=[345.60,0.00,0.00,206.87, (79.45,395.38)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'also  larger,  but  when  we  put  the  errors  into  perspective,  it  turns  out  that  the  bus\\nforecasts are actually slightly better than the rail forecasts.\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.792,498.305,396.003,565.505 'The MAE, MAPE, and MSE are among the most common metrics\\nyou  can  use  to  evaluate  your  forecasts.  As  always,  choosing  the\\nright  metric  depends  on  the  task.  For  example,  if  your  project\\nsuffers quadratically more from large errors than from small ones,\\nthen  the  MSE  may  be  preferable,  as  it  strongly  penalizes  large\\nerrors.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,420.316,432.004,481.216 'Looking at the time series, there doesn’t appear to be any significant monthly season‐\\nality, but let’s check whether there’s any yearly seasonality. We’ll look at the data from\\n2001 to 2019. To reduce the risk of data snooping, we’ll ignore more recent data for\\nnow. Let’s also plot a 12-month rolling average for each series to visualize long-term\\ntrends (see Figure 15-8):\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,384.430,395.000,413.330 'period = slice(\"2001\", \"2019\")\\ndf_monthly = df.resample(\\'M\\').mean()  # compute the mean for each month\\nrolling_average_12_months = df_monthly[period].rolling(window=12).mean()\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,333.430,352.500,372.530 'fig, ax = plt.subplots(figsize=(8, 4))\\ndf_monthly[period].plot(ax=ax, marker=\".\")\\nrolling_average_12_months.plot(ax=ax, grid=True, legend=False)\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,138.496,285.000,148.996 'Figure 15-8. Yearly seasonality and long-term trends\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,76.097,432.004,124.396 'Yep! There’s definitely some yearly seasonality as well, although it is noisier than the\\nweekly  seasonality,  and  more  visible  for  the  rail  series  than  the  bus  series:  we  see\\npeaks and troughs at roughly the same dates each year. Let’s check what we get if we\\nplot the 12-month difference (see Figure 15-9):\\n'>\n",
            "<LTTextBoxHorizontal(7) 326.730,40.500,402.519,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.987,40.500,432.003,49.500 '547\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,326.455,432.500,326.455>\n",
            "<LTLine 432.375,154.560,432.375,326.580>\n",
            "<LTLine 72.000,154.685,432.500,154.685>\n",
            "<LTLine 72.125,154.560,72.125,326.580>\n",
            "<LTFigure(I1) 85.000,515.523,126.760,571.300 matrix=[41.76,0.00,0.00,55.78, (85.00,515.52)]>\n",
            "<LTFigure(I2) 79.454,160.540,425.046,321.330 matrix=[345.59,0.00,0.00,160.79, (79.45,160.54)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,587.950,390.750,606.650 'df_monthly.diff(12)[period].plot(grid=True, marker=\".\", figsize=(8, 3))\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,437.897,223.175,448.397 'Figure 15-9. The 12-month difference\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,312.497,432.004,423.797 'Notice how differencing not only removed the yearly seasonality, but it also removed\\nthe  long-term  trends.  For  example,  the  linear  downward  trend  present  in  the  time\\nseries from 2016 to 2019 became a roughly constant negative value in the differenced\\ntime  series.  In  fact,  differencing  is  a  common  technique  used  to  remove  trend  and\\nseasonality  from  a  time  series:  it’s  easier  to  study  a  stationary  time  series,  meaning\\none whose statistical properties remain constant over time, without any seasonality or\\ntrends. Once you’re able to make accurate forecasts on the differenced time series, it’s\\neasy to turn them into forecasts for the actual time series by just adding back the past\\nvalues that were previously subtracted.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,205.696,432.004,304.397 'You  may  be  thinking  that  we’re  only  trying  to  predict  tomorrow’s  ridership,  so  the\\nlong-term patterns matter much less than the short-term ones. You’re right, but still,\\nwe  may  be  able  to  improve  performance  slightly  by  taking  long-term  patterns  into\\naccount. For example, daily bus ridership dropped by about 2,500 in October 2017,\\nwhich represents about 570 fewer passengers each week, so if we were at the end of\\nOctober 2017, it would make sense to forecast tomorrow’s ridership by copying the\\nvalue from last week, minus 570. Accounting for the trend will make your forecasts a\\nbit more accurate on average.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,149.296,432.004,197.596 'Now that you’re familiar with the ridership time series, as well as some of the most\\nimportant concepts in time series analysis, including seasonality, trend, differencing,\\nand  moving  averages,  let’s  take  a  quick  look  at  a  very  popular  family  of  statistical\\nmodels that are commonly used to analyze time series.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.003,40.500,84.441,49.500 '548 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.909,40.500,266.934,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,580.975,432.500,580.975>\n",
            "<LTLine 432.375,453.960,432.375,581.100>\n",
            "<LTLine 72.000,454.085,432.500,454.085>\n",
            "<LTLine 72.125,453.960,72.125,581.100>\n",
            "<LTFigure(I1) 79.450,459.940,425.050,575.850 matrix=[345.60,0.00,0.00,115.91, (79.45,459.94)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,511.055,432.004,607.894 'The ARMA Model Family\\nWe’ll  start  with  the  autoregressive  moving  average  (ARMA)  model,  developed  by\\nHerman  Wold  in  the  1930s:  it  computes  its  forecasts  using  a  simple  weighted  sum\\nof lagged values and corrects these forecasts by adding a moving average, very much\\nlike we just discussed. Specifically, the moving average component is computed using\\na weighted sum of the last few forecast errors. Equation 15-3 shows how the model\\nmakes its forecasts.\\n'>\n",
            "<LTTextBoxHorizontal(1) 86.995,484.768,292.774,495.268 'Equation 15-3. Forecasting using an ARMA model\\n'>\n",
            "<LTTextBoxHorizontal(2) 119.174,469.950,123.446,477.950 'p\\n'>\n",
            "<LTTextBoxHorizontal(3) 87.004,445.121,166.866,474.181 'y t = ∑i = 1\\nwith\\xa0ϵ t = y t − y t\\n'>\n",
            "<LTTextBoxHorizontal(4) 135.826,460.683,201.318,474.181 'αi y t − i + ∑i = 1\\n'>\n",
            "<LTTextBoxHorizontal(5) 202.432,461.177,231.594,474.181 'θi ϵ t − i\\n'>\n",
            "<LTTextBoxHorizontal(6) 185.726,469.948,189.742,477.948 'q\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.003,417.651,140.894,428.151 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.650,324.051,432.003,405.551 '• ŷ(t) is the model’s forecast for time step t.\\n•\\n•\\n• y(t) is the time series’ value at time step t.\\n•\\n• The  first  sum  is  the  weighted  sum  of  the  past  p  values  of  the  time  series,  using\\nthe  learned  weights  αi.  The  number  p  is  a  hyperparameter,  and  it  determines\\nhow far back into the past the model should look. This sum is the autoregressive\\ncomponent of the model: it performs regression based on past values.\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.659,282.251,432.002,317.951 '•\\n• The second sum is the weighted sum over the past q forecast errors ε(t), using the\\nlearned  weights  θi.  The  number  q  is  a  hyperparameter.  This  sum  is  the  moving\\naverage component of the model.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,196.650,432.003,270.151 'Importantly,  this  model  assumes  that  the  time  series  is  stationary.  If  it  is  not,  then\\ndifferencing  may  help.  Using  differencing  over  a  single  time  step  will  produce  an\\napproximation  of  the  derivative  of  the  time  series:  indeed,  it  will  give  the  slope\\nof  the  series  at  each  time  step.  This  means  that  it  will  eliminate  any  linear  trend,\\ntransforming it into a constant value. For example, if you apply one-step differencing\\nto the series [3, 5, 7, 9, 11], you get the differenced series [2, 2, 2, 2].\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.996,89.850,432.005,188.550 'If the original time series has a quadratic trend instead of a linear trend, then a single\\nround of differencing will not be enough. For example, the series [1, 4, 9, 16, 25, 36]\\nbecomes  [3,  5,  7,  9,  11]  after  one  round  of  differencing,  but  if  you  run  differencing\\nfor a second round, then you get [2, 2, 2, 2]. So, running two rounds of differencing\\nwill  eliminate  quadratic  trends.  More  generally,  running  d  consecutive  rounds  of\\ndifferencing computes an approximation of the dth order derivative of the time series,\\nso it will eliminate polynomial trends up to degree d. This hyperparameter d is called\\nthe order of integration.\\n'>\n",
            "<LTTextBoxHorizontal(12) 326.731,40.500,402.520,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.988,40.500,432.004,49.500 '549\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 87.633,472.514,93.293,474.214>\n",
            "<LTCurve 93.853,462.331,95.693,470.331>\n",
            "<LTCurve 99.441,462.331,101.281,470.331>\n",
            "<LTCurve 150.707,462.331,152.547,470.331>\n",
            "<LTCurve 167.158,462.331,168.998,470.331>\n",
            "<LTCurve 215.613,462.331,217.453,470.331>\n",
            "<LTCurve 232.064,462.331,233.904,470.331>\n",
            "<LTCurve 111.881,445.114,113.721,453.114>\n",
            "<LTCurve 117.469,445.114,119.309,453.114>\n",
            "<LTCurve 136.798,445.114,138.638,453.114>\n",
            "<LTCurve 142.385,445.114,144.225,453.114>\n",
            "<LTCurve 155.671,455.297,161.331,456.997>\n",
            "<LTCurve 161.891,445.114,163.731,453.114>\n",
            "<LTCurve 167.478,445.114,169.318,453.114>\n",
            "<LTTextBoxHorizontal(0) 71.997,532.037,432.005,605.537 'Differencing is the central contribution of the autoregressive integrated moving average\\n(ARIMA)  model,  introduced  in  1970  by  George  Box  and  Gwilym  Jenkins  in  their\\nbook Time Series Analysis (Wiley): this model runs d rounds of differencing to make\\nthe time series more stationary, then it applies a regular ARMA model. When making\\nforecasts, it uses this ARMA model, then it adds back the terms that were subtracted\\nby differencing.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,425.236,432.005,523.937 'One  last  member  of  the  ARMA  family  is  the  seasonal  ARIMA  (SARIMA)  model:\\nit  models  the  time  series  in  the  same  way  as  ARIMA,  but  it  additionally  models\\na  seasonal  component  for  a  given  frequency  (e.g.,  weekly),  using  the  exact  same\\nARIMA  approach.  It  has  a  total  of  seven  hyperparameters:  the  same  p,  d,  and  q\\nhyperparameters as ARIMA, plus additional P, D, and Q hyperparameters to model\\nthe  seasonal  pattern,  and  lastly  the  period  of  the  seasonal  pattern,  noted  s.  The\\nhyperparameters P, D, and Q are just like p, d, and q, but they are used to model the\\ntime series at t – s, t – 2s, t – 3s, etc.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,355.050,432.003,417.136 'Let’s  see  how  to  fit  a  SARIMA  model  to  the  rail  time  series,  and  use  it  to  make  a\\nforecast for tomorrow’s ridership. We’ll pretend today is the last day of May 2019, and\\nwe want to forecast the rail ridership for “tomorrow”, the 1st of June, 2019. For this,\\nwe can use the statsmodels library, which contains many different statistical models,\\nincluding the ARMA model and its variants, implemented by the ARIMA class:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,339.563,280.249,348.063 'from statsmodels.tsa.arima.model import ARIMA\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,257.963,318.499,327.663 'origin, today = \"2019-01-01\", \"2019-05-31\"\\nrail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\\nmodel = ARIMA(rail_series,\\n              order=(1, 0, 0),\\n              seasonal_order=(0, 1, 1, 7))\\nmodel = model.fit()\\ny_pred = model.forecast()  # returns 427,758.6\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,238.650,161.198,249.150 'In this code example:\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.655,163.870,432.002,227.516 '• We start by importing the ARIMA class, then we take the rail ridership data from\\n•\\nthe  start  of  2019  up  to  “today”,  and  we  use  asfreq(\"D\")  to  set  the  time  series’\\nfrequency to daily: this doesn’t change the data at all in this case, since it’s already\\ndaily, but without this the ARIMA class would have to guess the frequency, and it\\nwould display a warning.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.652,93.904,432.005,158.736 '• Next, we create an ARIMA instance, passing it all the data until “today”, and we set\\n•\\nthe model hyperparameters: order=(1, 0, 0) means that p = 1, d = 0, q = 0, and\\nseasonal_order=(0, 1, 1, 7) means that P = 0, D = 1, Q = 1, and s = 7. Notice\\nthat the statsmodels API differs a bit from Scikit-Learn’s API, since we pass the\\ndata to the model at construction time, instead of passing it to the fit() method.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,40.500,84.436,49.500 '550 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.904,40.500,266.929,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 80.651,595.037,432.004,605.537 '•\\n• Next, we fit the model, and we use it to make a forecast for “tomorrow”, the 1st of\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.998,582.437,136.229,592.937 'June, 2019.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,509.436,432.003,570.337 'The  forecast  is  427,759  passengers,  when  in  fact  there  were  379,044.  Yikes,  we’re\\n12.9% off—that’s pretty bad. It’s actually slightly worse than naive forecasting, which\\nforecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day? To check\\nthis, we can run the same code in a loop to make forecasts for every day in March,\\nApril, and May, and compute the MAE over that period:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,391.950,407.749,502.450 'origin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\\ntime_period = pd.date_range(start_date, end_date)\\nrail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\\ny_preds = []\\nfor today in time_period.shift(-1):\\n    model = ARIMA(rail_series[origin:today],  # train on data up to \"today\"\\n                  order=(1, 0, 0),\\n                  seasonal_order=(0, 1, 1, 7))\\n    model = model.fit()  # note that we retrain the model every day!\\n    y_pred = model.forecast()[0]\\n    y_preds.append(y_pred)\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,361.350,407.749,380.050 'y_preds = pd.Series(y_preds, index=time_period)\\nmae = (y_preds - rail_series[time_period]).abs().mean()  # returns 32,040.7\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,316.836,432.001,352.537 'Ah,  that’s  much  better!  The  MAE  is  about  32,041,  which  is  significantly  lower  than\\nthe MAE we got with naive forecasting (42,143). So although the model is not perfect,\\nit still beats naive forecasting by a large margin, on average.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,184.836,432.006,308.736 'At  this  point,  you  may  be  wondering  how  to  pick  good  hyperparameters  for  the\\nSARIMA  model.  There  are  several  methods,  but  the  simplest  to  understand  and\\nto  get  started  with  is  the  brute-force  approach:  just  run  a  grid  search.  For  each\\nmodel  you  want  to  evaluate  (i.e.,  each  hyperparameter  combination),  you  can  run\\nthe preceding code example, changing only the hyperparameter values. Good p, q, P,\\nand Q values are usually fairly small (typically 0 to 2, sometimes up to 5 or 6), and d\\nand D are typically 0 or 1, sometimes 2. As for s, it’s just the main seasonal pattern’s\\nperiod: in our case it’s 7 since there’s a strong weekly seasonality. The model with the\\nlowest MAE wins. Of course, you can replace the MAE with another metric if it better\\nmatches your business objective. And that’s it!4\\n'>\n",
            "<LTTextBoxHorizontal(7) 73.140,108.954,409.912,116.954 '4 There are other more principled approaches to selecting good hyperparameters, based on analyzing the\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.000,68.954,419.168,106.954 'autocorrelation function (ACF) and partial autocorrelation function (PACF), or minimizing the AIC or BIC\\nmetrics (introduced in Chapter 9) to penalize models that use too many parameters and reduce the risk of\\noverfitting the data, but grid search is a good place to start. For more details on the ACF-PACF approach,\\ncheck out this very nice post by Jason Brownlee.\\n'>\n",
            "<LTTextBoxHorizontal(9) 326.727,40.500,402.516,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.984,40.500,432.000,49.500 '551\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,124.900,162.000,124.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,498.455,432.005,607.894 'Preparing the Data for Machine Learning Models\\nNow that we have two baselines, naive forecasting and SARIMA, let’s try to use the\\nmachine  learning  models  we’ve  covered  so  far  to  forecast  this  time  series,  starting\\nwith  a  basic  linear  model.  Our  goal  will  be  to  forecast  tomorrow’s  ridership  based\\non  the  ridership  of  the  past  8  weeks  of  data  (56  days).  The  inputs  to  our  model\\nwill  therefore  be  sequences  (usually  a  single  sequence  per  day  once  the  model  is  in\\nproduction),  each  containing  56  values  from  time  steps  t  –  55  to  t.  For  each  input\\nsequence, the model will output a single value: the forecast for time step t + 1.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,454.655,432.001,490.355 'But what will we use as training data? Well, that’s the trick: we will use every 56-day\\nwindow  from  the  past  as  training  data,  and  the  target  for  each  window  will  be  the\\nvalue immediately following it.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,359.268,432.005,447.521 'Keras  actually  has  a  nice  utility  function  called  tf.keras.utils.timeseries_\\ndataset_from_array()  to  help  us  prepare  the  training  set.  It  takes  a  time  series\\nas  input,  and  it  builds  a  tf.data.Dataset  (introduced  in  Chapter  13)  containing  all\\nthe  windows  of  the  desired  length,  as  well  as  their  corresponding  targets.  Here’s  an\\nexample that takes a time series containing the numbers 0 to 5 and creates a dataset\\ncontaining  all  the  windows  of  length  3,  with  their  corresponding  targets,  grouped\\ninto batches of size 2:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.002,343.782,186.752,352.282 'import tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,262.182,382.252,331.882 'my_series = [0, 1, 2, 3, 4, 5]\\nmy_dataset = tf.keras.utils.timeseries_dataset_from_array(\\n    my_series,\\n    targets=my_series[3:],  # the targets are 3 steps into the future\\n    sequence_length=3,\\n    batch_size=2\\n)\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,242.868,239.045,253.368 'Let’s inspect the contents of this dataset:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.003,166.182,429.003,235.882 '>>> list(my_dataset)\\n[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[0, 1, 2],\\n         [1, 2, 3]], dtype=int32)>,\\n  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>),\\n (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]], dtype=int32)>,\\n  <tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>)]\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,96.468,432.004,157.368 'Each  sample  in  the  dataset  is  a  window  of  length  3,  along  with  its  corresponding\\ntarget  (i.e.,  the  value  immediately  after  the  window).  The  windows  are  [0,  1,  2],  [1,\\n2, 3], and [2, 3, 4], and their respective targets are 3, 4, and 5. Since there are three\\nwindows in total, which is not a multiple of the batch size, the last batch only contains\\none window instead of two.\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.003,40.500,84.441,49.500 '552 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.909,40.500,266.934,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,556.050,432.002,606.503 'Another way to get the same result is to use the window() method of tf.data’s Dataset\\nclass. It’s more complex, but it gives you full control, which will come in handy later\\nin this chapter, so let’s see how it works. The  window() method returns a dataset of\\nwindow datasets:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.003,438.563,386.503,549.063 '>>> for window_dataset in tf.data.Dataset.range(6).window(4, shift=1):\\n...     for element in window_dataset:\\n...         print(f\"{element}\", end=\" \")\\n...     print()\\n...\\n0 1 2 3\\n1 2 3 4\\n2 3 4 5\\n3 4 5\\n4 5\\n5\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,380.857,432.001,429.750 'In this example, the dataset contains six windows, each shifted by one step compared\\nto the previous one, and the last three windows are smaller because they’ve reached\\nthe  end  of  the  series.  In  general  you’ll  want  to  get  rid  of  these  smaller  windows  by\\npassing drop_remainder=True to the window() method.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,323.863,432.003,373.723 'The  window()  method  returns  a  nested  dataset,  analogous  to  a  list  of  lists.  This  is\\nuseful when you want to transform each window by calling its dataset methods (e.g.,\\nto shuffle them or batch them). However, we cannot use a nested dataset directly for\\ntraining, as our model will expect tensors as input, not datasets.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,266.870,432.004,316.729 'Therefore,  we  must  call  the  flat_map()  method:  it  converts  a  nested  dataset  into  a\\nflat  dataset  (one  that  contains  tensors,  not  datasets).  For  example,  suppose  {1,  2,  3}\\nrepresents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the\\nnested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,196.684,432.003,259.736 'Moreover, the flat_map() method takes a function as an argument, which allows you\\nto transform each dataset in the nested dataset before flattening. For example, if you\\npass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the\\nnested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset\\ncontaining 3 tensors, each of size 2.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,178.084,297.094,188.584 'With that in mind, we are ready to flatten our dataset:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.997,91.197,420.497,171.097 '>>> dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)\\n>>> dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))\\n>>> for window_tensor in dataset:\\n...     print(f\"{window_tensor}\")\\n...\\n[0 1 2 3]\\n[1 2 3 4]\\n[2 3 4 5]\\n'>\n",
            "<LTTextBoxHorizontal(8) 326.723,40.500,402.512,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.980,40.500,431.996,49.500 '553\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,556.643,432.000,606.503 'Since each window dataset contains exactly four items, calling batch(4) on a window\\nproduces a single tensor of size 4. Great! We now have a dataset containing consecu‐\\ntive  windows  represented  as  tensors.  Let’s  create  a  little  helper  function  to  make  it\\neasier to extract windows from a dataset:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,520.757,386.500,549.657 'def to_windows(dataset, length):\\n    dataset = dataset.window(length, shift=1, drop_remainder=True)\\n    return dataset.flat_map(lambda window_ds: window_ds.batch(length))\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,488.250,431.998,512.909 'The last step is to split each window into inputs and targets, using the map() method.\\nWe can also group the resulting windows into batches of size 2:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,391.163,429.000,481.263 '>>> dataset = to_windows(tf.data.Dataset.range(6), 4)  # 3 inputs + 1 target = 4\\n>>> dataset = dataset.map(lambda window: (window[:-1], window[-1]))\\n>>> list(dataset.batch(2))\\n[(<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\\n  array([[0, 1, 2],\\n         [1, 2, 3]])>,\\n  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])>),\\n (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]])>,\\n  <tf.Tensor: shape=(1,), dtype=int64, numpy=array([5])>)]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,346.057,432.003,382.350 'As  you  can  see,  we  now  have  the  same  output  as  we  got  earlier  with  the\\ntimeseries_dataset_from_array()  function  (with  a  bit  more  effort,  but  it  will  be\\nworthwhile soon).\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,289.657,432.003,337.957 'Now,  before  we  start  training,  we  need  to  split  our  data  into  a  training  period,  a\\nvalidation period, and a test period. We will focus on the rail ridership for now. We\\nwill also scale it down by a factor of one million, to ensure the values are near the 0–1\\nrange; this plays nicely with the default weight initialization and learning rate:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.004,253.770,301.504,282.670 'rail_train = df[\"rail\"][\"2016-01\":\"2018-12\"] / 1e6\\nrail_valid = df[\"rail\"][\"2019-01\":\"2019-05\"] / 1e6\\nrail_test = df[\"rail\"][\"2019-06\":] / 1e6\\n'>\n",
            "<LTTextBoxHorizontal(7) 136.786,111.325,395.999,236.125 'When  dealing  with  time  series,  you  generally  want  to  split  across\\ntime. However, in some cases you may be able to split along other\\ndimensions,  which  will  give  you  a  longer  time  period  to  train\\non.  For  example,  if  you  have  data  about  the  financial  health  of\\n10,000  companies  from  2001  to  2019,  you  might  be  able  to  split\\nthis  data  across  the  different  companies.  It’s  very  likely  that  many\\nof these companies will be strongly correlated, though (e.g., whole\\neconomic  sectors  may  go  up  or  down  jointly),  and  if  you  have\\ncorrelated companies across the training set and the test set, your\\ntest  set  will  not  be  as  useful,  as  its  measure  of  the  generalization\\nerror will be optimistically biased.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,40.500,84.435,49.500 '554 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.903,40.500,266.928,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,188.420,126.944,237.920 matrix=[37.94,0.00,0.00,49.50, (89.00,188.42)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,556.050,432.002,606.503 'Next, let’s use timeseries_dataset_from_array() to create datasets for training and\\nvalidation.  Since  gradient  descent  expects  the  instances  in  the  training  set  to  be\\nindependent and identically distributed (IID), as we saw in Chapter 4, we must set the\\nargument shuffle=True to shuffle the training windows (but not their contents):\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,397.764,327.002,549.064 'seq_length = 56\\ntrain_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    rail_train.to_numpy(),\\n    targets=rail_train[seq_length:],\\n    sequence_length=seq_length,\\n    batch_size=32,\\n    shuffle=True,\\n    seed=42\\n)\\nvalid_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    rail_valid.to_numpy(),\\n    targets=rail_valid[seq_length:],\\n    sequence_length=seq_length,\\n    batch_size=32\\n)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,378.450,368.499,388.950 'And now we’re ready to build and train any regression model we want!\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,307.841,432.005,366.880 'Forecasting Using a Linear Model\\nLet’s  try  a  basic  linear  model  first.  We  will  use  the  Huber  loss,  which  usually  works\\nbetter than minimizing the MAE directly, as discussed in Chapter 10. We’ll also use\\nearly stopping:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,200.554,407.750,300.854 'tf.random.set_seed(42)\\nmodel = tf.keras.Sequential([\\n    tf.keras.layers.Dense(1, input_shape=[seq_length])\\n])\\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\\n    monitor=\"val_mae\", patience=50, restore_best_weights=True)\\nopt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\\nhistory = model.fit(train_ds, validation_data=valid_ds, epochs=500,\\n                    callbacks=[early_stopping_cb])\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.002,168.641,431.784,191.741 'This model reaches a validation MAE of about 37,866 (your mileage may vary). That’s\\nbetter than naive forecasting, but worse than the SARIMA model.5\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.005,150.041,244.698,160.541 'Can we do better with an RNN? Let’s see!\\n'>\n",
            "<LTTextBoxHorizontal(7) 73.140,88.954,409.832,96.954 '5 Note that the validation period starts on the 1st of January 2019, so the first prediction is for the 26th of\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.000,68.954,427.224,86.954 'February 2019, eight weeks later. When we evaluated the baseline models we used predictions starting on the\\n1st of March instead, but this should be close enough.\\n'>\n",
            "<LTTextBoxHorizontal(9) 326.724,40.500,402.513,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.981,40.500,431.997,49.500 '555\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTTextBoxHorizontal(0) 72.000,561.455,432.003,607.894 'Forecasting Using a Simple RNN\\nLet’s  try  the  most  basic  RNN,  containing  a  single  recurrent  layer  with  just  one\\nrecurrent neuron, as we saw in Figure 15-1:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,525.568,322.749,554.468 'model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\\n])\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,391.075,432.005,516.755 'All recurrent layers in Keras expect 3D inputs of shape [batch size, time steps, dimen‐\\nsionality], where dimensionality is 1 for univariate time series and more for multivari‐\\nate  time  series.  Recall  that  the  input_shape  argument  ignores  the  first  dimension\\n(i.e.,  the  batch  size),  and  since  recurrent  layers  can  accept  input  sequences  of  any\\nlength, we can set the second dimension to None, which means “any size”. Lastly, since\\nwe’re dealing with a univariate time series, we need the last dimension’s size to be 1.\\nThis is why we specified the input shape [None, 1]: it means “univariate sequences\\nof any length”. Note that the datasets actually contain inputs of shape [batch size, time\\nsteps], so we’re missing the last dimension, of size 1, but Keras is kind enough to add\\nit for us in this case.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,259.075,432.002,382.975 'This  model  works  exactly  as  we  saw  earlier:  the  initial  state  h(init)  is  set  to  0,  and  it\\nis  passed  to  a  single  recurrent  neuron,  along  with  the  value  of  the  first  time  step,\\nx(0). The neuron computes a weighted sum of these values plus the bias term, and it\\napplies the activation function to the result, using the hyperbolic tangent function by\\ndefault. The result is the first output, y0. In a simple RNN, this output is also the new\\nstate  h0.  This  new  state  is  passed  to  the  same  recurrent  neuron  along  with  the  next\\ninput value, x(1), and the process is repeated until the last time step. At the end, the\\nlayer just outputs the last value: in our case the sequences are 56 steps long, so the last\\nvalue is y55. All of this is performed simultaneously for every sequence in the batch, of\\nwhich there are 32 in this case.\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.786,208.961,395.996,242.143 'By  default,  recurrent  layers  in  Keras  only  return  the  final  output.\\nTo  make  them  return  one  output  per  time  step,  you  must  set\\nreturn_sequences=True, as you will see.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,153.475,432.003,176.575 'So  that’s  our  first  recurrent  model!  It’s  a  sequence-to-vector  model.  Since  there’s  a\\nsingle output neuron, the output vector has a size of 1.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,109.675,432.000,145.375 'Now if you compile, train, and evaluate this model just like the previous model, you\\nwill  find  that  it’s  no  good  at  all:  its  validation  MAE  is  greater  than  100,000!  Ouch.\\nThat was to be expected, for two reasons:\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.001,40.500,84.439,49.500 '556 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.907,40.500,266.932,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,194.438,126.944,243.939 matrix=[37.94,0.00,0.00,49.50, (89.00,194.44)]>\n",
            "<LTTextBoxHorizontal(0) 77.316,490.236,432.005,601.537 '1. The  model  only  has  a  single  recurrent  neuron,  so  the  only  data  it  can  use  to\\n1.\\nmake  a  prediction  at  each  time  step  is  the  input  value  at  the  current  time  step\\nand the output value from the previous time step. That’s not much to go on! In\\nother  words,  the  RNN’s  memory  is  extremely  limited:  it’s  just  a  single  number,\\nits previous output. And let’s count how many parameters this model has: since\\nthere’s  just  one  recurrent  neuron  with  only  two  input  values,  the  whole  model\\nonly has three parameters (two weights plus a bias term). That’s far from enough\\nfor this time series. In contrast, our previous model could look at all 56 previous\\nvalues at once, and it had a total of 57 parameters.\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.315,448.436,432.002,484.137 '2. The time series contains values from 0 to about 1.4, but since the default activa‐\\n2.\\ntion function is tanh, the recurrent layer can only output values between –1 and\\n+1. There’s no way it can predict values between 1.0 and 1.4.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,362.836,432.005,436.336 'Let’s  fix  both  of  these  issues:  we  will  create  a  model  with  a  larger  recurrent  layer,\\ncontaining  32  recurrent  neurons,  and  we  will  add  a  dense  output  layer  on  top  of  it\\nwith  a  single  output  neuron  and  no  activation  function.  The  recurrent  layer  will  be\\nable to carry much more information from one time step to the next, and the dense\\noutput layer will project the final output from 32 dimensions down to 1, without any\\nvalue range constraints:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.998,316.750,365.248,355.850 'univar_model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\\n    tf.keras.layers.Dense(1)  # no activation function by default\\n])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,272.236,432.005,307.936 'Now if you compile, fit, and evaluate this model just like the previous one, you will\\nfind that its validation MAE reaches 27,703. That’s the best model we’ve trained so far,\\nand it even beats the SARIMA model: we’re doing pretty well!\\n'>\n",
            "<LTTextBoxHorizontal(5) 136.790,188.105,396.001,255.305 'We’ve  only  normalized  the  time  series,  without  removing  trend\\nand seasonality, and yet the model still performs well. This is con‐\\nvenient,  as  it  makes  it  possible  to  quickly  search  for  promising\\nmodels without worrying too much about preprocessing. However,\\nto get the best performance, you may want to try making the time\\nseries more stationary; for example, using differencing.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.998,126.935,432.005,173.374 'Forecasting Using a Deep RNN\\nIt  is  quite  common  to  stack  multiple  layers  of  cells,  as  shown  in  Figure  15-10.  This\\ngives you a deep RNN.\\n'>\n",
            "<LTTextBoxHorizontal(7) 326.731,40.500,402.520,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.988,40.500,432.004,49.500 '557\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,205.323,126.760,261.100 matrix=[41.76,0.00,0.00,55.78, (85.00,205.32)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,432.180,322.866,442.680 'Figure 15-10. A deep RNN (left) unrolled through time (right)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,317.012,432.005,418.080 'Implementing a deep RNN with Keras is straightforward: just stack recurrent layers.\\nIn the following example, we use three SimpleRNN layers (but we could use any other\\ntype  of  recurrent  layer  instead,  such  as  an  LSTM  layer  or  a  GRU  layer,  which  we  will\\ndiscuss  shortly).  The  first  two  are  sequence-to-sequence  layers,  and  the  last  one  is\\na  sequence-to-vector  layer.  Finally,  the  Dense  layer  produces  the  model’s  forecast\\n(you can think of it as a vector-to-vector layer). So this model is just like the model\\nrepresented in Figure 15-10, except the outputs Ŷ(0) to Ŷ(t–1_) are ignored, and there’s a\\ndense layer on top of Ŷ(t), which outputs the actual forecast:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,251.114,429.000,310.614 'deep_model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\\n    tf.keras.layers.SimpleRNN(32, return_sequences=True),\\n    tf.keras.layers.SimpleRNN(32),\\n    tf.keras.layers.Dense(1)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.785,154.206,396.005,234.352 'Make  sure  to  set  return_sequences=True  for  all  recurrent  layers\\n(except the last one, if you only care about the last output). If you\\nforget to set this parameter for one recurrent layer, it will output a\\n2D array containing only the output of the last time step, instead of\\na 3D array containing outputs for all time steps. The next recurrent\\nlayer  will  complain  that  you  are  not  feeding  it  sequences  in  the\\nexpected 3D format.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,101.418,431.999,137.118 'If  you  train  and  evaluate  this  model,  you  will  find  that  it  reaches  an  MAE  of  about\\n31,211. That’s better than both baselines, but it doesn’t beat our “shallower” RNN. It\\nlooks like this RNN is a bit too large for our task.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,40.500,84.438,49.500 '558 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.906,40.500,266.931,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,448.243,432.375,607.500>\n",
            "<LTLine 72.000,448.368,432.500,448.368>\n",
            "<LTLine 72.125,448.243,72.125,607.500>\n",
            "<LTFigure(I1) 83.000,186.954,132.680,234.264 matrix=[49.68,0.00,0.00,47.31, (83.00,186.95)]>\n",
            "<LTFigure(I2) 126.250,454.223,378.250,602.250 matrix=[252.00,0.00,0.00,148.03, (126.25,454.22)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,498.455,432.005,607.894 'Forecasting Multivariate Time Series\\nA great quality of neural networks is their flexibility: in particular, they can deal with\\nmultivariate  time  series  with  almost  no  change  to  their  architecture.  For  example,\\nlet’s  try  to  forecast  the  rail  time  series  using  both  the  bus  and  rail  data  as  input.  In\\nfact, let’s also throw in the day type! Since we can always know in advance whether\\ntomorrow is going to be a weekday, a weekend, or a holiday, we can shift the day type\\nseries one day into the future, so that the model is given tomorrow’s day type as input.\\nFor simplicity, we’ll do this processing using Pandas:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.997,462.568,428.997,491.468 'df_mulvar = df[[\"bus\", \"rail\"]] / 1e6  # use both bus & rail series as input\\ndf_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)  # we know tomorrow\\'s type\\ndf_mulvar = pd.get_dummies(df_mulvar)  # one-hot encode the day type\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,404.268,432.003,454.721 'Now df_mulvar is a DataFrame with five columns: the bus and rail data, plus three\\ncolumns containing the one-hot encoding of the next day’s type (recall that there are\\nthree possible day types, W, A, and U). Next we can proceed much like we did earlier.\\nFirst we split the data into three periods, for training, validation, and testing:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.002,368.382,280.252,397.282 'mulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\\nmulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\\nmulvar_test = df_mulvar[\"2019-06\":]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,349.068,189.158,359.568 'Then we create the datasets:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,241.782,424.752,342.082 'train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    mulvar_train.to_numpy(),  # use all 5 columns as input\\n    targets=mulvar_train[\"rail\"][seq_length:],  # forecast only the rail series\\n    [...]  # the other 4 arguments are the same as earlier\\n)\\nvalid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    mulvar_valid.to_numpy(),\\n    targets=mulvar_valid[\"rail\"][seq_length:],\\n    [...]  # the other 2 arguments are the same as earlier\\n)\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,222.468,203.187,232.968 'And finally we create the RNN:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,176.382,331.248,215.482 'mulvar_model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\\n    tf.keras.layers.Dense(1)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,131.275,432.004,168.534 'Notice  that  the  only  difference  from  the  univar_model  RNN  we  built  earlier  is  the\\ninput shape: at each time step, the model now receives five inputs instead of one. This\\nmodel actually reaches a validation MAE of 22,062. Now we’re making big progress!\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.995,73.689,432.003,123.175 'In  fact,  it’s  not  too  hard  to  make  the  RNN  forecast  both  the  bus  and  rail  rid‐\\nership.  You  just  need  to  change  the  targets  when  creating  the  datasets,  setting\\nthem  to  mulvar_train[[\"bus\",  \"rail\"]][seq_length:]  for  the  training  set,  and\\nmulvar_valid[[\"bus\",  \"rail\"]][seq_length:]  for  the  validation  set.  You  must\\n'>\n",
            "<LTTextBoxHorizontal(10) 326.731,40.500,402.520,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.988,40.500,432.004,49.500 '559\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.243,432.000,606.503 'also  add  an  extra  neuron  in  the  output  Dense  layer,  since  it  must  now  make  two\\nforecasts: one for tomorrow’s bus ridership, and the other for rail. That’s all there is to\\nit!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,449.843,432.005,561.143 'As we discussed in Chapter 10, using a single model for multiple related tasks often\\nresults in better performance than using a separate model for each task, since features\\nlearned  for  one  task  may  be  useful  for  the  other  tasks,  and  also  because  having  to\\nperform  well  across  multiple  tasks  prevents  the  model  from  overfitting  (it’s  a  form\\nof  regularization).  However,  it  depends  on  the  task,  and  in  this  particular  case  the\\nmultitask  RNN  that  forecasts  both  the  bus  and  the  rail  ridership  doesn’t  perform\\nquite as well as dedicated models that forecast one or the other (using all five columns\\nas  input).  Still,  it  reaches  a  validation  MAE  of  25,330  for  rail  and  26,369  for  bus,\\nwhich is pretty good.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.993,354.034,432.004,438.273 'Forecasting Several Time Steps Ahead\\nSo  far  we  have  only  predicted  the  value  at  the  next  time  step,  but  we  could  just  as\\neasily have predicted the value several steps ahead by changing the targets appropri‐\\nately (e.g., to predict the ridership 2 weeks from now, we could just change the targets\\nto be the value 14 days ahead instead of 1 day ahead). But what if we want to predict\\nthe next 14 values?\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,297.041,432.003,346.900 'The first option is to take the univar_model RNN we trained earlier for the rail time\\nseries, make it predict the next value, and add that value to the inputs, acting as if the\\npredicted value had actually occurred; we would then use the model again to predict\\nthe following value, and so on, as in the following code:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.001,281.554,165.501,290.054 'import numpy as np\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.001,230.554,361.001,269.654 'X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]\\nfor step_ahead in range(14):\\n    y_pred_one = univar_model.predict(X)\\n    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,160.248,432.004,221.741 'In  this  code,  we  take  the  rail  ridership  of  the  first  56  days  of  the  validation  period,\\nand  we  convert  the  data  to  a  NumPy  array  of  shape  [1,  56,  1]  (recall  that  recurrent\\nlayers expect 3D inputs). Then we repeatedly use the model to forecast the next value,\\nand  we  append  each  forecast  to  the  input  series,  along  the  time  axis  (axis=1).  The\\nresulting forecasts are plotted in Figure 15-11.\\n'>\n",
            "<LTTextBoxHorizontal(7) 136.785,99.156,396.005,143.316 'If  the  model  makes  an  error  at  one  time  step,  then  the  forecasts\\nfor the following time steps are impacted as well: the errors tend to\\naccumulate. So, it’s preferable to use this technique only for a small\\nnumber of steps.\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.002,40.500,84.440,49.500 '560 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.908,40.500,266.933,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,96.802,132.680,144.111 matrix=[49.68,0.00,0.00,47.31, (83.00,96.80)]>\n",
            "<LTTextBoxHorizontal(0) 72.005,442.606,303.016,453.106 'Figure 15-11. Forecasting 14 steps ahead, 1 step at a time\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,315.427,432.005,428.506 'The second option is to train an RNN to predict the next 14 values in one shot. We\\ncan  still  use  a  sequence-to-vector  model,  but  it  will  output  14  values  instead  of  1.\\nHowever,  we  first  need  to  change  the  targets  to  be  vectors  containing  the  next  14\\nvalues.  To  do  this,  we  can  use  timeseries_dataset_from_array()  again,  but  this\\ntime  asking  it  to  create  datasets  without  targets  (targets=None)  and  with  longer\\nsequences, of length seq_length + 14. Then we can use the datasets’ map() method\\nto apply a custom function to each batch of sequences, splitting them into inputs and\\ntargets.  In  this  example,  we  use  the  multivariate  time  series  as  input  (using  all  five\\ncolumns), and we forecast the rail ridership for the next 14 days:6\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,289.740,407.747,308.440 'def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\\n    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,157.140,352.497,277.840 'ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    mulvar_train.to_numpy(),\\n    targets=None,\\n    sequence_length=seq_length + 14,\\n    [...]  # the other 3 arguments are the same as earlier\\n).map(split_inputs_and_targets)\\nahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\\n    mulvar_valid.to_numpy(),\\n    targets=None,\\n    sequence_length=seq_length + 14,\\n    batch_size=32\\n).map(split_inputs_and_targets)\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,137.827,337.507,148.327 'Now we just need the output layer to have 14 units instead of 1:\\n'>\n",
            "<LTTextBoxHorizontal(5) 73.140,88.954,426.776,96.954 '6 Feel free to play around with this model. For example, you can try forecasting both the bus and rail ridership\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,403.312,86.954 'for the next 14 days. You’ll need to tweak the targets to include both, and make your model output 28\\nforecasts instead of 14.\\n'>\n",
            "<LTTextBoxHorizontal(7) 326.725,40.500,402.514,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.982,40.500,431.998,49.500 '561\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,458.670,432.375,607.500>\n",
            "<LTLine 72.000,458.795,432.500,458.795>\n",
            "<LTLine 72.125,458.670,72.125,607.500>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTFigure(I1) 79.450,464.650,425.050,602.250 matrix=[345.60,0.00,0.00,137.60, (79.45,464.65)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,567.550,331.250,606.650 'ahead_model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\\n    tf.keras.layers.Dense(14)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,548.236,394.283,558.736 'After training this model, you can predict the next 14 values at once like this:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,522.550,394.997,541.250 'X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\\nY_pred = ahead_model.predict(X)  # shape [1, 14]\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,465.436,432.003,513.736 'This  approach  works  quite  well.  Its  forecasts  for  the  next  day  are  obviously  better\\nthan its forecasts for 14 days into the future, but it doesn’t accumulate errors like the\\nprevious approach did. However, we can still do better, using a sequence-to-sequence\\n(or seq2seq) model.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,369.627,432.002,453.867 'Forecasting Using a Sequence-to-Sequence Model\\nInstead of training the model to forecast the next 14 values only at the very last time\\nstep, we can train it to forecast the next 14 values at each and every time step. In other\\nwords, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN.\\nThe advantage of this technique is that the loss will contain a term for the output of\\nthe RNN at each and every time step, not just for the output at the last time step.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,325.827,432.004,361.527 'This means there will be many more error gradients flowing through the model, and\\nthey won’t have to flow through time as much since they will come from the output of\\neach time step, not just the last one. This will both stabilize and speed up training.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.998,256.827,432.005,317.727 'To be clear, at time step 0 the model will output a vector containing the forecasts for\\ntime steps 1 to 14, then at time step 1 the model will forecast time steps 2 to 15, and\\nso on. In other words, the targets are sequences of consecutive windows, shifted by\\none time step at each time step. The target is not a vector anymore, but a sequence of\\nthe same length as the inputs, containing a 14-dimensional vector at each step.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,187.234,432.004,248.727 'Preparing  the  datasets  is  not  trivial,  since  each  instance  has  a  window  as  input  and\\na  sequence  of  windows  as  output.  One  way  to  do  this  is  to  use  the  to_windows()\\nutility  function  we  created  earlier,  twice  in  a  row,  to  get  windows  of  consecutive\\nwindows. For example, let’s turn the series of numbers 0 to 6 into a dataset containing\\nsequences of 4 consecutive windows, each of length 3:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.003,69.747,314.253,180.248 '>>> my_series = tf.data.Dataset.range(7)\\n>>> dataset = to_windows(to_windows(my_series, 3), 4)\\n>>> list(dataset)\\n[<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\\n array([[0, 1, 2],\\n        [1, 2, 3],\\n        [2, 3, 4],\\n        [3, 4, 5]])>,\\n <tf.Tensor: shape=(4, 3), dtype=int64, numpy=\\n array([[1, 2, 3],\\n        [2, 3, 4],\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.002,40.500,84.440,49.500 '562 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.908,40.500,266.933,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,587.950,178.250,606.650 '        [3, 4, 5],\\n        [4, 5, 6]])>]\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.001,555.443,432.005,580.103 'Now we can use the map() method to split these windows of windows into inputs and\\ntargets:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.004,407.357,369.504,548.457 '>>> dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))\\n>>> list(dataset)\\n[(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])>,\\n  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\\n  array([[1, 2],\\n         [2, 3],\\n         [3, 4],\\n         [4, 5]])>),\\n (<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4])>,\\n  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\\n  array([[2, 3],\\n         [3, 4],\\n         [4, 5],\\n         [5, 6]])>)]\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,337.643,432.004,398.543 'Now the dataset contains sequences of length 4 as inputs, and the targets are sequen‐\\nces  containing  the  next  two  steps,  for  each  time  step.  For  example,  the  first  input\\nsequence is [0, 1, 2, 3], and its corresponding targets are [[1, 2], [2, 3], [3, 4], [4, 5]],\\nwhich are the next two values for each time step. If you’re like me, you will probably\\nneed a few minutes to wrap your head around this. Take your time!\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.785,276.552,396.004,320.712 'It may be surprising that the targets contain values that appear in\\nthe inputs. Isn’t that cheating? Fortunately, not at all: at each time\\nstep,  an  RNN  only  knows  about  past  time  steps;  it  cannot  look\\nahead. It is said to be a causal model.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,232.043,431.998,255.143 'Let’s create another little utility function to prepare the datasets for our sequence-to-\\nsequence model. It will also take care of shuffling (optional) and batching:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.996,155.357,403.496,225.057 'def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,\\n                       batch_size=32, shuffle=False, seed=None):\\n    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)\\n    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))\\n    if shuffle:\\n        ds = ds.shuffle(8 * batch_size, seed=seed)\\n    return ds.batch(batch_size)\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.003,136.043,287.579,146.543 'Now we can use this function to create the datasets:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.002,110.357,390.752,129.057 'seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)\\nseq2seq_valid = to_seq2seq_dataset(mulvar_valid)\\n'>\n",
            "<LTTextBoxHorizontal(9) 326.725,40.500,402.514,49.500 'Forecasting a Time Series \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.982,40.500,431.998,49.500 '563\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,273.006,126.944,322.507 matrix=[37.94,0.00,0.00,49.50, (89.00,273.01)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,314.077,605.537 'And lastly, we can build the sequence-to-sequence model:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.998,548.950,428.998,588.050 'seq2seq_model = tf.keras.Sequential([\\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),\\n    tf.keras.layers.Dense(14)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,451.664,432.062,540.137 'It  is  almost  identical  to  our  previous  model:  the  only  difference  is  that  we  set\\nreturn_sequences=True in the SimpleRNN layer. This way, it will output a sequence of\\nvectors (each of size 32), instead of outputting a single vector at the last time step. The\\nDense layer is smart enough to handle sequences as input: it will be applied at each\\ntime step, taking a 32-dimensional vector as input and outputting a 14-dimensional\\nvector. In fact, another way to get the exact same result is to use a Conv1D layer with a\\nkernel size of 1: Conv1D(14, kernel_size=1).\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.790,354.927,396.002,435.615 'Keras  offers  a  TimeDistributed  layer  that  lets  you  apply  any\\nvector-to-vector  layer  to  every  vector  in  the  input  sequences,  at\\nevery  time  step.  It  does  this  efficiently,  by  reshaping  the  inputs  so\\nthat each time step is treated as a separate instance, then it reshapes\\nthe layer’s outputs to recover the time dimension. In our case, we\\ndon’t  need  it  since  the  Dense  layer  already  supports  sequences  as\\ninputs.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,289.539,432.003,337.839 'The  training  code  is  the  same  as  usual.  During  training,  all  the  model’s  outputs  are\\nused, but after training only the output of the very last time step matters, and the rest\\ncan be ignored. For example, we can forecast the rail ridership for the next 14 days\\nlike this:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,263.852,424.752,282.552 \"X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]\\ny_pred_14 = seq2seq_model.predict(X)[0, -1]  # only the last time step's output\\n\">\n",
            "<LTTextBoxHorizontal(6) 71.998,219.339,432.004,255.039 'If  you  evaluate  this  model’s  forecasts  for  t  +  1,  you  will  find  a  validation  MAE  of\\n25,519. For t + 2 it’s 26,274, and the performance continues to drop gradually as the\\nmodel tries to forecast further into the future. At t + 14, the MAE is 34,322.\\n'>\n",
            "<LTTextBoxHorizontal(7) 136.793,146.727,396.004,202.407 'You  can  combine  both  approaches  to  forecasting  multiple  steps\\nahead:  for  example,  you  can  train  a  model  that  forecasts  14  days\\nahead,  then  take  its  output  and  append  it  to  the  inputs,  then  run\\nthe  model  again  to  get  forecasts  for  the  following  14  days,  and\\npossibly repeat the process.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,93.939,431.998,129.639 'Simple  RNNs  can  be  quite  good  at  forecasting  time  series  or  handling  other  kinds\\nof sequences, but they do not perform as well on long time series or sequences. Let’s\\ndiscuss why and see what we can do about it.\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.004,40.500,84.442,49.500 '564 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.910,40.500,266.935,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,384.750,126.760,440.527 matrix=[41.76,0.00,0.00,55.78, (85.00,384.75)]>\n",
            "<LTFigure(I1) 85.000,152.425,126.760,208.202 matrix=[41.76,0.00,0.00,55.78, (85.00,152.42)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,507.312,432.003,607.973 'Handling Long Sequences\\nTo  train  an  RNN  on  long  sequences,  we  must  run  it  over  many  time  steps,  making\\nthe  unrolled  RNN  a  very  deep  network.  Just  like  any  deep  neural  network  it  may\\nsuffer  from  the  unstable  gradients  problem,  discussed  in  Chapter  11:  it  may  take\\nforever  to  train,  or  training  may  be  unstable.  Moreover,  when  an  RNN  processes  a\\nlong  sequence,  it  will  gradually  forget  the  first  inputs  in  the  sequence.  Let’s  look  at\\nboth these problems, starting with the unstable gradients problem.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,361.103,432.005,495.743 'Fighting the Unstable Gradients Problem\\nMany of the tricks we used in deep nets to alleviate the unstable gradients problem\\ncan also be used for RNNs: good parameter initialization, faster optimizers, dropout,\\nand so on. However, nonsaturating activation functions (e.g., ReLU) may not help as\\nmuch here. In fact, they may actually lead the RNN to be even more unstable during\\ntraining.  Why?  Well,  suppose  gradient  descent  updates  the  weights  in  a  way  that\\nincreases the outputs slightly at the first time step. Because the same weights are used\\nat every time step, the outputs at the second time step may also be slightly increased,\\nand  those  at  the  third,  and  so  on  until  the  outputs  explode—and  a  nonsaturating\\nactivation function does not prevent that.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,329.903,432.004,353.003 'You can reduce this risk by using a smaller learning rate, or you can use a saturating\\nactivation function like the hyperbolic tangent (this explains why it’s the default).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,286.103,432.001,321.803 'In  much  the  same  way,  the  gradients  themselves  can  explode.  If  you  notice  that\\ntraining  is  unstable,  you  may  want  to  monitor  the  size  of  the  gradients  (e.g.,  using\\nTensorBoard) and perhaps use gradient clipping.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,242.303,432.003,278.003 'Moreover,  batch  normalization  cannot  be  used  as  efficiently  with  RNNs  as  with\\ndeep  feedforward  nets.  In  fact,  you  cannot  use  it  between  time  steps,  only  between\\nrecurrent layers.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,122.903,432.004,234.203 'To be more precise, it is technically possible to add a BN layer to a memory cell (as\\nyou  will  see  shortly)  so  that  it  will  be  applied  at  each  time  step  (both  on  the  inputs\\nfor  that  time  step  and  on  the  hidden  state  from  the  previous  step).  However,  the\\nsame  BN  layer  will  be  used  at  each  time  step,  with  the  same  parameters,  regardless\\nof the actual scale and offset of the inputs and hidden state. In practice, this does not\\nyield good results, as was demonstrated by César Laurent et al. in a 2015 paper:7 the\\nauthors found that BN was slightly beneficial only when it was applied to the layer’s\\ninputs,  not  to  the  hidden  states.  In  other  words,  it  was  slightly  better  than  nothing\\nwhen applied between recurrent layers (i.e., vertically in Figure 15-10), but not within\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,78.954,418.712,86.954 '7 César Laurent et al., “Batch Normalized Recurrent Neural Networks”, Proceedings of the IEEE International\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,313.504,76.954 'Conference on Acoustics, Speech, and Signal Processing (2016): 2657–2661.\\n'>\n",
            "<LTTextBoxHorizontal(8) 325.141,40.500,402.514,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.982,40.500,431.998,49.500 '565\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.999,569.243,432.005,605.537 'recurrent layers (i.e., horizontally). In Keras, you can apply BN between layers simply\\nby  adding  a  BatchNormalization  layer  before  each  recurrent  layer,  but  it  will  slow\\ndown training, and it may not help much.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,424.643,432.003,561.143 'Another  form  of  normalization  often  works  better  with  RNNs:  layer  normalization.\\nThis  idea  was  introduced  by  Jimmy  Lei  Ba  et  al.  in  a  2016  paper:8  it  is  very  similar\\nto batch normalization, but instead of normalizing across the batch dimension, layer\\nnormalization normalizes across the features dimension. One advantage is that it can\\ncompute  the  required  statistics  on  the  fly,  at  each  time  step,  independently  for  each\\ninstance. This also means that it behaves the same way during training and testing (as\\nopposed to BN), and it does not need to use exponential moving averages to estimate\\nthe  feature  statistics  across  all  instances  in  the  training  set,  like  BN  does.  Like  BN,\\nlayer normalization learns a scale and an offset parameter for each input. In an RNN,\\nit  is  typically  used  right  after  the  linear  combination  of  the  inputs  and  the  hidden\\nstates.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,367.057,432.004,416.543 'Let’s use Keras to implement layer normalization within a simple memory cell. To do\\nthis, we need to define a custom memory cell, which is just like a regular layer, except\\nits call() method takes two arguments: the inputs at the current time step and the\\nhidden states from the previous time step.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,257.884,432.005,359.923 'Note that the states argument is a list containing one or more tensors. In the case\\nof a simple RNN cell it contains a single tensor equal to the outputs of the previous\\ntime  step,  but  other  cells  may  have  multiple  state  tensors  (e.g.,  an  LSTMCell  has  a\\nlong-term  state  and  a  short-term  state,  as  you  will  see  shortly).  A  cell  must  also\\nhave  a  state_size  attribute  and  an  output_size  attribute.  In  a  simple  RNN,  both\\nare  simply  equal  to  the  number  of  units.  The  following  code  implements  a  custom\\nmemory  cell  that  will  behave  like  a  SimpleRNNCell,  except  it  will  also  apply  layer\\nnormalization at each time step:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,160.797,416.252,250.897 'class LNSimpleRNNCell(tf.keras.layers.Layer):\\n    def __init__(self, units, activation=\"tanh\", **kwargs):\\n        super().__init__(**kwargs)\\n        self.state_size = units\\n        self.output_size = units\\n        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,\\n                                                             activation=None)\\n        self.layer_norm = tf.keras.layers.LayerNormalization()\\n        self.activation = tf.keras.activations.get(activation)\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,109.797,369.502,148.897 '    def call(self, inputs, states):\\n        outputs, new_states = self.simple_rnn_cell(inputs, states)\\n        norm_outputs = self.activation(self.layer_norm(outputs))\\n        return norm_outputs, [norm_outputs]\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,68.954,345.960,76.954 '8 Jimmy Lei Ba et al., “Layer Normalization”, arXiv preprint arXiv:1607.06450 (2016).\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,40.500,84.435,49.500 '566 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.903,40.500,266.928,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 71.999,595.037,191.258,605.537 'Let’s walk through this code:\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.654,571.843,431.996,583.903 '• Our LNSimpleRNNCell class inherits from the tf.keras.layers.Layer class, just\\n•\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.000,559.243,180.237,569.743 'like any custom layer.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.652,477.863,432.005,553.143 '• The  constructor  takes  the  number  of  units  and  the  desired  activation  function\\n•\\nand  sets  the  state_size  and  output_size  attributes,  then  creates  a  SimpleRNN\\nCell with no activation function (because we want to perform layer normaliza‐\\ntion  after  the  linear  operation  but  before  the  activation  function).9  Then  the\\nconstructor  creates  the  LayerNormalization  layer,  and  finally  it  fetches  the\\ndesired activation function.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.653,357.497,432.005,472.729 '• The  call()  method  starts  by  applying  the  simpleRNNCell,  which  computes  a\\n•\\nlinear  combination  of  the  current  inputs  and  the  previous  hidden  states,  and  it\\nreturns the result twice (indeed, in a  SimpleRNNCell, the outputs are just equal\\nto  the  hidden  states:  in  other  words,  new_states[0]  is  equal  to  outputs,  so  we\\ncan safely ignore new_states in the rest of the call() method). Next, the call()\\nmethod applies layer normalization, followed by the activation function. Finally,\\nit  returns  the  outputs  twice:  once  as  the  outputs,  and  once  as  the  new  hidden\\nstates. To use this custom cell, all we need to do is create a tf.keras.layers.RNN\\nlayer, passing it a cell instance:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.998,297.211,373.748,346.511 'custom_ln_model = tf.keras.Sequential([\\n    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,\\n                        input_shape=[None, 5]),\\n    tf.keras.layers.Dense(14)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,214.304,432.004,288.397 'Similarly,  you  could  create  a  custom  cell  to  apply  dropout  between  each  time  step.\\nBut  there’s  a  simpler  way:  most  recurrent  layers  and  cells  provided  by  Keras  have\\ndropout  and  recurrent_dropout  hyperparameters:  the  former  defines  the  dropout\\nrate  to  apply  to  the  inputs,  and  the  latter  defines  the  dropout  rate  for  the  hidden\\nstates, between time steps. So, there’s no need to create a custom cell to apply dropout\\nat each time step in an RNN.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.999,170.504,432.005,206.204 'With  these  techniques,  you  can  alleviate  the  unstable  gradients  problem  and  train\\nan  RNN  much  more  efficiently.  Now  let’s  look  at  how  to  deal  with  the  short-term\\nmemory problem.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,89.646,410.736,98.834 '9 It would have been simpler to inherit from SimpleRNNCell instead so that we wouldn’t have to create an\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,428.352,88.142 'internal SimpleRNNCell or handle the state_size and output_size attributes, but the goal here was to show\\nhow to create a custom cell from scratch.\\n'>\n",
            "<LTTextBoxHorizontal(10) 325.143,40.500,402.516,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.984,40.500,432.000,49.500 '567\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,106.284,162.000,106.284>\n",
            "<LTTextBoxHorizontal(0) 136.789,514.380,396.001,605.705 'When forecasting time series, it is often useful to have some error\\nbars along with your predictions. For this, one approach is to use\\nMC  dropout,  introduced  in  Chapter  11:  use  recurrent_dropout\\nduring  training,  then  keep  dropout  active  at  inference  time  by\\ncalling  the  model  using  model(X,  training=True).  Repeat  this\\nseveral times to get multiple slightly different forecasts, then com‐\\npute the mean and standard deviation of these predictions for each\\ntime step.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,377.610,432.004,499.649 'Tackling the Short-Term Memory Problem\\nDue  to  the  transformations  that  the  data  goes  through  when  traversing  an  RNN,\\nsome  information  is  lost  at  each  time  step.  After  a  while,  the  RNN’s  state  contains\\nvirtually  no  trace  of  the  first  inputs.  This  can  be  a  showstopper.  Imagine  Dory  the\\nfish10 trying to translate a long sentence; by the time she’s finished reading it, she has\\nno clue how it started. To tackle this problem, various types of cells with long-term\\nmemory  have  been  introduced.  They  have  proven  so  successful  that  the  basic  cells\\nare  not  used  much  anymore.  Let’s  first  look  at  the  most  popular  of  these  long-term\\nmemory cells: the LSTM cell.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,353.243,110.519,364.803 'LSTM cells\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,258.532,432.005,346.438 'The long short-term memory (LSTM) cell was proposed in 199711 by Sepp Hochreiter\\nand Jürgen Schmidhuber and gradually improved over the years by several research‐\\ners, such as Alex Graves, Haşim Sak,12 and Wojciech Zaremba.13 If you consider the\\nLSTM  cell  as  a  black  box,  it  can  be  used  very  much  like  a  basic  cell,  except  it  will\\nperform  much  better;  training  will  converge  faster,  and  it  will  detect  longer-term\\npatterns  in  the  data.  In  Keras,  you  can  simply  use  the  LSTM  layer  instead  of  the\\nSimpleRNN layer:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,212.445,407.747,251.545 'model = tf.keras.Sequential([\\n    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\\n    tf.keras.layers.Dense(14)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,166.745,432.003,204.598 'Alternatively,  you  could  use  the  general-purpose  tf.keras.layers.RNN  layer,  giv‐\\ning  it  an  LSTMCell  as  an  argument.  However,  the  LSTM  layer  uses  an  optimized\\nimplementation when running on a GPU (see Chapter 19), so in general it is prefera‐\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,127.954,416.872,135.954 '10 A character from the animated movies Finding Nemo and Finding Dory who has short-term memory loss.\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.783,114.954,425.779,122.954 '11 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory”, Neural Computation 9, no. 8 (1997):\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.003,104.954,116.707,112.954 '1735–1780.\\n'>\n",
            "<LTTextBoxHorizontal(9) 69.778,91.954,407.566,99.954 '12 Haşim Sak et al., “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large\\n'>\n",
            "<LTTextBoxHorizontal(10) 79.998,81.954,311.094,89.954 'Vocabulary Speech Recognition”, arXiv preprint arXiv:1402.1128 (2014).\\n'>\n",
            "<LTTextBoxHorizontal(11) 69.782,68.954,427.698,76.954 '13 Wojciech Zaremba et al., “Recurrent Neural Network Regularization”, arXiv preprint arXiv:1409.2329 (2014).\\n'>\n",
            "<LTTextBoxHorizontal(12) 72.000,40.500,84.438,49.500 '568 \\n'>\n",
            "<LTTextBoxHorizontal(13) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 102.906,40.500,266.931,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,143.900,162.000,143.900>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,581.843,432.000,606.503 'ble to use it (the RNN layer is mostly useful when you define custom cells, as we did\\nearlier).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,524.855,432.005,573.743 'So  how  does  an  LSTM  cell  work?  Its  architecture  is  shown  in  Figure  15-12.  If  you\\ndon’t  look  at  what’s  inside  the  box,  the  LSTM  cell  looks  exactly  like  a  regular  cell,\\nexcept that its state is split into two vectors: h(t) and c(t) (“c” stands for “cell”). You can\\nthink of h(t) as the short-term state and c(t) as the long-term state.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.998,263.293,184.968,273.793 'Figure 15-12. An LSTM cell\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,112.692,432.003,249.192 'Now let’s open the box! The key idea is that the network can learn what to store in the\\nlong-term state, what to throw away, and what to read from it. As the long-term state\\nc(t–1) traverses the network from left to right, you can see that it first goes through a\\nforget gate, dropping some memories, and then it adds some new memories via the\\naddition  operation  (which  adds  the  memories  that  were  selected  by  an  input  gate).\\nThe  result  c(t)  is  sent  straight  out,  without  any  further  transformation.  So,  at  each\\ntime  step,  some  memories  are  dropped  and  some  memories  are  added.  Moreover,\\nafter  the  addition  operation,  the  long-term  state  is  copied  and  passed  through  the\\ntanh  function,  and  then  the  result  is  filtered  by  the  output  gate.  This  produces  the\\nshort-term  state  h(t)  (which  is  equal  to  the  cell’s  output  for  this  time  step,  y(t)).  Now\\nlet’s look at where new memories come from and how the gates work.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,81.492,431.998,104.592 'First,  the  current  input  vector  x(t)  and  the  previous  short-term  state  h(t–1)  are  fed  to\\nfour different fully connected layers. They all serve a different purpose:\\n'>\n",
            "<LTTextBoxHorizontal(5) 325.147,40.500,402.520,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.988,40.500,432.004,49.500 '569\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,519.182,432.500,519.182>\n",
            "<LTLine 432.375,279.356,432.375,519.307>\n",
            "<LTLine 72.000,279.481,432.500,279.481>\n",
            "<LTLine 72.125,279.356,72.125,519.307>\n",
            "<LTFigure(I1) 81.849,285.336,422.651,514.057 matrix=[340.80,0.00,0.00,228.72, (81.85,285.34)]>\n",
            "<LTTextBoxHorizontal(0) 80.655,540.636,432.002,601.537 '• The main layer is the one that outputs g(t). It has the usual role of analyzing the\\n•\\ncurrent inputs x(t) and the previous (short-term) state h(t–1). In a basic cell, there\\nis  nothing  other  than  this  layer,  and  its  output  goes  straight  out  to  y(t)  and  h(t).\\nBut in an LSTM cell, this layer’s output does not go straight out; instead its most\\nimportant parts are stored in the long-term state (and the rest is dropped).\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.652,486.237,432.005,534.537 '• The  three  other  layers  are  gate  controllers.  Since  they  use  the  logistic  activation\\n•\\nfunction,  the  outputs  range  from  0  to  1.  As  you  can  see,  the  gate  controllers’\\noutputs are fed to element-wise multiplication operations: if they output 0s they\\nclose the gate, and if they output 1s they open it. Specifically:\\n'>\n",
            "<LTTextBoxHorizontal(2) 91.065,469.048,432.002,480.137 '—\\n— The forget gate (controlled by f(t)) controls which parts of the long-term state\\n'>\n",
            "<LTTextBoxHorizontal(3) 102.995,457.037,175.372,467.537 'should be erased.\\n'>\n",
            "<LTTextBoxHorizontal(4) 91.068,439.848,432.002,450.936 '—\\n— The input gate (controlled by i(t)) controls which parts of g(t) should be added\\n'>\n",
            "<LTTextBoxHorizontal(5) 103.005,427.836,196.077,438.336 'to the long-term state.\\n'>\n",
            "<LTTextBoxHorizontal(6) 91.065,398.048,431.999,421.736 '—\\n— Finally,  the  output  gate  (controlled  by  o(t))  controls  which  parts  of  the  long-\\nterm state should be read and output at this time step, both to h(t) and to y(t).\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,325.636,432.004,386.536 'In short, an LSTM cell can learn to recognize an important input (that’s the role of the\\ninput gate), store it in the long-term state, preserve it for as long as it is needed (that’s\\nthe  role  of  the  forget  gate),  and  extract  it  whenever  it  is  needed.  This  explains  why\\nthese  cells  have  been  amazingly  successful  at  capturing  long-term  patterns  in  time\\nseries, long texts, audio recordings, and more.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,281.836,432.003,317.536 'Equation 15-4 summarizes how to compute the cell’s long-term state, its short-term\\nstate, and its output at each time step for a single instance (the equations for a whole\\nmini-batch are very similar).\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.456,233.758,140.014,245.602 'i t = σ Wxi\\n'>\n",
            "<LTTextBoxHorizontal(10) 87.005,194.349,239.960,266.049 'Equation 15-4. LSTM computations\\n⊺h t − 1 + bi\\n⊺x t + Wℎi\\n⊺h t − 1 + b f\\n⊺x t + Wℎ f\\n⊺h t − 1 + bo\\n⊺x t + Wℎo\\n'>\n",
            "<LTTextBoxHorizontal(11) 87.286,195.052,141.580,206.895 'o t = σ Wxo\\n'>\n",
            "<LTTextBoxHorizontal(12) 88.104,214.405,140.932,226.249 'f t = σ Wx f\\n'>\n",
            "<LTTextBoxHorizontal(13) 87.004,141.414,211.394,190.672 '⊺x t + Wℎg\\ng t = tanh Wxg\\nc t = f t ⊗ c t − 1 + i t ⊗ g t\\ny t = h t = o t ⊗ tanh c t\\n'>\n",
            "<LTTextBoxHorizontal(14) 206.422,174.996,257.090,190.672 '⊺h t − 1 + bg\\n'>\n",
            "<LTTextBoxHorizontal(15) 71.997,113.943,140.888,124.443 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(16) 80.651,90.755,431.999,101.843 '•\\n• Wxi, Wxf, Wxo, and Wxg are the weight matrices of each of the four layers for their\\n'>\n",
            "<LTTextBoxHorizontal(17) 90.004,78.155,233.652,89.243 'connection to the input vector x(t).\\n'>\n",
            "<LTTextBoxHorizontal(18) 72.003,40.500,84.441,49.500 '570 \\n'>\n",
            "<LTTextBoxHorizontal(19) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 102.909,40.500,266.934,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 92.936,233.752,94.776,241.752>\n",
            "<LTCurve 98.524,233.752,100.364,241.752>\n",
            "<LTCurve 120.298,233.752,122.598,247.435>\n",
            "<LTCurve 150.579,233.752,152.419,241.752>\n",
            "<LTCurve 156.167,233.752,158.007,241.752>\n",
            "<LTCurve 197.037,233.752,198.877,241.752>\n",
            "<LTCurve 214.660,233.752,216.500,241.752>\n",
            "<LTCurve 236.031,233.752,238.331,247.435>\n",
            "<LTCurve 92.936,214.399,94.776,222.399>\n",
            "<LTCurve 98.524,214.399,100.364,222.399>\n",
            "<LTCurve 120.298,214.399,122.598,228.082>\n",
            "<LTCurve 152.122,214.399,153.962,222.399>\n",
            "<LTCurve 157.710,214.399,159.550,222.399>\n",
            "<LTCurve 200.123,214.399,201.963,222.399>\n",
            "<LTCurve 217.746,214.399,219.586,222.399>\n",
            "<LTCurve 240.660,214.399,242.960,228.082>\n",
            "<LTCurve 92.936,195.045,94.776,203.045>\n",
            "<LTCurve 98.524,195.045,100.364,203.045>\n",
            "<LTCurve 120.298,195.045,122.598,208.729>\n",
            "<LTCurve 152.040,195.045,153.880,203.045>\n",
            "<LTCurve 157.628,195.045,159.468,203.045>\n",
            "<LTCurve 199.959,195.045,201.799,203.045>\n",
            "<LTCurve 217.582,195.045,219.422,203.045>\n",
            "<LTCurve 240.414,195.045,242.714,208.729>\n",
            "<LTCurve 92.936,175.692,94.776,183.692>\n",
            "<LTCurve 98.524,175.692,100.364,183.692>\n",
            "<LTCurve 138.015,175.692,140.315,189.376>\n",
            "<LTCurve 169.561,175.692,171.401,183.692>\n",
            "<LTCurve 175.149,175.692,176.989,183.692>\n",
            "<LTCurve 217.285,175.692,219.125,183.692>\n",
            "<LTCurve 234.908,175.692,236.748,183.692>\n",
            "<LTCurve 257.545,175.692,259.845,189.376>\n",
            "<LTCurve 92.936,159.471,94.776,167.471>\n",
            "<LTCurve 98.524,159.471,100.364,167.471>\n",
            "<LTCurve 118.342,159.471,120.182,167.471>\n",
            "<LTCurve 123.929,159.471,125.769,167.471>\n",
            "<LTCurve 143.432,159.471,145.272,167.471>\n",
            "<LTCurve 161.055,159.471,162.895,167.471>\n",
            "<LTCurve 180.525,159.471,182.365,167.471>\n",
            "<LTCurve 186.113,159.471,187.953,167.471>\n",
            "<LTCurve 206.416,159.471,208.256,167.471>\n",
            "<LTCurve 212.003,159.471,213.843,167.471>\n",
            "<LTCurve 92.936,141.407,94.776,149.407>\n",
            "<LTCurve 98.524,141.407,100.364,149.407>\n",
            "<LTCurve 119.929,141.407,121.769,149.407>\n",
            "<LTCurve 125.516,141.407,127.356,149.407>\n",
            "<LTCurve 144.928,141.407,146.768,149.407>\n",
            "<LTCurve 150.515,141.407,152.355,149.407>\n",
            "<LTCurve 189.642,141.407,191.942,155.091>\n",
            "<LTCurve 197.283,141.407,199.123,149.407>\n",
            "<LTCurve 202.871,141.407,204.711,149.407>\n",
            "<LTCurve 205.729,141.407,208.029,155.091>\n",
            "<LTTextBoxHorizontal(0) 80.655,594.448,432.003,605.537 '•\\n• Whi, Whf, Who, and Whg are the weight matrices of each of the four layers for their\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.997,581.848,295.455,592.937 'connection to the previous short-term state h(t–1).\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.654,540.636,432.002,576.337 '• bi, bf, bo, and bg are the bias terms for each of the four layers. Note that Tensor‐\\n•\\nFlow  initializes  bf  to  a  vector  full  of  1s  instead  of  0s.  This  prevents  forgetting\\neverything at the beginning of training.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,505.436,431.999,528.536 'There  are  several  variants  of  the  LSTM  cell.  One  particularly  popular  variant  is  the\\nGRU cell, which we will look at now.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,481.069,106.368,492.629 'GRU cells\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,437.944,432.001,473.644 'The gated recurrent unit (GRU) cell (see Figure 15-13) was proposed by Kyunghyun\\nCho  et  al.  in  a  2014  paper14  that  also  introduced  the  encoder–decoder  network  we\\ndiscussed earlier.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,157.559,164.668,168.059 'Figure 15-13. GRU cell\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,88.954,404.544,96.954 '14 Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder–Decoder for Statistical\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.000,68.954,431.160,86.954 'Machine Translation”, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\\n(2014): 1724–1734.\\n'>\n",
            "<LTTextBoxHorizontal(9) 325.147,40.500,402.520,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.988,40.500,432.004,49.500 '571\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,431.683,432.500,431.683>\n",
            "<LTLine 432.375,173.623,432.375,431.808>\n",
            "<LTLine 72.000,173.748,432.500,173.748>\n",
            "<LTLine 72.125,173.623,72.125,431.808>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTFigure(I1) 108.250,179.603,396.250,426.558 matrix=[288.00,0.00,0.00,246.96, (108.25,179.60)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,582.437,431.995,605.537 'The GRU cell is a simplified version of the LSTM cell, and it seems to perform just as\\nwell15 (which explains its growing popularity). These are the main simplifications:\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.653,492.837,432.004,570.337 '•\\n• Both state vectors are merged into a single vector h(t).\\n• A  single  gate  controller  z(t)  controls  both  the  forget  gate  and  the  input  gate.  If\\n•\\nthe  gate  controller  outputs  a  1,  the  forget  gate  is  open  (=  1)  and  the  input  gate\\nis  closed  (1  –  1  =  0).  If  it  outputs  a  0,  the  opposite  happens.  In  other  words,\\nwhenever a memory must be stored, the location where it will be stored is erased\\nfirst. This is actually a frequent variant to the LSTM cell in and of itself.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.650,450.448,432.003,486.737 '• There is no output gate; the full state vector is output at every time step. How‐\\n•\\never,  there  is  a  new  gate  controller  r(t)  that  controls  which  part  of  the  previous\\nstate will be shown to the main layer (g(t)).\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,415.836,432.004,438.936 'Equation  15-5  summarizes  how  to  compute  the  cell’s  state  at  each  time  step  for  a\\nsingle instance.\\n'>\n",
            "<LTTextBoxHorizontal(4) 86.995,389.549,228.472,400.049 'Equation 15-5. GRU computations\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.622,367.758,141.594,379.602 'z t = σ Wxz\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.492,348.405,141.810,360.249 'r t = σ Wxr\\n'>\n",
            "<LTTextBoxHorizontal(7) 141.722,348.405,189.090,382.732 '⊺x t + Wℎz\\n⊺x t + Wℎr\\n'>\n",
            "<LTTextBoxHorizontal(8) 188.896,347.703,239.710,382.732 '⊺h t − 1 + bz\\n⊺h t − 1 + br\\n'>\n",
            "<LTTextBoxHorizontal(9) 86.996,310.286,233.316,340.896 'g t = tanh Wxg\\nh t = z t ⊗ h t − 1 + 1 − z t ⊗ g t\\n'>\n",
            "<LTTextBoxHorizontal(10) 159.584,329.052,206.904,344.025 '⊺x t + Wℎg\\n'>\n",
            "<LTTextBoxHorizontal(11) 206.904,328.350,289.626,344.025 '⊺ r t ⊗ h t − 1 + bg\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.998,257.132,432.001,294.984 'Keras  provides  a  tf.keras.layers.GRU  layer:  using  it  is  just  a  matter  of  replacing\\nSimpleRNN  or  LSTM  with  GRU.  It  also  provides  a  tf.keras.layers.GRUCell,  in  case\\nyou want to create a custom cell based on a GRU cell.\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.995,175.532,432.003,249.032 'LSTM  and  GRU  cells  are  one  of  the  main  reasons  behind  the  success  of  RNNs.\\nYet  while  they  can  tackle  much  longer  sequences  than  simple  RNNs,  they  still  have\\na  fairly  limited  short-term  memory,  and  they  have  a  hard  time  learning  long-term\\npatterns  in  sequences  of  100  time  steps  or  more,  such  as  audio  samples,  long  time\\nseries, or long sentences. One way to solve this is to shorten the input sequences; for\\nexample, using 1D convolutional layers.\\n'>\n",
            "<LTTextBoxHorizontal(14) 69.780,68.954,426.312,96.954 '15 See Klaus Greff et al., “LSTM: A Search Space Odyssey”, IEEE Transactions on Neural Networks and Learning\\nSystems 28, no. 10 (2017): 2222–2232.This paper seems to show that all LSTM variants perform roughly the\\nsame.\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.004,40.500,84.442,49.500 '572 \\n'>\n",
            "<LTTextBoxHorizontal(16) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(17) 102.910,40.500,266.935,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTCurve 93.419,367.752,95.259,375.752>\n",
            "<LTCurve 99.007,367.752,100.847,375.752>\n",
            "<LTCurve 120.782,367.752,123.082,381.435>\n",
            "<LTCurve 152.180,367.752,154.020,375.752>\n",
            "<LTCurve 157.767,367.752,159.607,375.752>\n",
            "<LTCurve 199.755,367.752,201.595,375.752>\n",
            "<LTCurve 217.378,367.752,219.218,375.752>\n",
            "<LTCurve 239.866,367.752,242.166,381.435>\n",
            "<LTCurve 93.419,348.399,95.259,356.399>\n",
            "<LTCurve 99.007,348.399,100.847,356.399>\n",
            "<LTCurve 120.782,348.399,123.082,362.082>\n",
            "<LTCurve 152.277,348.399,154.117,356.399>\n",
            "<LTCurve 157.865,348.399,159.705,356.399>\n",
            "<LTCurve 199.950,348.399,201.790,356.399>\n",
            "<LTCurve 217.573,348.399,219.413,356.399>\n",
            "<LTCurve 240.159,348.399,242.459,362.082>\n",
            "<LTCurve 93.419,329.046,95.259,337.046>\n",
            "<LTCurve 99.007,329.046,100.847,337.046>\n",
            "<LTCurve 138.498,329.046,140.798,342.729>\n",
            "<LTCurve 170.045,329.046,171.885,337.046>\n",
            "<LTCurve 175.632,329.046,177.472,337.046>\n",
            "<LTCurve 212.049,329.046,214.349,342.729>\n",
            "<LTCurve 219.734,329.046,221.574,337.046>\n",
            "<LTCurve 225.322,329.046,227.162,337.046>\n",
            "<LTCurve 246.362,329.046,248.202,337.046>\n",
            "<LTCurve 263.985,329.046,265.825,337.046>\n",
            "<LTCurve 266.843,329.046,269.143,342.729>\n",
            "<LTCurve 290.082,329.046,292.382,342.729>\n",
            "<LTCurve 93.419,310.982,95.259,318.982>\n",
            "<LTCurve 99.007,310.982,100.847,318.982>\n",
            "<LTCurve 118.786,310.982,120.626,318.982>\n",
            "<LTCurve 124.374,310.982,126.214,318.982>\n",
            "<LTCurve 145.414,310.982,147.254,318.982>\n",
            "<LTCurve 163.037,310.982,164.877,318.982>\n",
            "<LTCurve 176.390,310.982,178.690,324.666>\n",
            "<LTCurve 198.987,310.982,200.827,318.982>\n",
            "<LTCurve 204.575,310.982,206.415,318.982>\n",
            "<LTCurve 207.433,310.982,209.733,324.666>\n",
            "<LTCurve 228.338,310.982,230.178,318.982>\n",
            "<LTCurve 233.925,310.982,235.765,318.982>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.229,264.763,607.789 'Using 1D convolutional layers to process sequences\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,425.918,432.004,588.805 'In  Chapter  14,  we  saw  that  a  2D  convolutional  layer  works  by  sliding  several  fairly\\nsmall  kernels  (or  filters)  across  an  image,  producing  multiple  2D  feature  maps\\n(one  per  kernel).  Similarly,  a  1D  convolutional  layer  slides  several  kernels  across  a\\nsequence,  producing  a  1D  feature  map  per  kernel.  Each  kernel  will  learn  to  detect\\na  single  very  short  sequential  pattern  (no  longer  than  the  kernel  size).  If  you  use\\n10  kernels,  then  the  layer’s  output  will  be  composed  of  10  1D  sequences  (all  of  the\\nsame length), or equivalently you can view this output as a single 10D sequence. This\\nmeans that you can build a neural network composed of a mix of recurrent layers and\\n1D  convolutional  layers  (or  even  1D  pooling  layers).  If  you  use  a  1D  convolutional\\nlayer with a stride of 1 and \"same\" padding, then the output sequence will have the\\nsame length as the input sequence. But if you use \"valid\" padding or a stride greater\\nthan  1,  then  the  output  sequence  will  be  shorter  than  the  input  sequence,  so  make\\nsure you adjust the targets accordingly.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,280.725,432.004,417.818 'For  example,  the  following  model  is  the  same  as  earlier,  except  it  starts  with  a  1D\\nconvolutional  layer  that  downsamples  the  input  sequence  by  a  factor  of  2,  using  a\\nstride  of  2.  The  kernel  size  is  larger  than  the  stride,  so  all  inputs  will  be  used  to\\ncompute the layer’s output, and therefore the model can learn to preserve the useful\\ninformation, dropping only the unimportant details. By shortening the sequences the\\nconvolutional layer may help the GRU layers detect longer patterns, so we can afford\\nto double the input sequence length to 112 days. Note that we must also crop off the\\nfirst  three  time  steps  in  the  targets:  indeed,  the  kernel’s  size  is  4,  so  the  first  output\\nof the convolutional layer will be based on the input time steps 0 to 3, and the first\\nforecasts will be for time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we\\nmust downsample the targets by a factor of 2, because of the stride:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,214.238,382.247,273.738 'conv_rnn_model = tf.keras.Sequential([\\n    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2,\\n                           activation=\"relu\", input_shape=[None, 5]),\\n    tf.keras.layers.GRU(32, return_sequences=True),\\n    tf.keras.layers.Dense(14)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,142.838,369.497,202.338 'longer_train = to_seq2seq_dataset(mulvar_train, seq_length=112,\\n                                       shuffle=True, seed=42)\\nlonger_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\\ndownsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))\\ndownsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))\\n[...]  # compile and fit the model using the downsampled datasets\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,98.324,432.003,134.025 'If  you  train  and  evaluate  this  model,  you  will  find  that  it  outperforms  the  previous\\nmodel (by a small margin). In fact, it is actually possible to use only 1D convolutional\\nlayers and drop the recurrent layers entirely!\\n'>\n",
            "<LTTextBoxHorizontal(6) 325.142,40.500,402.515,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.983,40.500,431.999,49.500 '573\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.001,596.229,106.739,607.789 'WaveNet\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,490.104,432.005,589.425 'In a 2016 paper,16 Aaron van den Oord and other DeepMind researchers introduced\\na novel architecture called WaveNet. They stacked 1D convolutional layers, doubling\\nthe dilation rate (how spread apart each neuron’s inputs are) at every layer: the first\\nconvolutional layer gets a glimpse of just two time steps at a time, while the next one\\nsees four time steps (its receptive field is four time steps long), the next one sees eight\\ntime steps, and so on (see Figure 15-14). This way, the lower layers learn short-term\\npatterns,  while  the  higher  layers  learn  long-term  patterns.  Thanks  to  the  doubling\\ndilation rate, the network can process extremely large sequences very efficiently.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.999,321.074,216.490,331.574 'Figure 15-14. WaveNet architecture\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,195.674,432.004,306.974 'The authors of the paper actually stacked 10 convolutional layers with dilation rates\\nof  1,  2,  4,  8,  …,  256,  512,  then  they  stacked  another  group  of  10  identical  layers\\n(also  with  dilation  rates  1,  2,  4,  8,  …,  256,  512),  then  again  another  identical  group\\nof  10  layers.  They  justified  this  architecture  by  pointing  out  that  a  single  stack  of\\n10 convolutional layers with these dilation rates will act like a super-efficient convolu‐\\ntional layer with a kernel of size 1,024 (except way faster, more powerful, and using\\nsignificantly  fewer  parameters).  They  also  left-padded  the  input  sequences  with  a\\nnumber  of  zeros  equal  to  the  dilation  rate  before  every  layer,  to  preserve  the  same\\nsequence length throughout the network.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,164.474,431.997,187.574 'Here  is  how  to  implement  a  simplified  WaveNet  to  tackle  the  same  sequences  as\\nearlier:17\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.996,138.787,331.246,157.487 'wavenet_model = tf.keras.Sequential()\\nwavenet_model.add(tf.keras.layers.Input(shape=[None, 5]))\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,101.954,425.240,109.954 '16 Aaron van den Oord et al., “WaveNet: A Generative Model for Raw Audio”, arXiv preprint arXiv:1609.03499\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,91.954,102.720,99.954 '(2016).\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.777,78.954,428.357,86.954 '17 The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and gated activation\\n'>\n",
            "<LTTextBoxHorizontal(9) 79.997,68.954,356.701,76.954 'units similar to those found in a GRU cell. See this chapter’s notebook for more details.\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.001,40.500,84.439,49.500 '574 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.907,40.500,266.932,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,483.843,432.500,483.843>\n",
            "<LTLine 432.375,337.138,432.375,483.968>\n",
            "<LTLine 72.000,337.263,432.500,337.263>\n",
            "<LTLine 72.125,337.138,72.125,483.968>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTFigure(I1) 85.449,343.118,419.051,478.718 matrix=[333.60,0.00,0.00,135.60, (85.45,343.12)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,557.350,390.750,606.650 'for rate in (1, 2, 4, 8) * 2:\\n    wavenet_model.add(tf.keras.layers.Conv1D(\\n        filters=32, kernel_size=2, padding=\"causal\", activation=\"relu\",\\n        dilation_rate=rate))\\nwavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,397.063,432.005,549.503 'This Sequential model starts with an explicit input layer—this is simpler than trying\\nto set input_shape only on the first layer. Then it continues with a 1D convolutional\\nlayer  using  \"causal\"  padding,  which  is  like  \"same\"  padding  except  that  the  zeros\\nare  appended  only  at  the  start  of  the  input  sequence,  instead  of  on  both  sides.  This\\nensures  that  the  convolutional  layer  does  not  peek  into  the  future  when  making\\npredictions. Then we add similar pairs of layers using growing dilation rates: 1, 2, 4,\\n8,  and  again  1,  2,  4,  8.  Finally,  we  add  the  output  layer:  a  convolutional  layer  with\\n14  filters  of  size  1  and  without  any  activation  function.  As  we  saw  earlier,  such  a\\nconvolutional layer is equivalent to a Dense layer with 14 units. Thanks to the causal\\npadding, every convolutional layer outputs a sequence of the same length as its input\\nsequence, so the targets we use during training can be the full 112-day sequences: no\\nneed to crop them or downsample them.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,277.663,432.004,388.963 'The models we’ve discussed in this section offer similar performance for the ridership\\nforecasting  task,  but  they  may  vary  significantly  depending  on  the  task  and  the\\namount of available data. In the WaveNet paper, the authors achieved state-of-the-art\\nperformance on various audio tasks (hence the name of the architecture), including\\ntext-to-speech  tasks,  producing  incredibly  realistic  voices  across  several  languages.\\nThey also used the model to generate music, one audio sample at a time. This feat is\\nall  the  more  impressive  when  you  realize  that  a  single  second  of  audio  can  contain\\ntens  of  thousands  of  time  steps—even  LSTMs  and  GRUs  cannot  handle  such  long\\nsequences.\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.785,158.972,396.005,260.732 'If  you  evaluate  our  best  Chicago  ridership  models  on  the  test\\nperiod,  starting  in  2020,  you  will  find  that  they  perform  much\\nworse than expected! Why is that? Well, that’s when the Covid-19\\npandemic started, which greatly affected public transportation. As\\nmentioned earlier, these models will only work well if the patterns\\nthey  learned  from  the  past  continue  in  the  future.  In  any  case,\\nbefore  deploying  a  model  to  production,  verify  that  it  works  well\\non recent data. And once it’s in production, make sure to monitor\\nits performance regularly.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,118.783,431.995,141.883 'With that, you can now tackle all sorts of time series! In Chapter 16, we will continue\\nto explore RNNs, and we will see how they can tackle various NLP tasks as well.\\n'>\n",
            "<LTTextBoxHorizontal(5) 325.145,40.500,402.518,49.500 'Handling Long Sequences \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.986,40.500,432.002,49.500 '575\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,214.217,132.680,261.527 matrix=[49.68,0.00,0.00,47.31, (83.00,214.22)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,589.053,127.644,607.973 'Exercises\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.314,566.312,432.001,576.812 '1.\\n1. Can  you  think  of  a  few  applications  for  a  sequence-to-sequence  RNN?  What\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.995,553.712,367.479,564.212 'about a sequence-to-vector RNN, and a vector-to-sequence RNN?\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.314,537.112,431.995,547.613 '2.\\n2. How many dimensions must the inputs of an RNN layer have? What does each\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.000,524.513,283.714,535.013 'dimension represent? What about its outputs?\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.313,494.719,432.000,518.413 '3.\\n3. If  you  want  to  build  a  deep  sequence-to-sequence  RNN,  which  RNN  layers\\nshould have return_sequences=True? What about a sequence-to-vector RNN?\\n'>\n",
            "<LTTextBoxHorizontal(6) 77.313,478.119,432.004,488.619 '4. Suppose you have a daily univariate time series, and you want to forecast the next\\n4.\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.998,465.519,314.709,476.019 'seven days. Which RNN architecture should you use?\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.317,448.919,426.177,459.419 '5.\\n5. What are the main difficulties when training RNNs? How can you handle them?\\n'>\n",
            "<LTTextBoxHorizontal(9) 77.317,432.319,277.623,442.819 '6.\\n6. Can you sketch the LSTM cell’s architecture?\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.317,415.719,361.675,426.219 '7.\\n7. Why would you want to use 1D convolutional layers in an RNN?\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.317,399.119,375.703,409.619 '8.\\n8. Which neural network architecture could you use to classify videos?\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.312,382.519,431.998,393.019 '9.\\n9. Train a classification model for the SketchRNN dataset, available in TensorFlow\\n'>\n",
            "<LTTextBoxHorizontal(13) 90.003,369.919,127.708,380.419 'Datasets.\\n'>\n",
            "<LTTextBoxHorizontal(14) 72.271,227.319,432.005,363.819 '10.\\n10. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales\\ncomposed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long,\\nand each time step contains 4 integers, where each integer corresponds to a note’s\\nindex  on  a  piano  (except  for  the  value  0,  which  means  that  no  note  is  played).\\nTrain a model—recurrent, convolutional, or both—that can predict the next time\\nstep  (four  notes),  given  a  sequence  of  time  steps  from  a  chorale.  Then  use  this\\nmodel to generate Bach-like music, one note at a time: you can do this by giving\\nthe model the start of a chorale and asking it to predict the next time step, then\\nappending these time steps to the input sequence and asking the model for the\\nnext  note,  and  so  on.  Also  make  sure  to  check  out  Google’s  Coconet  model,\\nwhich was used for a nice Google doodle about Bach.\\n'>\n",
            "<LTTextBoxHorizontal(15) 71.996,192.119,431.999,215.219 'Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\\nhttps://homl.info/colab3.\\n'>\n",
            "<LTTextBoxHorizontal(16) 72.003,40.500,84.441,49.500 '576 \\n'>\n",
            "<LTTextBoxHorizontal(17) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 102.909,40.500,266.934,49.500 'Chapter 15: Processing Sequences Using RNNs and CNNs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 190.873,509.542,432.001,582.331 'CHAPTER 16\\nNatural Language Processing\\nwith RNNs and Attention\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,231.309,432.005,393.629 'When Alan Turing imagined his famous Turing test1 in 1950, he proposed a way to\\nevaluate  a  machine’s  ability  to  match  human  intelligence.  He  could  have  tested  for\\nmany  things,  such  as  the  ability  to  recognize  cats  in  pictures,  play  chess,  compose\\nmusic,  or  escape  a  maze,  but,  interestingly,  he  chose  a  linguistic  task.  More  specifi‐\\ncally,  he  devised  a  chatbot  capable  of  fooling  its  interlocutor  into  thinking  it  was\\nhuman.2 This test does have its weaknesses: a set of hardcoded rules can fool unsus‐\\npecting or naive humans (e.g., the machine could give vague predefined answers in\\nresponse to some keywords, it could pretend that it is joking or drunk to get a pass on\\nits weirdest answers, or it could escape difficult questions by answering them with its\\nown questions), and many aspects of human intelligence are utterly ignored (e.g., the\\nability to interpret nonverbal communication such as facial expressions, or to learn a\\nmanual task). But the test does highlight the fact that mastering language is arguably\\nHomo sapiens’s greatest cognitive ability.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,174.909,432.003,223.209 'Can  we  build  a  machine  that  can  master  written  and  spoken  language?  This  is  the\\nultimate  goal  of  NLP  research,  but  it’s  a  bit  too  broad,  so  in  practice  researchers\\nfocus  on  more  specific  tasks,  such  as  text  classification,  translation,  summarization,\\nquestion answering, and many more.\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,111.954,339.280,119.954 '1 Alan Turing, “Computing Machinery and Intelligence”, Mind 49 (1950): 433–460.\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.139,68.954,430.751,106.954 '2 Of course, the word chatbot came much later. Turing called his test the imitation game: machine A and human\\nB chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is\\nthe machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try\\nto help the interrogator.\\n'>\n",
            "<LTTextBoxHorizontal(5) 420.983,40.500,431.999,49.500 '577\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTLine 72.000,127.900,162.000,127.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,456.436,432.004,605.537 'A common approach for natural language tasks is to use recurrent neural networks.\\nWe will therefore continue to explore RNNs (introduced in Chapter 15), starting with\\na  character  RNN,  or  char-RNN,  trained  to  predict  the  next  character  in  a  sentence.\\nThis  will  allow  us  to  generate  some  original  text.  We  will  first  use  a  stateless  RNN\\n(which learns on random portions of text at each iteration, without any information\\non the rest of the text), then we will build a stateful RNN (which preserves the hidden\\nstate  between  training  iterations  and  continues  reading  where  it  left  off,  allowing  it\\nto learn longer patterns). Next, we will build an RNN to perform sentiment analysis\\n(e.g., reading movie reviews and extracting the rater’s feeling about the movie), this\\ntime  treating  sentences  as  sequences  of  words,  rather  than  characters.  Then  we  will\\nshow  how  RNNs  can  be  used  to  build  an  encoder–decoder  architecture  capable  of\\nperforming neural machine translation (NMT), translating English to Spanish.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,324.436,432.005,448.336 'In  the  second  part  of  this  chapter,  we  will  explore  attention  mechanisms.  As  their\\nname suggests, these are neural network components that learn to select the part of\\nthe inputs that the rest of the model should focus on at each time step. First, we will\\nboost the performance of an RNN-based encoder–decoder architecture using atten‐\\ntion.  Next,  we  will  drop  RNNs  altogether  and  use  a  very  successful  attention-only\\narchitecture, called the transformer, to build a translation model. We will then discuss\\nsome of the most important advances in NLP in the last few years, including incredi‐\\nbly powerful language models such as GPT and BERT, both based on transformers.\\nLastly, I will show you how to get started with the excellent Transformers library by\\nHugging Face.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.999,305.836,405.238,316.336 'Let’s start with a simple and fun model that can write like Shakespeare (sort of).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,205.436,432.002,293.496 'Generating Shakespearean Text Using a Character RNN\\nIn  a  famous  2015  blog  post  titled  “The  Unreasonable  Effectiveness  of  Recurrent\\nNeural Networks”, Andrej Karpathy showed how to train an RNN to predict the next\\ncharacter in a sentence. This char-RNN can then be used to generate novel text, one\\ncharacter at a time. Here is a small sample of the text generated by a char-RNN model\\nafter it was trained on all of Shakespeare’s works:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.998,186.760,139.511,196.060 'PANDARUS:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.998,171.600,290.301,180.900 'Alas, I think he shall be come approached and the day\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.998,156.440,295.630,165.740 'When little srain would be attain’d into being never fed,\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.998,141.280,271.394,150.580 'And who is but a chain and subjects of his death,\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.998,126.120,157.888,135.420 'I should not sleep.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.998,80.836,432.005,116.536 'Not  exactly  a  masterpiece,  but  it  is  still  impressive  that  the  model  was  able  to  learn\\nwords, grammar, proper punctuation, and more, just by learning to predict the next\\ncharacter  in  a  sentence.  This  is  our  first  example  of  a  language  model;  similar  (but\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.002,40.500,84.440,49.500 '578 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'much more powerful) language models, discussed later in this chapter, are at the core\\nof modern NLP. In the remainder of this section we’ll build a char-RNN step by step,\\nstarting with the creation of the dataset.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.993,511.234,431.767,558.267 'Creating the Training Dataset\\nFirst, using Keras’s handy tf.keras.utils.get_file() function, let’s download all of\\nShakespeare’s works. The data is loaded from Andrej Karpathy’s char-rnn project:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,495.748,186.752,504.248 'import tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.002,444.748,386.502,483.848 'shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\\nfilepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\\nwith open(filepath) as f:\\n    shakespeare_text = f.read()\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,425.434,189.934,435.934 'Let’s print the first few lines:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,389.548,280.247,418.448 '>>> print(shakespeare_text[:80])\\nFirst Citizen:\\nBefore we proceed any further, hear me speak.\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.997,358.948,144.247,377.648 'All:\\nSpeak, speak.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,339.634,206.795,350.134 'Looks like Shakespeare all right!\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,281.455,431.998,332.500 'Next,  we’ll  use  a  tf.keras.layers.TextVectorization  layer  (introduced  in  Chap‐\\nter 13) to encode this text. We set split=\"character\" to get character-level encoding\\nrather  than  the  default  word-level  encoding,  and  we  use  standardize=\"lower\"  to\\nconvert the text to lowercase (which will simplify the task):\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.003,235.368,390.753,274.468 'text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\\n                                                   standardize=\"lower\")\\ntext_vec_layer.adapt([shakespeare_text])\\nencoded = text_vec_layer([shakespeare_text])[0]\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.995,165.061,432.003,227.520 'Each  character  is  now  mapped  to  an  integer,  starting  at  2.  The  TextVectorization\\nlayer  reserved  the  value  0  for  padding  tokens,  and  it  reserved  1  for  unknown  char‐\\nacters.  We  won’t  need  either  of  these  tokens  for  now,  so  let’s  subtract  2  from  the\\ncharacter IDs and compute the number of distinct characters and the total number of\\ncharacters:\\n'>\n",
            "<LTTextBoxHorizontal(11) 88.997,129.175,428.997,158.075 'encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\\nn_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\\ndataset_size = len(encoded)  # total number of chars = 1,115,394\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.996,72.061,432.005,120.361 'Next,  just  like  we  did  in  Chapter  15,  we  can  turn  this  very  long  sequence  into  a\\ndataset of windows that we can then use to train a sequence-to-sequence RNN. The\\ntargets will be similar to the inputs, but shifted by one time step into the “future”. For\\nexample, one sample in the dataset may be a sequence of character IDs representing\\n'>\n",
            "<LTTextBoxHorizontal(13) 239.918,40.500,402.512,49.500 'Generating Shakespearean Text Using a Character RNN \\n'>\n",
            "<LTTextBoxHorizontal(14) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 420.980,40.500,431.996,49.500 '579\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'the  text  “to  be  or  not  to  b”  (without  the  final  “e”),  and  the  corresponding  target—a\\nsequence  of  character  IDs  representing  the  text  “o  be  or  not  to  be”  (with  the  final\\n“e”, but without the leading “t”). Let’s write a small utility function to convert a long\\nsequence of character IDs into a dataset of input/target window pairs:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,470.350,416.249,550.250 'def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\\n    ds = ds.window(length + 1, shift=1, drop_remainder=True)\\n    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\\n    if shuffle:\\n        ds = ds.shuffle(buffer_size=100_000, seed=seed)\\n    ds = ds.batch(batch_size)\\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,437.843,431.996,462.502 'This function starts much like the to_windows() custom utility function we created\\nin Chapter 15:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.652,415.243,432.000,425.743 '•\\n• It takes a sequence as input (i.e., the encoded text), and creates a dataset contain‐\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.005,402.643,262.341,413.143 'ing all the windows of the desired length.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.652,386.043,412.145,396.543 '•\\n• It increases the length by one, since we need the next character for the target.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.652,369.443,432.005,379.943 '•\\n• Then, it shuffles the windows (optionally), batches them, splits them into input/\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.999,356.843,253.316,367.343 'output pairs, and activates prefetching.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,321.643,432.001,344.743 'Figure 16-1 summarizes the dataset preparation steps: it shows windows of length 11,\\nand a batch size of 3. The start index of each window is indicated next to it.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.998,85.683,286.040,96.183 'Figure 16-1. Preparing a dataset of shuffled windows\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.002,40.500,84.440,49.500 '580 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,315.382,432.500,315.382>\n",
            "<LTLine 432.375,101.746,432.375,315.507>\n",
            "<LTLine 72.000,101.871,432.500,101.871>\n",
            "<LTLine 72.125,101.746,72.125,315.507>\n",
            "<LTFigure(I1) 108.250,107.726,396.250,310.257 matrix=[288.00,0.00,0.00,202.53, (108.25,107.73)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'Now we’re ready to create the training set, the validation set, and the test set. We will\\nuse roughly 90% of the text for training, 5% for validation, and 5% for testing:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.004,515.950,395.004,575.450 'length = 100\\ntf.random.set_seed(42)\\ntrain_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\\n                       seed=42)\\nvalid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\\ntest_set = to_dataset(encoded[1_060_000:], length=length)\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.787,453.602,395.998,498.305 'We  set  the  window  length  to  100,  but  you  can  try  tuning  it:  it’s\\neasier and faster to train RNNs on shorter input sequences, but the\\nRNN  will  not  be  able  to  learn  any  pattern  longer  than  length,  so\\ndon’t make it too small.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,422.237,402.998,432.737 'That’s it! Preparing the dataset was the hardest part. Now let’s create the model.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,338.434,432.005,410.667 'Building and Training the Char-RNN Model\\nSince our dataset is reasonably large, and modeling language is quite a difficult task,\\nwe need more than a simple RNN with a few recurrent neurons. Let’s build and train\\na model with one GRU layer composed of 128 units (you can try tweaking the number\\nof layers and units later, if needed):\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.999,220.948,394.999,331.448 'model = tf.keras.Sequential([\\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\\n    tf.keras.layers.GRU(128, return_sequences=True),\\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\\n])\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\\n    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\\nhistory = model.fit(train_set, validation_data=valid_set, epochs=10,\\n                    callbacks=[model_ckpt])\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,201.634,166.276,212.134 'Let’s go over this code:\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.652,101.061,432.005,190.500 '• We use an Embedding layer as the first layer, to encode the character IDs (embed‐\\n•\\ndings  were  introduced  in  Chapter  13).  The  Embedding  layer’s  number  of  input\\ndimensions  is  the  number  of  distinct  character  IDs,  and  the  number  of  output\\ndimensions is a hyperparameter you can tune—we’ll set it to 16 for now. Whereas\\nthe inputs of the Embedding layer will be 2D tensors of shape [batch size, window\\nlength], the output of the Embedding layer will be a 3D tensor of shape [batch size,\\nwindow length, embedding size].\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.652,71.268,432.004,95.927 '• We  use  a  Dense  layer  for  the  output  layer:  it  must  have  39  units  (n_tokens)\\n•\\nbecause  there  are  39  distinct  characters  in  the  text,  and  we  want  to  output  a\\n'>\n",
            "<LTTextBoxHorizontal(9) 239.920,40.500,402.514,49.500 'Generating Shakespearean Text Using a Character RNN \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.982,40.500,431.998,49.500 '581\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,448.323,126.760,504.100 matrix=[41.76,0.00,0.00,55.78, (85.00,448.32)]>\n",
            "<LTTextBoxHorizontal(0) 89.997,569.243,432.003,605.537 'probability for each possible character (at each time step). The 39 output proba‐\\nbilities should sum up to 1 at each time step, so we apply the softmax activation\\nfunction to the outputs of the Dense layer.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.655,513.657,432.002,564.109 '• Lastly,  we  compile  this  model,  using  the  \"sparse_categorical_crossentropy\"\\n•\\nloss  and  a  Nadam  optimizer,  and  we  train  the  model  for  several  epochs,3  using\\na ModelCheckpoint callback to save the best model (in terms of validation accu‐\\nracy) as training progresses.\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.785,419.525,396.005,486.725 'If you are running this code on Colab with a GPU activated, then\\ntraining  should  take  roughly  one  to  two  hours.  You  can  reduce\\nthe  number  of  epochs  if  you  don’t  want  to  wait  that  long,  but  of\\ncourse  the  model’s  accuracy  will  probably  be  lower.  If  the  Colab\\nsession times out, make sure to reconnect quickly, or else the Colab\\nruntime will be destroyed.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,352.950,432.000,402.437 'This  model  does  not  handle  text  preprocessing,  so  let’s  wrap  it  in  a  final  model\\ncontaining  the  tf.keras.layers.TextVectorization  layer  as  the  first  layer,  plus  a\\ntf.keras.layers.Lambda layer to subtract 2 from the character IDs since we’re not\\nusing the padding and unknown tokens for now:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,296.664,395.004,345.964 'shakespeare_model = tf.keras.Sequential([\\n    text_vec_layer,\\n    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\\n    model\\n])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,277.350,333.533,287.850 'And now let’s use it to predict the next character in a sentence:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.997,231.264,394.997,270.364 '>>> y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\\n>>> y_pred = tf.argmax(y_proba)  # choose the most probable character ID\\n>>> text_vec_layer.get_vocabulary()[y_pred + 2]\\n\\'e\\'\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.998,199.350,432.000,222.450 'Great,  the  model  correctly  predicted  the  next  character.  Now  let’s  use  this  model  to\\npretend we’re Shakespeare!\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,116.141,432.004,187.780 'Generating Fake Shakespearean Text\\nTo  generate  new  text  using  the  char-RNN  model,  we  could  feed  it  some  text,  make\\nthe model predict the most likely next letter, add it to the end of the text, then give\\nthe extended text to the model to guess the next letter, and so on. This is called greedy\\ndecoding. But in practice this often leads to the same words being repeated over and\\n'>\n",
            "<LTTextBoxHorizontal(9) 73.140,78.954,409.080,86.954 '3 Since the input windows overlap, the concept of epoch is not so clear in this case: during each epoch (as\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.000,68.954,353.664,76.954 'implemented by Keras), the model will actually see the same character multiple times.\\n'>\n",
            "<LTTextBoxHorizontal(11) 72.004,40.500,84.442,49.500 '582 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.910,40.500,299.362,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 85.000,436.743,126.760,492.520 matrix=[41.76,0.00,0.00,55.78, (85.00,436.74)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,543.450,432.005,605.537 'over  again.  Instead,  we  can  sample  the  next  character  randomly,  with  a  probability\\nequal  to  the  estimated  probability,  using  TensorFlow’s  tf.random.categorical()\\nfunction.  This  will  generate  more  diverse  and  interesting  text.  The  categorical()\\nfunction samples random class indices, given the class log probabilities (logits). For\\nexample:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,497.363,424.749,536.463 '>>> log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\\n>>> tf.random.set_seed(42)\\n>>> tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples\\n<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,389.257,432.003,488.550 'To have more control over the diversity of the generated text, we can divide the logits\\nby a number called the temperature, which we can tweak as we wish. A temperature\\nclose  to  zero  favors  high-probability  characters,  while  a  high  temperature  gives  all\\ncharacters  an  equal  probability.  Lower  temperatures  are  typically  preferred  when\\ngenerating fairly rigid and precise text, such as mathematical equations, while higher\\ntemperatures  are  preferred  when  generating  more  diverse  and  creative  text.  The\\nfollowing  next_char()  custom  helper  function  uses  this  approach  to  pick  the  next\\ncharacter to add to the input text:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,332.970,399.247,382.270 'def next_char(text, temperature=1):\\n    y_proba = shakespeare_model.predict([text])[0, -1:]\\n    rescaled_logits = tf.math.log(y_proba) / temperature\\n    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\\n    return text_vec_layer.get_vocabulary()[char_id + 2]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,300.463,431.997,324.157 'Next,  we  can  write  another  small  helper  function  that  will  repeatedly  call\\nnext_char() to get the next character and append it to the given text:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.001,254.377,297.251,293.477 'def extend_text(text, n_chars=50, temperature=1):\\n    for _ in range(n_chars):\\n        text += next_char(text, temperature)\\n    return text\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,235.063,421.603,245.563 'We are now ready to generate some text! Let’s try with different temperature values:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.999,158.377,352.499,228.077 '>>> tf.random.set_seed(42)\\n>>> print(extend_text(\"To be or not to be\", temperature=0.01))\\nTo be or not to be the duke\\nas it is a proper strange death,\\nand the\\n>>> print(extend_text(\"To be or not to be\", temperature=1))\\nTo be or not to behold?\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.999,97.177,348.249,146.477 'second push:\\ngremio, lord all, a sistermen,\\n>>> print(extend_text(\"To be or not to be\", temperature=100))\\nTo be or not to bef ,mt\\'&o3fpadm!$\\nwh!nse?bws3est--vgerdjw?c-y-ewznq\\n'>\n",
            "<LTTextBoxHorizontal(9) 239.923,40.500,402.517,49.500 'Generating Shakespearean Text Using a Character RNN \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.985,40.500,432.001,49.500 '583\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,480.450,432.005,605.537 'Shakespeare  seems  to  be  suffering  from  a  heatwave.  To  generate  more  convincing\\ntext, a common technique is to sample only from the top k characters, or only from\\nthe smallest set of top characters whose total probability exceeds some threshold (this\\nis called nucleus sampling). Alternatively, you could try using beam search, which we\\nwill  discuss  later  in  this  chapter,  or  using  more  GRU  layers  and  more  neurons  per\\nlayer,  training  for  longer,  and  adding  some  regularization  if  needed.  Also  note  that\\nthe  model  is  currently  incapable  of  learning  patterns  longer  than  length,  which  is\\njust  100  characters.  You  could  try  making  this  window  larger,  but  it  will  also  make\\ntraining harder, and even LSTM and GRU cells cannot handle very long sequences.\\nAn alternative approach is to use a stateful RNN.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,359.441,432.005,468.880 'Stateful RNN\\nUntil  now,  we  have  only  used  stateless  RNNs:  at  each  training  iteration  the  model\\nstarts  with  a  hidden  state  full  of  zeros,  then  it  updates  this  state  at  each  time  step,\\nand  after  the  last  time  step,  it  throws  it  away  as  it  is  not  needed  anymore.  What  if\\nwe  instructed  the  RNN  to  preserve  this  final  state  after  processing  a  training  batch\\nand  use  it  as  the  initial  state  for  the  next  training  batch?  This  way  the  model  could\\nlearn long-term patterns despite only backpropagating through short sequences. This\\nis called a stateful RNN. Let’s go over how to build one.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,263.461,432.004,351.341 'First,  note  that  a  stateful  RNN  only  makes  sense  if  each  input  sequence  in  a  batch\\nstarts exactly where the corresponding sequence in the previous batch left off. So the\\nfirst thing we need to do to build a stateful RNN is to use sequential and nonoverlap‐\\nping  input  sequences  (rather  than  the  shuffled  and  overlapping  sequences  we  used\\nto train stateless RNNs). When creating the tf.data.Dataset, we must therefore use\\nshift=length (instead of shift=1) when calling the window() method. Moreover, we\\nmust not call the shuffle() method.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,142.875,432.005,255.361 'Unfortunately,  batching  is  much  harder  when  preparing  a  dataset  for  a  stateful\\nRNN  than  it  is  for  a  stateless  RNN.  Indeed,  if  we  were  to  call  batch(32),  then  32\\nconsecutive windows would be put in the same batch, and the following batch would\\nnot  continue  each  of  these  windows  where  it  left  off.  The  first  batch  would  contain\\nwindows  1  to  32  and  the  second  batch  would  contain  windows  33  to  64,  so  if  you\\nconsider, say, the first window of each batch (i.e., windows 1 and 33), you can see that\\nthey are not consecutive. The simplest solution to this problem is to just use a batch\\nsize  of  1.  The  following  to_dataset_for_stateful_rnn()  custom  utility  function\\nuses this strategy to prepare a dataset for a stateful RNN:\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,40.500,84.441,49.500 '584 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.909,40.500,299.361,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,557.350,416.250,606.650 'def to_dataset_for_stateful_rnn(sequence, length):\\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\\n    ds = ds.window(length + 1, shift=length, drop_remainder=True)\\n    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,506.350,420.500,545.450 'stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\\nstateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\\n                                                 length)\\nstateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,487.037,306.045,497.537 'Figure 16-2 summarizes the main steps of this function.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,265.206,420.432,275.706 'Figure 16-2. Preparing a dataset of consecutive sequence fragments for a stateful RNN\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,176.419,432.004,251.106 'Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s\\ntext  into  32  texts  of  equal  length,  create  one  dataset  of  consecutive  input  sequen‐\\nces for each of them, and finally use  tf.data.Dataset.zip(datasets).map(lambda\\n*windows: tf.stack(windows)) to create proper consecutive batches, where the nth\\ninput sequence in a batch starts off exactly where the nth input sequence ended in the\\nprevious batch (see the notebook for the full code).\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,93.633,432.004,169.285 'Now,  let’s  create  the  stateful  RNN.  We  need  to  set  the  stateful  argument  to  True\\nwhen  creating  each  recurrent  layer,  and  because  the  stateful  RNN  needs  to  know\\nthe  batch  size  (since  it  will  preserve  a  state  for  each  input  sequence  in  the  batch).\\nTherefore we must set the batch_input_shape argument in the first layer. Note that\\nwe can leave the second dimension unspecified, since the input sequences could have\\nany length:\\n'>\n",
            "<LTTextBoxHorizontal(6) 239.923,40.500,402.517,49.500 'Generating Shakespearean Text Using a Character RNN \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.985,40.500,432.001,49.500 '585\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,480.775,432.500,480.775>\n",
            "<LTLine 432.375,281.269,432.375,480.900>\n",
            "<LTLine 72.000,281.394,432.500,281.394>\n",
            "<LTLine 72.125,281.269,72.125,480.900>\n",
            "<LTFigure(I1) 111.130,287.249,393.370,475.650 matrix=[282.24,0.00,0.00,188.40, (111.13,287.25)]>\n",
            "<LTTextBoxHorizontal(0) 88.999,547.150,373.749,606.650 'model = tf.keras.Sequential([\\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\\n                              batch_input_shape=[1, None]),\\n    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\\n])\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,515.236,432.005,538.337 'At  the  end  of  each  epoch,  we  need  to  reset  the  states  before  we  go  back  to  the\\nbeginning of the text. For this, we can use a small custom Keras callback:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,479.350,322.749,508.250 'class ResetStatesCallback(tf.keras.callbacks.Callback):\\n    def on_epoch_begin(self, epoch, logs):\\n        self.model.reset_states()\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,460.037,353.664,470.537 'And now we can compile the model and train it using our callback:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,413.950,416.249,453.050 'model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nhistory = model.fit(stateful_train_set, validation_data=stateful_valid_set,\\n                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])\\n'>\n",
            "<LTTextBoxHorizontal(5) 136.793,352.145,396.042,396.305 'After this model is trained, it will only be possible to use it to make\\npredictions for batches of the same size as were used during train‐\\ning. To avoid this restriction, create an identical stateless model, and\\ncopy the stateful model’s weights to this model.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,194.236,432.004,330.737 'Interestingly, although a char-RNN model is just trained to predict the next character,\\nthis  seemingly  simple  task  actually  requires  it  to  learn  some  higher-level  tasks  as\\nwell. For example, to find the next character after “Great movie, I really”, it’s helpful\\nto  understand  that  the  sentence  is  positive,  so  what  follows  is  more  likely  to  be\\nthe  letter  “l”  (for  “loved”)  rather  than  “h”  (for  “hated”).  In  fact,  a  2017  paper4  by\\nAlec Radford and other OpenAI researchers describes how the authors trained a big\\nchar-RNN-like model on a large dataset, and found that one of the neurons acted as\\nan excellent sentiment analysis classifier: although the model was trained without any\\nlabels, the sentiment neuron—as they called it—reached state-of-the-art performance\\non  sentiment  analysis  benchmarks.  This  foreshadowed  and  motivated  unsupervised\\npretraining in NLP.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,150.436,432.003,186.136 'But before we explore unsupervised pretraining, let’s turn our attention to word-level\\nmodels  and  how  to  use  them  in  a  supervised  fashion  for  sentiment  analysis.  In  the\\nprocess, you will learn how to handle sequences of variable lengths using masking.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,78.954,382.376,86.954 '4 Alec Radford et al., “Learning to Generate Reviews and Discovering Sentiment”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,160.552,76.954 'arXiv:1704.01444 (2017).\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,40.500,84.435,49.500 '586 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.903,40.500,299.355,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 85.000,346.323,126.760,402.100 matrix=[41.76,0.00,0.00,55.78, (85.00,346.32)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,456.912,432.005,607.973 'Sentiment Analysis\\nGenerating text can be fun and instructive, but in real-life projects, one of the most\\ncommon  applications  of  NLP  is  text  classification—especially  sentiment  analysis.  If\\nimage classification on the MNIST dataset is the “Hello world!” of computer vision,\\nthen sentiment analysis on the IMDb reviews dataset is the “Hello world!” of natural\\nlanguage processing. The IMDb dataset consists of 50,000 movie reviews in English\\n(25,000  for  training,  25,000  for  testing)  extracted  from  the  famous  Internet  Movie\\nDatabase, along with a simple binary target for each review indicating whether it is\\nnegative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for\\ngood reasons: it is simple enough to be tackled on a laptop in a reasonable amount of\\ntime, but challenging enough to be fun and rewarding.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,413.112,432.004,448.812 'Let’s  load  the  IMDb  dataset  using  the  TensorFlow  Datasets  library  (introduced  in\\nChapter 13). We’ll use the first 90% of the training set for training, and the remaining\\n10% for validation:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,397.626,233.498,406.126 'import tensorflow_datasets as tfds\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.998,295.626,386.498,385.726 'raw_train_set, raw_valid_set, raw_test_set = tfds.load(\\n    name=\"imdb_reviews\",\\n    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\\n    as_supervised=True\\n)\\ntf.random.set_seed(42)\\ntrain_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\\nvalid_set = raw_valid_set.batch(32).prefetch(1)\\ntest_set = raw_test_set.batch(32).prefetch(1)\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.786,244.798,395.996,277.981 'Keras  also  includes  a  function  for  loading  the  IMDb  dataset,  if\\nyou  prefer:  tf.keras.datasets.imdb.load_data().  The  reviews\\nare already preprocessed as sequences of word IDs.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,201.912,183.089,212.412 'Let’s inspect a few reviews:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.002,74.226,429.003,194.926 '>>> for review, label in raw_train_set.take(4):\\n...     print(review.numpy().decode(\"utf-8\"))\\n...     print(\"Label:\", label.numpy())\\n...\\nThis was an absolutely terrible movie. Don\\'t be lured in by Christopher [...]\\nLabel: 0\\nI have been known to fall asleep during films, but this is usually due to [...]\\nLabel: 0\\nMann photographs the Alberta Rocky Mountains in a superb fashion, and [...]\\nLabel: 0\\nThis is the kind of film for a snowy Sunday afternoon when the rest of the [...]\\nLabel: 1\\n'>\n",
            "<LTTextBoxHorizontal(7) 344.376,40.500,402.516,49.500 'Sentiment Analysis \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.984,40.500,432.000,49.500 '587\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,227.999,126.760,283.776 matrix=[41.76,0.00,0.00,55.78, (85.00,228.00)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,557.236,432.003,605.537 'Some  reviews  are  easy  to  classify.  For  example,  the  first  review  includes  the  words\\n“terrible  movie”  in  the  very  first  sentence.  But  in  many  cases  things  are  not  that\\nsimple. For example, the third review starts off positively, even though it’s ultimately a\\nnegative review (label 0).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,449.250,432.004,549.136 'To  build  a  model  for  this  task,  we  need  to  preprocess  the  text,  but  this  time  we\\nwill  chop  it  into  words  instead  of  characters.  For  this,  we  can  use  the  tf.keras.\\nlayers.TextVectorization  layer  again.  Note  that  it  uses  spaces  to  identify  word\\nboundaries,  which  will  not  work  well  in  some  languages.  For  example,  Chinese\\nwriting  does  not  use  spaces  between  words,  Vietnamese  uses  spaces  even  within\\nwords, and German often attaches multiple words together, without spaces. Even in\\nEnglish, spaces are not always the best way to tokenize text: think of “San Francisco”\\nor “#ILoveDeepLearning”.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,317.250,432.004,441.770 'Fortunately, there are solutions to address these issues. In a 2016 paper,5 Rico Senn‐\\nrich  et  al.  from  the  University  of  Edinburgh  explored  several  methods  to  tokenize\\nand  detokenize  text  at  the  subword  level.  This  way,  even  if  your  model  encounters\\na  rare  word  it  has  never  seen  before,  it  can  still  reasonably  guess  what  it  means.\\nFor  example,  even  if  the  model  never  saw  the  word  “smartest”  during  training,  if  it\\nlearned  the  word  “smart”  and  it  also  learned  that  the  suffix  “est”  means  “the  most”,\\nit  can  infer  the  meaning  of  “smartest”.  One  of  the  techniques  the  authors  evaluated\\nis  byte  pair  encoding  (BPE).  BPE  works  by  splitting  the  whole  training  set  into\\nindividual characters (including spaces), then repeatedly merging the most frequent\\nadjacent pairs until the vocabulary reaches the desired size.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,210.450,432.005,309.770 'A  2018  paper6  by  Taku  Kudo  at  Google  further  improved  subword  tokenization,\\noften  removing  the  need  for  language-specific  preprocessing  prior  to  tokenization.\\nMoreover, the paper proposed a novel regularization technique called subword regula‐\\nrization, which improves accuracy and robustness by introducing some randomness\\nin  tokenization  during  training:  for  example,  “New  England”  may  be  tokenized  as\\n“New”  +  “England”,  or  “New”  +  “Eng”  +  “land”,  or  simply  “New  England”  (just\\none token). Google’s SentencePiece project provides an open source implementation,\\nwhich is described in a paper7 by Taku Kudo and John Richardson.\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.140,124.954,429.808,132.954 '5 Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units”, Proceedings of the 54th\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,114.954,350.408,122.954 'Annual Meeting of the Association for Computational Linguistics 1 (2016): 1715–1725.\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.137,101.954,430.389,109.954 '6 Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword\\n'>\n",
            "<LTTextBoxHorizontal(7) 79.997,91.954,249.445,99.954 'Candidates”, arXiv preprint arXiv:1804.10959 (2018).\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.142,78.954,423.442,86.954 '7 Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.002,68.954,353.850,76.954 'and Detokenizer for Neural Text Processing”, arXiv preprint arXiv:1808.06226 (2018).\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.998,40.500,84.436,49.500 '588 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.904,40.500,299.356,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,140.900,162.000,140.900>\n",
            "<LTTextBoxHorizontal(0) 71.998,569.837,432.004,605.537 'The TensorFlow Text library also implements various tokenization strategies, includ‐\\ning  WordPiece8  (a  variant  of  BPE),  and  last  but  not  least,  the  Tokenizers  library  by\\nHugging Face implements a wide range of extremely fast tokenizers.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,487.643,432.003,561.736 'However, for the IMDb task in English, using spaces for token boundaries should be\\ngood enough. So let’s go ahead with creating a TextVectorization layer and adapting\\nit to the training set. We will limit the vocabulary to 1,000 tokens, including the most\\nfrequent  998  words  plus  a  padding  token  and  a  token  for  unknown  words,  since\\nit’s  unlikely  that  very  rare  words  will  be  important  for  this  task,  and  limiting  the\\nvocabulary size will reduce the number of parameters the model needs to learn:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,451.757,399.251,480.657 'vocab_size = 1000\\ntext_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\\ntext_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,432.443,257.335,442.943 'Finally, we can create the model and train it:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.001,314.957,373.751,425.457 'embed_size = 128\\ntf.random.set_seed(42)\\nmodel = tf.keras.Sequential([\\n    text_vec_layer,\\n    tf.keras.layers.Embedding(vocab_size, embed_size),\\n    tf.keras.layers.GRU(128),\\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\\n])\\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nhistory = model.fit(train_set, validation_data=valid_set, epochs=2)\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,179.870,432.005,307.109 'The  first  layer  is  the  TextVectorization  layer  we  just  prepared,  followed  by  an\\nEmbedding layer that will convert word IDs into embeddings. The embedding matrix\\nneeds to have one row per token in the vocabulary (vocab_size) and one column per\\nembedding dimension (this example uses 128 dimensions, but this is a hyperparame‐\\nter you could tune). Next we use a GRU layer and a Dense layer with a single neuron\\nand  the  sigmoid  activation  function,  since  this  is  a  binary  classification  task:  the\\nmodel’s output will be the estimated probability that the review expresses a positive\\nsentiment  regarding  the  movie.  We  then  compile  the  model,  and  we  fit  it  on  the\\ndataset we prepared earlier for a couple of epochs (or you can train for longer to get\\nbetter results).\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,78.954,414.488,86.954 '8 Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,279.568,76.954 'Machine Translation”, arXiv preprint arXiv:1609.08144 (2016).\\n'>\n",
            "<LTTextBoxHorizontal(8) 344.375,40.500,402.515,49.500 'Sentiment Analysis \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.983,40.500,431.999,49.500 '589\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,454.657,432.005,605.537 'Sadly,  if  you  run  this  code,  you  will  generally  find  that  the  model  fails  to  learn\\nanything  at  all:  the  accuracy  remains  close  to  50%,  no  better  than  random  chance.\\nWhy  is  that?  The  reviews  have  different  lengths,  so  when  the  TextVectorization\\nlayer converts them to sequences of token IDs, it pads the shorter sequences using the\\npadding token (with ID 0) to make them as long as the longest sequence in the batch.\\nAs  a  result,  most  sequences  end  with  many  padding  tokens—often  dozens  or  even\\nhundreds  of  them.  Even  though  we’re  using  a  GRU  layer,  which  is  much  better  than\\na SimpleRNN layer, its short-term memory is still not great, so when it goes through\\nmany padding tokens, it ends up forgetting what the review was about! One solution\\nis  to  feed  the  model  with  batches  of  equal-length  sentences  (which  also  speeds  up\\ntraining). Another solution is to make the RNN ignore the padding tokens. This can\\nbe done using masking.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,358.254,432.004,443.087 'Masking\\nMaking  the  model  ignore  padding  tokens  is  trivial  using  Keras:  simply  add\\nmask_zero=True when creating the Embedding layer. This means that padding tokens\\n(whose ID is 0) will be ignored by all downstream layers. That’s all! If you retrain the\\nprevious  model  for  a  few  epochs,  you  will  find  that  the  validation  accuracy  quickly\\nreaches over 80%.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,235.888,432.005,351.120 'The  way  this  works  is  that  the  Embedding  layer  creates  a  mask  tensor  equal  to\\ntf.math.not_equal(inputs,  0):  it  is  a  Boolean  tensor  with  the  same  shape  as  the\\ninputs, and it is equal to False anywhere the token IDs are 0, or True otherwise. This\\nmask tensor is then automatically propagated by the model to the next layer. If that\\nlayer’s call() method has a mask argument, then it automatically receives the mask.\\nThis  allows  the  layer  to  ignore  the  appropriate  time  steps.  Each  layer  may  handle\\nthe mask differently, but in general they simply ignore masked time steps (i.e., time\\nsteps for which the mask is False). For example, when a recurrent layer encounters a\\nmasked time step, it simply copies the output from the previous time step.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,73.349,432.005,228.754 'Next,  if  the  layer’s  supports_masking  attribute  is  True,  then  the  mask  is  automati‐\\ncally  propagated  to  the  next  layer.  It  keeps  propagating  this  way  for  as  long  as  the\\nlayers  have  supports_masking=True.  As  an  example,  a  recurrent  layer’s  supports_\\nmasking attribute is True when return_sequences=True, but it’s False when return_\\nsequences=False  since  there’s  no  need  for  a  mask  anymore  in  this  case.  So  if  you\\nhave  a  model  with  several  recurrent  layers  with  return_sequences=True,  followed\\nby a recurrent layer with return_sequences=False, then the mask will automatically\\npropagate  up  to  the  last  recurrent  layer:  that  layer  will  use  the  mask  to  ignore\\nmasked  steps,  but  it  will  not  propagate  the  mask  any  further.  Similarly,  if  you  set\\nmask_zero=True when creating the Embedding layer in the sentiment analysis model\\nwe just built, then the GRU layer will receive and use the mask automatically, but it will\\nnot propagate it any further, since return_sequences is not set to True.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,40.500,84.441,49.500 '590 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.909,40.500,299.361,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 136.790,537.420,396.002,605.705 'Some  layers  need  to  update  the  mask  before  propagating  it  to\\nthe  next  layer:  they  do  so  by  implementing  the  compute_mask()\\nmethod,  which  takes  two  arguments:  the  inputs  and  the  previous\\nmask.  It  then  computes  the  updated  mask  and  returns  it.  The\\ndefault  implementation  of  compute_mask()  just  returns  the  previ‐\\nous mask unchanged.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,470.252,432.002,521.298 'Many  Keras  layers  support  masking:  SimpleRNN,  GRU,  LSTM,  Bidirectional,  Dense,\\nTimeDistributed,  Add,  and  a  few  others  (all  in  the  tf.keras.layers  package).\\nHowever,  convolutional  layers  (including  Conv1D)  do  not  support  masking—it’s  not\\nobvious how they would do so anyway.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,413.852,432.004,462.152 'If the mask propagates all the way to the output, then it gets applied to the losses as\\nwell, so the masked time steps will not contribute to the loss (their loss will be 0). This\\nassumes  that  the  model  outputs  sequences,  which  is  not  the  case  in  our  sentiment\\nanalysis model.\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.787,305.053,396.001,397.803 'The  LSTM  and  GRU  layers  have  an  optimized  implementation  for\\nGPUs, based on Nvidia’s cuDNN library. However, this implemen‐\\ntation  only  supports  masking  if  all  the  padding  tokens  are  at  the\\nend of the sequences. It also requires you to use the default values\\nfor  several  hyperparameters:  activation,  recurrent_activation,\\nrecurrent_dropout,  unroll,  use_bias, and  reset_after. If that’s\\nnot  the  case,  then  these  layers  will  fall  back  to  the  (much  slower)\\ndefault GPU implementation.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,212.685,432.003,287.965 'If you want to implement your own custom layer with masking support, you should\\nadd  a  mask  argument  to  the  call()  method,  and  obviously  make  the  method  use\\nthe  mask.  Additionally,  if  the  mask  must  be  propagated  to  the  next  layers,  then\\nyou  should  set  self.supports_masking=True  in  the  constructor.  If  the  mask  must\\nbe  updated  before  it  is  propagated,  then  you  must  implement  the  compute_mask()\\nmethod.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,154.505,432.004,205.551 'If  your  model  does  not  start  with  an  Embedding  layer,  you  may  use  the  tf.\\nkeras.layers.Masking  layer  instead:  by  default,  it  sets  the  mask  to  tf.math.\\nreduce_any(tf.math.not_equal(X,  0),  axis=-1),  meaning  that  time  steps  where\\nthe last dimension is full of zeros will be masked out in subsequent layers.\\n'>\n",
            "<LTTextBoxHorizontal(6) 344.378,40.500,402.518,49.500 'Sentiment Analysis \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.986,40.500,432.002,49.500 '591\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTFigure(I2) 83.000,350.406,132.680,397.715 matrix=[49.68,0.00,0.00,47.31, (83.00,350.41)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,518.843,432.005,605.537 'Using masking layers and automatic mask propagation works best for simple models.\\nIt  will  not  always  work  for  more  complex  models,  such  as  when  you  need  to  mix\\nConv1D layers with recurrent layers. In such cases, you will need to explicitly compute\\nthe  mask  and  pass  it  to  the  appropriate  layers,  using  either  the  functional  API  or\\nthe subclassing API. For example, the following model is equivalent to the previous\\nmodel, except it is built using the functional API and handles masking manually. It\\nalso adds a bit of dropout since the previous model was overfitting slightly:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.001,442.157,361.001,511.857 'inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\\ntoken_ids = text_vec_layer(inputs)\\nmask = tf.math.not_equal(token_ids, 0)\\nZ = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\\nZ = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.003,397.050,431.998,433.963 'One last approach to masking is to feed the model with ragged tensors.9 In practice,\\nall you need to do is to set ragged=True when creating the TextVectorization layer,\\nso that the input sequences are represented as ragged tensors:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,330.563,424.751,390.063 '>>> text_vec_layer_ragged = tf.keras.layers.TextVectorization(\\n...     max_tokens=vocab_size, ragged=True)\\n...\\n>>> text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\\n>>> text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio\\'s best role.\"])\\n<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,298.650,432.002,321.750 'Compare  this  ragged  tensor  representation  with  the  regular  tensor  representation,\\nwhich uses padding tokens:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,252.563,382.250,291.663 '>>> text_vec_layer([\"Great movie!\", \"This is DiCaprio\\'s best role.\"])\\n<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\\narray([[ 86,  18,   0,   0,   0],\\n       [ 11,   7,   1, 116, 217]])>\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,156.463,432.004,243.750 'Keras’s  recurrent  layers  have  built-in  support  for  ragged  tensors,  so  there’s  nothing\\nelse  you  need  to  do:  just  use  this  TextVectorization  layer  in  your  model.  There’s\\nno need to pass mask_zero=True or handle masks explicitly—it’s all implemented for\\nyou. That’s convenient! However, as of early 2022, the support for ragged tensors in\\nKeras is still fairly recent, so there are a few rough edges. For example, it is currently\\nnot possible to use ragged tensors as targets when running on the GPU (but this may\\nbe resolved by the time you read these lines).\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,112.070,432.004,148.363 'Whichever masking approach you prefer, after training this model for a few epochs,\\nit  will  become  quite  good  at  judging  whether  a  review  is  positive  or  not.  If  you  use\\nthe tf.keras.callbacks.TensorBoard() callback, you can visualize the embeddings\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,68.954,349.352,76.954 '9 Ragged tensors were introduced in Chapter 12, and they are detailed in Appendix C.\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.004,40.500,84.442,49.500 '592 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.910,40.500,299.362,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,544.636,432.003,605.537 'in TensorBoard as they are being learned: it is fascinating to see words like “awesome”\\nand “amazing” gradually cluster on one side of the embedding space, while words like\\n“awful” and “terrible” cluster on the other side. Some words are not as positive as you\\nmight expect (at least with this model), such as the word “good”, presumably because\\nmany negative reviews contain the phrase “not good”.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,335.427,432.005,533.067 'Reusing Pretrained Embeddings and Language Models\\nIt’s impressive that the model is able to learn useful word embeddings based on just\\n25,000 movie reviews. Imagine how good the embeddings would be if we had billions\\nof  reviews  to  train  on!  Unfortunately,  we  don’t,  but  perhaps  we  can  reuse  word\\nembeddings  trained  on  some  other  (very)  large  text  corpus  (e.g.,  Amazon  reviews,\\navailable  on  TensorFlow  Datasets),  even  if  it  is  not  composed  of  movie  reviews?\\nAfter all, the word “amazing” generally has the same meaning whether you use it to\\ntalk about movies or anything else. Moreover, perhaps embeddings would be useful\\nfor  sentiment  analysis  even  if  they  were  trained  on  another  task:  since  words  like\\n“awesome”  and  “amazing”  have  a  similar  meaning,  they  will  likely  cluster  in  the\\nembedding  space  even  for  tasks  such  as  predicting  the  next  word  in  a  sentence.  If\\nall positive words and all negative words form clusters, then this will be helpful for\\nsentiment analysis. So, instead of training word embeddings, we could just download\\nand  use  pretrained  embeddings,  such  as  Google’s  Word2vec  embeddings,  Stanford’s\\nGloVe embeddings, or Facebook’s FastText embeddings.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,228.627,432.004,327.327 'Using pretrained word embeddings was popular for several years, but this approach\\nhas its limits. In particular, a word has a single representation, no matter the context.\\nFor example, the word “right” is encoded the same way in “left and right” and “right\\nand wrong”, even though it means two very different things. To address this limita‐\\ntion, a 2018 paper10 by Matthew Peters introduced Embeddings from Language Models\\n(ELMo): these are contextualized word embeddings learned from the internal states\\nof a deep bidirectional language model. Instead of just using pretrained embeddings\\nin your model, you reuse part of a pretrained language model.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,147.027,432.005,220.527 'At  roughly  the  same  time,  the  Universal  Language  Model  Fine-Tuning  (ULMFiT)\\npaper11  by  Jeremy  Howard  and  Sebastian  Ruder  demonstrated  the  effectiveness  of\\nunsupervised  pretraining  for  NLP  tasks:  the  authors  trained  an  LSTM  language\\nmodel  on  a  huge  text  corpus  using  self-supervised  learning  (i.e.,  generating  the\\nlabels  automatically  from  the  data),  then  they  fine-tuned  it  on  various  tasks.  Their\\nmodel  outperformed  the  state  of  the  art  on  six  text  classification  tasks  by  a  large\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,111.954,415.760,119.954 '10 Matthew Peters et al., “Deep Contextualized Word Representations”, Proceedings of the 2018 Conference of\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,91.954,427.584,109.954 'the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1\\n(2018): 2227–2237.\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.783,68.954,421.899,86.954 '11 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification”, Pro‐\\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328–339.\\n'>\n",
            "<LTTextBoxHorizontal(7) 344.375,40.500,402.515,49.500 'Sentiment Analysis \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.983,40.500,431.999,49.500 '593\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,127.900,162.000,127.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,519.436,432.005,605.537 'margin  (reducing  the  error  rate  by  18–24%  in  most  cases).  Moreover,  the  authors\\nshowed  a  pretrained  model  fine-tuned  on  just  100  labeled  examples  could  achieve\\nthe  same  performance  as  one  trained  from  scratch  on  10,000  examples.  Before  the\\nULMFiT  paper,  using  pretrained  models  was  only  the  norm  in  computer  vision;  in\\nthe context of NLP, pretraining was limited to word embeddings. This paper marked\\nthe beginning of a new era in NLP: today, reusing pretrained language models is the\\nnorm.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,463.036,432.003,511.336 'For example, let’s build a classifier based on the Universal Sentence Encoder, a model\\narchitecture  introduced  in  a  2018  paper12  by  a  team  of  Google  researchers.  This\\nmodel  is  based  on  the  transformer  architecture,  which  we  will  look  at  later  in  this\\nchapter. Conveniently, the model is available on TensorFlow Hub:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.996,437.350,207.996,456.050 'import os\\nimport tensorflow_hub as hub\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.996,325.150,407.746,425.450 'os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\\nmodel = tf.keras.Sequential([\\n    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\\n                   trainable=True, dtype=tf.string, input_shape=[]),\\n    tf.keras.layers.Dense(64, activation=\"relu\"),\\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\\n])\\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nmodel.fit(train_set, validation_data=valid_set, epochs=10)\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.786,228.242,395.998,307.505 'This model is quite large—close to 1 GB in size—so it may take a\\nwhile to download. By default, TensorFlow Hub modules are saved\\nto a temporary directory, and they get downloaded again and again\\nevery  time  you  run  your  program.  To  avoid  that,  you  must  set\\nthe  TFHUB_CACHE_DIR  environment  variable  to  a  directory  of  your\\nchoice: the modules will then be saved there, and only downloaded\\nonce.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,162.854,432.005,211.154 'Note  that  the  last  part  of  the  TensorFlow  Hub  module  URL  specifies  that  we  want\\nversion  4  of  the  model.  This  versioning  ensures  that  if  a  new  module  version  is\\nreleased on TF Hub, it will not break our model. Conveniently, if you just enter this\\nURL in a web browser, you will get the documentation for this module.\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,68.954,361.968,76.954 '12 Daniel Cer et al., “Universal Sentence Encoder”, arXiv preprint arXiv:1803.11175 (2018).\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.002,40.500,84.440,49.500 '594 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 85.000,257.523,126.760,313.300 matrix=[41.76,0.00,0.00,55.78, (85.00,257.52)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,544.043,432.005,606.503 'Also note that we set trainable=True when creating the hub.KerasLayer. This way,\\nthe pretrained Universal Sentence Encoder is fine-tuned during training: some of its\\nweights are tweaked via backprop. Not all TensorFlow Hub modules are fine-tunable,\\nso make sure to check the documentation for each pretrained module you’re interes‐\\nted in.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,487.643,432.003,535.943 'After  training,  this  model  should  reach  a  validation  accuracy  of  over  90%.  That’s\\nactually really good: if you try to perform the task yourself, you will probably do only\\nmarginally better since many reviews contain both positive and negative comments.\\nClassifying these ambiguous reviews is like flipping a coin.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,431.243,432.005,479.543 'So  far  we  have  looked  at  text  generation  using  a  char-RNN,  and  sentiment  analysis\\nwith word-level RNN models (based on trainable embeddings) and using a powerful\\npretrained language model from TensorFlow Hub. In the next section, we will explore\\nanother important NLP task: neural machine translation (NMT).\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,350.669,432.003,418.903 'An Encoder–Decoder Network for Neural Machine\\nTranslation\\nLet’s begin with a simple NMT model13 that will translate English sentences to Span‐\\nish (see Figure 16-3).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,231.269,432.004,342.569 'In  short,  the  architecture  is  as  follows:  English  sentences  are  fed  as  inputs  to  the\\nencoder,  and  the  decoder  outputs  the  Spanish  translations.  Note  that  the  Spanish\\ntranslations are also used as inputs to the decoder during training, but shifted back\\nby one step. In other words, during training the decoder is given as input the word\\nthat it should have output at the previous step, regardless of what it actually output.\\nThis  is  called  teacher  forcing—a  technique  that  significantly  speeds  up  training  and\\nimproves the model’s performance. For the very first word, the decoder is given the\\nstart-of-sequence (SOS) token, and the decoder is expected to end the sentence with\\nan end-of-sequence (EOS) token.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,186.282,431.999,224.135 'Each word is initially represented by its ID (e.g., 854 for the word “soccer”). Next, an\\nEmbedding layer returns the word embedding. These word embeddings are then fed\\nto the encoder and the decoder.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,117.282,432.004,178.182 'At each step, the decoder outputs a score for each word in the output vocabulary (i.e.,\\nSpanish),  then  the  softmax  activation  function  turns  these  scores  into  probabilities.\\nFor example, at the first step the word “Me” may have a probability of 7%, “Yo” may\\nhave a probability of 1%, and so on. The word with the highest probability is output.\\nThis  is  very  much  like  a  regular  classification  task,  and  indeed  you  can  train  the\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,68.954,397.392,76.954 '13 Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks”, arXiv preprint (2014).\\n'>\n",
            "<LTTextBoxHorizontal(8) 219.919,40.500,402.520,49.500 'An Encoder–Decoder Network for Neural Machine Translation \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.988,40.500,432.004,49.500 '595\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,581.843,431.999,606.503 'model using the \"sparse_categorical_crossentropy\" loss, much like we did in the\\nchar-RNN model.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,291.372,270.740,301.872 'Figure 16-3. A simple machine translation model\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,228.972,432.004,277.272 'Note that at inference time (after training), you will not have the target sentence to\\nfeed to the decoder. Instead, you need to feed it the word that it has just output at the\\nprevious step, as shown in Figure 16-4 (this will require an embedding lookup that is\\nnot shown in the diagram).\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.785,179.401,396.001,212.937 'In a 2015 paper,14 Samy Bengio et al. proposed gradually switching\\nfrom feeding the decoder the previous target token to feeding it the\\nprevious output token during training.\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,78.954,418.104,86.954 '14 Samy Bengio et al., “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,68.954,188.896,76.954 'preprint arXiv:1506.03099 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,40.500,84.438,49.500 '596 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,575.582,432.500,575.582>\n",
            "<LTLine 432.375,307.436,432.375,575.707>\n",
            "<LTLine 72.000,307.561,432.500,307.561>\n",
            "<LTLine 72.125,307.436,72.125,575.707>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 85.000,162.059,126.760,217.836 matrix=[41.76,0.00,0.00,55.78, (85.00,162.06)]>\n",
            "<LTFigure(I2) 103.570,313.416,400.930,570.457 matrix=[297.36,0.00,0.00,257.04, (103.57,313.42)]>\n",
            "<LTTextBoxHorizontal(0) 72.002,428.961,422.965,452.061 'Figure 16-4. At inference time, the decoder is fed as input the word it just output at the\\nprevious time step\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,391.761,432.003,414.861 'Let’s  build  and  train  this  model!  First,  we  need  to  download  a  dataset  of  English/\\nSpanish sentence pairs:15\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,345.675,424.749,384.775 'url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\\npath = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\\n                               extract=True)\\ntext = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,287.968,432.004,336.861 'Each  line  contains  an  English  sentence  and  the  corresponding  Spanish  translation,\\nseparated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”, which\\nthe  TextVectorization  layer  doesn’t  handle,  then  we  will  parse  the  sentence  pairs\\nand shuffle them. Finally, we will split them into two separate lists, one per language:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,272.481,165.500,280.981 'import numpy as np\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,221.481,412.000,260.581 'text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\\npairs = [line.split(\"\\\\t\") for line in text.splitlines()]\\nnp.random.shuffle(pairs)\\nsentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.999,202.168,270.974,212.668 'Let’s take a look at the first three sentence pairs:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,135.681,411.998,195.181 '>>> for i in range(3):\\n...     print(sentences_en[i], \"=>\", sentences_es[i])\\n...\\nHow boring! => Qué aburrimiento!\\nI love sports. => Adoro el deporte.\\nWould you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.780,88.954,413.200,96.954 '15 This dataset is composed of sentence pairs created by contributors of the Tatoeba project. About 120,000\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,428.072,86.954 'sentence pairs were selected by the authors of the website https://manythings.org/anki. This dataset is released\\nunder the Creative Commons Attribution 2.0 France license. Other language pairs are available.\\n'>\n",
            "<LTTextBoxHorizontal(10) 219.911,40.500,402.512,49.500 'An Encoder–Decoder Network for Neural Machine Translation \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.980,40.500,431.996,49.500 '597\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,457.625,432.375,607.500>\n",
            "<LTLine 72.000,457.750,432.500,457.750>\n",
            "<LTLine 72.125,457.625,72.125,607.500>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTFigure(I1) 108.250,463.605,396.250,602.250 matrix=[288.00,0.00,0.00,138.65, (108.25,463.60)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,581.843,432.000,606.503 'Next, let’s create two TextVectorization layers—one per language—and adapt them\\nto the text:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,494.957,407.749,574.857 'vocab_size = 1000\\nmax_length = 50\\ntext_vec_layer_en = tf.keras.layers.TextVectorization(\\n    vocab_size, output_sequence_length=max_length)\\ntext_vec_layer_es = tf.keras.layers.TextVectorization(\\n    vocab_size, output_sequence_length=max_length)\\ntext_vec_layer_en.adapt(sentences_en)\\ntext_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,475.643,220.209,486.143 'There are a few things to note here:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.650,377.443,432.003,463.543 '•\\n• We  limit  the  vocabulary  size  to  1,000,  which  is  quite  small.  That’s  because  the\\ntraining  set  is  not  very  large,  and  because  using  a  small  value  will  speed  up\\ntraining. State-of-the-art translation models typically use a much larger vocabu‐\\nlary  (e.g.,  30,000),  a  much  larger  training  set  (gigabytes),  and  a  much  larger\\nmodel (hundreds or even thousands of megabytes). For example, check out the\\nOpus-MT  models  by  the  University  of  Helsinki,  or  the  M2M-100  model  by\\nFacebook.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.652,322.450,432.004,371.343 '•\\n• Since  all  sentences  in  the  dataset  have  a  maximum  of  50  words,  we  set\\noutput_sequence_length to 50: this way the input sequences will automatically\\nbe padded with zeros until they are all 50 tokens long. If there was any sentence\\nlonger than 50 tokens in the training set, it would be cropped to 50 tokens.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.652,267.457,432.005,316.350 '•\\n• For the Spanish text, we add “startofseq” and “endofseq” to each sentence when\\nadapting the TextVectorization layer: we will use these words as SOS and EOS\\ntokens.  You  could  use  any  other  words,  as  long  as  they  are  not  actual  Spanish\\nwords.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,219.657,432.003,255.357 'Let’s  inspect  the  first  10  tokens  in  both  vocabularies.  They  start  with  the  padding\\ntoken,  the  unknown  token,  the  SOS  and  EOS  tokens  (only  in  the  Spanish  vocabu‐\\nlary), then the actual words, sorted by decreasing frequency:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.000,173.570,412.000,212.670 \">>> text_vec_layer_en.get_vocabulary()[:10]\\n['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\\n>>> text_vec_layer_es.get_vocabulary()[:10]\\n['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\\n\">\n",
            "<LTTextBoxHorizontal(8) 71.996,116.457,432.004,164.757 'Next,  let’s  create  the  training  set  and  the  validation  set  (you  could  also  create  a  test\\nset if you needed it). We will use the first 100,000 sentence pairs for training, and the\\nrest for validation. The decoder’s inputs are the Spanish sentences plus an SOS token\\nprefix. The targets are the Spanish sentences plus an EOS suffix:\\n'>\n",
            "<LTTextBoxHorizontal(9) 88.999,70.370,420.499,109.470 'X_train = tf.constant(sentences_en[:100_000])\\nX_valid = tf.constant(sentences_en[100_000:])\\nX_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\\nX_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.001,40.500,84.439,49.500 '598 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.907,40.500,299.359,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 88.999,587.950,420.500,606.650 'Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\\nY_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,543.437,432.005,579.137 'OK, we’re now ready to build our translation model. We will use the functional API\\nfor  that  since  the  model  is  not  sequential.  It  requires  two  text  inputs—one  for  the\\nencoder and one for the decoder—so let’s start with that:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,517.750,365.248,536.450 'encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\\ndecoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,458.857,432.004,509.902 'Next,  we  need  to  encode  these  sentences  using  the  TextVectorization  layers\\nwe  prepared  earlier,  followed  by  an  Embedding  layer  for  each  language,  with\\nmask_zero=True  to  ensure  masking  is  handled  automatically.  The  embedding  size\\nis a hyperparameter you can tune, as always:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.996,361.770,407.747,451.870 'embed_size = 128\\nencoder_input_ids = text_vec_layer_en(encoder_inputs)\\ndecoder_input_ids = text_vec_layer_es(decoder_inputs)\\nencoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\\n                                                    mask_zero=True)\\ndecoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\\n                                                    mask_zero=True)\\nencoder_embeddings = encoder_embedding_layer(encoder_input_ids)\\ndecoder_embeddings = decoder_embedding_layer(decoder_input_ids)\\n'>\n",
            "<LTTextBoxHorizontal(5) 136.791,311.485,396.002,344.125 'When the languages share many words, you may get better perfor‐\\nmance  using  the  same  embedding  layer  for  both  the  encoder  and\\nthe decoder.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,268.057,329.046,278.557 'Now let’s create the encoder and pass it the embedded inputs:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,242.370,348.248,261.070 'encoder = tf.keras.layers.LSTM(512, return_state=True)\\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,157.684,432.005,234.523 'To  keep  things  simple,  we  just  used  a  single  LSTM  layer,  but  you  could  stack  several\\nof  them.  We  also  set  return_state=True  to  get  a  reference  to  the  layer’s  final  state.\\nSince we’re using an LSTM layer, there are actually two states: the short-term state and\\nthe long-term state. The layer returns these states separately, which is why we had to\\nwrite *encoder_state to group both states in a list.16 Now we can use this (double)\\nstate as the initial state of the decoder:\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.003,131.997,403.503,150.697 'decoder = tf.keras.layers.LSTM(512, return_sequences=True)\\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.780,68.954,356.248,78.142 '16 In Python, if you run a, *b = [1, 2, 3, 4], then a equals 1 and b equals [2, 3, 4].\\n'>\n",
            "<LTTextBoxHorizontal(11) 219.912,40.500,402.513,49.500 'An Encoder–Decoder Network for Neural Machine Translation \\n'>\n",
            "<LTTextBoxHorizontal(12) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 420.981,40.500,431.997,49.500 '599\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,85.592,162.000,85.592>\n",
            "<LTFigure(I1) 85.000,294.143,126.760,349.920 matrix=[41.76,0.00,0.00,55.78, (85.00,294.14)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,581.843,432.000,606.503 'Next,  we  can  pass  the  decoder’s  outputs  through  a  Dense  layer  with  the  softmax\\nactivation function to get the word probabilities for each step:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.997,556.157,386.497,574.857 'output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\\nY_proba = output_layer(decoder_outputs)\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.252,383.372,423.752,535.432 'Optimizing the Output Layer\\nWhen  the  output  vocabulary  is  large,  outputting  a  probability  for  each  and  every\\npossible  word  can  be  quite  slow.  If  the  target  vocabulary  contained,  say,  50,000\\nSpanish words instead of 1,000, then the decoder would output 50,000-dimensional\\nvectors, and computing the softmax function over such a large vector would be very\\ncomputationally intensive. To avoid this, one solution is to look only at the logits out‐\\nput by the model for the correct word and for a random sample of incorrect words,\\nthen compute an approximation of the loss based only on these logits. This sampled\\nsoftmax  technique  was  introduced  in  2015  by  Sébastien  Jean  et  al.17  In  TensorFlow\\nyou  can  use  the  tf.nn.sampled_softmax_loss()  function  for  this  during  training\\nand use the normal softmax function at inference time (sampled softmax cannot be\\nused at inference time because it requires knowing the target).\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.248,245.372,423.752,375.372 'Another  thing  you  can  do  to  speed  up  training—which  is  compatible  with  sampled\\nsoftmax—is  to  tie  the  weights  of  the  output  layer  to  the  transpose  of  the  decoder’s\\nembedding matrix (you will see how to tie weights in Chapter 17). This significantly\\nreduces the number of model parameters, which speeds up training and may some‐\\ntimes  improve  the  model’s  accuracy  as  well,  especially  if  you  don’t  have  a  lot  of\\ntraining data. The embedding matrix is equivalent to one-hot encoding followed by\\na  linear  layer  with  no  bias  term  and  no  activation  function  that  maps  the  one-hot\\nvectors to the embedding space. The output layer does the reverse. So, if the model\\ncan find an embedding matrix whose transpose is close to its inverse (such a matrix is\\ncalled an orthogonal matrix), then there’s no need to learn a separate set of weights for\\nthe output layer.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.001,210.935,390.004,222.994 'And that’s it! We just need to create the Keras Model, compile it, and train it:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,144.448,395.002,203.949 'model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\\n                       outputs=[Y_proba])\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\\n          validation_data=((X_valid, X_valid_dec), Y_valid))\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,68.954,421.496,106.954 '17 Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation”, Proceedings\\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint\\nConference on Natural Language Processing of the Asian Federation of Natural Language Processing 1 (2015):\\n1–10.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,40.500,84.442,49.500 '600 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.910,40.500,299.362,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,545.182,432.000,545.182>\n",
            "<LTLine 431.875,233.992,431.875,545.307>\n",
            "<LTLine 72.000,234.117,432.000,234.117>\n",
            "<LTLine 72.125,233.992,72.125,545.307>\n",
            "<LTLine 72.000,114.900,162.000,114.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,518.843,432.005,605.537 'After training, we can use the model to translate new English sentences to Spanish.\\nBut  it’s  not  as  simple  as  calling  model.predict(),  because  the  decoder  expects  as\\ninput  the  word  that  was  predicted  at  the  previous  time  step.  One  way  to  do  this  is\\nto  write  a  custom  memory  cell  that  keeps  track  of  the  previous  output  and  feeds  it\\nto the encoder at the next time step. However, to keep things simple, we can just call\\nthe model multiple times, predicting one extra word at each round. Let’s write a little\\nutility function for that:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.997,391.157,424.747,511.857 'def translate(sentence_en):\\n    translation = \"\"\\n    for word_idx in range(max_length):\\n        X = np.array([sentence_en])  # encoder input\\n        X_dec = np.array([\"startofseq \" + translation])  # decoder input\\n        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token\\'s probas\\n        predicted_word_id = np.argmax(y_proba)\\n        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\\n        if predicted_word == \"endofseq\":\\n            break\\n        translation += \" \" + predicted_word\\n    return translation.strip()\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,359.243,431.996,382.343 'The  function  simply  keeps  predicting  one  word  at  a  time,  gradually  completing  the\\ntranslation, and it stops once it reaches the EOS token. Let’s give it a try!\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,333.557,216.499,352.257 '>>> translate(\"I like soccer\")\\n\\'me gusta el fútbol\\'\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,289.043,432.002,324.743 'Hurray,  it  works!  Well,  at  least  it  does  with  very  short  sentences.  If  you  try  playing\\nwith this model for a while, you will find that it’s not bilingual yet, and in particular it\\nreally struggles with longer sentences. For example:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.003,263.357,335.503,282.057 '>>> translate(\"I like soccer and also going to the beach\")\\n\\'me gusta el fútbol y a veces mismo al bus\\'\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,205.650,432.001,254.543 'The  translation  says  “I  like  soccer  and  sometimes  even  the  bus”.  So  how  can  you\\nimprove it? One way is to increase the training set size and add more LSTM layers in\\nboth  the  encoder  and  the  decoder.  But  this  will  only  get  you  so  far,  so  let’s  look  at\\nmore sophisticated techniques, starting with bidirectional recurrent layers.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,97.241,432.003,194.080 'Bidirectional RNNs\\nAt  each  time  step,  a  regular  recurrent  layer  only  looks  at  past  and  present  inputs\\nbefore  generating  its  output.  In  other  words,  it  is  causal,  meaning  it  cannot  look\\ninto  the  future.  This  type  of  RNN  makes  sense  when  forecasting  time  series,  or\\nin  the  decoder  of  a  sequence-to-sequence  (seq2seq)  model.  But  for  tasks  like  text\\nclassification,  or  in  the  encoder  of  a  seq2seq  model,  it  is  often  preferable  to  look\\nahead at the next words before encoding a given word.\\n'>\n",
            "<LTTextBoxHorizontal(8) 219.918,40.500,402.519,49.500 'An Encoder–Decoder Network for Neural Machine Translation \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.987,40.500,432.003,49.500 '601\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,605.537 'For example, consider the phrases “the right arm”, “the right person”, and “the right to\\ncriticize”: to properly encode the word “right”, you need to look ahead. One solution\\nis  to  run  two  recurrent  layers  on  the  same  inputs,  one  reading  the  words  from  left\\nto right and the other reading them from right to left, then combine their outputs at\\neach time step, typically by concatenating them. This is what a bidirectional recurrent\\nlayer does (see Figure 16-5).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,343.643,247.167,354.143 'Figure 16-5. A bidirectional recurrent layer\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,293.249,431.999,329.543 'To implement a bidirectional recurrent layer in Keras, just wrap a recurrent layer in\\na tf.keras.layers.Bidirectional layer. For example, the following Bidirectional\\nlayer could be used as the encoder in our translation model:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,267.563,297.250,286.263 'encoder = tf.keras.layers.Bidirectional(\\n    tf.keras.layers.LSTM(256, return_state=True))\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.786,204.673,395.998,250.801 'The  Bidirectional  layer  will  create  a  clone  of  the  GRU  layer  (but\\nin the reverse direction), and it will run both and concatenate their\\noutputs. So although the GRU layer has 10 units, the Bidirectional\\nlayer will output 20 values per time step.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,83.870,432.004,184.349 'There’s  just  one  problem.  This  layer  will  now  return  four  states  instead  of  two:  the\\nfinal short-term and long-term states of the forward LSTM layer, and the final short-\\nterm and long-term states of the backward LSTM layer. We cannot use this quadruple\\nstate  directly  as  the  initial  state  of  the  decoder’s  LSTM  layer,  since  it  expects  just  two\\nstates (short-term and long-term). We cannot make the decoder bidirectional, since it\\nmust remain causal: otherwise it would cheat during training and it would not work.\\nInstead, we can concatenate the two short-term states, and also concatenate the two\\nlong-term states:\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,40.500,84.434,49.500 '602 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.902,40.500,299.354,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,525.775,432.500,525.775>\n",
            "<LTLine 432.375,359.706,432.375,525.900>\n",
            "<LTLine 72.000,359.831,432.500,359.831>\n",
            "<LTLine 72.125,359.706,72.125,525.900>\n",
            "<LTFigure(I1) 89.000,202.212,126.944,251.713 matrix=[37.94,0.00,0.00,49.50, (89.00,202.21)]>\n",
            "<LTFigure(I2) 144.250,365.686,360.250,520.650 matrix=[216.00,0.00,0.00,154.96, (144.25,365.69)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,577.750,420.500,606.650 'encoder_outputs, *encoder_state = encoder(encoder_embeddings)\\nencoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\\n                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.005,545.836,431.976,568.937 'Now let’s look at another popular technique that can greatly improve the performance\\nof a translation model at inference time: beam search.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,349.227,432.004,534.267 'Beam Search\\nSuppose you have trained an encoder–decoder model, and you use it to translate the\\nsentence  “I  like  soccer”  to  Spanish.  You  are  hoping  that  it  will  output  the  proper\\ntranslation  “me  gusta  el  fútbol”,  but  unfortunately  it  outputs  “me  gustan  los  juga‐\\ndores”, which means “I like the players”. Looking at the training set, you notice many\\nsentences such as “I like cars”, which translates to “me gustan los autos”, so it wasn’t\\nabsurd for the model to output “me gustan los” after seeing “I like”. Unfortunately, in\\nthis case it was a mistake since “soccer” is singular. The model could not go back and\\nfix it, so it tried to complete the sentence as best it could, in this case using the word\\n“jugadores”. How can we give the model a chance to go back and fix mistakes it made\\nearlier? One of the most common solutions is beam search: it keeps track of a short\\nlist of the k most promising sentences (say, the top three), and at each decoder step\\nit  tries  to  extend  them  by  one  word,  keeping  only  the  k  most  likely  sentences.  The\\nparameter k is called the beam width.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,192.027,432.005,341.127 'For  example,  suppose  you  use  the  model  to  translate  the  sentence  “I  like  soccer”\\nusing  beam  search  with  a  beam  width  of  3  (see  Figure  16-6).  At  the  first  decoder\\nstep,  the  model  will  output  an  estimated  probability  for  each  possible  first  word\\nin  the  translated  sentence.  Suppose  the  top  three  words  are  “me”  (75%  estimated\\nprobability), “a” (3%), and “como” (1%). That’s our short list so far. Next, we use the\\nmodel to find the next word for each sentence. For the first sentence (“me”), perhaps\\nthe  model  outputs  a  probability  of  36%  for  the  word  “gustan”,  32%  for  the  word\\n“gusta”, 16% for the word “encanta”, and so on. Note that these are actually conditional\\nprobabilities, given that the sentence starts with “me”. For the second sentence (“a”),\\nthe  model  might  output  a  conditional  probability  of  50%  for  the  word  “mi”,  and  so\\non. Assuming the vocabulary has 1,000 words, we will end up with 1,000 probabilities\\nper sentence.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,85.227,432.005,183.927 'Next,  we  compute  the  probabilities  of  each  of  the  3,000  two-word  sentences  we\\nconsidered (3 × 1,000). We do this by multiplying the estimated conditional proba‐\\nbility  of  each  word  by  the  estimated  probability  of  the  sentence  it  completes.  For\\nexample, the estimated probability of the sentence “me” was 75%, while the estimated\\nconditional  probability  of  the  word  “gustan”  (given  that  the  first  word  is  “me”)  was\\n36%, so the estimated probability of the sentence “me gustan” is 75% × 36% = 27%.\\nAfter  computing  the  probabilities  of  all  3,000  two-word  sentences,  we  keep  only\\nthe  top  3.  In  this  example  they  all  start  with  the  word  “me”:  “me  gustan”  (27%),\\n'>\n",
            "<LTTextBoxHorizontal(5) 219.914,40.500,402.515,49.500 'An Encoder–Decoder Network for Neural Machine Translation \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.983,40.500,431.999,49.500 '603\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 '“me  gusta”  (24%),  and  “me  encanta”  (12%).  Right  now,  the  sentence  “me  gustan”  is\\nwinning, but “me gusta” has not been eliminated.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,400.688,274.416,411.188 'Figure 16-6. Beam search, with a beam width of 3\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,287.888,432.009,386.588 'Then we repeat the same process: we use the model to predict the next word in each\\nof  these  three  sentences,  and  we  compute  the  probabilities  of  all  3,000  three-word\\nsentences we considered. Perhaps the top three are now “me gustan los” (10%), “me\\ngusta el” (8%), and “me gusta mucho” (2%). At the next step we may get “me gusta el\\nfútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte” (0.2%). Notice that\\n“me gustan” was eliminated, and the correct translation is now ahead. We boosted our\\nencoder–decoder model’s performance without any extra training, simply by using it\\nmore wisely.\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.788,215.277,395.999,270.957 'The  TensorFlow  Addons  library  includes  a  full  seq2seq  API  that\\nlets  you  build  encoder–decoder  models  with  attention,  including\\nbeam  search,  and  more.  However,  its  documentation  is  currently\\nvery limited. Implementing beam search is a good exercise, so give\\nit a try! Check out this chapter’s notebook for a possible solution.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,149.888,432.002,198.188 'With  all  this,  you  can  get  reasonably  good  translations  for  fairly  short  sentences.\\nUnfortunately,  this  model  will  be  really  bad  at  translating  long  sentences.  Once\\nagain, the problem comes from the limited short-term memory of RNNs. Attention\\nmechanisms are the game-changing innovation that addressed this problem.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.993,87.288,432.002,137.549 'Attention Mechanisms\\nConsider  the  path  from  the  word  “soccer”  to  its  translation  “fútbol”  back  in  Fig‐\\nure 16-3: it is quite long! This means that a representation of this word (along with all\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.002,40.500,84.440,49.500 '604 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,576.175,432.500,576.175>\n",
            "<LTLine 432.375,416.752,432.375,576.300>\n",
            "<LTLine 72.000,416.877,432.500,416.877>\n",
            "<LTLine 72.125,416.752,72.125,576.300>\n",
            "<LTFigure(I1) 85.000,220.975,126.760,276.752 matrix=[41.76,0.00,0.00,55.78, (85.00,220.97)]>\n",
            "<LTFigure(I2) 79.450,422.732,425.050,571.050 matrix=[345.60,0.00,0.00,148.32, (79.45,422.73)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,582.437,432.003,605.537 'the other words) needs to be carried over many steps before it is actually used. Can’t\\nwe make this path shorter?\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,463.036,432.004,574.957 'This  was  the  core  idea  in  a  landmark  2014  paper18  by  Dzmitry  Bahdanau  et  al.,\\nwhere  the  authors  introduced  a  technique  that  allowed  the  decoder  to  focus  on  the\\nappropriate  words  (as  encoded  by  the  encoder)  at  each  time  step.  For  example,  at\\nthe  time  step  where  the  decoder  needs  to  output  the  word  “fútbol”,  it  will  focus  its\\nattention  on  the  word  “soccer”.  This  means  that  the  path  from  an  input  word  to  its\\ntranslation is now much shorter, so the short-term memory limitations of RNNs have\\nmuch less impact. Attention mechanisms revolutionized neural machine translation\\n(and deep learning in general), allowing a significant improvement in the state of the\\nart, especially for long sentences (e.g., over 30 words).\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.786,367.385,395.998,446.105 'The  most  common  metric  used  in  NMT  is  the  bilingual  evalua‐\\ntion  understudy  (BLEU)  score,  which  compares  each  translation\\nproduced by the model with several good translations produced by\\nhumans: it counts the number of n-grams (sequences of n words)\\nthat  appear  in  any  of  the  target  translations  and  adjusts  the  score\\nto take into account the frequency of the produced n-grams in the\\ntarget translations.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,150.796,432.005,350.296 'Figure 16-7 shows our encoder–decoder model with an added attention mechanism.\\nOn  the  left,  you  have  the  encoder  and  the  decoder.  Instead  of  just  sending  the\\nencoder’s final hidden state to the decoder, as well as the previous target word at each\\nstep  (which  is  still  done,  although  it  is  not  shown  in  the  figure),  we  now  send  all\\nof  the  encoder’s  outputs  to  the  decoder  as  well.  Since  the  decoder  cannot  deal  with\\nall these encoder outputs at once, they need to be aggregated: at each time step, the\\ndecoder’s  memory  cell  computes  a  weighted  sum  of  all  the  encoder  outputs.  This\\ndetermines  which  words  it  will  focus  on  at  this  step.  The  weight  α(t,i)  is  the  weight\\nof  the  ith  encoder  output  at  the  tth  decoder  time  step.  For  example,  if  the  weight\\nα(3,2) is much larger than the weights α(3,0) and α(3,1), then the decoder will pay much\\nmore attention to the encoder’s output for word #2 (“soccer”) than to the other two\\noutputs,  at  least  at  this  time  step.  The  rest  of  the  decoder  works  just  like  earlier:  at\\neach time step the memory cell receives the inputs we just discussed, plus the hidden\\nstate  from  the  previous  time  step,  and  finally  (although  it  is  not  represented  in  the\\ndiagram) it receives the target word from the previous time step (or at inference time,\\nthe output from the previous time step).\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,78.954,413.744,86.954 '18 Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,68.954,185.056,76.954 'preprint arXiv:1409.0473 (2014).\\n'>\n",
            "<LTTextBoxHorizontal(6) 334.442,40.500,402.518,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.986,40.500,432.002,49.500 '605\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 89.000,398.399,126.944,447.900 matrix=[37.94,0.00,0.00,49.50, (89.00,398.40)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,303.606,413.450,326.706 'Figure 16-7. Neural machine translation using an encoder–decoder network with an\\nattention model\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,89.412,432.005,289.506 'But  where  do  these  α(t,i)  weights  come  from?  Well,  they  are  generated  by  a  small\\nneural  network  called  an  alignment  model  (or  an  attention  layer),  which  is  trained\\njointly with the rest of the encoder–decoder model. This alignment model is illustra‐\\nted on the righthand side of Figure 16-7. It starts with a Dense layer composed of a\\nsingle neuron that processes each of the encoder’s outputs, along with the decoder’s\\nprevious  hidden  state  (e.g.,  h(2)).  This  layer  outputs  a  score  (or  energy)  for  each\\nencoder  output  (e.g.,  e(3,  2)):  this  score  measures  how  well  each  output  is  aligned\\nwith  the  decoder’s  previous  hidden  state.  For  example,  in  Figure  16-7,  the  model\\nhas  already  output  “me  gusta  el”  (meaning  “I  like”),  so  it’s  now  expecting  a  noun:\\nthe word “soccer” is the one that best aligns with the current state, so it gets a high\\nscore.  Finally,  all  the  scores  go  through  a  softmax  layer  to  get  a  final  weight  for\\neach  encoder  output  (e.g.,  α(3,2)).  All  the  weights  for  a  given  decoder  time  step  add\\nup  to  1.  This  particular  attention  mechanism  is  called  Bahdanau  attention  (named\\nafter  the  2014  paper’s  first  author).  Since  it  concatenates  the  encoder  output  with\\nthe decoder’s previous hidden state, it is sometimes called concatenative attention (or\\nadditive attention).\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,40.500,84.439,49.500 '606 \\n'>\n",
            "<LTTextBoxHorizontal(3) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(4) 102.907,40.500,299.359,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,332.269,432.375,607.500>\n",
            "<LTLine 72.000,332.394,432.500,332.394>\n",
            "<LTLine 72.125,332.269,72.125,607.500>\n",
            "<LTFigure(I1) 90.129,338.249,414.371,602.250 matrix=[324.24,0.00,0.00,264.00, (90.13,338.25)]>\n",
            "<LTTextBoxHorizontal(0) 136.785,550.025,396.003,605.705 'If the input sentence is n words long, and assuming the output sen‐\\ntence is about as long, then this model will need to compute about\\nn2 weights. Fortunately, this quadratic computational complexity is\\nstill tractable because even long sentences don’t have thousands of\\nwords.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,268.609,432.005,532.936 'Another  common  attention  mechanism,  known  as  Luong  attention  or  multiplicative\\nattention, was proposed shortly after, in 2015,19 by Minh-Thang Luong et al. Because\\nthe  goal  of  the  alignment  model  is  to  measure  the  similarity  between  one  of  the\\nencoder’s  outputs  and  the  decoder’s  previous  hidden  state,  the  authors  proposed  to\\nsimply compute the dot product (see Chapter 4) of these two vectors, as this is often a\\nfairly good similarity measure, and modern hardware can compute it very efficiently.\\nFor  this  to  be  possible,  both  vectors  must  have  the  same  dimensionality.  The  dot\\nproduct gives a score, and all the scores (at a given decoder time step) go through a\\nsoftmax layer to give the final weights, just like in Bahdanau attention. Another sim‐\\nplification Luong et al. proposed was to use the decoder’s hidden state at the current\\ntime step rather than at the previous time step (i.e., h(t) rather than h(t–1)), then to use\\nthe output of the attention mechanism (noted h t ) directly to compute the decoder’s\\npredictions, rather than using it to compute the decoder’s current hidden state. The\\nresearchers also proposed a variant of the dot product mechanism where the encoder\\noutputs  first  go  through  a  fully  connected  layer  (without  a  bias  term)  before  the\\ndot  products  are  computed.  This  is  called  the  “general”  dot  product  approach.  The\\nresearchers compared both dot product approaches with the concatenative attention\\nmechanism  (adding  a  rescaling  parameter  vector  v),  and  they  observed  that  the\\ndot product variants performed better than concatenative attention. For this reason,\\nconcatenative attention is much less used now. The equations for these three attention\\nmechanisms are summarized in Equation 16-1.\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.995,242.321,242.112,252.821 'Equation 16-1. Attention mechanisms\\n'>\n",
            "<LTTextBoxHorizontal(3) 87.002,215.502,120.912,233.676 'h t = ∑\\ni\\n'>\n",
            "<LTTextBoxHorizontal(4) 122.014,220.413,152.748,233.416 'α t, i y i\\n'>\n",
            "<LTTextBoxHorizontal(5) 86.998,193.098,138.066,206.101 '\\xa0with\\xa0α t, i =\\n'>\n",
            "<LTTextBoxHorizontal(6) 87.000,145.941,133.950,158.944 '\\xa0and\\xa0e t, i =\\n'>\n",
            "<LTTextBoxHorizontal(7) 140.200,145.130,186.220,213.662 'exp e t, i\\n∑\\nexp e t, i′\\ni′\\n⊺y i\\nh t\\n⊺ Wy i\\n'>\n",
            "<LTTextBoxHorizontal(8) 140.200,126.802,214.258,156.973 'h t\\nv⊺tanh W h t ; y i\\n'>\n",
            "<LTTextBoxHorizontal(9) 228.044,166.568,242.374,176.568 'dot\\n'>\n",
            "<LTTextBoxHorizontal(10) 228.050,146.973,261.740,156.973 'general\\n'>\n",
            "<LTTextBoxHorizontal(11) 228.048,128.646,257.908,138.646 'concat\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.780,78.954,431.200,86.954 '19 Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation”, Proceedings\\n'>\n",
            "<LTTextBoxHorizontal(13) 80.000,68.954,384.600,76.954 'of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412–1421.\\n'>\n",
            "<LTTextBoxHorizontal(14) 334.438,40.500,402.514,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(15) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 420.982,40.500,431.998,49.500 '607\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 89.000,557.999,126.944,607.500 matrix=[37.94,0.00,0.00,49.50, (89.00,558.00)]>\n",
            "<LTCurve 269.934,394.840,276.086,396.300>\n",
            "<LTCurve 276.646,381.872,278.486,389.872>\n",
            "<LTCurve 282.233,381.872,284.073,389.872>\n",
            "<LTCurve 87.000,234.107,92.859,235.497>\n",
            "<LTCurve 93.419,221.566,95.259,229.566>\n",
            "<LTCurve 99.007,221.566,100.847,229.566>\n",
            "<LTCurve 128.226,221.566,130.066,229.566>\n",
            "<LTCurve 139.639,221.566,141.479,229.566>\n",
            "<LTCurve 147.983,221.566,149.823,229.566>\n",
            "<LTCurve 153.219,221.566,155.059,229.566>\n",
            "<LTCurve 115.670,194.251,117.510,202.251>\n",
            "<LTCurve 127.083,194.251,128.923,202.251>\n",
            "<LTLine 141.402,200.521,193.007,200.521>\n",
            "<LTCurve 162.201,201.812,164.501,215.495>\n",
            "<LTCurve 169.837,201.812,171.677,209.812>\n",
            "<LTCurve 181.250,201.812,183.090,209.812>\n",
            "<LTCurve 184.108,201.812,186.408,215.495>\n",
            "<LTCurve 165.557,186.004,167.857,199.688>\n",
            "<LTCurve 173.194,186.004,175.034,194.004>\n",
            "<LTCurve 186.584,186.004,188.424,194.004>\n",
            "<LTCurve 189.442,186.004,191.742,199.688>\n",
            "<LTCurve 111.557,147.094,113.397,155.094>\n",
            "<LTCurve 122.970,147.094,124.810,155.094>\n",
            "<LTCurve 137.383,126.796,139.663,179.933>\n",
            "<LTCurve 146.623,164.718,148.463,172.718>\n",
            "<LTCurve 152.210,164.718,154.050,172.718>\n",
            "<LTCurve 165.995,164.718,167.835,172.718>\n",
            "<LTCurve 171.231,164.718,173.071,172.718>\n",
            "<LTCurve 146.623,145.123,148.463,153.123>\n",
            "<LTCurve 152.210,145.123,154.050,153.123>\n",
            "<LTCurve 177.044,145.123,178.884,153.123>\n",
            "<LTCurve 182.280,145.123,184.120,153.123>\n",
            "<LTCurve 168.936,126.796,171.236,140.479>\n",
            "<LTCurve 182.945,126.796,184.645,140.479>\n",
            "<LTCurve 191.615,126.796,193.455,134.796>\n",
            "<LTCurve 197.202,126.796,199.042,134.796>\n",
            "<LTCurve 209.493,126.796,211.333,134.796>\n",
            "<LTCurve 214.729,126.796,216.569,134.796>\n",
            "<LTCurve 217.687,126.796,219.397,140.479>\n",
            "<LTCurve 221.037,126.796,223.337,140.479>\n",
            "<LTTextBoxHorizontal(0) 71.999,542.857,432.002,606.503 'Keras  provides  a  tf.keras.layers.Attention  layer  for  Luong  attention,  and  an\\nAdditiveAttention  layer  for  Bahdanau  attention.  Let’s  add  Luong  attention  to  our\\nencoder–decoder  model.  Since  we  will  need  to  pass  all  the  encoder’s  outputs  to  the\\nAttention  layer,  we  first  need  to  set  return_sequences=True  when  creating  the\\nencoder:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,517.170,394.999,535.870 'encoder = tf.keras.layers.Bidirectional(\\n    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,434.857,432.003,508.357 'Next,  we  need  to  create  the  attention  layer  and  pass  it  the  decoder’s  states  and  the\\nencoder’s outputs. However, to access the decoder’s states at each step we would need\\nto write a custom memory cell. For simplicity, let’s use the decoder’s outputs instead\\nof  its  states:  in  practice  this  works  well  too,  and  it’s  much  easier  to  code.  Then  we\\njust pass the attention layer’s outputs directly to the output layer, as suggested in the\\nLuong attention paper:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,388.770,390.749,427.870 'attention_layer = tf.keras.layers.Attention()\\nattention_outputs = attention_layer([decoder_outputs, encoder_outputs])\\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\\nY_proba = output_layer(attention_outputs)\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,356.857,432.002,379.957 'And that’s it! If you train this model, you will find that it now handles much longer\\nsentences. For example:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,331.170,335.497,349.870 '>>> translate(\"I like soccer and also going to the beach\")\\n\\'me gusta el fútbol y también ir a la playa\\'\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,286.657,432.003,322.357 'In short, the attention layer provides a way to focus the attention of the model on part\\nof the inputs. But there’s another way to think of this layer: it acts as a differentiable\\nmemory retrieval mechanism.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,179.857,432.005,278.557 'For  example,  let’s  suppose  the  encoder  analyzed  the  input  sentence  “I  like  soccer”,\\nand it managed to understand that the word “I” is the subject and the word “like” is\\nthe verb, so it encoded this information in its outputs for these words. Now suppose\\nthe  decoder  has  already  translated  the  subject,  and  it  thinks  that  it  should  translate\\nthe  verb  next.  For  this,  it  needs  to  fetch  the  verb  from  the  input  sentence.  This\\nis  analogous  to  a  dictionary  lookup:  it’s  as  if  the  encoder  had  created  a  dictionary\\n{\"subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value\\nthat corresponds to the key “verb”.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,73.057,432.005,171.757 'However, the model does not have discrete tokens to represent the keys (like “subject”\\nor “verb”); instead, it has vectorized representations of these concepts that it learned\\nduring training, so the query it will use for the lookup will not perfectly match any\\nkey  in  the  dictionary.  The  solution  is  to  compute  a  similarity  measure  between  the\\nquery and each key in the dictionary, and then use the softmax function to convert\\nthese  similarity  scores  to  weights  that  add  up  to  1.  As  we  saw  earlier,  that’s  exactly\\nwhat  the  attention  layer  does.  If  the  key  that  represents  the  verb  is  by  far  the  most\\nsimilar to the query, then that key’s weight will be close to 1.\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.003,40.500,84.441,49.500 '608 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.909,40.500,299.361,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.003,605.537 'Next, the attention layer computes a weighted sum of the corresponding values: if the\\nweight of the “verb” key is close to 1, then the weighted sum will be very close to the\\nrepresentation of the word “played”.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,475.043,432.004,562.703 'This is why the Keras Attention and AdditiveAttention layers both expect a list as\\ninput, containing two or three items: the queries, the keys, and optionally the values. If\\nyou do not pass any values, then they are automatically equal to the keys. So, looking\\nat  the  previous  code  example  again,  the  decoder  outputs  are  the  queries,  and  the\\nencoder outputs are both the keys and the values. For each decoder output (i.e., each\\nquery),  the  attention  layer  returns  a  weighted  sum  of  the  encoder  outputs  (i.e.,  the\\nkeys/values) that are most similar to the decoder output.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,431.243,432.003,466.943 'The  bottom  line  is  that  an  attention  mechanism  is  a  trainable  memory  retrieval\\nsystem. It is so powerful that you can actually build state-of-the-art models using only\\nattention mechanisms. Enter the transformer architecture.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,285.034,432.004,419.673 'Attention Is All You Need: The Original Transformer Architecture\\nIn  a  groundbreaking  2017  paper,20  a  team  of  Google  researchers  suggested  that\\n“Attention  Is  All  You  Need”.  They  created  an  architecture  called  the  transformer,\\nwhich  significantly  improved  the  state-of-the-art  in  NMT  without  using  any  recur‐\\nrent  or  convolutional  layers,21  just  attention  mechanisms  (plus  embedding  layers,\\ndense  layers,  normalization  layers,  and  a  few  other  bits  and  pieces).  Because  the\\nmodel  is  not  recurrent,  it  doesn’t  suffer  as  much  from  the  vanishing  or  exploding\\ngradients problems as RNNs, it can be trained in fewer steps, it’s easier to parallelize\\nacross multiple GPUs, and it can better capture long-range patterns than RNNs. The\\noriginal 2017 transformer architecture is represented in Figure 16-8.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,228.634,432.004,276.934 'In  short,  the  left  part  of  Figure  16-8  is  the  encoder,  and  the  right  part  is  the\\ndecoder.  Each  embedding  layer  outputs  a  3D  tensor  of  shape  [batch  size,  sequence\\nlength, embedding size]. After that, the tensors are gradually transformed as they flow\\nthrough the transformer, but their shape remains the same.\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,101.954,423.384,109.954 '20 Ashish Vaswani et al., “Attention Is All You Need”, Proceedings of the 31st International Conference on Neural\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,91.954,240.608,99.954 'Information Processing Systems (2017): 6000–6010.\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.779,78.954,428.503,86.954 '21 Since the transformer uses time-distributed dense layers, you could argue that it uses 1D convolutional layers\\n'>\n",
            "<LTTextBoxHorizontal(8) 79.999,68.954,151.015,76.954 'with a kernel size of 1.\\n'>\n",
            "<LTTextBoxHorizontal(9) 334.436,40.500,402.512,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.980,40.500,431.996,49.500 '609\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTTextBoxHorizontal(0) 72.003,235.916,302.757,247.036 'Figure 16-8. The original 2017 transformer architecture22\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,147.723,432.003,221.816 'If you use the transformer for NMT, then during training you must feed the English\\nsentences to the encoder and the corresponding Spanish translations to the decoder,\\nwith an extra SOS token inserted at the start of each sentence. At inference time, you\\nmust  call  the  transformer  multiple  times,  producing  the  translations  one  word  at  a\\ntime and feeding the partial translations to the decoder at each round, just like we did\\nearlier in the translate() function.\\n'>\n",
            "<LTTextBoxHorizontal(2) 69.780,78.954,408.800,86.954 '22 This is figure 1 from the “Attention Is All You Need” paper, reproduced with the kind permission of the\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.000,68.954,106.128,76.954 'authors.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,40.500,84.435,49.500 '610 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.903,40.500,299.355,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,251.980,432.375,607.500>\n",
            "<LTLine 72.000,252.105,432.500,252.105>\n",
            "<LTLine 72.125,251.980,72.125,607.500>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 135.250,257.960,369.250,602.250 matrix=[234.00,0.00,0.00,344.29, (135.25,257.96)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,494.236,432.005,605.537 'The encoder’s role is to gradually transform the inputs—word representations of the\\nEnglish sentence—until each word’s representation perfectly captures the meaning of\\nthe  word,  in  the  context  of  the  sentence.  For  example,  if  you  feed  the  encoder  with\\nthe  sentence  “I  like  soccer”,  then  the  word  “like”  will  start  off  with  a  rather  vague\\nrepresentation,  since  this  word  could  mean  different  things  in  different  contexts:\\nthink  of  “I  like  soccer”  versus  “It’s  like  that”.  But  after  going  through  the  encoder,\\nthe  word’s  representation  should  capture  the  correct  meaning  of  “like”  in  the  given\\nsentence (i.e., to be fond of), as well as any other information that may be required\\nfor translation (e.g., it’s a verb).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,400.036,432.005,486.136 'The  decoder’s  role  is  to  gradually  transform  each  word  representation  in  the  trans‐\\nlated  sentence  into  a  word  representation  of  the  next  word  in  the  translation.  For\\nexample, if the sentence to translate is “I like soccer”, and the decoder’s input sentence\\nis  “<SOS>  me  gusta  el  fútbol”,  then  after  going  through  the  decoder,  the  word\\nrepresentation of the word “el” will end up transformed into a representation of the\\nword “fútbol”. Similarly, the representation of the word “fútbol” will be transformed\\ninto a representation of the EOS token.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,343.043,432.003,391.936 'After  going  through  the  decoder,  each  word  representation  goes  through  a  final\\nDense  layer  with  a  softmax  activation  function,  which  will  hopefully  output  a  high\\nprobability for the correct next word and a low probability for all other words. The\\npredicted sentence should be “me gusta el fútbol <EOS>”.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,324.443,386.934,334.943 'That was the big picture; now let’s walk through Figure 16-8 in more detail:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.652,276.643,432.004,312.343 '•\\n• First,  notice  that  both  the  encoder  and  the  decoder  contain  modules  that  are\\nstacked N times. In the paper, N = 6. The final outputs of the whole encoder stack\\nare fed to the decoder at each of these N levels.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.650,146.643,432.004,270.543 '•\\n• Zooming  in,  you  can  see  that  you  are  already  familiar  with  most  components:\\nthere are two embedding layers; several skip connections, each of them followed\\nby  a  layer  normalization  layer;  several  feedforward  modules  that  are  composed\\nof  two  dense  layers  each  (the  first  one  using  the  ReLU  activation  function,  the\\nsecond with no activation function); and finally the output layer is a dense layer\\nusing  the  softmax  activation  function.  You  can  also  sprinkle  a  bit  of  dropout\\nafter  the  attention  layers  and  the  feedforward  modules,  if  needed.  Since  all  of\\nthese layers are time-distributed, each word is treated independently from all the\\nothers. But how can we translate a sentence by looking at the words completely\\nseparately? Well, we can’t, so that’s where the new components come in:\\n'>\n",
            "<LTTextBoxHorizontal(6) 91.067,79.643,432.004,140.543 '— The encoder’s multi-head attention layer updates each word representation by\\n—\\nattending  to  (i.e.,  paying  attention  to)  all  other  words  in  the  same  sentence.\\nThat’s  where  the  vague  representation  of  the  word  “like”  becomes  a  richer\\nand more accurate representation, capturing its precise meaning in the given\\nsentence. We will discuss exactly how this works shortly.\\n'>\n",
            "<LTTextBoxHorizontal(7) 334.442,40.500,402.518,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.986,40.500,432.002,49.500 '611\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 91.069,544.636,432.003,605.537 '— The  decoder’s  masked  multi-head  attention  layer  does  the  same  thing,  but\\n—\\nwhen it processes a word, it doesn’t attend to words located after it: it’s a causal\\nlayer. For example, when it processes the word “gusta”, it only attends to the\\nwords  “<SOS>  me  gusta”,  and  it  ignores  the  words  “el  fútbol”  (or  else  that\\nwould be cheating).\\n'>\n",
            "<LTTextBoxHorizontal(1) 91.066,477.637,432.003,538.537 '— The  decoder’s  upper  multi-head  attention  layer  is  where  the  decoder  pays\\n—\\nattention  to  the  words  in  the  English  sentence.  This  is  called  cross-attention,\\nnot self-attention in this case. For example, the decoder will probably pay close\\nattention to the word “soccer” when it processes the word “el” and transforms\\nits representation into a representation of the word “fútbol”.\\n'>\n",
            "<LTTextBoxHorizontal(2) 91.066,360.236,432.003,471.537 '—\\n— The  positional  encodings  are  dense  vectors  (much  like  word  embeddings)\\nthat  represent  the  position  of  each  word  in  the  sentence.  The  nth  positional\\nencoding  is  added  to  the  word  embedding  of  the  nth  word  in  each  sentence.\\nThis is needed because all layers in the transformer architecture ignore word\\npositions:  without  positional  encodings,  you  could  shuffle  the  input  sequen‐\\nces, and it would just shuffle the output sequences in the same way. Obviously,\\nthe order of words matters, which is why we need to give positional informa‐\\ntion  to  the  transformer  somehow:  adding  positional  encodings  to  the  word\\nrepresentations is a good way to achieve this.\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.792,254.585,396.004,333.305 'The  first  two  arrows  going  into  each  multi-head  attention  layer\\nin  Figure  16-8  represent  the  keys  and  values,  and  the  third  arrow\\nrepresents  the  queries.  In  the  self-attention  layers,  all  three  are\\nequal  to  the  word  representations  output  by  the  previous  layer,\\nwhile in the decoder’s upper attention layer, the keys and values are\\nequal  to  the  encoder’s  final  word  representations,  and  the  queries\\nare equal to the word representations output by the previous layer.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,214.396,431.997,237.496 'Let’s go through the novel components of the transformer architecture in more detail,\\nstarting with the positional encodings.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.002,190.029,150.621,201.589 'Positional encodings\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,95.911,432.003,182.604 'A positional encoding is a dense vector that encodes the position of a word within a\\nsentence: the ith positional encoding is added to the word embedding of the ith word\\nin the sentence. The easiest way to implement this is to use an Embedding layer and\\nmake it encode all the positions from 0 to the maximum sequence length in the batch,\\nthen  add  the  result  to  the  word  embeddings.  The  rules  of  broadcasting  will  ensure\\nthat the positional encodings get applied to every input sequence. For example, here\\nis how to add positional encodings to the encoder and decoder inputs:\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,40.500,84.438,49.500 '612 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,285.599,126.944,335.100 matrix=[37.94,0.00,0.00,49.50, (89.00,285.60)]>\n",
            "<LTTextBoxHorizontal(0) 89.003,536.950,420.503,606.650 'max_length = 50  # max length in the whole training set\\nembed_size = 128\\npos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\\nbatch_max_len_enc = tf.shape(encoder_embeddings)[1]\\nencoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\\nbatch_max_len_dec = tf.shape(decoder_embeddings)[1]\\ndecoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,479.243,432.003,528.136 'Note that this implementation assumes that the embeddings are represented as reg‐\\nular  tensors,  not  ragged  tensors.23  The  encoder  and  the  decoder  share  the  same\\nEmbedding  layer  for  the  positional  encodings,  since  they  have  the  same  embedding\\nsize (this is often the case).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,410.243,432.005,471.143 'Instead of using trainable positional encodings, the authors of the transformer paper\\nchose  to  use  fixed  positional  encodings,  based  on  the  sine  and  cosine  functions  at\\ndifferent frequencies. The positional encoding matrix P is defined in Equation 16-2\\nand represented at the top of Figure 16-9 (transposed), where Pp,i is the ith component\\nof the encoding for the word located at the pth position in the sentence.\\n'>\n",
            "<LTTextBoxHorizontal(3) 87.000,383.956,280.463,394.456 'Equation 16-2. Sine/cosine positional encodings\\n'>\n",
            "<LTTextBoxHorizontal(4) 87.003,351.077,112.315,364.080 'Pp, i =\\n'>\n",
            "<LTTextBoxHorizontal(5) 118.565,362.948,167.675,374.398 'sin p/10000\\n'>\n",
            "<LTTextBoxHorizontal(6) 167.675,367.978,178.155,377.138 'i/d\\n'>\n",
            "<LTTextBoxHorizontal(7) 204.655,362.948,245.385,374.398 'if\\xa0i\\xa0is even\\n'>\n",
            "<LTTextBoxHorizontal(8) 118.565,342.312,168.855,353.762 'cos p/10000\\n'>\n",
            "<LTTextBoxHorizontal(9) 171.623,347.342,196.911,356.502 'i − 1 /d\\n'>\n",
            "<LTTextBoxHorizontal(10) 206.121,342.312,243.921,353.762 'if\\xa0i\\xa0is odd\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.998,108.741,416.115,131.841 'Figure 16-9. Sine/cosine positional encoding matrix (transposed, top) with a focus on\\ntwo values of i (bottom)\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.780,68.954,367.880,76.954 '23 It’s possible to use ragged tensors instead, if you are using the latest version of TensorFlow.\\n'>\n",
            "<LTTextBoxHorizontal(13) 334.438,40.500,402.514,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(14) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 420.982,40.500,431.998,49.500 '613\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,324.744,432.500,324.744>\n",
            "<LTLine 432.375,137.405,432.375,324.869>\n",
            "<LTLine 72.000,137.530,432.500,137.530>\n",
            "<LTLine 72.125,137.405,72.125,324.869>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTCurve 115.741,339.869,118.021,377.132>\n",
            "<LTCurve 131.081,360.876,133.381,376.761>\n",
            "<LTCurve 178.890,360.876,181.190,376.761>\n",
            "<LTCurve 132.261,339.869,134.561,356.496>\n",
            "<LTCurve 169.413,348.496,171.253,356.496>\n",
            "<LTCurve 186.684,348.496,188.524,356.496>\n",
            "<LTCurve 197.642,339.869,199.942,356.496>\n",
            "<LTFigure(I1) 90.250,143.385,414.250,319.619 matrix=[324.00,0.00,0.00,176.23, (90.25,143.38)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,406.036,432.005,605.537 'This solution can give the same performance as trainable positional encodings, and it\\ncan extend to arbitrarily long sentences without adding any parameters to the model\\n(however,  when  there  is  a  large  amount  of  pretraining  data,  trainable  positional\\nencodings  are  usually  favored).  After  these  positional  encodings  are  added  to  the\\nword  embeddings,  the  rest  of  the  model  has  access  to  the  absolute  position  of  each\\nword in the sentence because there is a unique positional encoding for each position\\n(e.g., the positional encoding for the word located at the 22nd position in a sentence\\nis represented by the vertical dashed line at the top left of Figure 16-9, and you can\\nsee  that  it  is  unique  to  that  position).  Moreover,  the  choice  of  oscillating  functions\\n(sine and cosine) makes it possible for the model to learn relative positions as well.\\nFor  example,  words  located  38  words  apart  (e.g.,  at  positions  p  =  22  and  p  =  60)\\nalways have the same positional encoding values in the encoding dimensions i = 100\\nand i = 101, as you can see in Figure 16-9. This explains why we need both the sine\\nand the cosine for each frequency: if we only used the sine (the blue wave at i = 100),\\nthe model would not be able to distinguish positions p = 22 and p = 35 (marked by a\\ncross).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,335.257,432.005,398.902 'There  is  no  PositionalEncoding  layer  in  TensorFlow,  but  it  is  not  too  hard  to\\ncreate  one.  For  efficiency  reasons,  we  precompute  the  positional  encoding  matrix\\nin  the  constructor.  The  call()  method  just  truncates  this  encoding  matrix  to  the\\nmax  length  of  the  input  sequences,  and  it  adds  them  to  the  inputs.  We  also  set\\nsupports_masking=True to propagate the input’s automatic mask to the next layer:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,217.770,407.751,328.270 'class PositionalEncoding(tf.keras.layers.Layer):\\n    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\\n        super().__init__(dtype=dtype, **kwargs)\\n        assert embed_size % 2 == 0, \"embed_size must be even\"\\n        p, i = np.meshgrid(np.arange(max_length),\\n                           2 * np.arange(embed_size // 2))\\n        pos_emb = np.empty((1, max_length, embed_size))\\n        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\\n        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\\n        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\\n        self.supports_masking = True\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,176.970,361.001,205.870 '    def call(self, inputs):\\n        batch_max_length = tf.shape(inputs)[1]\\n        return inputs + self.pos_encodings[:, :batch_max_length]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,157.657,376.720,168.157 'Let’s use this layer to add the positional encoding to the encoder’s inputs:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.003,121.770,344.003,150.670 'pos_embed_layer = PositionalEncoding(max_length, embed_size)\\nencoder_in = pos_embed_layer(encoder_embeddings)\\ndecoder_in = pos_embed_layer(decoder_embeddings)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,89.857,431.999,112.957 'Now  let’s  look  deeper  into  the  heart  of  the  transformer  model,  at  the  multi-head\\nattention layer.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,40.500,84.434,49.500 '614 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.902,40.500,299.354,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.229,152.400,607.789 'Multi-head attention\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,540.504,432.001,588.805 'To  understand  how  a  multi-head  attention  layer  works,  we  must  first  understand\\nthe  scaled  dot-product  attention  layer,  which  it  is  based  on.  Its  equation  is  shown  in\\nEquation  16-3,  in  a  vectorized  form.  It’s  the  same  as  Luong  attention,  except  for  a\\nscaling factor.\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.997,514.217,266.809,524.717 'Equation 16-3. Scaled dot-product attention\\n'>\n",
            "<LTTextBoxHorizontal(3) 86.999,487.459,211.289,498.909 'Attention Q, K, V = softmax\\n'>\n",
            "<LTTextBoxHorizontal(4) 223.169,479.272,246.187,507.351 'QK⊺\\ndkeys\\n'>\n",
            "<LTTextBoxHorizontal(5) 250.767,488.909,258.137,498.909 'V\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.998,451.801,140.888,462.301 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.653,404.001,432.002,439.701 '• Q  is  a  matrix  containing  one  row  per  query.  Its  shape  is  [nqueries,  dkeys],  where\\n•\\nnqueries  is  the  number  of  queries  and  dkeys  is  the  number  of  dimensions  of  each\\nquery and each key.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.654,386.813,432.002,397.901 '•\\n• K is a matrix containing one row per key. Its shape is [nkeys, dkeys], where nkeys is\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.996,374.801,219.713,385.301 'the number of keys and values.\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.654,357.613,432.002,368.701 '•\\n• V is a matrix containing one row per value. Its shape is [nkeys, dvalues], where dvalues\\n'>\n",
            "<LTTextBoxHorizontal(11) 89.996,345.601,269.420,356.101 'is the number of dimensions of each value.\\n'>\n",
            "<LTTextBoxHorizontal(12) 80.650,247.477,432.001,341.227 '• The  shape  of  Q  K⊺  is  [nqueries,  nkeys]:  it  contains  one  similarity  score  for  each\\n•\\nquery/key pair. To prevent this matrix from being huge, the input sequences must\\nnot  be  too  long  (we  will  discuss  how  to  overcome  this  limitation  later  in  this\\nchapter).  The  output  of  the  softmax  function  has  the  same  shape,  but  all  rows\\nsum up to 1. The final output has a shape of [nqueries, dvalues]: there is one row per\\nquery, where each row represents the query result (a weighted sum of the values).\\n• The scaling factor 1 / ( dkeys) scales down the similarity scores to avoid saturat‐\\n•\\n'>\n",
            "<LTTextBoxHorizontal(13) 89.995,235.709,345.639,246.209 'ing the softmax function, which would lead to tiny gradients.\\n'>\n",
            "<LTTextBoxHorizontal(14) 80.653,193.909,432.000,229.609 '• It  is  possible  to  mask  out  some  key/value  pairs  by  adding  a  very  large  negative\\n•\\nvalue to the corresponding similarity scores, just before computing the softmax.\\nThis is useful in the masked multi-head attention layer.\\n'>\n",
            "<LTTextBoxHorizontal(15) 71.995,74.237,432.004,182.775 'If you set use_scale=True when creating a tf.keras.layers.Attention layer, then\\nit will create an additional parameter that lets the layer learn how to properly down‐\\nscale the similarity scores. The scaled dot-product attention used in the transformer\\nmodel  is  almost  the  same,  except  it  always  scales  the  similarity  scores  by  the  same\\nfactor, 1 / ( dkeys).\\nNote that the Attention layer’s inputs are just like Q, K, and V, except with an extra\\nbatch dimension (the first dimension). Internally, the layer computes all the attention\\nscores for all sentences in the batch with just one call to tf.matmul(queries, keys):\\n'>\n",
            "<LTTextBoxHorizontal(16) 334.439,40.500,402.515,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(17) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 420.983,40.500,431.999,49.500 '615\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 129.508,488.901,131.808,498.901>\n",
            "<LTCurve 162.061,488.901,164.361,498.901>\n",
            "<LTCurve 214.792,479.265,217.092,507.393>\n",
            "<LTLine 218.108,493.329,246.753,493.329>\n",
            "<LTCurve 218.664,479.265,246.198,492.496>\n",
            "<LTCurve 247.759,479.265,250.059,507.393>\n",
            "<LTCurve 187.459,248.173,213.414,261.865>\n",
            "<LTCurve 117.644,119.087,143.598,132.779>\n",
            "<LTTextBoxHorizontal(0) 71.996,517.063,432.002,606.503 'this  makes  it  extremely  efficient.  Indeed,  in  TensorFlow,  if  A  and  B  are  tensors  with\\nmore than two dimensions—say, of shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively—\\nthen tf.matmul(A, B) will treat these tensors as 2 × 3 arrays where each cell contains\\na matrix, and it will multiply the corresponding matrices: the matrix at the ith row and\\njth column in A will be multiplied by the matrix at the ith row and jth column in B. Since\\nthe product of a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6 matrix, tf.matmul(A, B)\\nwill return an array of shape [2, 3, 4, 6].\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,485.863,431.997,508.963 'Now we’re ready to look at the multi-head attention layer. Its architecture is shown in\\nFigure 16-10.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.004,211.573,293.382,222.693 'Figure 16-10. Multi-head attention layer architecture24\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,149.173,432.004,197.473 'As you can see, it is just a bunch of scaled dot-product attention layers, each preceded\\nby  a  linear  transformation  of  the  values,  keys,  and  queries  (i.e.,  a  time-distributed\\ndense layer with no activation function). All the outputs are simply concatenated, and\\nthey go through a final linear transformation (again, time-distributed).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,117.973,432.001,141.073 'But  why?  What  is  the  intuition  behind  this  architecture?  Well,  consider  once  again\\nthe  word  “like”  in  the  sentence  “I  like  soccer”.  The  encoder  was  smart  enough  to\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,78.954,422.216,86.954 '24 This is the righthand part of figure 2 from “Attention Is All You Need”, reproduced with the kind authoriza‐\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,141.176,76.954 'tion of the authors.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.998,40.500,84.436,49.500 '616 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.904,40.500,299.356,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,479.602,432.500,479.602>\n",
            "<LTLine 432.375,227.637,432.375,479.727>\n",
            "<LTLine 72.000,227.762,432.500,227.762>\n",
            "<LTLine 72.125,227.637,72.125,479.727>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 162.250,233.617,342.250,474.477 matrix=[180.00,0.00,0.00,240.86, (162.25,233.62)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,532.036,432.004,605.537 'encode the fact that it is a verb. But the word representation also includes its position\\nin the text, thanks to the positional encodings, and it probably includes many other\\nfeatures  that  are  useful  for  its  translation,  such  as  the  fact  that  it  is  in  the  present\\ntense. In short, the word representation encodes many different characteristics of the\\nword.  If  we  just  used  a  single  scaled  dot-product  attention  layer,  we  would  only  be\\nable to query all of these characteristics in one shot.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,425.236,432.005,523.936 'This  is  why  the  multi-head  attention  layer  applies  multiple  different  linear  transfor‐\\nmations of the values, keys, and queries: this allows the model to apply many different\\nprojections  of  the  word  representation  into  different  subspaces,  each  focusing  on  a\\nsubset  of  the  word’s  characteristics.  Perhaps  one  of  the  linear  layers  will  project  the\\nword representation into a subspace where all that remains is the information that the\\nword is a verb, another linear layer will extract just the fact that it is present tense, and\\nso on. Then the scaled dot-product attention layers implement the lookup phase, and\\nfinally we concatenate all the results and project them back to the original space.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,354.457,432.005,418.102 'Keras  includes  a  tf.keras.layers.MultiHeadAttention  layer,  so  we  now  have\\neverything  we  need  to  build  the  rest  of  the  transformer.  Let’s  start  with  the  full\\nencoder, which is exactly like in Figure 16-8, except we use a stack of two blocks (N =\\n2) instead of six, since we don’t have a huge training set, and we add a bit of dropout\\nas well:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,175.770,420.501,347.470 'N = 2  # instead of 6\\nnum_heads = 8\\ndropout_rate = 0.1\\nn_units = 128  # for the first dense layer in each feedforward block\\nencoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\\nZ = encoder_in\\nfor _ in range(N):\\n    skip = Z\\n    attn_layer = tf.keras.layers.MultiHeadAttention(\\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\\n    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\\n    skip = Z\\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\\n    Z = tf.keras.layers.Dense(embed_size)(Z)\\n    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,130.663,431.998,166.956 'This code should be mostly straightforward, except for one thing: masking. As of the\\ntime of writing, the MultiHeadAttention layer does not support automatic masking,25\\nso we must handle it manually. How can we do that?\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,68.954,423.120,98.338 '25 This will most likely change by the time you read this; check out Keras issue #16248 for more details. When\\nthis happens, there will be no need to set the attention_mask argument, and therefore no need to create\\nencoder_pad_mask.\\n'>\n",
            "<LTTextBoxHorizontal(6) 334.440,40.500,402.516,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.984,40.500,432.000,49.500 '617\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,106.284,162.000,106.284>\n",
            "<LTTextBoxHorizontal(0) 71.996,465.477,432.005,606.503 'The  MultiHeadAttention  layer  accepts  an  attention_mask  argument,  which  is  a\\nBoolean  tensor  of  shape  [batch  size,  max  query  length,  max  value  length]:  for  every\\ntoken in every query sequence, this mask indicates which tokens in the correspond‐\\ning  value  sequence  should  be  attended  to.  We  want  to  tell  the  MultiHeadAttention\\nlayer to ignore all the padding tokens in the values. So, we first compute the padding\\nmask  using  tf.math.not_equal(encoder_input_ids,  0).  This  returns  a  Boolean\\ntensor  of  shape  [batch  size,  max  sequence  length].  We  then  insert  a  second  axis\\nusing [:, tf.newaxis], to get a mask of shape [batch size, 1, max sequence length].\\nThis  allows  us  to  use  this  mask  as  the  attention_mask  when  calling  the  MultiHead\\nAttention layer: thanks to broadcasting, the same mask will be used for all tokens in\\neach query. This way, the padding tokens in the values will be ignored correctly.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,342.517,432.005,457.377 'However,  the  layer  will  compute  outputs  for  every  single  query  token,  including\\nthe  padding  tokens.  We  need  to  mask  the  outputs  that  correspond  to  these  pad‐\\nding  tokens.  Recall  that  we  used  mask_zero  in  the  Embedding  layers,  and  we  set\\nsupports_masking to True in the PositionalEncoding layer, so the automatic mask\\nwas propagated all the way to the  MultiHeadAttention layer’s inputs (encoder_in).\\nWe  can  use  this  to  our  advantage  in  the  skip  connection:  indeed,  the  Add  layer\\nsupports  automatic  masking,  so  when  we  add  Z  and  skip  (which  is  initially  equal\\nto  encoder_in),  the  outputs  get  automatically  masked  correctly.26  Yikes!  Masking\\nrequired much more explanation than code.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,273.517,432.003,334.417 'Now on to the decoder! Once again, masking is going to be the only tricky part, so\\nlet’s  start  with  that.  The  first  multi-head  attention  layer  is  a  self-attention  layer,  like\\nin the encoder, but it is a masked multi-head attention layer, meaning it is causal: it\\nshould ignore all tokens in the future. So, we need two masks: a padding mask and a\\ncausal mask. Let’s create them:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,237.631,399.251,266.531 'decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\\ncausal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\\n    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,128.931,432.005,228.817 'The padding mask is exactly like the one we created for the encoder, except it’s based\\non  the  decoder’s  inputs  rather  than  the  encoder’s.  The  causal  mask  is  created  using\\nthe tf.linalg.band_part() function, which takes a tensor and returns a copy with\\nall  the  values  outside  a  diagonal  band  set  to  zero.  With  these  arguments,  we  get  a\\nsquare matrix of size batch_max_len_dec (the max length of the input sequences in\\nthe batch), with 1s in the lower-left triangle and 0s in the upper right. If we use this\\nmask as the attention mask, we will get exactly what we want: the first query token\\nwill only attend to the first value token, the second will only attend to the first two,\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,79.646,394.656,88.834 '26 Currently Z + skip does not support automatic masking, which is why we had to write tf.keras.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.001,68.954,353.537,78.142 'layers.Add()([Z, skip]) instead. Again, this may change by the time you read this.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,40.500,84.438,49.500 '618 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,96.284,162.000,96.284>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'the third will only attend to the first three, and so on. In other words, query tokens\\ncannot attend to any value token in the future.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,563.837,189.597,574.337 'Let’s now build the decoder:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,385.150,420.500,556.850 'encoder_outputs = Z  # let\\'s save the encoder\\'s final outputs\\nZ = decoder_in  # the decoder starts with its own inputs\\nfor _ in range(N):\\n    skip = Z\\n    attn_layer = tf.keras.layers.MultiHeadAttention(\\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\\n    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\\n    skip = Z\\n    attn_layer = tf.keras.layers.MultiHeadAttention(\\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\\n    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\\n    skip = Z\\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\\n    Z = tf.keras.layers.Dense(embed_size)(Z)\\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,327.443,432.005,377.303 'For the first attention layer, we use causal_mask & decoder_pad_mask to mask both\\nthe padding tokens and future tokens. The causal mask only has two dimensions: it’s\\nmissing  the  batch  dimension,  but  that’s  okay  since  broadcasting  ensures  that  it  gets\\ncopied across all the instances in the batch.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,283.050,432.003,319.343 'For the second attention layer, there’s nothing special. The only thing to note is that\\nwe are using encoder_pad_mask, not decoder_pad_mask, because this attention layer\\nuses the encoder’s final outputs as its values.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,251.850,432.000,274.950 'We’re  almost  done.  We  just  need  to  add  the  final  output  layer,  create  the  model,\\ncompile it, and train it:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.002,175.163,395.002,244.863 'Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\\n                       outputs=[Y_proba])\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\\n              metrics=[\"accuracy\"])\\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\\n          validation_data=((X_valid, X_valid_dec), Y_valid))\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,143.250,432.000,166.350 'Congratulations!  You’ve  built  a  full  transformer  from  scratch,  and  trained  it  for\\nautomatic translation. This is getting quite advanced!\\n'>\n",
            "<LTTextBoxHorizontal(8) 136.788,93.678,395.998,126.318 'The Keras team has created a new Keras NLP project, including an\\nAPI to build a transformer more easily. You may also be interested\\nin the new Keras CV project for computer vision.\\n'>\n",
            "<LTTextBoxHorizontal(9) 334.442,40.500,402.518,49.500 'Attention Mechanisms \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.986,40.500,432.002,49.500 '619\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,76.336,126.760,132.113 matrix=[41.76,0.00,0.00,55.78, (85.00,76.34)]>\n",
            "<LTTextBoxHorizontal(0) 72.004,595.037,394.049,605.537 'But the field didn’t stop there. Let’s now explore some of the recent advances.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.993,519.836,432.002,582.697 'An Avalanche of Transformer Models\\nThe year 2018 has been called the “ImageNet moment for NLP”. Since then, progress\\nhas been astounding, with larger and larger transformer-based architectures trained\\non immense datasets.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,337.436,432.005,512.356 'First,  the  GPT  paper27  by  Alec  Radford  and  other  OpenAI  researchers  once  again\\ndemonstrated  the  effectiveness  of  unsupervised  pretraining,  like  the  ELMo  and\\nULMFiT  papers  before  it,  but  this  time  using  a  transformer-like  architecture.  The\\nauthors  pretrained  a  large  but  fairly  simple  architecture  composed  of  a  stack  of\\n12  transformer  modules  using  only  masked  multi-head  attention  layers,  like  in  the\\noriginal  transformer’s  decoder.  They  trained  it  on  a  very  large  dataset,  using  the\\nsame autoregressive technique we used for our Shakespearean char-RNN: just predict\\nthe  next  token.  This  is  a  form  of  self-supervised  learning.  Then  they  fine-tuned  it\\non  various  language  tasks,  using  only  minor  adaptations  for  each  task.  The  tasks\\nwere quite diverse: they included text classification, entailment (whether sentence A\\nimposes,  involves,  or  implies  sentence  B  as  a  necessary  consequence),28  similarity\\n(e.g., “Nice weather today” is very similar to “It is sunny”), and question answering\\n(given  a  few  paragraphs  of  text  giving  some  context,  the  model  must  answer  some\\nmultiple-choice questions).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,255.836,432.004,329.956 'Then  Google’s  BERT  paper29  came  out:  it  also  demonstrated  the  effectiveness  of\\nself-supervised  pretraining  on  a  large  corpus,  using  a  similar  architecture  to  GPT\\nbut with nonmasked multi-head attention layers only, like in the original transform‐\\ner’s  encoder.  This  means  that  the  model  is  naturally  bidirectional;  hence  the  B  in\\nBERT  (Bidirectional  Encoder  Representations  from  Transformers).  Most  importantly,\\nthe authors proposed two pretraining tasks that explain most of the model’s strength:\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,236.236,203.633,246.736 'Masked language model (MLM)\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.995,160.636,432.004,234.136 'Each word in a sentence has a 15% probability of being masked, and the model\\nis  trained  to  predict  the  masked  words.  For  example,  if  the  original  sentence  is\\n“She  had  fun  at  the  birthday  party”,  then  the  model  may  be  given  the  sentence\\n“She <mask> fun at the <mask> party” and it must predict the words “had” and\\n“birthday” (the other outputs will be ignored). To be more precise, each selected\\nword has an 80% chance of being masked, a 10% chance of being replaced by a\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,124.954,380.312,132.954 '27 Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018).\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.777,111.954,430.013,119.954 '28 For example, the sentence “Jane had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”,\\n'>\n",
            "<LTTextBoxHorizontal(8) 79.997,101.954,371.917,109.954 'but it is contradicted by “Everyone hated the party” and it is unrelated to “The Earth is flat”.\\n'>\n",
            "<LTTextBoxHorizontal(9) 69.782,68.954,421.018,96.954 '29 Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”,\\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies 1 (2019).\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.004,40.500,84.442,49.500 '620 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.910,40.500,299.362,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,140.900,162.000,140.900>\n",
            "<LTTextBoxHorizontal(0) 89.999,569.837,432.005,605.537 'random  word  (to  reduce  the  discrepancy  between  pretraining  and  fine-tuning,\\nsince  the  model  will  not  see  <mask>  tokens  during  fine-tuning),  and  a  10%\\nchance of being left alone (to bias the model toward the correct answer).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,550.236,199.109,560.736 'Next sentence prediction (NSP)\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.996,487.236,432.003,548.136 'The  model  is  trained  to  predict  whether  two  sentences  are  consecutive  or  not.\\nFor  example,  it  should  predict  that  “The  dog  sleeps”  and  “It  snores  loudly”  are\\nconsecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are\\nnot  consecutive.  Later  research  showed  that  NSP  was  not  as  important  as  was\\ninitially thought, so it was dropped in most later architectures.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,342.636,432.005,479.136 'The  model  is  trained  on  these  two  tasks  simultaneously  (see  Figure  16-11).  For  the\\nNSP task, the authors inserted a class token (<CLS>) at the start of every input, and\\nthe corresponding output token represents the model’s prediction: sentence B follows\\nsentence A, or it does not. The two input sentences are concatenated, separated only\\nby a special separation token (<SEP>), and they are fed as input to the model. To help\\nthe  model  know  which  sentence  each  input  token  belongs  to,  a  segment  embedding\\nis  added  on  top  of  each  token’s  positional  embeddings:  there  are  just  two  possible\\nsegment embeddings, one for sentence A and one for sentence B. For the MLM task,\\nsome  input  words  are  masked  (as  we  just  saw)  and  the  model  tries  to  predict  what\\nthose words were. The loss is only computed on the NSP prediction and the masked\\ntokens, not on the unmasked ones.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,171.443,290.400,182.563 'Figure 16-11. BERT training and fine-tuning process30\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,121.643,432.004,157.343 'After  this  unsupervised  pretraining  phase  on  a  very  large  corpus  of  text,  the  model\\nis  then  fine-tuned  on  many  different  tasks,  changing  very  little  for  each  task.  For\\nexample,  for  text  classification  such  as  sentiment  analysis,  all  output  tokens  are\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,68.954,355.224,76.954 '30 This is figure 1 from the paper, reproduced with the kind authorization of the authors.\\n'>\n",
            "<LTTextBoxHorizontal(7) 293.098,40.500,402.520,49.500 'An Avalanche of Transformer Models \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.988,40.500,432.004,49.500 '621\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,336.375,432.500,336.375>\n",
            "<LTLine 432.375,187.507,432.375,336.500>\n",
            "<LTLine 72.000,187.632,432.500,187.632>\n",
            "<LTLine 72.125,187.507,72.125,336.500>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 79.450,193.487,425.050,331.250 matrix=[345.60,0.00,0.00,137.76, (79.45,193.49)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,582.437,431.999,605.537 'ignored except for the first one, corresponding to the class token, and a new output\\nlayer replaces the previous one, which was just a binary classification layer for NSP.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,463.036,432.005,574.337 'In February 2019, just a few months after BERT was published, Alec Radford, Jeffrey\\nWu,  and  other  OpenAI  researchers  published  the  GPT-2  paper,31  which  proposed  a\\nvery  similar  architecture  to  GPT,  but  larger  still  (with  over  1.5  billion  parameters!).\\nThe  researchers  showed  that  the  new  and  improved  GPT  model  could  perform\\nzero-shot learning (ZSL), meaning it could achieve good performance on many tasks\\nwithout  any  fine-tuning.  This  was  just  the  start  of  a  race  toward  larger  and  larger\\nmodels: Google’s Switch Transformers32 (introduced in January 2021) used 1 trillion\\nparameters, and soon much larger models came out, such as the Wu Dao 2.0 model\\nby the Beijing Academy of Artificial Intelligence (BAII), announced in June 2021.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,368.836,432.003,454.936 'An unfortunate consequence of this trend toward gigantic models is that only well-\\nfunded  organizations  can  afford  to  train  such  models:  it  can  easily  cost  hundreds\\nof  thousands  of  dollars  or  more.  And  the  energy  required  to  train  a  single  model\\ncorresponds  to  an  American  household’s  electricity  consumption  for  several  years;\\nit’s not eco-friendly at all. Many of these models are just too big to even be used on\\nregular hardware: they wouldn’t fit in RAM, and they would be horribly slow. Lastly,\\nsome are so costly that they are not released publicly.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,299.836,432.005,360.736 'Luckily,  ingenious  researchers  are  finding  new  ways  to  downsize  transformers  and\\nmake them more data-efficient. For example, the DistilBERT model,33 introduced in\\nOctober 2019 by Victor Sanh et al. from Hugging Face, is a small and fast transformer\\nmodel  based  on  BERT.  It  is  available  on  Hugging  Face’s  excellent  model  hub,  along\\nwith thousands of others—you’ll see an example later in this chapter.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,218.236,432.005,291.736 'DistilBERT  was  trained  using  distillation  (hence  the  name):  this  means  transferring\\nknowledge  from  a  teacher  model  to  a  student  one,  which  is  usually  much  smaller\\nthan the teacher model. This is typically done by using the teacher’s predicted proba‐\\nbilities  for  each  training  instance  as  targets  for  the  student.  Surprisingly,  distillation\\noften works better than training the student from scratch on the same dataset as the\\nteacher! Indeed, the student benefits from the teacher’s more nuanced labels.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,161.836,432.004,210.136 'Many  more  transformer  architectures  came  out  after  BERT,  almost  on  a  monthly\\nbasis, often improving on the state of the art across all NLP tasks: XLNet (June 2019),\\nRoBERTa  (July  2019),  StructBERT  (August  2019),  ALBERT  (September  2019),  T5\\n(October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020),\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,114.954,351.856,122.954 '31 Alec Radford et al., “Language Models Are Unsupervised Multitask Learners” (2019).\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.783,101.954,420.427,109.954 '32 William Fedus et al., “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.003,91.954,133.403,99.954 'Sparsity” (2021).\\n'>\n",
            "<LTTextBoxHorizontal(9) 69.779,78.954,408.343,86.954 '33 Victor Sanh et al., “DistilBERT, A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(10) 79.999,68.954,188.895,76.954 'preprint arXiv:1910.01108 (2019).\\n'>\n",
            "<LTTextBoxHorizontal(11) 72.000,40.500,84.438,49.500 '622 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,130.900,162.000,130.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,469.037,432.004,605.537 'Switch  Transformers  (January  2021),  Wu  Dao  2.0  (June  2021),  Gopher  (December\\n2021), GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022),\\nand the list goes on and on. Each of these models brought new ideas and techniques,34\\nbut  I  particularly  like  the  T5  paper35  by  Google  researchers:  it  frames  all  NLP  tasks\\nas  text-to-text,  using  an  encoder–decoder  transformer.  For  example,  to  translate  “I\\nlike soccer” to Spanish, you can just call the model with the input sentence “translate\\nEnglish to Spanish: I like soccer” and it outputs “me gusta el fútbol”. To summarize\\na paragraph, you just enter “summarize:” followed by the paragraph, and it outputs\\nthe summary. For classification, you only need to change the prefix to “classify:” and\\nthe model outputs the class name, as text. This simplifies using the model, and it also\\nmakes it possible to pretrain it on even more tasks.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,311.836,432.004,460.936 'Last but not least, in April 2022, Google researchers used a new large-scale training\\nplatform  named  Pathways  (which  we  will  briefly  discuss  in  Chapter  19)  to  train  a\\nhumongous  language  model  named  the  Pathways  Language  Model  (PaLM),36  with  a\\nwhopping  540  billion  parameters,  using  over  6,000  TPUs.  Other  than  its  incredible\\nsize,  this  model  is  a  standard  transformer,  using  decoders  only  (i.e.,  with  masked\\nmulti-head  attention  layers),  with  just  a  few  tweaks  (see  the  paper  for  details).\\nThis  model  achieved  incredible  performance  on  all  sorts  of  NLP  tasks,  particularly\\nin  natural  language  understanding  (NLU).  It’s  capable  of  impressive  feats,  such  as\\nexplaining jokes, giving detailed step-by-step answers to questions, and even coding.\\nThis is in part due to the model’s size, but also thanks to a technique called Chain of\\nthought prompting,37 which was introduced a couple months earlier by another team\\nof Google researchers.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,217.636,432.004,303.736 'In question answering tasks, regular prompting typically includes a few examples of\\nquestions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more cans of\\ntennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A:\\n11.” The prompt then continues with the actual question, such as “Q: John takes care\\nof 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How\\nmany hours a week does he spend taking care of dogs? A:”, and the model’s job is to\\nappend the answer: in this case, “35.”\\n'>\n",
            "<LTTextBoxHorizontal(3) 69.780,137.954,350.824,145.954 '34 Mariya Yao summarized many of these models in this post: https://homl.info/yaopost.\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,124.954,430.240,132.954 '35 Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,114.954,188.896,122.954 'preprint arXiv:1910.10683 (2019).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.783,101.954,389.035,109.954 '36 Aakanksha Chowdhery et al., “PaLM: Scaling Language Modeling with Pathways”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.003,91.954,160.555,99.954 'arXiv:2204.02311 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.778,78.954,423.302,86.954 '37 Jason Wei et al., “Chain of Thought Prompting Elicits Reasoning in Large Language Models”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(9) 79.998,68.954,160.550,76.954 'arXiv:2201.11903 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(10) 293.095,40.500,402.517,49.500 'An Avalanche of Transformer Models \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.985,40.500,432.001,49.500 '623\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,153.900,162.000,153.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,506.836,432.004,605.537 'But with chain of thought prompting, the example answers include all the reasoning\\nsteps that lead to the conclusion. For example, instead of “A: 11”, the prompt contains\\n“A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 =\\n11.” This encourages the model to give a detailed answer to the actual question, such\\nas “John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care\\nof their business. So that is 10 × .5 = 5 hours a day. 5 hours a day × 7 days a week =\\n35 hours a week. The answer is 35 hours a week.” This is an actual example from the\\npaper!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,450.436,432.004,498.736 'Not  only  does  the  model  give  the  right  answer  much  more  frequently  than  using\\nregular  prompting—we’re  encouraging  the  model  to  think  things  through—but  it\\nalso  provides  all  the  reasoning  steps,  which  can  be  useful  to  better  understand  the\\nrationale behind a model’s answer.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,419.236,431.998,442.336 'Transformers have taken over NLP, but they didn’t stop there: they soon expanded to\\ncomputer vision as well.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,331.436,432.005,406.897 'Vision Transformers\\nOne of the first applications of attention mechanisms beyond NMT was in generating\\nimage captions using visual attention:38 a convolutional neural network first processes\\nthe  image  and  outputs  some  feature  maps,  then  a  decoder  RNN  equipped  with  an\\nattention mechanism generates the caption, one word at a time. \\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,262.436,432.002,323.336 'At each decoder time step (i.e., each word), the decoder uses the attention model to\\nfocus  on  just  the  right  part  of  the  image.  For  example,  in  Figure  16-12,  the  model\\ngenerated the caption “A woman is throwing a frisbee in a park”, and you can see what\\npart  of  the  input  image  the  decoder  focused  its  attention  on  when  it  was  about  to\\noutput the word “frisbee”: clearly, most of its attention was focused on the frisbee.\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,78.954,430.576,86.954 '38 Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, Proceedings\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,325.960,76.954 'of the 32nd International Conference on Machine Learning (2015): 2048–2057.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.002,40.500,84.440,49.500 '624 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,402.134,398.672,425.234 'Figure 16-12. Visual attention: an input image (left) and the model’s focus before\\nproducing the word “frisbee” (right)39\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.248,216.627,423.748,380.122 'Explainability\\nOne extra benefit of attention mechanisms is that they make it easier to understand\\nwhat  led  the  model  to  produce  its  output.  This  is  called  explainability.  It  can  be\\nespecially useful when the model makes a mistake: for example, if an image of a dog\\nwalking in the snow is labeled as “a wolf walking in the snow”, then you can go back\\nand check what the model focused on when it output the word “wolf ”. You may find\\nthat  it  was  paying  attention  not  only  to  the  dog,  but  also  to  the  snow,  hinting  at  a\\npossible  explanation:  perhaps  the  way  the  model  learned  to  distinguish  dogs  from\\nwolves  is  by  checking  whether  or  not  there’s  a  lot  of  snow  around.  You  can  then\\nfix  this  by  training  the  model  with  more  images  of  wolves  without  snow,  and  dogs\\nwith snow. This example comes from a great 2016 paper40 by Marco Tulio Ribeiro et\\nal.  that  uses  a  different  approach  to  explainability:  learning  an  interpretable  model\\nlocally around a classifier’s prediction.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.248,174.627,423.748,208.627 'In  some  applications,  explainability  is  not  just  a  tool  to  debug  a  model;  it  can  be  a\\nlegal requirement—think of a system deciding whether or not it should grant you a\\nloan.\\n'>\n",
            "<LTTextBoxHorizontal(3) 69.780,101.954,397.312,109.954 '39 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.781,68.954,428.865,96.954 '40 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier”, Proceed‐\\nings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\\n1135–1144.\\n'>\n",
            "<LTTextBoxHorizontal(5) 342.451,40.500,402.517,49.500 'Vision Transformers \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.985,40.500,432.001,49.500 '625\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,430.797,432.375,607.500>\n",
            "<LTLine 72.000,430.922,432.500,430.922>\n",
            "<LTLine 72.125,430.797,72.125,607.500>\n",
            "<LTLine 72.000,389.872,432.000,389.872>\n",
            "<LTLine 431.875,163.247,431.875,389.997>\n",
            "<LTLine 72.000,163.372,432.000,163.372>\n",
            "<LTLine 72.125,163.247,72.125,389.997>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTFigure(I1) 79.453,436.777,425.048,602.250 matrix=[345.60,0.00,0.00,165.47, (79.45,436.78)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,494.236,432.005,605.537 'When  transformers  came  out  in  2017  and  people  started  to  experiment  with  them\\nbeyond NLP, they were first used alongside CNNs, without replacing them. Instead,\\ntransformers were generally used to replace RNNs, for example, in image captioning\\nmodels.  Transformers  became  slightly  more  visual  in  a  2020  paper41  by  Facebook\\nresearchers,  which  proposed  a  hybrid  CNN–transformer  architecture  for  object\\ndetection.  Once  again,  the  CNN  first  processes  the  input  images  and  outputs  a  set\\nof  feature  maps,  then  these  feature  maps  are  converted  to  sequences  and  fed  to  a\\ntransformer, which outputs bounding box predictions. But again, most of the visual\\nwork is still done by the CNN.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,324.436,432.005,486.756 'Then,  in  October  2020,  a  team  of  Google  researchers  released  a  paper42  that  intro‐\\nduced a fully transformer-based vision model, called a vision transformer (ViT). The\\nidea is surprisingly simple: just chop the image into little 16 × 16 squares, and treat\\nthe  sequence  of  squares  as  if  it  were  a  sequence  of  word  representations.  To  be\\nmore  precise,  the  squares  are  first  flattened  into  16  ×  16  ×  3  =  768-dimensional\\nvectors—the 3 is for the RGB color channels—then these vectors go through a linear\\nlayer  that  transforms  them  but  retains  their  dimensionality.  The  resulting  sequence\\nof  vectors  can  then  be  treated  just  like  a  sequence  of  word  embeddings:  this  means\\nadding  positional  embeddings,  and  passing  the  result  to  the  transformer.  That’s  it!\\nThis model beat the state of the art on ImageNet image classification, but to be fair\\nthe  authors  had  to  use  over  300  million  additional  images  for  training.  This  makes\\nsense  since  transformers  don’t  have  as  many  inductive  biases  as  convolution  neural\\nnets, so they need extra data just to learn things that CNNs implicitly assume.\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.786,194.225,396.005,307.505 'An  inductive  bias  is  an  implicit  assumption  made  by  the  model,\\ndue  to  its  architecture.  For  example,  linear  models  implicitly\\nassume  that  the  data  is,  well,  linear.  CNNs  implicitly  assume  that\\npatterns learned in one location will likely be useful in other loca‐\\ntions as well. RNNs implicitly assume that the inputs are ordered,\\nand  that  recent  tokens  are  more  important  than  older  ones.  The\\nmore  inductive  biases  a  model  has,  assuming  they  are  correct,\\nthe  less  training  data  the  model  will  require.  But  if  the  implicit\\nassumptions are wrong, then the model may perform poorly even\\nif it is trained on a large dataset.\\n'>\n",
            "<LTTextBoxHorizontal(3) 69.780,101.954,415.704,109.954 '41 Nicolas Carion et al., “End-to-End Object Detection with Transformers”, arXiv preprint arxiv:2005.12872\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,91.954,102.720,99.954 '(2020).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.777,78.954,419.293,86.954 '42 Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”,\\n'>\n",
            "<LTTextBoxHorizontal(6) 79.997,68.954,206.973,76.954 'arXiv preprint arxiv:2010.11929 (2020).\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.002,40.500,84.440,49.500 '626 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTFigure(I1) 89.000,259.799,126.944,309.300 matrix=[37.94,0.00,0.00,49.50, (89.00,259.80)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,606.157 'Just  two  months  later,  a  team  of  Facebook  researchers  released  a  paper43  that  intro‐\\nduced  data-efficient  image  transformers  (DeiTs).  Their  model  achieved  competitive\\nresults on ImageNet without requiring any additional data for training. The model’s\\narchitecture  is  virtually  the  same  as  the  original  ViT,  but  the  authors  used  a  distil‐\\nlation  technique  to  transfer  knowledge  from  state-of-the-art  CNN  models  to  their\\nmodel.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,248.836,432.005,524.556 'Then,  in  March  2021,  DeepMind  released  an  important  paper44  that  introduced  the\\nPerceiver  architecture.  It  is  a  multimodal  transformer,  meaning  you  can  feed  it  text,\\nimages,  audio,  or  virtually  any  other  modality.  Until  then,  transformers  had  been\\nrestricted to fairly short sequences because of the performance and RAM bottleneck\\nin the attention layers. This excluded modalities such as audio or video, and it forced\\nresearchers to treat images as sequences of patches, rather than sequences of pixels.\\nThe bottleneck is due to self-attention, where every token must attend to every other\\ntoken:  if  the  input  sequence  has  M  tokens,  then  the  attention  layer  must  compute\\nan  M  ×  M  matrix,  which  can  be  huge  if  M  is  very  large.  The  Perceiver  solves  this\\nproblem  by  gradually  improving  a  fairly  short  latent  representation  of  the  inputs,\\ncomposed of N tokens—typically just a few hundred. (The word latent means hidden,\\nor  internal.)  The  model  uses  cross-attention  layers  only,  feeding  them  the  latent\\nrepresentation as the queries, and the (possibly large) inputs as the values. This only\\nrequires computing an M × N matrix, so the computational complexity is linear with\\nregard to M, instead of quadratic. After going through several cross-attention layers,\\nif  everything  goes  well,  the  latent  representation  ends  up  capturing  everything  that\\nmatters  in  the  inputs.  The  authors  also  suggested  sharing  the  weights  between  con‐\\nsecutive cross-attention layers: if you do that, then the Perceiver effectively becomes\\nan RNN. Indeed, the shared cross-attention layers can be seen as the same memory\\ncell  at  different  time  steps,  and  the  latent  representation  corresponds  to  the  cell’s\\ncontext vector. The same inputs are repeatedly fed to the memory cell at every time\\nstep. It looks like RNNs are not dead after all!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,167.236,432.003,241.356 'Just  a  month  later,  Mathilde  Caron  et  al.  introduced  DINO,45  an  impressive  vision\\ntransformer  trained  entirely  without  labels,  using  self-supervision,  and  capable  of\\nhigh-accuracy semantic segmentation. The model is duplicated during training, with\\none network acting as a teacher and the other acting as a student. Gradient descent\\nonly  affects  the  student,  while  the  teacher’s  weights  are  just  an  exponential  moving\\naverage of the student’s weights. The student is trained to match the teacher’s predic‐\\n'>\n",
            "<LTTextBoxHorizontal(3) 69.780,124.954,423.160,132.954 '43 Hugo Touvron et al., “Training Data-Efficient Image Transformers & Distillation Through Attention”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,114.954,187.600,122.954 'preprint arxiv:2012.12877 (2020).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.782,101.954,429.386,109.954 '44 Andrew Jaegle et al., “Perceiver: General Perception with Iterative Attention”, arXiv preprint arxiv:2103.03206\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.002,91.954,102.722,99.954 '(2021).\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.779,78.954,399.815,86.954 '45 Mathilde Caron et al., “Emerging Properties in Self-Supervised Vision Transformers”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(8) 79.999,68.954,159.255,76.954 'arxiv:2104.14294 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(9) 342.447,40.500,402.513,49.500 'Vision Transformers \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.981,40.500,431.997,49.500 '627\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,140.900,162.000,140.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,481.636,432.005,605.537 'tions:  since  they’re  almost  the  same  model,  this  is  called  self-distillation.  At  each\\ntraining  step,  the  input  images  are  augmented  in  different  ways  for  the  teacher  and\\nthe  student,  so  they  don’t  see  the  exact  same  image,  but  their  predictions  must\\nmatch.  This  forces  them  to  come  up  with  high-level  representations.  To  prevent\\nmode collapse, where both the student and the teacher would always output the same\\nthing, completely ignoring the inputs, DINO keeps track of a moving average of the\\nteacher’s outputs, and it tweaks the teacher’s predictions to ensure that they remain\\ncentered on zero, on average. DINO also forces the teacher to have high confidence in\\nits predictions: this is called sharpening. Together, these techniques preserve diversity\\nin the teacher’s outputs.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,412.636,432.003,474.156 'In a 2021 paper,46 Google researchers showed how to scale ViTs up or down, depend‐\\ning on the amount of data. They managed to create a huge 2 billion parameter model\\nthat reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a\\nscaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only\\n10,000 images: that’s just 10 images per class!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,343.636,432.005,404.536 'And progress in visual transformers has continued steadily to this day. For example,\\nin March 2022, a paper47 by Mitchell Wortsman et al. demonstrated that it’s possible\\nto  first  train  multiple  transformers,  then  average  their  weights  to  create  a  new  and\\nimproved  model.  This  is  similar  to  an  ensemble  (see  Chapter  7),  except  there’s  just\\none model in the end, which means there’s no inference time penalty.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,236.836,432.004,335.536 'The latest trend in transformers consists in building large multimodal models, often\\ncapable of zero-shot or few-shot learning. For example, OpenAI’s 2021 CLIP paper48\\nproposed  a  large  transformer  model  pretrained  to  match  captions  with  images:  this\\ntask  allows  it  to  learn  excellent  image  representations,  and  the  model  can  then  be\\nused directly for tasks such as image classification using simple text prompts such as\\n“a  photo  of  a  cat”.  Soon  after,  OpenAI  announced  DALL·E,49  capable  of  generating\\namazing images based on text prompts. The DALL·E 2,50 which generates even higher\\nquality images using a diffusion model (see Chapter 17).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,193.036,432.002,229.356 'In  April  2022,  DeepMind  released  the  Flamingo  paper,51  which  introduced  a  family\\nof models pretrained on a wide variety of tasks across multiple modalities, including\\ntext,  images,  and  videos.  A  single  model  can  be  used  across  very  different  tasks,\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,150.954,377.536,158.954 '46 Xiaohua Zhai et al., “Scaling Vision Transformers”, arXiv preprint arxiv:2106.04560v1 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,137.954,425.024,145.954 '47 Mitchell Wortsman et al., “Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accu‐\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,127.954,348.152,135.954 'racy Without Increasing Inference Time”, arXiv preprint arxiv:2203.05482v1 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.782,114.954,420.250,122.954 '48 Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision”, arXiv pre‐\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.002,104.954,177.042,112.954 'print arxiv:2103.00020 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.780,91.954,402.976,99.954 '49 Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation”, arXiv preprint arxiv:2102.12092 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(11) 69.782,78.954,420.282,86.954 '50 Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP Latents”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(12) 80.002,68.954,159.258,76.954 'arxiv:2204.06125 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.002,40.500,84.440,49.500 '628 \\n'>\n",
            "<LTTextBoxHorizontal(14) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 102.908,40.500,299.360,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,166.900,162.000,166.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,532.036,432.004,605.537 'such  as  question  answering,  image  captioning,  and  more.  Soon  after,  in  May  2022,\\nDeepMind  introduced  GATO,52  a  multimodal  model  that  can  be  used  as  a  policy\\nfor a reinforcement learning agent (RL will be introduced in Chapter 18). The same\\ntransformer can chat with you, caption images, play Atari games, control (simulated)\\nrobotic  arms,  and  more,  all  with  “only”  1.2  billion  parameters.  And  the  adventure\\ncontinues!\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.790,447.905,396.001,515.105 'These astounding advances have led some researchers to claim that\\nhuman-level AI is near, that “scale is all you need”, and that some\\nof these models may be “slightly conscious”. Others point out that\\ndespite the amazing progress, these models still lack the reliability\\nand adaptability of human intelligence, our ability to reason sym‐\\nbolically, to generalize based on a single example, and more.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,369.916,432.004,430.816 'As you can see, transformers are everywhere! And the good news is that you gener‐\\nally  won’t  have  to  implement  transformers  yourself  since  many  excellent  pretrained\\nmodels  are  readily  available  for  download  via  TensorFlow  Hub  or  Hugging  Face’s\\nmodel hub. You’ve already seen how to use a model from TF Hub, so let’s close this\\nchapter by taking a quick look at Hugging Face’s ecosystem.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,256.916,432.003,357.577 'Hugging Face’s Transformers Library\\nIt’s  impossible  to  talk  about  transformers  today  without  mentioning  Hugging  Face,\\nan AI company that has built a whole ecosystem of easy-to-use open source tools for\\nNLP, vision, and beyond. The central component of their ecosystem is the Transform‐\\ners  library,  which  allows  you  to  easily  download  a  pretrained  model,  including  its\\ncorresponding tokenizer, and then fine-tune it on your own dataset, if needed. Plus,\\nthe library supports TensorFlow, PyTorch, and JAX (with the Flax library).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,199.329,432.005,249.782 'The  simplest  way  to  use  the  Transformers  library  is  to  use  the  transformers.\\npipeline() function: you just specify which task you want, such as sentiment analy‐\\nsis, and it downloads a default pretrained model, ready to be used—it really couldn’t\\nbe any simpler:\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,91.954,412.456,99.954 '51 Jean-Baptiste Alayrac et al., “Flamingo: a Visual Language Model for Few-Shot Learning”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,81.954,159.256,89.954 'arxiv:2204.14198 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.781,68.954,330.145,76.954 '52 Scott Reed et al., “A Generalist Agent”, arXiv preprint arxiv:2205.06175 (2022).\\n'>\n",
            "<LTTextBoxHorizontal(8) 293.649,40.500,402.513,49.500 'Hugging Face’s Transformers Library \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.981,40.500,431.997,49.500 '629\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,110.900,162.000,110.900>\n",
            "<LTFigure(I1) 89.000,467.399,126.944,516.900 matrix=[37.94,0.00,0.00,49.50, (89.00,467.40)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,598.150,229.250,606.650 'from transformers import pipeline\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,567.550,416.250,586.250 'classifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\\nresult = classifier(\"The actors were very convincing\".)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,548.236,351.856,558.736 'The result is a Python list containing one dictionary per input text:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.002,522.550,310.002,541.250 \">>> result\\n[{'label': 'POSITIVE', 'score': 0.9998071789741516}]\\n\">\n",
            "<LTTextBoxHorizontal(4) 71.996,490.637,431.999,513.736 'In this example, the model correctly found that the sentence is positive, with around\\n99.98% confidence. Of course, you can also pass a batch of sentences to the model:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,454.750,322.752,483.650 '>>> classifier([\"I am from India.\", \"I am from Iraq.\"])\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9896161556243896},\\n {\\'label\\': \\'NEGATIVE\\', \\'score\\': 0.9811071157455444}]\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.252,282.530,423.755,434.025 'Bias and Fairness\\nAs  the  output  suggests,  this  specific  classifier  loves  Indians,  but  is  severely  biased\\nagainst Iraqis. You can try this code with your own country or city. Such an undesira‐\\nble bias generally comes in large part from the training data itself: in this case, there\\nwere plenty of negative sentences related to the wars in Iraq in the training data. This\\nbias was then amplified during the fine-tuning process since the model was forced to\\nchoose between just two classes: positive or negative. If you add a neutral class when\\nfine-tuning, then the country bias mostly disappears. But the training data is not the\\nonly source of bias: the model’s architecture, the type of loss or regularization used for\\ntraining, the optimizer; all of these can affect what the model ends up learning. Even\\na mostly unbiased model can be used in a biased way, much like survey questions can\\nbe biased.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.255,156.530,423.755,274.530 'Understanding bias in AI and mitigating its negative effects is still an area of active\\nresearch,  but  one  thing  is  certain:  you  should  pause  and  think  before  you  rush  to\\ndeploy  a  model  to  production.  Ask  yourself  how  the  model  could  do  harm,  even\\nindirectly. For example, if the model’s predictions are used to decide whether or not\\nto  give  someone  a  loan,  the  process  should  be  fair.  So,  make  sure  you  evaluate  the\\nmodel’s  performance  not  just  on  average  over  the  whole  test  set,  but  across  various\\nsubsets as well: for example, you may find that although the model works very well\\non average, its performance is abysmal for some categories of people. You may also\\nwant to run counterfactual tests: for example, you may want to check that the model’s\\npredictions do not change when you simply switch someone’s gender.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.255,78.530,423.755,148.530 'If  the  model  works  well  on  average,  it’s  tempting  to  push  it  to  production  and\\nmove  on  to  something  else,  especially  if  it’s  just  one  component  of  a  much  larger\\nsystem. But in general, if you don’t fix such issues, no one else will, and your model\\nmay  end  up  doing  more  harm  than  good.  The  solution  depends  on  the  problem:  it\\nmay require rebalancing the dataset, fine-tuning on a different dataset, switching to\\nanother pretrained model, tweaking the model’s architecture or hyperparameters, etc.\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.000,40.500,84.438,49.500 '630 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,443.775,432.000,443.775>\n",
            "<LTLine 431.875,67.150,431.875,443.900>\n",
            "<LTLine 72.000,67.275,432.000,67.275>\n",
            "<LTLine 72.125,67.150,72.125,443.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,493.050,432.004,606.503 'The pipeline() function uses the default model for the given task. For example, for\\ntext classification tasks such as sentiment analysis, at the time of writing, it defaults to\\ndistilbert-base-uncased-finetuned-sst-2-english—a  DistilBERT  model  with\\nan uncased tokenizer, trained on English Wikipedia and a corpus of English books,\\nand fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task. It’s also possible\\nto  manually  specify  a  different  model.  For  example,  you  could  use  a  DistilBERT\\nmodel  fine-tuned  on  the  Multi-Genre  Natural  Language  Inference  (MultiNLI)  task,\\nwhich classifies two sentences into three classes: contradiction, neutral, or entailment.\\nHere is how:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,446.963,390.750,486.063 '>>> model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\\n>>> classifier_mnli = pipeline(\"text-classification\", model=model_name)\\n>>> classifier_mnli(\"She loves me. [SEP] She loves me not.\")\\n[{\\'label\\': \\'contradiction\\', \\'score\\': 0.9790192246437073}]\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.792,408.198,396.002,429.318 'You can find the available models at https://huggingface.co/models,\\nand the list of tasks at https://huggingface.co/tasks.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,302.257,432.004,363.750 'The pipeline API is very simple and convenient, but sometimes you will need more\\ncontrol. For such cases, the Transformers library provides many classes, including all\\nsorts of tokenizers, models, configurations, callbacks, and much more. For example,\\nlet’s load the same DistilBERT model, along with its corresponding tokenizer, using\\nthe TFAutoModelForSequenceClassification and AutoTokenizer classes:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,286.770,411.999,295.270 'from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.999,256.170,394.999,274.870 'tokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,224.257,431.998,247.357 'Next,  let’s  tokenize  a  couple  of  pairs  of  sentences.  In  this  code,  we  activate  padding\\nand specify that we want TensorFlow tensors instead of Python lists:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,188.370,411.998,217.270 'token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\\n                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\\n                      padding=True, return_tensors=\"tf\")\\n'>\n",
            "<LTTextBoxHorizontal(8) 136.786,136.458,396.004,171.608 'Instead  of  passing  \"Sentence  1  [SEP]  Sentence  2\"  to  the\\ntokenizer,  you  can  equivalently  pass  it  a  tuple:  (\"Sentence  1\",\\n\"Sentence 2\").\\n'>\n",
            "<LTTextBoxHorizontal(9) 293.655,40.500,402.519,49.500 'Hugging Face’s Transformers Library \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.987,40.500,432.003,49.500 '631\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,379.336,126.760,435.114 matrix=[41.76,0.00,0.00,55.78, (85.00,379.34)]>\n",
            "<LTFigure(I1) 85.000,120.743,126.760,176.520 matrix=[41.76,0.00,0.00,55.78, (85.00,120.74)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,581.843,432.000,606.503 'The output is a dictionary-like instance of the BatchEncoding class, which contains\\nthe sequences of token IDs, as well as a mask containing 0s for the padding tokens:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.004,484.757,399.254,574.857 \">>> token_ids\\n{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\\narray([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\\n         102,    0,    0,    0],\\n       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\\n        2003, 2214, 1012,  102]], dtype=int32)>,\\n 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\\n\">\n",
            "<LTTextBoxHorizontal(2) 71.997,439.650,432.000,476.909 'If you set return_token_type_ids=True when calling the tokenizer, you will also get\\nan extra tensor that indicates which sentence each token belongs to. This is needed by\\nsome models, but not DistilBERT.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,407.263,432.005,432.516 'Next,  we  can  directly  pass  this  BatchEncoding  object  to  the  model;  it  returns  a\\nTFSequenceClassifierOutput object containing its predicted class logits:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,350.977,399.247,400.277 '>>> outputs = model(token_ids)\\n>>> outputs\\nTFSequenceClassifierOutput(loss=None, logits=[<tf.Tensor: [...] numpy=\\narray([[-2.1123817 ,  1.1786783 ,  1.4101017 ],\\n       [-0.01478387,  1.0962474 , -0.9919954 ]], dtype=float32)>], [...])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,305.870,432.004,342.163 'Lastly,  we  can  apply  the  softmax  activation  function  to  convert  these  logits  to  class\\nprobabilities,  and  use  the  argmax()  function  to  predict  the  class  with  the  highest\\nprobability for each input sentence pair:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.998,218.984,348.248,298.884 '>>> Y_probas = tf.keras.activations.softmax(outputs.logits)\\n>>> Y_probas\\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\\narray([[0.01619702, 0.43523544, 0.5485676 ],\\n       [0.08672056, 0.85204804, 0.06123142]], dtype=float32)>\\n>>> Y_pred = tf.argmax(Y_probas, axis=1)\\n>>> Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral\\n<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])>\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,174.470,432.003,210.170 'In  this  example,  the  model  correctly  classifies  the  first  sentence  pair  as  neutral  (the\\nfact that I like soccer does not imply that everyone else does) and the second pair as\\nan entailment (Joe must indeed be quite old).\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,78.490,432.004,166.370 'If  you  wish  to  fine-tune  this  model  on  your  own  dataset,  you  can  train  the  model\\nas  usual  with  Keras  since  it’s  just  a  regular  Keras  model  with  a  few  extra  methods.\\nHowever,  because  the  model  outputs  logits  instead  of  probabilities,  you  must  use\\nthe  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \\nloss\\ninstead of the usual \"sparse_categorical_crossentropy\" loss. Moreover, the model\\ndoes  not  support  BatchEncoding  inputs  during  training,  so  you  must  use  its  data\\nattribute to get a regular dictionary instead:\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.000,40.500,84.438,49.500 '632 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.906,40.500,299.358,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,547.150,407.750,606.650 'sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\\nX_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\\ny_train = tf.constant([0, 2])  # contradiction, neutral\\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\nmodel.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\\nhistory = model.fit(X_train, y_train, epochs=2)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,477.437,432.003,538.337 'Hugging Face has also built a Datasets library that you can use to easily download a\\nstandard dataset (such as IMDb) or a custom one, and use it to fine-tune your model.\\nIt’s  similar  to  TensorFlow  Datasets,  but  it  also  provides  tools  to  perform  common\\npreprocessing  tasks  on  the  fly,  such  as  masking.  The  list  of  datasets  is  available  at\\nhttps://huggingface.co/datasets.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,395.836,432.005,469.337 'This should get you started with Hugging Face’s ecosystem. To learn more, you can\\nhead over to https://huggingface.co/docs for the documentation, which includes many\\ntutorial notebooks, videos, the full API, and more. I also recommend you check out\\nthe O’Reilly book Natural Language Processing with Transformers: Building Language\\nApplications with Hugging Face by Lewis Tunstall, Leandro von Werra, and Thomas\\nWolf—all from the Hugging Face team.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,352.036,432.003,387.736 'In the next chapter we will discuss how to learn deep representations in an unsuper‐\\nvised  way  using  autoencoders,  and  we  will  use  generative  adversarial  networks  to\\nproduce images and more!\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.994,320.777,127.638,339.697 'Exercises\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.314,298.036,406.654,308.536 '1.\\n1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\\n'>\n",
            "<LTTextBoxHorizontal(6) 77.314,281.436,431.995,291.936 '2.\\n2. Why  do  people  use  encoder–decoder  RNNs  rather  than  plain  sequence-to-\\n'>\n",
            "<LTTextBoxHorizontal(7) 90.000,268.836,267.429,279.336 'sequence RNNs for automatic translation?\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.313,252.236,432.000,262.736 '3.\\n3. How  can  you  deal  with  variable-length  input  sequences?  What  about  variable-\\n'>\n",
            "<LTTextBoxHorizontal(9) 90.004,239.636,195.970,250.136 'length output sequences?\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.313,223.036,432.004,233.536 '4. What  is  beam  search,  and  why  would  you  use  it?  What  tool  can  you  use  to\\n4.\\n'>\n",
            "<LTTextBoxHorizontal(11) 89.998,210.436,148.021,220.936 'implement it?\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.317,193.836,307.915,204.336 '5.\\n5. What is an attention mechanism? How does it help?\\n'>\n",
            "<LTTextBoxHorizontal(13) 77.311,177.236,431.998,187.736 '6.\\n6. What  is  the  most  important  layer  in  the  transformer  architecture?  What  is  its\\n'>\n",
            "<LTTextBoxHorizontal(14) 90.003,164.636,128.128,175.136 'purpose?\\n'>\n",
            "<LTTextBoxHorizontal(15) 77.311,148.036,290.122,158.536 '7. When would you need to use sampled softmax?\\n7.\\n'>\n",
            "<LTTextBoxHorizontal(16) 77.311,81.036,432.003,141.936 '8. Embedded  Reber  grammars  were  used  by  Hochreiter  and  Schmidhuber  in  their\\n8.\\npaper  about  LSTMs.  They  are  artificial  grammars  that  produce  strings  such  as\\n“BPBTSXXVPSEPE”. Check out Jenny Orr’s nice introduction to this topic, then\\nchoose  a  particular  embedded  Reber  grammar  (such  as  the  one  represented\\non  Orr’s  page),  then  train  an  RNN  to  identify  whether  a  string  respects  that\\n'>\n",
            "<LTTextBoxHorizontal(17) 374.625,40.500,402.516,49.500 'Exercises \\n'>\n",
            "<LTTextBoxHorizontal(18) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(19) 420.984,40.500,432.000,49.500 '633\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.996,569.837,432.002,605.537 'grammar  or  not.  You  will  first  need  to  write  a  function  capable  of  generating  a\\ntraining batch containing about 50% strings that respect the grammar, and 50%\\nthat don’t.\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.315,553.236,431.996,563.737 '9.\\n9. Train an encoder–decoder model that can convert a date string from one format\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.000,540.637,323.562,551.137 'to another (e.g., from “April 22, 2019” to “2019-04-22”).\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.273,473.637,432.005,534.537 '10. Go  through  the  example  on  the  Keras  website  for  “Natural  language  image\\n10.\\nsearch  with  a  Dual  Encoder”.  You  will  learn  how  to  build  a  model  capable  of\\nrepresenting both images and text within the same embedding space. This makes\\nit  possible  to  search  for  images  using  a  text  prompt,  like  in  the  CLIP  model  by\\nOpenAI.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.276,418.643,432.002,467.537 '11.\\n11. Use  the  Hugging  Face  Transformers  library  to  download  a  pretrained  language\\nmodel capable of generating text (e.g., GPT), and try generating more convincing\\nShakespearean  text.  You  will  need  to  use  the  model’s  generate()  method—see\\nHugging Face’s documentation for more details.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,383.443,432.003,406.543 'Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\\nhttps://homl.info/colab3.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.999,40.500,84.437,49.500 '634 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.905,40.500,299.357,49.500 'Chapter 16: Natural Language Processing with RNNs and Attention\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 82.222,533.502,431.998,582.331 'CHAPTER 17\\nAutoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,293.068,432.004,416.969 'Autoencoders are artificial neural networks capable of learning dense representations\\nof the input data, called latent representations or codings, without any supervision (i.e.,\\nthe training set is unlabeled). These codings typically have a much lower dimension‐\\nality  than  the  input  data,  making  autoencoders  useful  for  dimensionality  reduction\\n(see Chapter 8), especially for visualization purposes. Autoencoders also act as feature\\ndetectors, and they can be used for unsupervised pretraining of deep neural networks\\n(as we discussed in Chapter 11). Lastly, some autoencoders are generative models: they\\nare capable of randomly generating new data that looks very similar to the training\\ndata. For example, you could train an autoencoder on pictures of faces, and it would\\nthen be able to generate new faces.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,135.868,432.004,284.968 'Generative  adversarial  networks  (GANs)  are  also  neural  nets  capable  of  generating\\ndata.  In  fact,  they  can  generate  pictures  of  faces  so  convincing  that  it  is  hard  to\\nbelieve  the  people  they  represent  do  not  exist.  You  can  judge  so  for  yourself  by\\nvisiting  https://thispersondoesnotexist.com,  a  website  that  shows  faces  generated  by\\na GAN architecture called StyleGAN. You can also check out https://thisrentaldoesno\\ntexist.com  to  see  some  generated  Airbnb  listings.  GANs  are  now  widely  used  for\\nsuper resolution (increasing the resolution of an image), colorization, powerful image\\nediting  (e.g.,  replacing  photo  bombers  with  realistic  background),  turning  simple\\nsketches into photorealistic images, predicting the next frames in a video, augmenting\\na dataset (to train other models), generating other types of data (such as text, audio,\\nand time series), identifying the weaknesses in other models to strengthen them, and\\nmore.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,92.068,432.003,127.768 'A more recent addition to the generative learning party is diffusion models. In 2021,\\nthey managed to generate more diverse and higher-quality images than GANs, while\\nalso being much easier to train. However, diffusion models are much slower to run.\\n'>\n",
            "<LTTextBoxHorizontal(4) 420.981,40.500,431.997,49.500 '635\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'Autoencoders, GANs, and diffusion models are all unsupervised, they all learn latent\\nrepresentations,  they  can  all  be  used  as  generative  models,  and  they  have  many\\nsimilar applications. However, they work very differently:\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.654,459.036,432.005,557.737 '• Autoencoders simply learn to copy their inputs to their outputs. This may sound\\n•\\nlike a trivial task, but as you will see, constraining the network in various ways\\ncan make it rather difficult. For example, you can limit the size of the latent rep‐\\nresentations, or you can add noise to the inputs and train the network to recover\\nthe  original  inputs.  These  constraints  prevent  the  autoencoder  from  trivially\\ncopying the inputs directly to the outputs, which forces it to learn efficient ways\\nof representing the data. In short, the codings are byproducts of the autoencoder\\nlearning the identity function under some constraints.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.654,329.036,432.005,452.936 '• GANs  are  composed  of  two  neural  networks:  a  generator  that  tries  to  generate\\n•\\ndata  that  looks  similar  to  the  training  data,  and  a  discriminator  that  tries  to  tell\\nreal data from fake data. This architecture is very original in deep learning in that\\nthe generator and the discriminator compete against each other during training:\\nthe generator is often compared to a criminal trying to make realistic counterfeit\\nmoney,  while  the  discriminator  is  like  the  police  investigator  trying  to  tell  real\\nmoney  from  fake.  Adversarial  training  (training  competing  neural  networks)  is\\nwidely considered one of the most important innovations of the 2010s. In 2016,\\nYann LeCun even said that it was “the most interesting idea in the last 10 years in\\nmachine learning”.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.653,274.636,432.005,322.936 '•\\n• A denoising diffusion probabilistic model (DDPM) is trained to remove a tiny bit\\nof noise from an image. If you then take an image entirely full of Gaussian noise\\nand repeatedly run the diffusion model on that image, a high-quality image will\\ngradually emerge, similar to the training images (but not identical).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,163.836,432.005,262.536 'In this chapter we will start by exploring in more depth how autoencoders work and\\nhow to use them for dimensionality reduction, feature extraction, unsupervised pre‐\\ntraining, or as generative models. This will naturally lead us to GANs. We will build\\na  simple  GAN  to  generate  fake  images,  but  we  will  see  that  training  is  often  quite\\ndifficult.  We  will  discuss  the  main  difficulties  you  will  encounter  with  adversarial\\ntraining,  as  well  as  some  of  the  main  techniques  to  work  around  these  difficulties.\\nAnd lastly, we will build and train a DDPM and use it to generate images. Let’s start\\nwith autoencoders!\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,40.500,84.436,49.500 '636 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.904,40.500,263.194,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.999,570.312,405.952,607.973 'Efficient Data Representations\\nWhich of the following number sequences do you find the easiest to memorize?\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.654,547.712,233.794,558.213 '•\\n• 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.654,531.112,367.511,541.613 '•\\n• 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,382.512,432.005,519.013 'At first glance, it would seem that the first sequence should be easier, since it is much\\nshorter. However, if you look carefully at the second sequence, you will notice that it\\nis just the list of even numbers from 50 down to 14. Once you notice this pattern, the\\nsecond sequence becomes much easier to memorize than the first because you only\\nneed  to  remember  the  pattern  (i.e.,  decreasing  even  numbers)  and  the  starting  and\\nending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize\\nvery  long  sequences,  you  would  not  care  much  about  the  existence  of  a  pattern  in\\nthe  second  sequence.  You  would  just  learn  every  number  by  heart,  and  that  would\\nbe that. The fact that it is hard to memorize long sequences is what makes it useful\\nto  recognize  patterns,  and  hopefully  this  clarifies  why  constraining  an  autoencoder\\nduring training pushes it to discover and exploit patterns in the data.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,263.112,432.004,374.412 'The  relationship  between  memory,  perception,  and  pattern  matching  was  famously\\nstudied by William Chase and Herbert Simon1 in the early 1970s. They observed that\\nexpert chess players were able to memorize the positions of all the pieces in a game\\nby  looking  at  the  board  for  just  five  seconds,  a  task  that  most  people  would  find\\nimpossible. However, this was only the case when the pieces were placed in realistic\\npositions  (from  actual  games),  not  when  the  pieces  were  placed  randomly.  Chess\\nexperts don’t have a much better memory than you and I; they just see chess patterns\\nmore easily, thanks to their experience with the game. Noticing patterns helps them\\nstore information efficiently.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,181.512,432.003,255.012 'Just  like  the  chess  players  in  this  memory  experiment,  an  autoencoder  looks  at\\nthe  inputs,  converts  them  to  an  efficient  latent  representation,  and  then  spits  out\\nsomething that (hopefully) looks very close to the inputs. An autoencoder is always\\ncomposed of two parts: an encoder (or recognition network) that converts the inputs to\\na latent representation, followed by a decoder (or generative network) that converts the\\ninternal representation to the outputs (see Figure 17-1).\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,68.954,423.064,76.954 '1 William G. Chase and Herbert A. Simon, “Perception in Chess”, Cognitive Psychology 4, no. 1 (1973): 55–81.\\n'>\n",
            "<LTTextBoxHorizontal(7) 311.434,40.500,402.514,49.500 'Efficient Data Representations \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.982,40.500,431.998,49.500 '637\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 72.000,363.430,404.451,373.930 'Figure 17-1. The chess memory experiment (left) and a simple autoencoder (right)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,250.630,432.005,349.330 'As  you  can  see,  an  autoencoder  typically  has  the  same  architecture  as  a  multilayer\\nperceptron (MLP; see Chapter 10), except that the number of neurons in the output\\nlayer must be equal to the number of inputs. In this example, there is just one hidden\\nlayer  composed  of  two  neurons  (the  encoder),  and  one  output  layer  composed  of\\nthree neurons (the decoder). The outputs are often called the reconstructions because\\nthe  autoencoder  tries  to  reconstruct  the  inputs.  The  cost  function  contains  a  recon‐\\nstruction loss that penalizes the model when the reconstructions are different from the\\ninputs.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,181.630,432.005,242.530 'Because the internal representation has a lower dimensionality than the input data (it\\nis 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete\\nautoencoder cannot trivially copy its inputs to the codings, yet it must find a way to\\noutput  a  copy  of  its  inputs.  It  is  forced  to  learn  the  most  important  features  in  the\\ninput data (and drop the unimportant ones).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,150.430,432.002,173.530 'Let’s see how to implement a very simple undercomplete autoencoder for dimension‐\\nality reduction.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,40.500,84.441,49.500 '638 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.909,40.500,263.199,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,379.494,432.375,607.500>\n",
            "<LTLine 72.000,379.619,432.500,379.619>\n",
            "<LTLine 72.125,379.494,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,385.474,378.250,602.250 matrix=[252.00,0.00,0.00,216.78, (126.25,385.47)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,527.138,432.000,607.973 'Performing PCA with an Undercomplete Linear\\nAutoencoder\\nIf  the  autoencoder  uses  only  linear  activations  and  the  cost  function  is  the  mean\\nsquared  error  (MSE),  then  it  ends  up  performing  principal  component  analysis\\n(PCA; see Chapter 8).\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,495.938,432.005,519.038 'The  following  code  builds  a  simple  linear  autoencoder  to  perform  PCA  on  a  3D\\ndataset, projecting it to 2D:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,480.452,186.749,488.952 'import tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,439.652,331.249,468.552 'encoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\\ndecoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\\nautoencoder = tf.keras.Sequential([encoder, decoder])\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,409.052,318.499,427.752 'optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\\nautoencoder.compile(loss=\"mse\", optimizer=optimizer)\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,377.139,431.995,400.239 'This code is really not very different from all the MLPs we built in past chapters, but\\nthere are a few things to note:\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.652,315.552,432.005,365.039 '•\\n• We  organized  the  autoencoder  into  two  subcomponents:  the  encoder  and  the\\ndecoder. Both are regular Sequential models with a single Dense layer each, and\\nthe autoencoder is a Sequential model containing the encoder followed by the\\ndecoder (remember that a model can be used as a layer in another model).\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.657,298.952,418.008,309.452 '•\\n• The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.651,257.152,432.003,292.852 '• To  perform  PCA,  we  do  not  use  any  activation  function  (i.e.,  all  neurons  are\\n•\\nlinear), and the cost function is the MSE. That’s because PCA is a linear transfor‐\\nmation. We will see more complex and nonlinear autoencoders shortly.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.997,221.952,432.000,245.052 'Now  let’s  train  the  model  on  the  same  simple  generated  3D  dataset  we  used  in\\nChapter 8 and use it to encode that dataset (i.e., project it to 2D):\\n'>\n",
            "<LTTextBoxHorizontal(10) 89.001,186.065,386.501,214.965 'X_train = [...]  # generate a 3D dataset, like in Chapter 8\\nhistory = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\\ncodings = encoder.predict(X_train)\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.995,115.759,432.003,178.218 'Note that X_train is used as both the inputs and the targets. Figure 17-2 shows the\\noriginal 3D dataset (on the left) and the output of the autoencoder’s hidden layer (i.e.,\\nthe  coding  layer,  on  the  right).  As  you  can  see,  the  autoencoder  found  the  best  2D\\nplane  to  project  the  data  onto,  preserving  as  much  variance  in  the  data  as  it  could\\n(just like PCA).\\n'>\n",
            "<LTTextBoxHorizontal(12) 223.869,40.500,402.519,49.500 'Performing PCA with an Undercomplete Linear Autoencoder \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.096,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.986,40.500,432.002,49.500 '639\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,436.122,410.184,446.622 'Figure 17-2. Approximate PCA performed by an undercomplete linear autoencoder\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.786,375.030,395.996,419.190 'You  can  think  of  an  autoencoder  as  performing  a  form  of  self-\\nsupervised learning, since it is based on a supervised learning tech‐\\nnique with automatically generated labels (in this case simply equal\\nto the inputs).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,230.198,432.004,356.058 'Stacked Autoencoders\\nJust  like  other  neural  networks  we  have  discussed,  autoencoders  can  have  multiple\\nhidden layers. In this case they are called stacked autoencoders (or deep autoencoders).\\nAdding  more  layers  helps  the  autoencoder  learn  more  complex  codings.  That  said,\\none must be careful not to make the autoencoder too powerful. Imagine an encoder\\nso powerful that it just learns to map each input to a single arbitrary number (and the\\ndecoder learns the reverse mapping). Obviously such an autoencoder will reconstruct\\nthe training data perfectly, but it will not have learned any useful data representation\\nin the process, and it is unlikely to generalize well to new instances.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,148.598,432.003,222.098 'The architecture of a stacked autoencoder is typically symmetrical with regard to the\\ncentral hidden layer (the coding layer). To put it simply, it looks like a sandwich. For\\nexample,  an  autoencoder  for  Fashion  MNIST  (introduced  in  Chapter  10)  may  have\\n784 inputs, followed by a hidden layer with 100 neurons, then a central hidden layer\\nof 30 neurons, then another hidden layer with 100 neurons, and an output layer with\\n784 neurons. This stacked autoencoder is represented in Figure 17-3.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,40.500,84.442,49.500 '640 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.910,40.500,263.200,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,452.185,432.375,607.500>\n",
            "<LTLine 72.000,452.310,432.500,452.310>\n",
            "<LTLine 72.125,452.185,72.125,607.500>\n",
            "<LTFigure(I1) 89.000,371.485,126.944,420.985 matrix=[37.94,0.00,0.00,49.50, (89.00,371.48)]>\n",
            "<LTFigure(I2) 79.450,458.165,425.050,602.250 matrix=[345.60,0.00,0.00,144.08, (79.45,458.17)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,387.592,207.660,398.092 'Figure 17-3. Stacked autoencoder\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.003,342.011,401.136,375.850 'Implementing a Stacked Autoencoder Using Keras\\nYou can implement a stacked autoencoder very much like a regular deep MLP:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,224.524,377.999,335.024 'stacked_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(30, activation=\"relu\"),\\n])\\nstacked_decoder = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(28 * 28),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\nstacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,183.724,343.999,212.624 'stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\\nhistory = stacked_ae.fit(X_train, X_train, epochs=20,\\n                         validation_data=(X_valid, X_valid))\\n'>\n",
            "<LTTextBoxHorizontal(4) 335.755,40.500,402.517,49.500 'Stacked Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.985,40.500,432.001,49.500 '641\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,403.656,432.375,607.500>\n",
            "<LTLine 72.000,403.781,432.500,403.781>\n",
            "<LTLine 72.125,403.656,72.125,607.500>\n",
            "<LTFigure(I1) 135.250,409.636,369.250,602.250 matrix=[234.00,0.00,0.00,192.61, (135.25,409.64)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,181.862,605.537 'Let’s go through this code:\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.655,572.437,431.997,582.937 '•\\n• Just like earlier, we split the autoencoder model into two submodels: the encoder\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.001,559.837,159.375,570.337 'and the decoder.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.654,492.243,432.005,553.736 '• The  encoder  takes  28  ×  28–pixel  grayscale  images,  flattens  them  so  that  each\\n•\\nimage is represented as a vector of size 784, then processes these vectors through\\ntwo  Dense  layers  of  diminishing  sizes  (100  units  then  30  units),  both  using  the\\nReLU activation function. For each input image, the encoder outputs a vector of\\nsize 30.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.651,437.250,432.003,486.143 '• The decoder takes codings of size 30 (output by the encoder) and processes them\\n•\\nthrough  two  Dense  layers  of  increasing  sizes  (100  units  then  784  units),  and  it\\nreshapes  the  final  vectors  into  28  ×  28  arrays  so  the  decoder’s  outputs  have  the\\nsame shape as the encoder’s inputs.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.654,420.650,432.002,431.150 '•\\n• When  compiling  the  stacked  autoencoder,  we  use  the  MSE  loss  and  Nadam\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.996,408.050,146.612,418.550 'optimization.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.654,390.857,431.996,402.916 '• Finally,  we  train  the  model  using  X_train  as  both  the  inputs  and  the  targets.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(8) 90.000,377.663,369.930,389.723 'Similarly, we use X_valid as both the validation inputs and targets.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.998,307.054,432.004,366.094 'Visualizing the Reconstructions\\nOne way to ensure that an autoencoder is properly trained is to compare the inputs\\nand the outputs: the differences should not be too significant. Let’s plot a few images\\nfrom the validation set, as well as their reconstructions:\\n'>\n",
            "<LTTextBoxHorizontal(10) 88.996,291.568,165.496,300.068 'import numpy as np\\n'>\n",
            "<LTTextBoxHorizontal(11) 88.996,179.368,382.246,279.668 'def plot_reconstructions(model, images=X_valid, n_images=5):\\n    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\\n    for image_index in range(n_images):\\n        plt.subplot(2, n_images, 1 + image_index)\\n        plt.imshow(images[image_index], cmap=\"binary\")\\n        plt.axis(\"off\")\\n        plt.subplot(2, n_images, 1 + n_images + image_index)\\n        plt.imshow(reconstructions[image_index], cmap=\"binary\")\\n        plt.axis(\"off\")\\n'>\n",
            "<LTTextBoxHorizontal(12) 88.996,148.768,224.996,167.468 'plot_reconstructions(stacked_ae)\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.003,129.454,237.892,139.954 'Figure 17-4 shows the resulting images.\\n'>\n",
            "<LTTextBoxHorizontal(14) 72.004,40.500,84.442,49.500 '642 \\n'>\n",
            "<LTTextBoxHorizontal(15) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 102.910,40.500,263.200,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,487.767,354.282,498.267 'Figure 17-4. Original images (top) and their reconstructions (bottom)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,412.767,432.004,473.667 'The  reconstructions  are  recognizable,  but  a  bit  too  lossy.  We  may  need  to  train  the\\nmodel  for  longer,  or  make  the  encoder  and  decoder  deeper,  or  make  the  codings\\nlarger.  But  if  we  make  the  network  too  powerful,  it  will  manage  to  make  perfect\\nreconstructions without having learned any useful patterns in the data. For now, let’s\\ngo with this model.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,253.958,432.005,401.197 'Visualizing the Fashion MNIST Dataset\\nNow that we have trained a stacked autoencoder, we can use it to reduce the dataset’s\\ndimensionality. For visualization, this does not give great results compared to other\\ndimensionality reduction algorithms (such as those we discussed in Chapter 8), but\\none big advantage of autoencoders is that they can handle large datasets with many\\ninstances  and  many  features.  So,  one  strategy  is  to  use  an  autoencoder  to  reduce\\nthe  dimensionality  down  to  a  reasonable  level,  then  use  another  dimensionality\\nreduction  algorithm  for  visualization.  Let’s  use  this  strategy  to  visualize  Fashion\\nMNIST.  First  we’ll  use  the  encoder  from  our  stacked  autoencoder  to  reduce  the\\ndimensionality down to 30, then we’ll use Scikit-Learn’s implementation of the t-SNE\\nalgorithm to reduce the dimensionality down to 2 for visualization:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.004,238.471,229.254,246.971 'from sklearn.manifold import TSNE\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,197.671,352.504,226.571 'X_valid_compressed = stacked_encoder.predict(X_valid)\\ntsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\\nX_valid_2D = tsne.fit_transform(X_valid_compressed)\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.003,178.357,192.123,188.857 'Now we can plot the dataset:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.001,152.671,420.501,171.371 'plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\\nplt.show()\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,108.157,432.005,143.857 'Figure  17-5  shows  the  resulting  scatterplot,  beautified  a  bit  by  displaying  some  of\\nthe  images.  The  t-SNE  algorithm  identified  several  clusters  that  match  the  classes\\nreasonably well (each class is represented by a different color).\\n'>\n",
            "<LTTextBoxHorizontal(8) 335.757,40.500,402.519,49.500 'Stacked Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.987,40.500,432.003,49.500 '643\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,503.830,432.375,607.500>\n",
            "<LTLine 72.000,503.955,432.500,503.955>\n",
            "<LTLine 72.125,503.830,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,509.810,378.250,602.250 matrix=[252.00,0.00,0.00,92.44, (126.25,509.81)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,304.594,410.058,315.094 'Figure 17-5. Fashion MNIST visualization using an autoencoder followed by t-SNE\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,267.394,432.000,290.494 'So, autoencoders can be used for dimensionality reduction. Another application is for\\nunsupervised pretraining.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,158.985,432.005,255.824 'Unsupervised Pretraining Using Stacked Autoencoders\\nAs we discussed in Chapter 11, if you are tackling a complex supervised task but you\\ndo not have a lot of labeled training data, one solution is to find a neural network that\\nperforms  a  similar  task  and  reuse  its  lower  layers.  This  makes  it  possible  to  train  a\\nhigh-performance model using little training data because your neural network won’t\\nhave to learn all the low-level features; it will just reuse the feature detectors learned\\nby the existing network.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,89.985,432.005,150.885 'Similarly,  if  you  have  a  large  dataset  but  most  of  it  is  unlabeled,  you  can  first  train\\na  stacked  autoencoder  using  all  the  data,  then  reuse  the  lower  layers  to  create  a\\nneural network for your actual task and train it using the labeled data. For example,\\nFigure  17-6  shows  how  to  use  a  stacked  autoencoder  to  perform  unsupervised  pre‐\\ntraining for a classification neural network. When training the classifier, if you really\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,40.500,84.437,49.500 '644 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.905,40.500,263.195,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,320.657,432.375,607.500>\n",
            "<LTLine 72.000,320.782,432.500,320.782>\n",
            "<LTLine 72.125,320.657,72.125,607.500>\n",
            "<LTFigure(I1) 79.451,326.637,425.049,602.250 matrix=[345.60,0.00,0.00,275.61, (79.45,326.64)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'don’t have much labeled training data, you may want to freeze the pretrained layers\\n(at least the lower ones).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,371.646,307.292,382.146 'Figure 17-6. Unsupervised pretraining using autoencoders\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.786,275.994,395.998,354.714 'Having  plenty  of  unlabeled  data  and  little  labeled  data  is  com‐\\nmon.  Building  a  large  unlabeled  dataset  is  often  cheap  (e.g.,  a\\nsimple  script  can  download  millions  of  images  off  the  internet),\\nbut  labeling  those  images  (e.g.,  classifying  them  as  cute  or  not)\\ncan  usually  be  done  reliably  only  by  humans.  Labeling  instances\\nis  time-consuming  and  costly,  so  it’s  normal  to  have  only  a  few\\nthousand human-labeled instances, or even less.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,223.206,431.999,258.906 'There  is  nothing  special  about  the  implementation:  just  train  an  autoencoder  using\\nall the training data (labeled plus unlabeled), then reuse its encoder layers to create a\\nnew neural network (see the exercises at the end of this chapter for an example).\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,204.606,360.827,215.106 'Next, let’s look at a few techniques for training stacked autoencoders.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,70.997,432.005,193.036 'Tying Weights\\nWhen  an  autoencoder  is  neatly  symmetrical,  like  the  one  we  just  built,  a  common\\ntechnique  is  to  tie  the  weights  of  the  decoder  layers  to  the  weights  of  the  encoder\\nlayers.  This  halves  the  number  of  weights  in  the  model,  speeding  up  training  and\\nlimiting the risk of overfitting. Specifically, if the autoencoder has a total of N layers\\n(not counting the input layer), and WL represents the connection weights of the Lth\\nlayer (e.g., layer 1 is the first hidden layer, layer N/2 is the coding layer, and layer N\\n⊺\\nis the output layer), then the decoder layer weights can be defined as WL = WN–L+1\\n(with L = N / 2 + 1, …, N).\\n'>\n",
            "<LTTextBoxHorizontal(6) 335.755,40.500,402.517,49.500 'Stacked Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.985,40.500,432.001,49.500 '645\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,576.175,432.500,576.175>\n",
            "<LTLine 432.375,387.709,432.375,576.300>\n",
            "<LTLine 72.000,387.834,432.500,387.834>\n",
            "<LTLine 72.125,387.709,72.125,576.300>\n",
            "<LTFigure(I1) 89.000,307.009,126.944,356.509 matrix=[37.94,0.00,0.00,49.50, (89.00,307.01)]>\n",
            "<LTFigure(I2) 147.130,393.689,357.370,571.050 matrix=[210.24,0.00,0.00,177.36, (147.13,393.69)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,359.154,605.537 'To tie weights between layers using Keras, let’s define a custom layer:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,538.750,352.499,588.050 'class DenseTranspose(tf.keras.layers.Layer):\\n    def __init__(self, dense, activation=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = dense\\n        self.activation = tf.keras.activations.get(activation)\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,477.550,390.749,526.850 '    def build(self, batch_input_shape):\\n        self.biases = self.add_weight(name=\"bias\",\\n                                      shape=self.dense.input_shape[-1],\\n                                      initializer=\"zeros\")\\n        super().build(batch_input_shape)\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,436.750,386.499,465.650 '    def call(self, inputs):\\n        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\\n        return self.activation(Z + self.biases)\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,351.470,432.005,428.903 'This  custom  layer  acts  like  a  regular  Dense  layer,  but  it  uses  another  Dense  layer’s\\nweights, transposed (setting transpose_b=True is equivalent to transposing the sec‐\\nond argument, but it’s more efficient as it performs the transposition on the fly within\\nthe  matmul()  operation).  However,  it  uses  its  own  bias  vector.  Now  we  can  build  a\\nnew  stacked  autoencoder,  much  like  the  previous  one  but  with  the  decoder’s  Dense\\nlayers tied to the encoder’s Dense layers:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.004,325.784,322.754,344.484 'dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\\ndense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.004,264.584,242.004,313.884 'tied_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    dense_1,\\n    dense_2\\n])\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.004,203.384,288.754,252.684 'tied_decoder = tf.keras.Sequential([\\n    DenseTranspose(dense_2, activation=\"relu\"),\\n    DenseTranspose(dense_1),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.004,182.984,339.754,191.484 'tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.001,151.070,432.004,174.170 'This  model  achieves  roughly  the  same  reconstruction  error  as  the  previous  model,\\nusing almost half the number of parameters.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.999,80.461,432.005,139.500 'Training One Autoencoder at a Time\\nRather  than  training  the  whole  stacked  autoencoder  in  one  go  like  we  just  did,\\nit  is  possible  to  train  one  shallow  autoencoder  at  a  time,  then  stack  all  of  them\\ninto  a  single  stacked  autoencoder  (hence  the  name),  as  shown  in  Figure  17-7.  This\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.999,40.500,84.437,49.500 '646 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.905,40.500,263.195,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,582.437,432.003,605.537 'technique is not used so much these days, but you may still run into papers that talk\\nabout “greedy layerwise training”, so it’s good to know what it means.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,374.526,267.038,385.026 'Figure 17-7. Training one autoencoder at a time\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,249.126,432.005,360.426 'During  the  first  phase  of  training,  the  first  autoencoder  learns  to  reconstruct  the\\ninputs.  Then  we  encode  the  whole  training  set  using  this  first  autoencoder,  and\\nthis  gives  us  a  new  (compressed)  training  set.  We  then  train  a  second  autoencoder\\non  this  new  dataset.  This  is  the  second  phase  of  training.  Finally,  we  build  a  big\\nsandwich  using  all  these  autoencoders,  as  shown  in  Figure  17-7  (i.e.,  we  first  stack\\nthe hidden layers of each autoencoder, then the output layers in reverse order). This\\ngives us the final stacked autoencoder (see the “Training One Autoencoder at a Time”\\nsection in the chapter’s notebook for an implementation). We could easily train more\\nautoencoders this way, building a very deep stacked autoencoder.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,142.326,432.005,241.026 'As  I  mentioned  earlier,  one  of  the  triggers  of  the  deep  learning  tsunami  was  the\\ndiscovery  in  2006  by  Geoffrey  Hinton  et  al.  that  deep  neural  networks  can  be\\npretrained  in  an  unsupervised  fashion,  using  this  greedy  layerwise  approach.  They\\nused restricted Boltzmann machines (RBMs; see https://homl.info/extra-anns) for this\\npurpose, but in 2007 Yoshua Bengio et al.2 showed that autoencoders worked just as\\nwell. For several years this was the only efficient way to train deep nets, until many of\\nthe techniques introduced in Chapter 11 made it possible to just train a deep net in\\none shot.\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.140,78.954,421.232,86.954 '2 Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks”, Proceedings of the 19th International\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,68.954,302.352,76.954 'Conference on Neural Information Processing Systems (2006): 153–160.\\n'>\n",
            "<LTTextBoxHorizontal(6) 335.756,40.500,402.518,49.500 'Stacked Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.986,40.500,432.002,49.500 '647\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,576.175,432.500,576.175>\n",
            "<LTLine 432.375,390.589,432.375,576.300>\n",
            "<LTLine 72.000,390.714,432.500,390.714>\n",
            "<LTLine 72.125,390.589,72.125,576.300>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 110.410,396.569,394.090,571.050 matrix=[283.68,0.00,0.00,174.48, (110.41,396.57)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'Autoencoders  are  not  limited  to  dense  networks:  you  can  also  build  convolutional\\nautoencoders. Let’s look at these now.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,393.836,432.004,570.097 'Convolutional Autoencoders\\nIf  you  are  dealing  with  images,  then  the  autoencoders  we  have  seen  so  far  will  not\\nwork well (unless the images are very small): as you saw in Chapter 14, convolutional\\nneural  networks  are  far  better  suited  than  dense  networks  to  working  with  images.\\nSo if you want to build an autoencoder for images (e.g., for unsupervised pretraining\\nor  dimensionality  reduction),  you  will  need  to  build  a  convolutional  autoencoder.3\\nThe encoder is a regular CNN composed of convolutional layers and pooling layers.\\nIt  typically  reduces  the  spatial  dimensionality  of  the  inputs  (i.e.,  height  and  width)\\nwhile increasing the depth (i.e., the number of feature maps). The decoder must do\\nthe reverse (upscale the image and reduce its depth back to the original dimensions),\\nand  for  this  you  can  use  transpose  convolutional  layers  (alternatively,  you  could\\ncombine upsampling layers with convolutional layers). Here is a basic convolutional\\nautoencoder for Fashion MNIST:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,174.349,399.247,386.850 'conv_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Reshape([28, 28, 1]),\\n    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\\n    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\\n    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\\n    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\\n    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\\n    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\\n    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\\n    tf.keras.layers.GlobalAvgPool2D()  # output: 30\\n])\\nconv_decoder = tf.keras.Sequential([\\n    tf.keras.layers.Dense(3 * 3 * 16),\\n    tf.keras.layers.Reshape((3, 3, 16)),\\n    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\\n    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\\n                                    activation=\"relu\"),\\n    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\nconv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,78.954,431.888,86.954 '3 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction”, Proceedings\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,68.954,338.784,76.954 'of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,40.500,84.435,49.500 '648 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.903,40.500,263.193,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.998,582.437,432.001,605.537 'It’s also possible to create autoencoders with other architecture types, such as RNNs\\n(see the notebook for an example).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,526.036,432.002,574.337 'OK, let’s step back for a second. So far we have looked at various kinds of autoencod‐\\ners  (basic,  stacked,  and  convolutional),  and  how  to  train  them  (either  in  one  shot\\nor layer by layer). We also looked at a couple of applications: data visualization and\\nunsupervised pretraining.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,444.436,432.004,517.936 'Up  to  now,  in  order  to  force  the  autoencoder  to  learn  interesting  features,  we  have\\nlimited  the  size  of  the  coding  layer,  making  it  undercomplete.  There  are  actually\\nmany other kinds of constraints that can be used, including ones that allow the cod‐\\ning layer to be just as large as the inputs, or even larger, resulting in an overcomplete\\nautoencoder. Then, in the following sections we’ll look at a few more kinds of autoen‐\\ncoders: denoising autoencoders, sparse autoencoders, and variational autoencoders.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,344.036,432.003,432.097 'Denoising Autoencoders\\nAnother  way  to  force  the  autoencoder  to  learn  useful  features  is  to  add  noise  to  its\\ninputs, training it to recover the original, noise-free inputs. This idea has been around\\nsince the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008\\npaper,4 Pascal Vincent et al. showed that autoencoders could also be used for feature\\nextraction. In a 2010 paper,5 Vincent et al. introduced stacked denoising autoencoders.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,300.236,432.000,335.936 'The  noise  can  be  pure  Gaussian  noise  added  to  the  inputs,  or  it  can  be  randomly\\nswitched-off  inputs,  just  like  in  dropout  (introduced  in  Chapter  11).  Figure  17-8\\nshows both options.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.001,242.056,432.005,292.136 'The  implementation  is  straightforward:  it  is  a  regular  stacked  autoencoder  with\\nan  additional  Dropout  layer  applied  to  the  encoder’s  inputs  (or  you  could  use  a\\nGaussianNoise  layer  instead).  Recall  that  the  Dropout  layer  is  only  active  during\\ntraining (and so is the GaussianNoise layer):\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,101.954,429.304,109.954 '4 Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders”, Proceedings\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,91.954,324.168,99.954 'of the 25th International Conference on Machine Learning (2008): 1096–1103.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,78.954,426.432,86.954 '5 Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,381.520,76.954 'with a Local Denoising Criterion”, Journal of Machine Learning Research 11 (2010): 3371–3408.\\n'>\n",
            "<LTTextBoxHorizontal(10) 329.397,40.500,402.513,49.500 'Denoising Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.981,40.500,431.997,49.500 '649\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTTextBoxHorizontal(0) 89.000,485.950,378.000,606.650 'dropout_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(30, activation=\"relu\")\\n])\\ndropout_decoder = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(28 * 28),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\ndropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,252.551,404.901,263.051 'Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,152.351,432.005,238.451 'Figure  17-9  shows  a  few  noisy  images  (with  half  the  pixels  turned  off),  and  the\\nimages  reconstructed  by  the  dropout-based  denoising  autoencoder.  Notice  how  the\\nautoencoder  guesses  details  that  are  actually  not  in  the  input,  such  as  the  top  of\\nthe white shirt (bottom row, fourth image). As you can see, not only can denoising\\nautoencoders  be  used  for  data  visualization  or  unsupervised  pretraining,  like  the\\nother autoencoders we’ve discussed so far, but they can also be used quite simply and\\nefficiently to remove noise from images.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,40.500,84.437,49.500 '650 \\n'>\n",
            "<LTTextBoxHorizontal(4) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 102.905,40.500,263.195,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,478.975,432.500,478.975>\n",
            "<LTLine 432.375,268.614,432.375,479.100>\n",
            "<LTLine 72.000,268.739,432.500,268.739>\n",
            "<LTLine 72.125,268.614,72.125,479.100>\n",
            "<LTFigure(I1) 144.250,274.594,360.250,473.850 matrix=[216.00,0.00,0.00,199.26, (144.25,274.59)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,487.767,342.680,498.267 'Figure 17-9. Noisy images (top) and their reconstructions (bottom)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.991,349.566,432.005,475.427 'Sparse Autoencoders\\nAnother  kind  of  constraint  that  often  leads  to  good  feature  extraction  is  sparsity:\\nby  adding  an  appropriate  term  to  the  cost  function,  the  autoencoder  is  pushed  to\\nreduce  the  number  of  active  neurons  in  the  coding  layer.  For  example,  it  may  be\\npushed to have on average only 5% significantly active neurons in the coding layer.\\nThis  forces  the  autoencoder  to  represent  each  input  as  a  combination  of  a  small\\nnumber of activations. As a result, each neuron in the coding layer typically ends up\\nrepresenting  a  useful  feature  (if  you  could  speak  only  a  few  words  per  month,  you\\nwould probably try to make them worth listening to).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,293.166,432.000,341.466 'A  simple  approach  is  to  use  the  sigmoid  activation  function  in  the  coding  layer  (to\\nconstrain  the  codings  to  values  between  0  and  1),  use  a  large  coding  layer  (e.g.,\\nwith 300 units), and add some ℓ1 regularization to the coding layer’s activations. The\\ndecoder is just a regular decoder:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,165.480,403.499,286.180 'sparse_l1_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\\n    tf.keras.layers.ActivityRegularization(l1=1e-4)\\n])\\nsparse_l1_decoder = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(28 * 28),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\nsparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(4) 339.392,40.500,402.518,49.500 'Sparse Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.986,40.500,432.002,49.500 '651\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,503.830,432.375,607.500>\n",
            "<LTLine 72.000,503.955,432.500,503.955>\n",
            "<LTLine 72.125,503.830,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,509.810,378.250,602.250 matrix=[252.00,0.00,0.00,92.44, (126.25,509.81)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,479.857,432.005,606.503 'This  ActivityRegularization  layer  just  returns  its  inputs,  but  as  a  side  effect  it\\nadds  a  training  loss  equal  to  the  sum  of  the  absolute  values  of  its  inputs.  This  only\\naffects  training.  Equivalently,  you  could  remove  the  ActivityRegularization  layer\\nand  set  activity_regularizer=tf.keras.regularizers.l1(1e-4)  in  the  previous\\nlayer.  This  penalty  will  encourage  the  neural  network  to  produce  codings  close  to\\n0,  but  since  it  will  also  be  penalized  if  it  does  not  reconstruct  the  inputs  correctly,\\nit  will  have  to  output  at  least  a  few  nonzero  values.  Using  the  ℓ1  norm  rather  than\\nthe  ℓ2  norm  will  push  the  neural  network  to  preserve  the  most  important  codings\\nwhile eliminating the ones that are not needed for the input image (rather than just\\nreducing all codings).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,410.857,432.005,471.757 'Another approach, which often yields better results, is to measure the actual sparsity\\nof  the  coding  layer  at  each  training  iteration,  and  penalize  the  model  when  the\\nmeasured sparsity differs from a target sparsity. We do so by computing the average\\nactivation  of  each  neuron  in  the  coding  layer,  over  the  whole  training  batch.  The\\nbatch size must not be too small, or else the mean will not be accurate.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,304.057,432.005,402.757 'Once we have the mean activation per neuron, we want to penalize the neurons that\\nare  too  active,  or  not  active  enough,  by  adding  a  sparsity  loss  to  the  cost  function.\\nFor  example,  if  we  measure  that  a  neuron  has  an  average  activation  of  0.3,  but  the\\ntarget  sparsity  is  0.1,  it  must  be  penalized  to  activate  less.  One  approach  could  be\\nsimply  adding  the  squared  error  (0.3  –  0.1)2  to  the  cost  function,  but  in  practice  a\\nbetter approach is to use the Kullback–Leibler (KL) divergence (briefly discussed in\\nChapter 4), which has much stronger gradients than the mean squared error, as you\\ncan see in Figure 17-10.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.004,85.249,178.190,95.749 'Figure 17-10. Sparsity loss\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,40.500,84.437,49.500 '652 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.905,40.500,263.195,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,297.795,432.500,297.795>\n",
            "<LTLine 432.375,101.313,432.375,297.920>\n",
            "<LTLine 72.000,101.438,432.500,101.438>\n",
            "<LTLine 72.125,101.313,72.125,297.920>\n",
            "<LTFigure(I1) 108.250,107.293,396.250,292.670 matrix=[288.00,0.00,0.00,185.38, (108.25,107.29)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,581.848,432.000,605.537 'Given  two  discrete  probability  distributions  P  and  Q,  the  KL  divergence  between\\nthese distributions, noted DKL(P ∥ Q), can be computed using Equation 17-1.\\n'>\n",
            "<LTTextBoxHorizontal(1) 86.997,556.149,265.098,566.649 'Equation 17-1. Kullback–Leibler divergence\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.998,526.157,155.236,544.331 'DKL P ∥ Q = ∑\\ni\\n'>\n",
            "<LTTextBoxHorizontal(3) 156.346,532.622,188.676,544.072 'P i log\\n'>\n",
            "<LTTextBoxHorizontal(4) 192.556,527.667,206.766,549.333 'P i\\nQ i\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,460.887,432.003,509.187 'In  our  case,  we  want  to  measure  the  divergence  between  the  target  probability  p\\nthat a neuron in the coding layer will activate and the actual probability q, estimated\\nby  measuring  the  mean  activation  over  the  training  batch.  So,  the  KL  divergence\\nsimplifies to Equation 17-2.\\n'>\n",
            "<LTTextBoxHorizontal(6) 86.999,434.600,428.375,445.100 'Equation 17-2. KL divergence between the target sparsity p and the actual sparsity q\\n'>\n",
            "<LTTextBoxHorizontal(7) 87.005,409.405,165.179,421.951 'DKL p ∥ q = p log\\n'>\n",
            "<LTTextBoxHorizontal(8) 169.129,405.545,228.289,427.212 'p\\nq + 1 − p log\\n'>\n",
            "<LTTextBoxHorizontal(9) 232.179,404.095,252.629,427.784 '1 − p\\n1 − q\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.995,301.902,432.003,388.002 'Once  we  have  computed  the  sparsity  loss  for  each  neuron  in  the  coding  layer,  we\\nsum  up  these  losses  and  add  the  result  to  the  cost  function.  In  order  to  control  the\\nrelative importance of the sparsity loss and the reconstruction loss, we can multiply\\nthe  sparsity  loss  by  a  sparsity  weight  hyperparameter.  If  this  weight  is  too  high,  the\\nmodel  will  stick  closely  to  the  target  sparsity,  but  it  may  not  reconstruct  the  inputs\\nproperly, making the model useless. Conversely, if it is too low, the model will mostly\\nignore the sparsity objective and will not learn any interesting features.\\n'>\n",
            "<LTTextBoxHorizontal(11) 72.001,270.702,432.004,293.802 'We now have all we need to implement a sparse autoencoder based on the KL diver‐\\ngence. First, let’s create a custom regularizer to apply KL divergence regularization:\\n'>\n",
            "<LTTextBoxHorizontal(12) 89.002,255.215,339.752,263.715 'kl_divergence = tf.keras.losses.kullback_leibler_divergence\\n'>\n",
            "<LTTextBoxHorizontal(13) 89.002,204.215,365.252,243.315 'class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\\n    def __init__(self, weight, target):\\n        self.weight = weight\\n        self.target = target\\n'>\n",
            "<LTTextBoxHorizontal(14) 89.002,143.015,369.502,192.315 '    def __call__(self, inputs):\\n        mean_activities = tf.reduce_mean(inputs, axis=0)\\n        return self.weight * (\\n            kl_divergence(self.target, mean_activities) +\\n            kl_divergence(1. - self.target, 1. - mean_activities))\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.003,110.508,431.995,135.168 'Now  we  can  build  the  sparse  autoencoder,  using  the  KLDivergenceRegularizer  for\\nthe coding layer’s activations:\\n'>\n",
            "<LTTextBoxHorizontal(16) 339.390,40.500,402.516,49.500 'Sparse Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(17) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 420.984,40.500,432.000,49.500 '653\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 104.439,534.063,106.739,544.063>\n",
            "<LTCurve 132.736,534.063,135.036,544.063>\n",
            "<LTCurve 164.030,534.063,166.330,544.063>\n",
            "<LTCurve 170.575,534.063,172.875,544.063>\n",
            "<LTLine 192.006,538.492,210.921,538.492>\n",
            "<LTCurve 200.527,539.325,202.827,549.325>\n",
            "<LTCurve 207.072,539.325,209.372,549.325>\n",
            "<LTCurve 200.810,527.658,203.110,537.658>\n",
            "<LTCurve 207.355,527.658,209.655,537.658>\n",
            "<LTCurve 104.439,411.942,106.739,421.942>\n",
            "<LTCurve 128.644,411.942,130.944,421.942>\n",
            "<LTLine 168.508,416.371,175.029,416.371>\n",
            "<LTCurve 186.529,411.942,188.829,421.942>\n",
            "<LTCurve 210.194,411.942,212.494,421.942>\n",
            "<LTLine 231.625,416.371,253.190,416.371>\n",
            "<LTTextBoxHorizontal(0) 89.001,475.750,403.501,606.650 'kld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\\nsparse_kl_encoder = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(300, activation=\"sigmoid\",\\n                          activity_regularizer=kld_reg)\\n])\\nsparse_kl_decoder = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100, activation=\"relu\"),\\n    tf.keras.layers.Dense(28 * 28),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\nsparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,443.836,431.995,466.937 'After training this sparse autoencoder on Fashion MNIST, the coding layer will have\\nroughly 10% sparsity.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.994,368.636,432.003,431.497 'Variational Autoencoders\\nAn important category of autoencoders was introduced in 2013 by Diederik Kingma\\nand Max Welling6 and quickly became one of the most popular variants: variational\\nautoencoders (VAEs).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,337.436,431.997,360.536 'VAEs are quite different from all the autoencoders we have discussed so far, in these\\nparticular ways:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.653,289.636,432.001,325.336 '• They are probabilistic autoencoders, meaning that their outputs are partly deter‐\\n•\\nmined  by  chance,  even  after  training  (as  opposed  to  denoising  autoencoders,\\nwhich use randomness only during training).\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.658,273.036,431.995,283.536 '•\\n• Most importantly, they are generative autoencoders, meaning that they can gener‐\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.000,260.436,391.948,270.936 'ate new instances that look like they were sampled from the training set.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,124.436,432.004,248.336 'Both these properties make VAEs rather similar to RBMs, but they are easier to train,\\nand the sampling process is much faster (with RBMs you need to wait for the network\\nto  stabilize  into  a  “thermal  equilibrium”  before  you  can  sample  a  new  instance).  As\\ntheir name suggests, variational autoencoders perform variational Bayesian inference,\\nwhich is an efficient way of carrying out approximate Bayesian inference. Recall that\\nBayesian  inference  means  updating  a  probability  distribution  based  on  new  data,\\nusing equations derived from Bayes’ theorem. The original distribution is called the\\nprior,  while  the  updated  distribution  is  called  the  posterior.  In  our  case,  we  want  to\\nfind a good approximation of the data distribution. Once we have that, we can sample\\nfrom it.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,78.954,409.752,86.954 '6 Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes”, arXiv preprint arXiv:1312.6114\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,102.720,76.954 '(2013).\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,40.500,84.438,49.500 '654 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.906,40.500,263.196,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,469.036,432.005,605.537 'Let’s  take  a  look  at  how  VAEs  work.  Figure  17-11  (left)  shows  a  variational  autoen‐\\ncoder.  You  can  recognize  the  basic  structure  of  all  autoencoders,  with  an  encoder\\nfollowed by a decoder (in this example, they both have two hidden layers), but there\\nis  a  twist:  instead  of  directly  producing  a  coding  for  a  given  input,  the  encoder\\nproduces  a  mean  coding  μ  and  a  standard  deviation  σ.  The  actual  coding  is  then\\nsampled randomly from a Gaussian distribution with mean μ and standard deviation\\nσ.  After  that  the  decoder  decodes  the  sampled  coding  normally.  The  right  part  of\\nthe  diagram  shows  a  training  instance  going  through  this  autoencoder.  First,  the\\nencoder produces μ and σ, then a coding is sampled randomly (notice that it is not\\nexactly located at μ), and finally this coding is decoded; the final output resembles the\\ntraining instance.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.003,156.301,421.317,166.801 'Figure 17-11. A variational autoencoder (left) and an instance going through it (right)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,106.501,432.003,142.201 'As  you  can  see  in  the  diagram,  although  the  inputs  may  have  a  very  convoluted\\ndistribution, a variational autoencoder tends to produce codings that look as though\\nthey  were  sampled  from  a  simple  Gaussian  distribution:7  during  training,  the  cost\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,68.954,416.264,76.954 '7 Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions.\\n'>\n",
            "<LTTextBoxHorizontal(4) 326.121,40.500,402.513,49.500 'Variational Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.981,40.500,431.997,49.500 '655\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,462.775,432.500,462.775>\n",
            "<LTLine 432.375,172.364,432.375,462.900>\n",
            "<LTLine 72.000,172.489,432.500,172.489>\n",
            "<LTLine 72.125,172.364,72.125,462.900>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 108.250,178.344,396.250,457.650 matrix=[288.00,0.00,0.00,279.31, (108.25,178.34)]>\n",
            "<LTTextBoxHorizontal(0) 71.998,544.636,432.005,605.537 'function (discussed next) pushes the codings to gradually migrate within the coding\\nspace (also called the latent space) to end up looking like a cloud of Gaussian points.\\nOne great consequence is that after training a variational autoencoder, you can very\\neasily  generate  a  new  instance:  just  sample  a  random  coding  from  the  Gaussian\\ndistribution, decode it, and voilà!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,412.636,432.005,536.536 'Now, let’s look at the cost function. It is composed of two parts. The first is the usual\\nreconstruction loss that pushes the autoencoder to reproduce its inputs. We can use\\nthe  MSE  for  this,  as  we  did  earlier.  The  second  is  the  latent  loss  that  pushes  the\\nautoencoder  to  have  codings  that  look  as  though  they  were  sampled  from  a  simple\\nGaussian  distribution:  it  is  the  KL  divergence  between  the  target  distribution  (i.e.,\\nthe  Gaussian  distribution)  and  the  actual  distribution  of  the  codings.  The  math  is\\na  bit  more  complex  than  with  the  sparse  autoencoder,  in  particular  because  of  the\\nGaussian  noise,  which  limits  the  amount  of  information  that  can  be  transmitted  to\\nthe  coding  layer.  This  pushes  the  autoencoder  to  learn  useful  features.  Luckily,  the\\nequations simplify, so the latent loss can be computed using Equation 17-3.8\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.995,386.349,297.152,396.849 'Equation 17-3. Variational autoencoder’s latent loss\\n'>\n",
            "<LTTextBoxHorizontal(3) 87.002,359.674,117.672,371.124 'ℒ = −\\n'>\n",
            "<LTTextBoxHorizontal(4) 132.771,371.532,137.651,379.532 'n\\n'>\n",
            "<LTTextBoxHorizontal(5) 121.002,352.049,143.067,375.507 '1\\n2 ∑\\ni = 1\\n'>\n",
            "<LTTextBoxHorizontal(6) 147.621,359.280,186.687,371.124 '1 + log σi\\n'>\n",
            "<LTTextBoxHorizontal(7) 186.799,359.280,212.585,372.604 '2 − σi\\n'>\n",
            "<LTTextBoxHorizontal(8) 212.697,359.280,234.741,372.604 '2 − μi\\n'>\n",
            "<LTTextBoxHorizontal(9) 234.845,364.604,238.685,372.604 '2\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,287.482,432.000,337.304 'In  this  equation,  ℒ  is  the  latent  loss,  n  is  the  codings’  dimensionality,  and  μi  and  σi\\nare the mean and standard deviation of the ith component of the codings. The vectors\\nμ  and  σ  (which  contain  all  the  μi  and  σi)  are  output  by  the  encoder,  as  shown  in\\nFigure 17-11 (left).\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.998,243.682,432.004,279.382 'A common tweak to the variational autoencoder’s architecture is to make the encoder\\noutput γ = log(σ2) rather than σ. The latent loss can then be computed as shown in\\nEquation 17-4. This approach is more numerically stable and speeds up training.\\n'>\n",
            "<LTTextBoxHorizontal(12) 87.003,217.394,409.773,227.894 'Equation 17-4. Variational autoencoder’s latent loss, rewritten using γ = log(σ²)\\n'>\n",
            "<LTTextBoxHorizontal(13) 87.003,190.719,117.673,202.169 'ℒ = −\\n'>\n",
            "<LTTextBoxHorizontal(14) 132.772,202.577,137.652,210.577 'n\\n'>\n",
            "<LTTextBoxHorizontal(15) 121.003,183.095,143.060,206.552 '1\\n2 ∑\\ni = 1\\n'>\n",
            "<LTTextBoxHorizontal(16) 147.622,190.326,229.560,202.169 '1 + γi − exp γi − μi\\n'>\n",
            "<LTTextBoxHorizontal(17) 229.672,195.649,233.512,203.649 '2\\n'>\n",
            "<LTTextBoxHorizontal(18) 71.995,131.127,432.003,166.827 'Let’s  start  building  a  variational  autoencoder  for  Fashion  MNIST  (as  shown  in  Fig‐\\nure  17-11,  but  using  the  γ  tweak).  First,  we  will  need  a  custom  layer  to  sample  the\\ncodings, given μ and γ:\\n'>\n",
            "<LTTextBoxHorizontal(19) 73.140,78.954,420.424,86.954 '8 For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s\\n'>\n",
            "<LTTextBoxHorizontal(20) 80.000,68.954,146.120,76.954 'great tutorial (2016).\\n'>\n",
            "<LTTextBoxHorizontal(21) 71.997,40.500,84.435,49.500 '656 \\n'>\n",
            "<LTTextBoxHorizontal(22) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(23) 102.903,40.500,263.193,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTLine 120.449,365.544,126.360,365.544>\n",
            "<LTCurve 145.375,359.668,147.075,372.564>\n",
            "<LTCurve 175.679,359.668,177.979,372.564>\n",
            "<LTCurve 191.086,359.668,193.386,372.564>\n",
            "<LTCurve 239.236,359.668,240.946,372.564>\n",
            "<LTLine 120.449,196.589,126.360,196.589>\n",
            "<LTCurve 145.375,190.713,147.075,203.609>\n",
            "<LTCurve 196.483,190.713,198.783,203.609>\n",
            "<LTCurve 208.353,190.713,210.653,203.609>\n",
            "<LTCurve 234.061,190.713,235.771,203.609>\n",
            "<LTTextBoxHorizontal(0) 89.000,567.550,424.750,606.650 'class Sampling(tf.keras.layers.Layer):\\n    def call(self, inputs):\\n        mean, log_var = inputs\\n        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,484.050,432.005,559.703 'This Sampling layer takes two inputs: mean (μ) and log_var (γ). It uses the function\\ntf.random.normal()  to  sample  a  random  vector  (of  the  same  shape  as  γ)  from  the\\nGaussian distribution, with mean 0 and standard deviation 1. Then it multiplies it by\\nexp(γ / 2) (which is equal to σ, as you can verify mathematically), and finally it adds μ\\nand returns the result. This samples a codings vector from the Gaussian distribution\\nwith mean μ and standard deviation σ.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,452.850,432.004,475.950 'Next, we can create the encoder, using the functional API because the model is not\\nentirely sequential:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.003,437.363,161.253,445.863 'codings_size = 10\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.003,335.363,386.503,425.463 'inputs = tf.keras.layers.Input(shape=[28, 28])\\nZ = tf.keras.layers.Flatten()(inputs)\\nZ = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\\nZ = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\\ncodings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μ\\ncodings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γ\\ncodings = Sampling()([codings_mean, codings_log_var])\\nvariational_encoder = tf.keras.Model(\\n    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,250.084,432.005,327.516 'Note  that  the  Dense  layers  that  output  codings_mean  (μ)  and  codings_log_var\\n(γ)  have  the  same  inputs  (i.e.,  the  outputs  of  the  second  Dense  layer).  We  then\\npass  both  codings_mean  and  codings_log_var  to  the  Sampling  layer.  Finally,  the\\nvariational_encoder model has three outputs. Only the codings are required, but\\nwe add codings_mean and codings_log_var as well, in case we want to inspect their\\nvalues. Now let’s build the decoder:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.003,183.597,429.003,243.097 'decoder_inputs = tf.keras.layers.Input(shape=[codings_size])\\nx = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\\nx = tf.keras.layers.Dense(150, activation=\"relu\")(x)\\nx = tf.keras.layers.Dense(28 * 28)(x)\\noutputs = tf.keras.layers.Reshape([28, 28])(x)\\nvariational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,139.084,432.003,174.784 'For  this  decoder,  we  could  have  used  the  sequential  API  instead  of  the  functional\\nAPI, since it is really just a simple stack of layers, virtually identical to many of the\\ndecoders we have built so far. Finally, let’s build the variational autoencoder model:\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.997,103.197,407.747,132.097 '_, _, codings = variational_encoder(inputs)\\nreconstructions = variational_decoder(codings)\\nvariational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,71.284,431.999,94.384 'We ignore the first two outputs of the encoder (we only want to feed the codings to\\nthe decoder). Lastly, we must add the latent loss and the reconstruction loss:\\n'>\n",
            "<LTTextBoxHorizontal(10) 326.128,40.500,402.520,49.500 'Variational Autoencoders \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.988,40.500,432.004,49.500 '657\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,567.550,412.000,606.650 'latent_loss = -0.5 * tf.reduce_sum(\\n    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\\n    axis=-1)\\nvariational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,434.243,432.005,558.736 'We first apply Equation 17-4 to compute the latent loss for each instance in the batch,\\nsumming over the last axis. Then we compute the mean loss over all the instances in\\nthe batch, and we divide the result by 784 to ensure it has the appropriate scale com‐\\npared to the reconstruction loss. Indeed, the variational autoencoder’s reconstruction\\nloss  is  supposed  to  be  the  sum  of  the  pixel  reconstruction  errors,  but  when  Keras\\ncomputes  the  \"mse\"  loss  it  computes  the  mean  over  all  784  pixels,  rather  than  the\\nsum. So, the reconstruction loss is 784 times smaller than we need it to be. We could\\ndefine  a  custom  loss  to  compute  the  sum  rather  than  the  mean,  but  it  is  simpler  to\\ndivide the latent loss by 784 (the final loss will be 784 times smaller than it should be,\\nbut this just means that we should use a larger learning rate).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,415.643,291.363,426.143 'And finally, we can compile and fit the autoencoder!\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.004,379.757,399.254,408.657 'variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\\nhistory = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\\n                             validation_data=(X_valid, X_valid))\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,303.843,432.005,366.703 'Generating Fashion MNIST Images\\nNow  let’s  use  this  variational  autoencoder  to  generate  images  that  look  like  fashion\\nitems. All we need to do is sample random codings from a Gaussian distribution and\\ndecode them:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,278.156,322.750,296.856 'codings = tf.random.normal(shape=[3 * 7, codings_size])\\nimages = variational_decoder(codings).numpy()\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.004,258.843,259.377,269.343 'Figure 17-12 shows the 12 generated images.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,90.479,394.134,100.979 'Figure 17-12. Fashion MNIST images generated by the variational autoencoder\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,40.500,84.435,49.500 '658 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.903,40.500,263.193,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,252.581,432.500,252.581>\n",
            "<LTLine 432.375,106.543,432.375,252.706>\n",
            "<LTLine 72.000,106.668,432.500,106.668>\n",
            "<LTLine 72.125,106.543,72.125,252.706>\n",
            "<LTFigure(I1) 90.250,112.523,414.250,247.456 matrix=[324.00,0.00,0.00,134.93, (90.25,112.52)]>\n",
            "<LTTextBoxHorizontal(0) 71.998,582.437,432.001,605.537 'The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not\\ngreat, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,513.437,432.003,574.337 'Variational autoencoders make it possible to perform semantic interpolation: instead\\nof interpolating between two images at the pixel level, which would look as if the two\\nimages were just overlaid, we can interpolate at the codings level. For example, let’s\\ntake a few codings along an arbitrary line in latent space and decode them. We get a\\nsequence of images that gradually go from pants to sweaters (see Figure 17-13):\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,477.550,407.752,506.450 'codings = np.zeros([7, codings_size])\\ncodings[:, 3] = np.linspace(-0.8, 0.8, 7)  # axis 3 looks best in this case\\nimages = variational_decoder(codings).numpy()\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.005,408.379,222.029,418.879 'Figure 17-13. Semantic interpolation\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.002,371.179,432.005,394.279 'Let’s now turn our attention to GANs: they are harder to train, but when you manage\\nto get them to work, they produce pretty amazing images.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,258.179,432.005,358.840 'Generative Adversarial Networks\\nGenerative adversarial networks were proposed in a 2014 paper9 by Ian Goodfellow et\\nal., and although the idea got researchers excited almost instantly, it took a few years\\nto overcome some of the difficulties of training GANs. Like many great ideas, it seems\\nsimple  in  hindsight:  make  neural  networks  compete  against  each  other  in  the  hope\\nthat  this  competition  will  push  them  to  excel.  As  shown  in  Figure  17-14,  a  GAN  is\\ncomposed of two neural networks:\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,238.579,113.450,249.079 'Generator\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.996,150.379,432.005,236.479 'Takes  a  random  distribution  as  input  (typically  Gaussian)  and  outputs  some\\ndata—typically,  an  image.  You  can  think  of  the  random  inputs  as  the  latent\\nrepresentations (i.e., codings) of the image to be generated. So, as you can see, the\\ngenerator offers the same functionality as a decoder in a variational autoencoder,\\nand  it  can  be  used  in  the  same  way  to  generate  new  images:  just  feed  it  some\\nGaussian  noise,  and  it  outputs  a  brand-new  image.  However,  it  is  trained  very\\ndifferently, as you will soon see.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,78.954,408.912,86.954 '9 Ian Goodfellow et al., “Generative Adversarial Nets”, Proceedings of the 27th International Conference on\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,269.464,76.954 'Neural Information Processing Systems 2 (2014): 2672–2680.\\n'>\n",
            "<LTTextBoxHorizontal(10) 304.181,40.500,402.515,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.983,40.500,431.999,49.500 '659\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,470.575,432.500,470.575>\n",
            "<LTLine 432.375,424.443,432.375,470.700>\n",
            "<LTLine 72.000,424.568,432.500,424.568>\n",
            "<LTLine 72.125,424.443,72.125,470.700>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 108.250,430.423,396.250,465.450 matrix=[288.00,0.00,0.00,35.03, (108.25,430.42)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,129.666,605.537 'Discriminator\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.997,569.837,431.997,592.937 'Takes either a fake image from the generator or a real image from the training set\\nas input, and must guess whether the input image is fake or real.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.005,289.341,263.735,299.841 'Figure 17-14. A generative adversarial network\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,214.341,432.004,275.241 'During  training,  the  generator  and  the  discriminator  have  opposite  goals:  the  dis‐\\ncriminator  tries  to  tell  fake  images  from  real  images,  while  the  generator  tries  to\\nproduce  images  that  look  real  enough  to  trick  the  discriminator.  Because  the  GAN\\nis  composed  of  two  networks  with  different  objectives,  it  cannot  be  trained  like  a\\nregular neural network. Each training iteration is divided into two phases:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.652,128.741,432.005,202.241 '• In the first phase, we train the discriminator. A batch of real images is sampled\\n•\\nfrom  the  training  set  and  is  completed  with  an  equal  number  of  fake  images\\nproduced by the generator. The labels are set to 0 for fake images and 1 for real\\nimages, and the discriminator is trained on this labeled batch for one step, using\\nthe  binary  cross-entropy  loss.  Importantly,  backpropagation  only  optimizes  the\\nweights of the discriminator during this phase.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.654,74.341,432.005,122.641 '•\\n• In  the  second  phase,  we  train  the  generator.  We  first  use  it  to  produce  another\\nbatch  of  fake  images,  and  once  again  the  discriminator  is  used  to  tell  whether\\nthe  images  are  fake  or  real.  This  time  we  do  not  add  real  images  in  the  batch,\\nand  all  the  labels  are  set  to  1  (real):  in  other  words,  we  want  the  generator  to\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,40.500,84.434,49.500 '660 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.902,40.500,263.192,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,563.575,432.500,563.575>\n",
            "<LTLine 432.375,305.405,432.375,563.700>\n",
            "<LTLine 72.000,305.530,432.500,305.530>\n",
            "<LTLine 72.125,305.405,72.125,563.700>\n",
            "<LTFigure(I1) 144.250,311.385,360.250,558.450 matrix=[216.00,0.00,0.00,247.07, (144.25,311.38)]>\n",
            "<LTTextBoxHorizontal(0) 89.996,569.837,432.002,605.537 'produce images that the discriminator will (wrongly) believe to be real! Crucially,\\nthe weights of the discriminator are frozen during this step, so backpropagation\\nonly affects the weights of the generator.\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.788,475.705,396.000,542.905 'The generator never actually sees any real images, yet it gradually\\nlearns  to  produce  convincing  fake  images!  All  it  gets  is  the  gradi‐\\nents flowing back through the discriminator. Fortunately, the better\\nthe discriminator gets, the more information about the real images\\nis  contained  in  these  secondhand  gradients,  so  the  generator  can\\nmake significant progress.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,448.116,321.076,458.616 'Let’s go ahead and build a simple GAN for Fashion MNIST.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,365.923,432.004,440.016 'First, we need to build the generator and the discriminator. The generator is similar\\nto  an  autoencoder’s  decoder,  and  the  discriminator  is  a  regular  binary  classifier:\\nit  takes  an  image  as  input  and  ends  with  a  Dense  layer  containing  a  single  unit\\nand  using  the  sigmoid  activation  function.  For  the  second  phase  of  each  training\\niteration, we also need the full GAN model containing the generator followed by the\\ndiscriminator:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,350.437,161.247,358.937 'codings_size = 30\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,197.437,369.497,338.537 'Dense = tf.keras.layers.Dense\\ngenerator = tf.keras.Sequential([\\n    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\\n    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\\n    Dense(28 * 28, activation=\"sigmoid\"),\\n    tf.keras.layers.Reshape([28, 28])\\n])\\ndiscriminator = tf.keras.Sequential([\\n    tf.keras.layers.Flatten(),\\n    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\\n    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\\n    Dense(1, activation=\"sigmoid\")\\n])\\ngan = tf.keras.Sequential([generator, discriminator])\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,113.343,432.004,188.623 'Next,  we  need  to  compile  these  models.  As  the  discriminator  is  a  binary  classifier,\\nwe  can  naturally  use  the  binary  cross-entropy  loss.  The  gan  model  is  also  a  binary\\nclassifier, so it can use the binary cross-entropy loss as well. However, the generator\\nwill  only  be  trained  through  the  gan  model,  so  we  do  not  need  to  compile  it  at  all.\\nImportantly, the discriminator should not be trained during the second phase, so we\\nmake it non-trainable before compiling the gan model:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.997,77.457,386.497,106.357 'discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\\ndiscriminator.trainable = False\\ngan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\\n'>\n",
            "<LTTextBoxHorizontal(8) 304.181,40.500,402.515,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.983,40.500,431.999,49.500 '661\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,495.199,126.944,544.700 matrix=[37.94,0.00,0.00,49.50, (89.00,495.20)]>\n",
            "<LTTextBoxHorizontal(0) 136.790,547.855,396.001,606.588 'The trainable attribute is taken into account by Keras only when\\ncompiling a model, so after running this code, the discriminator\\nis  trainable  if  we  call  its  fit()  method  or  its  train_on_batch()\\nmethod (which we will be using), while it is not trainable when we\\ncall these methods on the gan model.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,493.880,432.003,531.733 'Since the training loop is unusual, we cannot use the regular fit() method. Instead,\\nwe  will  write  a  custom  training  loop.  For  this,  we  first  need  to  create  a  Dataset  to\\niterate through the images:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.999,457.994,424.749,486.894 'batch_size = 32\\ndataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=1000)\\ndataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,438.087,425.722,450.146 'We are now ready to write the training loop. Let’s wrap it in a train_gan() function:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.003,290.001,412.003,431.101 'def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\\n    generator, discriminator = gan.layers\\n    for epoch in range(n_epochs):\\n        for X_batch in dataset:\\n            # phase 1 - training the discriminator\\n            noise = tf.random.normal(shape=[batch_size, codings_size])\\n            generated_images = generator(noise)\\n            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\\n            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\\n            discriminator.train_on_batch(X_fake_and_real, y1)\\n            # phase 2 - training the generator\\n            noise = tf.random.normal(shape=[batch_size, codings_size])\\n            y2 = tf.constant([[1.]] * batch_size)\\n            gan.train_on_batch(noise, y2)\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.003,269.601,352.503,278.101 'train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,250.287,343.053,260.787 'As discussed earlier, you can see the two phases at each iteration:\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.650,176.694,432.003,238.187 '• In  phase  one  we  feed  Gaussian  noise  to  the  generator  to  produce  fake  images,\\n•\\nand  we  complete  this  batch  by  concatenating  an  equal  number  of  real  images.\\nThe targets y1 are set to 0 for fake images and 1 for real images. Then we train\\nthe discriminator on this batch. Remember that the discriminator is trainable in\\nthis phase, but we are not touching the generator.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.652,95.907,432.004,170.594 '• In phase two, we feed the GAN some Gaussian noise. Its generator will start by\\n•\\nproducing  fake  images,  then  the  discriminator  will  try  to  guess  whether  these\\nimages  are  fake  or  real.  In  this  phase,  we  are  trying  to  improve  the  generator,\\nwhich means that we want the discriminator to fail: this is why the targets y2 are\\nall set to 1, although the images are fake. In this phase, the discriminator is not\\ntrainable, so the only part of the gan model that will improve is the generator.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,40.500,84.434,49.500 '662 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.902,40.500,263.192,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,557.999,126.944,607.500 matrix=[37.94,0.00,0.00,49.50, (89.00,558.00)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'That’s  it!  After  training,  you  can  randomly  sample  some  codings  from  a  Gaussian\\ndistribution, and feed them to the generator to produce new images:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.998,556.750,343.998,575.450 'codings = tf.random.normal(shape=[batch_size, codings_size])\\ngenerated_images = generator.predict(codings)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.998,524.836,432.001,547.937 'If you display the generated images (see Figure 17-15), you will see that at the end of\\nthe first epoch, they already start to look like (very noisy) Fashion MNIST images.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,323.066,358.995,333.566 'Figure 17-15. Images generated by the GAN after one epoch of training\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,273.266,431.998,308.966 'Unfortunately, the images never really get much better than that, and you may even\\nfind epochs where the GAN seems to be forgetting what it learned. Why is that? Well,\\nit turns out that training a GAN can be challenging. Let’s see why.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,76.657,432.065,261.696 'The Difficulties of Training GANs\\nDuring training, the generator and the discriminator constantly try to outsmart each\\nother, in a zero-sum game. As training advances, the game may end up in a state that\\ngame  theorists  call  a  Nash  equilibrium,  named  after  the  mathematician  John  Nash:\\nthis is when no player would be better off changing their own strategy, assuming the\\nother players do not change theirs. For example, a Nash equilibrium is reached when\\neveryone drives on the left side of the road: no driver would be better off being the\\nonly one to switch sides. Of course, there is a second possible Nash equilibrium: when\\neveryone  drives  on  the  right  side  of  the  road.  Different  initial  states  and  dynamics\\nmay  lead  to  one  equilibrium  or  the  other.  In  this  example,  there  is  a  single  optimal\\nstrategy  once  an  equilibrium  is  reached  (i.e.,  driving  on  the  same  side  as  everyone\\nelse), but a Nash equilibrium can involve multiple competing strategies (e.g., a preda‐\\ntor chases its prey, the prey tries to escape, and neither would be better off changing\\ntheir strategy).\\n'>\n",
            "<LTTextBoxHorizontal(6) 304.183,40.500,402.517,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.985,40.500,432.001,49.500 '663\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,518.575,432.500,518.575>\n",
            "<LTLine 432.375,339.129,432.375,518.700>\n",
            "<LTLine 72.000,339.254,432.500,339.254>\n",
            "<LTLine 72.125,339.129,72.125,518.700>\n",
            "<LTFigure(I1) 79.450,345.109,425.050,513.450 matrix=[345.60,0.00,0.00,168.34, (79.45,345.11)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,519.436,432.005,605.537 'So how does this apply to GANs? Well, the authors of the GAN paper demonstrated\\nthat  a  GAN  can  only  reach  a  single  Nash  equilibrium:  that’s  when  the  generator\\nproduces perfectly realistic images, and the discriminator is forced to guess (50% real,\\n50%  fake).  This  fact  is  very  encouraging:  it  would  seem  that  you  just  need  to  train\\nthe  GAN  for  long  enough,  and  it  will  eventually  reach  this  equilibrium,  giving  you\\na  perfect  generator.  Unfortunately,  it’s  not  that  simple:  nothing  guarantees  that  the\\nequilibrium will ever be reached.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,374.836,432.005,511.336 'The  biggest  difficulty  is  called  mode  collapse:  this  is  when  the  generator’s  outputs\\ngradually become less diverse. How can this happen? Suppose that the generator gets\\nbetter at producing convincing shoes than any other class. It will fool the discrimina‐\\ntor  a  bit  more  with  shoes,  and  this  will  encourage  it  to  produce  even  more  images\\nof  shoes.  Gradually,  it  will  forget  how  to  produce  anything  else.  Meanwhile,  the\\nonly  fake  images  that  the  discriminator  will  see  will  be  shoes,  so  it  will  also  forget\\nhow to discriminate fake images of other classes. Eventually, when the discriminator\\nmanages  to  discriminate  the  fake  shoes  from  the  real  ones,  the  generator  will  be\\nforced to move to another class. It may then become good at shirts, forgetting about\\nshoes, and the discriminator will follow. The GAN may gradually cycle across a few\\nclasses, never really becoming very good at any of them.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,280.636,432.005,366.736 'Moreover, because the generator and the discriminator are constantly pushing against\\neach other, their parameters may end up oscillating and becoming unstable. Training\\nmay  begin  properly,  then  suddenly  diverge  for  no  apparent  reason,  due  to  these\\ninstabilities. And since many factors affect these complex dynamics, GANs are very\\nsensitive  to  the  hyperparameters:  you  may  have  to  spend  a  lot  of  effort  fine-tuning\\nthem.  In  fact,  that’s  why  I  used  RMSProp  rather  than  Nadam  when  compiling  the\\nmodels: when using Nadam, I ran into a severe mode collapse.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,136.036,432.004,272.536 'These problems have kept researchers very busy since 2014: many papers have been\\npublished on this topic, some proposing new cost functions10 (though a 2018 paper11\\nby Google researchers questions their efficiency) or techniques to stabilize training or\\nto avoid the mode collapse issue. For example, a popular technique called experience\\nreplay  consists  of  storing  the  images  produced  by  the  generator  at  each  iteration  in\\na replay buffer (gradually dropping older generated images) and training the discrim‐\\ninator  using  real  images  plus  fake  images  drawn  from  this  buffer  (rather  than  just\\nfake  images  produced  by  the  current  generator).  This  reduces  the  chances  that  the\\ndiscriminator will overfit the latest generator’s outputs. Another common technique\\nis  called  mini-batch  discrimination:  it  measures  how  similar  images  are  across  the\\nbatch  and  provides  this  statistic  to  the  discriminator,  so  it  can  easily  reject  a  whole\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,91.954,399.880,99.954 '10 For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee.\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,78.954,413.264,86.954 '11 Mario Lucic et al., “Are GANs Created Equal? A Large-Scale Study”, Proceedings of the 32nd International\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,302.352,76.954 'Conference on Neural Information Processing Systems (2018): 698–707.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,40.500,84.442,49.500 '664 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.910,40.500,263.200,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,107.900,162.000,107.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.003,605.537 'batch of fake images that lack diversity. This encourages the generator to produce a\\ngreater variety of images, reducing the chance of mode collapse. Other papers simply\\npropose specific architectures that happen to perform well.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,488.236,432.004,561.736 'In  short,  this  is  still  a  very  active  field  of  research,  and  the  dynamics  of  GANs  are\\nstill  not  perfectly  understood.  But  the  good  news  is  that  great  progress  has  been\\nmade, and some of the results are truly astounding! So let’s look at some of the most\\nsuccessful architectures, starting with deep convolutional GANs, which were the state\\nof  the  art  just  a  few  years  ago.  Then  we  will  look  at  two  more  recent  (and  more\\ncomplex) architectures.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.993,367.227,432.003,476.667 'Deep Convolutional GANs\\nThe authors of the original GAN paper experimented with convolutional layers, but\\nonly tried to generate small images. Soon after, many researchers tried to build GANs\\nbased  on  deeper  convolutional  nets  for  larger  images.  This  proved  to  be  tricky,  as\\ntraining  was  very  unstable,  but  Alec  Radford  et  al.  finally  succeeded  in  late  2015,\\nafter  experimenting  with  many  different  architectures  and  hyperparameters.  They\\ncalled  their  architecture  deep  convolutional  GANs  (DCGANs).12  Here  are  the  main\\nguidelines they proposed for building stable convolutional GANs:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.654,344.627,432.001,355.127 '•\\n• Replace any pooling layers with strided convolutions (in the discriminator) and\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.996,332.027,272.097,342.527 'transposed convolutions (in the generator).\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.654,315.427,431.996,325.927 '•\\n• Use  batch  normalization  in  both  the  generator  and  the  discriminator,  except  in\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.000,302.827,352.920,313.327 'the generator’s output layer and the discriminator’s input layer.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.658,286.227,354.527,296.727 '•\\n• Remove fully connected hidden layers for deeper architectures.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.652,269.627,432.000,280.127 '•\\n• Use ReLU activation in the generator for all layers except the output layer, which\\n'>\n",
            "<LTTextBoxHorizontal(9) 90.005,257.027,158.811,267.527 'should use tanh.\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.652,240.427,342.089,250.927 '•\\n• Use leaky ReLU activation in the discriminator for all layers.\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.997,180.027,432.005,228.327 'These  guidelines  will  work  in  many  cases,  but  not  always,  so  you  may  still  need\\nto  experiment  with  different  hyperparameters.  In  fact,  just  changing  the  random\\nseed and training the exact same model again will sometimes work. Here is a small\\nDCGAN that works reasonably well with Fashion MNIST:\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.780,78.954,429.520,86.954 '12 Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial\\n'>\n",
            "<LTTextBoxHorizontal(13) 80.000,68.954,244.528,76.954 'Networks”, arXiv preprint arXiv:1511.06434 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(14) 304.182,40.500,402.516,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(15) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 420.984,40.500,432.000,49.500 '665\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 89.000,598.150,165.500,606.650 'codings_size = 100\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,373.750,399.250,586.250 'generator = tf.keras.Sequential([\\n    tf.keras.layers.Dense(7 * 7 * 128),\\n    tf.keras.layers.Reshape([7, 7, 128]),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\\n                                    padding=\"same\", activation=\"relu\"),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\\n                                    padding=\"same\", activation=\"tanh\"),\\n])\\ndiscriminator = tf.keras.Sequential([\\n    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\\n                           activation=tf.keras.layers.LeakyReLU(0.2)),\\n    tf.keras.layers.Dropout(0.4),\\n    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\\n                           activation=tf.keras.layers.LeakyReLU(0.2)),\\n    tf.keras.layers.Dropout(0.4),\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\\n])\\ngan = tf.keras.Sequential([generator, discriminator])\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,253.636,432.005,364.937 'The  generator  takes  codings  of  size  100,  projects  them  to  6,272  dimensions  (7  *  7\\n*  128),  and  reshapes  the  result  to  get  a  7  ×  7  ×  128  tensor.  This  tensor  is  batch\\nnormalized  and  fed  to  a  transposed  convolutional  layer  with  a  stride  of  2,  which\\nupsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result\\nis  batch  normalized  again  and  fed  to  another  transposed  convolutional  layer  with  a\\nstride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64\\nto 1. This layer uses the tanh activation function, so the outputs will range from –1 to\\n1. For this reason, before training the GAN, we need to rescale the training set to that\\nsame range. We also need to reshape it to add the channel dimension:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.004,238.150,420.504,246.650 'X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,141.457,432.005,229.336 'The  discriminator  looks  much  like  a  regular  CNN  for  binary  classification,  except\\ninstead of using max pooling layers to downsample the image, we use strided convo‐\\nlutions  (strides=2).  Note  that  we  use  the  leaky  ReLU  activation  function.  Overall,\\nwe  respected  the  DCGAN  guidelines,  except  we  replaced  the  BatchNormalization\\nlayers  in  the  discriminator  with  Dropout  layers;  otherwise,  training  was  unstable  in\\nthis  case.  Feel  free  to  tweak  this  architecture:  you  will  see  how  sensitive  it  is  to  the\\nhyperparameters, especially the relative learning rates of the two networks.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,85.057,432.002,133.357 'Lastly,  to  build  the  dataset  and  then  compile  and  train  this  model,  we  can  use  the\\nsame code as earlier. After 50 epochs of training, the generator produces images like\\nthose shown in Figure 17-16. It’s still not perfect, but many of these images are pretty\\nconvincing.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,40.500,84.435,49.500 '666 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.903,40.500,263.193,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,411.866,372.384,422.366 'Figure 17-16. Images generated by the DCGAN after 50 epochs of training\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,185.666,432.005,397.766 'If you scale up this architecture and train it on a large dataset of faces, you can get\\nfairly realistic images. In fact, DCGANs can learn quite meaningful latent representa‐\\ntions, as you can see in Figure 17-17: many images were generated, and nine of them\\nwere picked manually (top left), including three representing men with glasses, three\\nmen without glasses, and three women without glasses. For each of these categories,\\nthe codings that were used to generate the images were averaged, and an image was\\ngenerated  based  on  the  resulting  mean  codings  (lower  left).  In  short,  each  of  the\\nthree lower-left images represents the mean of the three images located above it. But\\nthis  is  not  a  simple  mean  computed  at  the  pixel  level  (this  would  result  in  three\\noverlapping faces), it is a mean computed in the latent space, so the images still look\\nlike normal faces. Amazingly, if you compute men with glasses, minus men without\\nglasses,  plus  women  without  glasses—where  each  term  corresponds  to  one  of  the\\nmean codings—and you generate the image that corresponds to this coding, you get\\nthe image at the center of the 3 × 3 grid of faces on the right: a woman with glasses!\\nThe eight other images around it were generated based on the same vector plus a bit\\nof noise, to illustrate the semantic interpolation capabilities of DCGANs. Being able\\nto do arithmetic on faces feels like science fiction!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,129.266,432.005,177.566 'DCGANs  aren’t  perfect,  though.  For  example,  when  you  try  to  generate  very  large\\nimages using DCGANs, you often end up with locally convincing features but overall\\ninconsistencies, such as shirts with one sleeve much longer than the other, different\\nearrings, or eyes looking in opposite directions. How can you fix this?\\n'>\n",
            "<LTTextBoxHorizontal(3) 304.181,40.500,402.515,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(4) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 420.983,40.500,431.999,49.500 '667\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,427.929,432.375,607.500>\n",
            "<LTLine 72.000,428.054,432.500,428.054>\n",
            "<LTLine 72.125,427.929,72.125,607.500>\n",
            "<LTFigure(I1) 79.450,433.909,425.050,602.250 matrix=[345.60,0.00,0.00,168.34, (79.45,433.91)]>\n",
            "<LTTextBoxHorizontal(0) 72.003,389.766,418.482,412.866 'Figure 17-17. Vector arithmetic for visual concepts (part of figure 7 from the DCGAN\\npaper)13\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.789,317.154,396.000,372.834 'If  you  add  each  image’s  class  as  an  extra  input  to  both  the  gener‐\\nator  and  the  discriminator,  they  will  both  learn  what  each  class\\nlooks  like,  and  thus  you  will  be  able  to  control  the  class  of  each\\nimage  produced  by  the  generator.  This  is  called  a  conditional\\nGAN(CGAN).14\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,192.984,432.005,302.424 'Progressive Growing of GANs\\nIn  a  2018  paper,15  Nvidia  researchers  Tero  Kerras  et  al.  proposed  an  important\\ntechnique: they suggested generating small images at the beginning of training, then\\ngradually  adding  convolutional  layers  to  both  the  generator  and  the  discriminator\\nto  produce  larger  and  larger  images  (4  ×  4,  8  ×  8,  16  ×  16,  …,  512  ×  512,  1,024  ×\\n1,024). This approach resembles greedy layer-wise training of stacked autoencoders.\\nThe  extra  layers  get  added  at  the  end  of  the  generator  and  at  the  beginning  of  the\\ndiscriminator, and previously trained layers remain trainable.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,161.784,432.004,184.884 'For  example,  when  growing  the  generator’s  outputs  from  4  ×  4  to  8  ×  8  (see  Fig‐\\nure  17-18),  an  upsampling  layer  (using  nearest  neighbor  filtering)  is  added  to  the\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,114.954,256.920,122.954 '13 Reproduced with the kind authorization of the authors.\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.781,101.954,378.257,109.954 '14 Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.001,91.954,156.713,99.954 'arXiv:1411.1784 (2014).\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.779,78.954,426.543,86.954 '15 Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability, and Variation”, Proceedings\\n'>\n",
            "<LTTextBoxHorizontal(8) 79.999,68.954,291.631,76.954 'of the International Conference on Learning Representations (2018).\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.001,40.500,84.439,49.500 '668 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.907,40.500,263.197,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,418.429,432.375,607.500>\n",
            "<LTLine 72.000,418.554,432.500,418.554>\n",
            "<LTLine 72.125,418.429,72.125,607.500>\n",
            "<LTLine 72.000,130.900,162.000,130.900>\n",
            "<LTFigure(I1) 85.000,322.852,126.760,378.629 matrix=[41.76,0.00,0.00,55.78, (85.00,322.85)]>\n",
            "<LTFigure(I2) 79.569,424.409,424.931,602.250 matrix=[345.36,0.00,0.00,177.84, (79.57,424.41)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,443.243,432.005,605.537 'existing convolutional layer (“Conv 1”) to produce 8 × 8 feature maps. These are fed\\nto the new convolutional layer (“Conv 2”), which in turn feeds into a new output con‐\\nvolutional layer. To avoid breaking the trained weights of Conv 1, we gradually fade\\nin the two new convolutional layers (represented with dashed lines in Figure 17-18)\\nand  fade  out  the  original  output  layer.  The  final  outputs  are  a  weighted  sum  of  the\\nnew  outputs  (with  weight  α)  and  the  original  outputs  (with  weight  1  –  α),  slowly\\nincreasing  α  from  0  to  1.  A  similar  fade-in/fade-out  technique  is  used  when  a  new\\nconvolutional  layer  is  added  to  the  discriminator  (followed  by  an  average  pooling\\nlayer for downsampling). Note that all convolutional layers use \"same\" padding and\\nstrides of 1, so they preserve the height and width of their inputs. This includes the\\noriginal  convolutional  layer,  so  it  now  produces  8  ×  8  outputs  (since  its  inputs  are\\nnow 8 × 8). Lastly, the output layers use kernel size 1. They just project their inputs\\ndown to the desired number of color channels (typically 3).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,170.412,402.336,193.512 'Figure 17-18. A progressively growing GAN: a GAN generator outputs 4 × 4 color\\nimages (left); we extend it to output 8 × 8 images (right)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,133.212,431.996,156.312 'The paper also introduced several other techniques aimed at increasing the diversity\\nof the outputs (to avoid mode collapse) and making training more stable:\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.003,113.612,219.854,124.112 'Mini-batch standard deviation layer\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.999,75.219,432.005,111.512 'Added  near  the  end  of  the  discriminator.  For  each  position  in  the  inputs,  it\\ncomputes the standard deviation across all channels and all instances in the batch\\n(S = tf.math.reduce_std(inputs, axis=[0, -1])). These standard deviations\\n'>\n",
            "<LTTextBoxHorizontal(5) 304.181,40.500,402.515,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.983,40.500,431.999,49.500 '669\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,436.982,432.500,436.982>\n",
            "<LTLine 432.375,199.076,432.375,437.107>\n",
            "<LTLine 72.000,199.201,432.500,199.201>\n",
            "<LTLine 72.125,199.076,72.125,437.107>\n",
            "<LTFigure(I1) 151.570,205.056,352.930,431.857 matrix=[201.36,0.00,0.00,226.80, (151.57,205.06)]>\n",
            "<LTTextBoxHorizontal(0) 89.997,491.864,432.005,606.503 'are  then  averaged  across  all  points  to  get  a  single  value  (v  =  tf.reduce_\\nmean(S)). Finally, an extra feature map is added to each instance in the batch and\\nfilled  with  the  computed  value  (tf.concat([inputs,  tf.fill([batch_size,\\nheight, width, 1], v)], axis=-1)). How does this help? Well, if the gener‐\\nator  produces  images  with  little  variety,  then  there  will  be  a  small  standard\\ndeviation  across  feature  maps  in  the  discriminator.  Thanks  to  this  layer,  the\\ndiscriminator  will  have  easy  access  to  this  statistic,  making  it  less  likely  to  be\\nfooled  by  a  generator  that  produces  too  little  diversity.  This  will  encourage  the\\ngenerator to produce more diverse outputs, reducing the risk of mode collapse.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.005,472.264,166.925,482.764 'Equalized learning rate\\n'>\n",
            "<LTTextBoxHorizontal(2) 276.636,424.173,280.476,432.173 '2\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.995,291.595,432.004,470.163 'Initializes  all  weights  using  a  Gaussian  distribution  with  mean  0  and  standard\\ndeviation  1  rather  than  using  He  initialization.  However,  the  weights  are  scaled\\ndown  at  runtime  (i.e.,  every  time  the  layer  is  executed)  by  the  same  factor  as\\nninputs,  where  ninputs  is  the  number\\nin  He  initialization:  they  are  divided  by \\nof  inputs  to  the  layer.  The  paper  demonstrated  that  this  technique  significantly\\nimproved the GAN’s performance when using RMSProp, Adam, or other adap‐\\ntive gradient optimizers. Indeed, these optimizers normalize the gradient updates\\nby their estimated standard deviation (see Chapter 11), so parameters that have\\na larger dynamic range16 will take longer to train, while parameters with a small\\ndynamic range may be updated too quickly, leading to instabilities. By rescaling\\nthe  weights  as  part  of  the  model  itself  rather  than  just  rescaling  them  upon\\ninitialization,  this  approach  ensures  that  the  dynamic  range  is  the  same  for  all\\nparameters  throughout  training,  so  they  all  learn  at  the  same  speed.  This  both\\nspeeds up and stabilizes training.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,271.995,193.489,282.495 'Pixelwise normalization layer\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.995,182.608,432.004,269.895 'Added after each convolutional layer in the generator. It normalizes each activa‐\\ntion based on all the activations in the same image and at the same location, but\\nacross all channels (dividing by the square root of the mean squared activation).\\nIn TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),\\naxis=-1,  keepdims=True)  +  1e-8)  (the  smoothing  term  1e-8  is  needed  to\\navoid division by zero). This technique avoids explosions in the activations due\\nto excessive competition between the generator and the discriminator.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,113.608,432.005,174.508 'The  combination  of  all  these  techniques  allowed  the  authors  to  generate  extremely\\nconvincing high-definition images of faces. But what exactly do we call “convincing”?\\nEvaluation  is  one  of  the  big  challenges  when  working  with  GANs:  although  it  is\\npossible to automatically evaluate the diversity of the generated images, judging their\\nquality is a much trickier and subjective task. One technique is to use human raters,\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,68.954,389.304,76.954 '16 The dynamic range of a variable is the ratio between the highest and the lowest value it may take.\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.002,40.500,84.440,49.500 '670 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.908,40.500,263.198,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTCurve 270.215,417.458,307.139,434.327>\n",
            "<LTCurve 277.779,417.458,284.009,432.869>\n",
            "<LTTextBoxHorizontal(0) 71.995,557.236,432.003,605.537 'but  this  is  costly  and  time-consuming.  So,  the  authors  proposed  to  measure  the\\nsimilarity between the local image structure of the generated images and the training\\nimages, considering every scale. This idea led them to another groundbreaking inno‐\\nvation: StyleGANs.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,436.227,432.004,545.667 'StyleGANs\\nThe  state  of  the  art  in  high-resolution  image  generation  was  advanced  once  again\\nby  the  same  Nvidia  team  in  a  2018  paper17  that  introduced  the  popular  StyleGAN\\narchitecture. The authors used style transfer techniques in the generator to ensure that\\nthe  generated  images  have  the  same  local  structure  as  the  training  images,  at  every\\nscale,  greatly  improving  the  quality  of  the  generated  images.  The  discriminator  and\\nthe  loss  function  were  not  modified,  only  the  generator.  A  StyleGAN  generator  is\\ncomposed of two networks (see Figure 17-19):\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,416.627,144.293,427.127 'Mapping network\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.996,327.834,432.002,414.527 'An  eight-layer  MLP  that  maps  the  latent  representations  z  (i.e.,  the  codings)\\nto  a  vector  w.  This  vector  is  then  sent  through  multiple  affine  transformations\\n(i.e.,  Dense  layers  with  no  activation  functions,  represented  by  the  “A”  boxes  in\\nFigure 17-19), which produces multiple vectors. These vectors control the style of\\nthe generated image at different levels, from fine-grained texture (e.g., hair color)\\nto high-level features (e.g., adult or child). In short, the mapping network maps\\nthe codings to multiple style vectors.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,308.234,144.968,318.734 'Synthesis network\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.995,182.234,432.005,306.134 'Responsible  for  generating  the  images.  It  has  a  constant  learned  input  (to  be\\nclear, this input will be constant after training, but during training it keeps getting\\ntweaked by backpropagation). It processes this input through multiple convolu‐\\ntional and upsampling layers, as earlier, but there are two twists. First, some noise\\nis added to the input and to all the outputs of the convolutional layers (before the\\nactivation function). Second, each noise layer is followed by an adaptive instance\\nnormalization (AdaIN) layer: it standardizes each feature map independently (by\\nsubtracting the feature map’s mean and dividing by its standard deviation), then\\nit uses the style vector to determine the scale and offset of each feature map (the\\nstyle vector contains one scale and one bias term for each feature map).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,78.954,431.832,86.954 '17 Tero Karras et al., “A Style-Based Generator Architecture for Generative Adversarial Networks”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,160.552,76.954 'arXiv:1812.04948 (2018).\\n'>\n",
            "<LTTextBoxHorizontal(8) 304.178,40.500,402.512,49.500 'Generative Adversarial Networks \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.980,40.500,431.996,49.500 '671\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 72.005,287.074,415.523,310.174 'Figure 17-19. StyleGAN’s generator architecture (part of Figure 1 from the StyleGAN\\npaper)18\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,111.274,432.005,272.974 'The  idea  of  adding  noise  independently  from  the  codings  is  very  important.  Some\\nparts of an image are quite random, such as the exact position of each freckle or hair.\\nIn  earlier  GANs,  this  randomness  had  to  either  come  from  the  codings  or  be  some\\npseudorandom  noise  produced  by  the  generator  itself.  If  it  came  from  the  codings,\\nit meant that the generator had to dedicate a significant portion of the codings’ repre‐\\nsentational power to storing noise, which this is quite wasteful. Moreover, the noise\\nhad to be able to flow through the network and reach the final layers of the generator:\\nthis  seems  like  an  unnecessary  constraint  that  probably  slowed  down  training.  And\\nfinally, some visual artifacts may appear because the same noise was used at different\\nlevels.  If  instead  the  generator  tried  to  produce  its  own  pseudorandom  noise,  this\\nnoise might not look very convincing, leading to more visual artifacts. Plus, part of\\nthe generator’s weights would be dedicated to generating pseudorandom noise, which\\nagain  seems  wasteful.  By  adding  extra  noise  inputs,  all  these  issues  are  avoided;  the\\n'>\n",
            "<LTTextBoxHorizontal(2) 69.780,68.954,256.920,76.954 '18 Reproduced with the kind authorization of the authors.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,40.500,84.434,49.500 '672 \\n'>\n",
            "<LTTextBoxHorizontal(4) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 102.902,40.500,263.192,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,315.737,432.375,607.500>\n",
            "<LTLine 72.000,315.862,432.500,315.862>\n",
            "<LTLine 72.125,315.737,72.125,607.500>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 126.250,321.717,378.250,602.250 matrix=[252.00,0.00,0.00,280.53, (126.25,321.72)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,582.437,431.999,605.537 'GAN is able to use the provided noise to add the right amount of stochasticity to each\\npart of the image.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,526.036,432.004,574.337 'The  added  noise  is  different  for  each  level.  Each  noise  input  consists  of  a  single\\nfeature  map  full  of  Gaussian  noise,  which  is  broadcast  to  all  feature  maps  (of  the\\ngiven level) and scaled using learned per-feature scaling factors (this is represented by\\nthe “B” boxes in Figure 17-19) before it is added.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,419.236,432.003,517.936 'Finally,  StyleGAN  uses  a  technique  called  mixing  regularization  (or  style  mixing),\\nwhere a percentage of the generated images are produced using two different codings.\\nSpecifically, the codings c1 and c2 are sent through the mapping network, giving two\\nstyle vectors w1 and w2. Then the synthesis network generates an image based on the\\nstyles w1 for the first levels and the styles w2 for the remaining levels. The cutoff level\\nis picked randomly. This prevents the network from assuming that styles at adjacent\\nlevels  are  correlated,  which  in  turn  encourages  locality  in  the  GAN,  meaning  that\\neach style vector only affects a limited number of traits in the generated image.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,299.836,432.005,411.136 'There is such a wide variety of GANs out there that it would require a whole book to\\ncover them all. Hopefully this introduction has given you the main ideas, and most\\nimportantly the desire to learn more. Go ahead and implement your own GAN, and\\ndo not get discouraged if it has trouble learning at first: unfortunately, this is normal,\\nand  it  will  require  quite  a  bit  of  patience  to  get  it  working,  but  the  result  is  worth\\nit.  If  you’re  struggling  with  an  implementation  detail,  there  are  plenty  of  Keras  or\\nTensorFlow  implementations  that  you  can  look  at.  In  fact,  if  all  you  want  is  to  get\\nsome  amazing  results  quickly,  then  you  can  just  use  a  pretrained  model  (e.g.,  there\\nare pretrained StyleGAN models available for Keras).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,268.636,432.002,291.736 'Now  that  we’ve  examined  autoencoders  and  GANs,  let’s  look  at  one  last  type  of\\narchitecture: diffusion models.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,130.436,432.005,256.296 'Diffusion Models\\nThe ideas behind diffusion models have been around for many years, but they were\\nfirst  formalized  in  their  modern  form  in  a  2015  paper19  by  Jascha  Sohl-Dickstein\\net  al.  from  Stanford  University  and  UC  Berkeley.  The  authors  applied  tools  from\\nthermodynamics to model a diffusion process, similar to a drop of milk diffusing in\\na cup of tea. The core idea is to train a model to learn the reverse process: start from\\nthe completely mixed state, and gradually “unmix” the milk from the tea. Using this\\nidea, they obtained promising results in image generation, but since GANs produced\\nmore convincing images back then, diffusion models did not get as much attention.\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,78.954,421.744,86.954 '19 Jascha Sohl-Dickstein et al., “Deep Unsupervised Learning using Nonequilibrium Thermodynamics”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,188.896,76.954 'preprint arXiv:1503.03585 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(8) 351.778,40.500,402.520,49.500 'Diffusion Models \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.988,40.500,432.004,49.500 '673\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,506.837,432.005,605.537 'Then, in 2020, Jonathan Ho et al., also from UC Berkeley, managed to build a diffu‐\\nsion model capable of generating highly realistic images, which they called a denoising\\ndiffusion probabilistic model (DDPM).20 A few months later, a 2021 paper21 by OpenAI\\nresearchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM architecture and\\nproposed several improvements that allowed DDPMs to finally beat GANs: not only\\nare  DDPMs  much  easier  to  train  than  GANs,  but  the  generated  images  are  more\\ndiverse and of even higher quality. The main downside of DDPMs, as you will see, is\\nthat they take a very long time to generate images, compared to GANs or VAEs.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,362.236,432.004,498.737 'So how exactly does a DDPM work? Well, suppose you start with a picture of a cat\\n(like the one you’ll see in Figure 17-20), noted x0, and at each time step t you add a\\nlittle  bit  of  Gaussian  noise  to  the  image,  with  mean  0  and  variance  βt.  This  noise  is\\nindependent for each pixel: we call it isotropic. You first obtain the image x1, then x2,\\nand so on, until the cat is completely hidden by the noise, impossible to see. The last\\ntime step is noted T. In the original DDPM paper, the authors used T = 1,000, and\\nthey scheduled the variance βt in such a way that the cat signal fades linearly between\\ntime steps 0 and T. In the improved DDPM paper, T was bumped up to 4,000, and\\nthe  variance  schedule  was  tweaked  to  change  more  slowly  at  the  beginning  and  at\\nthe end. In short, we’re gradually drowning the cat in noise: this is called the forward\\nprocess.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,214.864,432.005,354.136 'As we add more and more Gaussian noise in the forward process, the distribution of\\npixel values becomes more and more Gaussian. One important detail I left out is that\\nthe pixel values get rescaled slightly at each step, by a factor of  1 − βt. This ensures\\nthat the mean of the pixel values gradually approaches 0, since the scaling factor is a\\nbit smaller than 1 (imagine repeatedly multiplying a number by 0.99). It also ensures\\nthat the variance will gradually converge to 1. This is because the standard deviation\\nof  the  pixel  values  also  gets  scaled  by  1 − βt,  so  the  variance  gets  scaled  by  1  –  βt\\n(i.e., the square of the scaling factor). But the variance cannot shrink to 0 since we’re\\nadding Gaussian noise with variance βt at each step. And since variances add up when\\nyou sum Gaussian distributions, you can see that the variance can only converge to\\n1 – βt + βt = 1.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,133.264,432.003,207.352 'The forward diffusion process is summarized in Equation 17-5. This equation won’t\\nteach you anything new about the forward process, but it’s useful to understand this\\ntype of mathematical notation, as it’s often used in ML papers. This equation defines\\nthe probability distribution q of xt given xt–1 as a Gaussian distribution with mean xt–1\\ntimes the scaling factor, and with a covariance matrix equal to βtI. This is the identity\\nmatrix I multiplied by βt, which means that the noise is isotropic with variance βt.\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,81.954,304.840,89.954 '20 Jonathan Ho et al., “Denoising Diffusion Probabilistic Models” (2020).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.779,68.954,391.551,76.954 '21 Alex Nichol and Prafulla Dhariwal, “Improved Denoising Diffusion Probabilistic Models” (2021).\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.001,40.500,84.439,49.500 '674 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.907,40.500,263.197,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,97.900,162.000,97.900>\n",
            "<LTCurve 341.889,317.208,372.899,330.900>\n",
            "<LTCurve 237.927,265.715,268.937,279.408>\n",
            "<LTTextBoxHorizontal(0) 87.005,596.349,382.810,606.849 'Equation 17-5. Probability distribution q of the forward diffusion process\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.000,575.145,229.026,587.691 'q xt xt − 1 = N 1 − βtxt − 1 , βtI\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,497.978,432.003,558.878 'Interestingly,  there’s  a  shortcut  for  the  forward  process:  it’s  possible  to  sample  an\\nimage xt given x0 without having to first compute x1, x2, …, xt–1. Indeed, since the sum\\nof multiple Gaussian distributions is also a Gaussian distribution, all the noise can be\\nadded in just one shot using Equation 17-6. This is the equation we will be using, as it\\nis much faster.\\n'>\n",
            "<LTTextBoxHorizontal(3) 86.997,471.690,318.259,482.190 'Equation 17-6. Shortcut for the forward diffusion process\\n'>\n",
            "<LTTextBoxHorizontal(4) 86.999,450.487,212.595,463.033 'q xt x0 = N αtx0 , 1 − αt I\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,348.119,432.005,434.219 'Our  goal,  of  course,  is  not  to  drown  cats  in  noise.  On  the  contrary,  we  want  to\\ncreate many new cats! We can do so by training a model that can perform the reverse\\nprocess: going from xt to xt–1. We can then use it to remove a tiny bit of noise from an\\nimage, and repeat the operation many times until all the noise is gone. If we train the\\nmodel on a dataset containing many cat images, then we can give it a picture entirely\\nfull of Gaussian noise, and the model will gradually make a brand new cat appear (see\\nFigure 17-20).\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.999,185.465,304.627,195.965 'Figure 17-20. The forward process q and reverse process p\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,69.432,432.005,171.365 'OK, so let’s start coding! The first thing we need to do is to code the forward process.\\nFor this, we will first need to implement the variance schedule. How can we control\\nhow  fast  the  cat  disappears?  Initially,  100%  of  the  variance  comes  from  the  original\\ncat image. Then at each time step t, the variance gets multiplied by 1 – βt, as explained\\nearlier, and noise gets added. So, the part of the variance that comes from the initial\\ndistribution shrinks by a factor of 1 – βt at each step. If we define αt = 1 – βt, then after\\nt time steps, the cat signal will have been multiplied by a factor of α̅t = α1×α2×…×αt =\\nαt. It’s this “cat signal” factor α̅t that we want to schedule so it shrinks down\\nαt = ∏i = 1\\n'>\n",
            "<LTTextBoxHorizontal(8) 101.414,77.135,104.182,85.135 't\\n'>\n",
            "<LTTextBoxHorizontal(9) 351.776,40.500,402.518,49.500 'Diffusion Models \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.986,40.500,432.002,49.500 '675\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,341.858,432.500,341.858>\n",
            "<LTLine 432.375,201.528,432.375,341.983>\n",
            "<LTLine 72.000,201.653,432.500,201.653>\n",
            "<LTLine 72.125,201.528,72.125,341.983>\n",
            "<LTCurve 92.729,575.841,95.029,589.525>\n",
            "<LTCurve 106.069,575.841,106.709,589.525>\n",
            "<LTCurve 129.784,575.841,132.084,589.525>\n",
            "<LTCurve 155.729,575.841,158.029,589.525>\n",
            "<LTCurve 158.489,575.841,188.167,589.072>\n",
            "<LTCurve 229.477,575.841,231.777,589.525>\n",
            "<LTCurve 92.729,451.183,95.029,464.866>\n",
            "<LTCurve 106.069,451.183,106.709,464.866>\n",
            "<LTCurve 118.569,451.183,120.869,464.866>\n",
            "<LTCurve 144.513,451.183,146.813,464.866>\n",
            "<LTCurve 147.273,451.183,162.449,464.414>\n",
            "<LTCurve 154.901,461.365,159.429,461.915>\n",
            "<LTCurve 178.395,451.183,180.695,464.866>\n",
            "<LTCurve 197.721,461.365,202.249,461.915>\n",
            "<LTCurve 205.718,451.183,208.018,464.866>\n",
            "<LTCurve 213.050,451.183,215.350,464.866>\n",
            "<LTFigure(I1) 79.450,207.508,425.050,336.733 matrix=[345.60,0.00,0.00,129.22, (79.45,207.51)]>\n",
            "<LTCurve 73.598,80.681,78.352,81.259>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'from  1  to  0  gradually  between  time  steps  0  and  T.  In  the  improved  DDPM  paper,\\nthe authors schedule α̅t according to Equation 17-7. This schedule is represented in\\nFigure 17-21.\\n'>\n",
            "<LTTextBoxHorizontal(1) 86.999,543.549,398.565,554.049 'Equation 17-7. Variance schedule equations for the forward diffusion process\\n'>\n",
            "<LTTextBoxHorizontal(2) 87.005,516.662,119.703,528.505 'βt = 1 −\\n'>\n",
            "<LTTextBoxHorizontal(3) 123.035,509.554,144.145,535.609 'αt\\nαt − 1\\n'>\n",
            "<LTTextBoxHorizontal(4) 145.255,516.662,187.639,528.505 ', with\\xa0αt =\\n'>\n",
            "<LTTextBoxHorizontal(5) 192.599,510.650,205.059,533.767 'f t\\nf 0\\n'>\n",
            "<LTTextBoxHorizontal(6) 209.629,517.055,269.619,528.505 '\\xa0and\\xa0 f t = cos\\n'>\n",
            "<LTTextBoxHorizontal(7) 324.771,528.029,328.611,536.029 '2\\n'>\n",
            "<LTTextBoxHorizontal(8) 274.189,510.650,310.659,534.339 't/T + s\\n1 + s ·\\n'>\n",
            "<LTTextBoxHorizontal(9) 313.989,511.222,319.899,533.767 'π\\n2\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,482.787,150.851,493.287 'In these equations:\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.651,459.599,432.003,470.687 '•\\n• s is a tiny value which prevents βt from being too small near t = 0. In the paper,\\n'>\n",
            "<LTTextBoxHorizontal(12) 89.997,447.587,201.098,458.087 'the authors used s = 0.008.\\n'>\n",
            "<LTTextBoxHorizontal(13) 80.655,430.399,385.112,441.487 '•\\n• βt is clipped to be no larger than 0.999, to avoid instabilities near t = T.\\n'>\n",
            "<LTTextBoxHorizontal(14) 72.002,235.915,389.493,248.526 'Figure 17-21. Noise variance schedule βt, and the remaining signal variance α̅t\\n'>\n",
            "<LTTextBoxHorizontal(15) 71.996,211.315,402.034,223.926 'Let’s create a small function to compute αt, βt, and α̅t, and call it with T = 4,000:\\n'>\n",
            "<LTTextBoxHorizontal(16) 88.996,124.167,372.735,204.917 'def variance_schedule(T, s=0.008, max_beta=0.999):\\n    t = np.arange(T + 1)\\n    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\\n    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\\n    alpha = np.append(1, alpha).astype(np.float32)  # add α₀ = 1\\n    beta = 1 - alpha\\n    alpha_cumprod = np.cumprod(alpha)\\n    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\\n'>\n",
            "<LTTextBoxHorizontal(17) 88.996,94.417,297.246,113.117 'T = 4000\\nalpha, alpha_cumprod, beta = variance_schedule(T)\\n'>\n",
            "<LTTextBoxHorizontal(18) 72.003,40.500,84.441,49.500 '676 \\n'>\n",
            "<LTTextBoxHorizontal(19) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 102.909,40.500,263.199,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,420.725,432.500,420.725>\n",
            "<LTLine 432.375,252.567,432.375,420.850>\n",
            "<LTLine 72.000,252.692,432.500,252.692>\n",
            "<LTLine 72.125,252.567,72.125,420.850>\n",
            "<LTLine 122.483,522.925,144.699,522.925>\n",
            "<LTCurve 130.578,533.941,135.106,534.491>\n",
            "<LTCurve 124.560,520.433,129.089,520.983>\n",
            "<LTCurve 171.516,526.838,176.045,527.388>\n",
            "<LTLine 190.975,522.925,209.070,522.925>\n",
            "<LTCurve 198.007,523.759,200.307,533.759>\n",
            "<LTCurve 204.992,523.759,207.292,533.759>\n",
            "<LTCurve 197.495,511.310,199.795,521.731>\n",
            "<LTCurve 205.505,511.310,207.805,521.731>\n",
            "<LTCurve 235.269,511.310,237.569,534.541>\n",
            "<LTCurve 242.254,511.310,244.554,534.541>\n",
            "<LTCurve 270.319,511.310,272.619,534.541>\n",
            "<LTLine 273.635,522.925,305.622,522.925>\n",
            "<LTLine 313.438,522.925,320.755,522.925>\n",
            "<LTCurve 321.761,511.310,324.061,534.541>\n",
            "<LTFigure(I1) 79.450,258.547,425.050,415.600 matrix=[345.60,0.00,0.00,157.05, (79.45,258.55)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.243,432.000,605.537 'To train our model to reverse the diffusion process, we will need noisy images from\\ndifferent  time  steps  of  the  forward  process.  For  this,  let’s  create  a  prepare_batch()\\nfunction that will take a batch of clean images from the dataset and prepare them:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.003,451.757,424.753,562.257 'def prepare_batch(X):\\n    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # scale from –1 to +1\\n    X_shape = tf.shape(X)\\n    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\\n    alpha_cm = tf.gather(alpha_cumprod, t)\\n    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\\n    noise = tf.random.normal(X_shape)\\n    return {\\n        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\\n        \"time\": t,\\n    }, noise\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.005,432.443,181.866,442.943 'Let’s go through this code:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.654,384.643,432.002,420.343 '•\\n• For  simplicity  we  will  use  Fashion  MNIST,  so  the  function  must  first  add  a\\nchannel axis. It will also help to scale the pixel values from –1 to 1, so it’s closer to\\nthe final Gaussian distribution with mean 0 and variance 1.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.652,367.450,432.000,379.509 '• Next,  the  function  creates  t,  a  vector  containing  a  random  time  step  for  each\\n•\\n'>\n",
            "<LTTextBoxHorizontal(5) 90.005,354.850,243.830,365.350 'image in the batch, between 1 and T.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.652,311.864,432.746,349.716 '• Then it uses tf.gather() to get the value of alpha_cumprod for each of the time\\n•\\nsteps in the vector t. This gives us the vector alpha_cm, containing one value of α̅t\\nfor each image.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.654,294.670,432.005,306.730 '• The next line reshapes the alpha_cm from [batch size] to [batch size, 1, 1, 1]. This\\n•\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.999,281.477,367.461,293.536 'is needed to ensure alpha_cm can be broadcasted with the batch X.\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.657,264.877,374.622,275.377 '•\\n• Then we generate some Gaussian noise with mean 0 and variance 1.\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.651,196.691,432.005,258.777 '• Lastly,  we  use  Equation  17-6  to  apply  the  diffusion  process  to  the  images.  Note\\n•\\nthat  x  **  0.5  is  equal  to  the  square  root  of  x.  The  function  returns  a  tuple\\ncontaining  the  inputs  and  the  targets.  The  inputs  are  represented  as  a  Python\\ndict containing the noisy images and the time steps used to generate them. The\\ntargets are the Gaussian noise used to generate each image.\\n'>\n",
            "<LTTextBoxHorizontal(11) 136.792,125.599,396.003,169.759 'With  this  setup,  the  model  will  predict  the  noise  that  should  be\\nsubtracted  from  the  input  image  to  get  the  original  image.  Why\\nnot  predict  the  original  image  directly?  Well,  the  authors  tried:  it\\nsimply doesn’t work as well.\\n'>\n",
            "<LTTextBoxHorizontal(12) 351.773,40.500,402.515,49.500 'Diffusion Models \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.983,40.500,431.999,49.500 '677\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,122.054,126.944,171.554 matrix=[37.94,0.00,0.00,49.50, (89.00,122.05)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,569.243,431.997,605.537 'Next,  we’ll  create  a  training  dataset  and  a  validation  set  that  will  apply  the\\nprepare_batch()  function  to  every  batch.  As  earlier,  X_train  and  X_valid  contain\\nthe Fashion MNIST images with pixel values ranging from 0 to 1:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.001,512.957,352.501,562.257 'def prepare_dataset(X, batch_size=32, shuffle=False):\\n    ds = tf.data.Dataset.from_tensor_slices(X)\\n    if shuffle:\\n        ds = ds.shuffle(buffer_size=10_000)\\n    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,482.357,365.251,501.057 'train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\\nvalid_set = prepare_dataset(X_valid, batch_size=32)\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,437.843,432.000,473.543 'Now we’re ready to build the actual diffusion model itself. It can be any model you\\nwant, as long as it takes the noisy images and time steps as inputs, and predicts the\\nnoise to subtract from the input images:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,371.357,416.250,430.857 'def build_diffusion_model():\\n    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\\n    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\\n    [...]  # build the model based on the noisy images and the time steps\\n    outputs = [...]  # predict the noise (same shape as the input images)\\n    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,212.850,432.005,363.163 'The DDPM authors used a modified U-Net architecture,22 which has many similari‐\\nties with the FCN architecture we discussed in Chapter 14 for semantic segmentation:\\nit’s  a  convolutional  neural  network  that  gradually  downsamples  the  input  images,\\nthen gradually upsamples them again, with skip connections crossing over from each\\nlevel of the downsampling part to the corresponding level in the upsampling part. To\\ntake into account the time steps, they encoded them using the same technique as the\\npositional encodings in the transformer architecture (see Chapter 16). At every level\\nin the U-Net architecture, they passed these time encodings through Dense layers and\\nfed  them  to  the  U-Net.  Lastly,  they  also  used  multi-head  attention  layers  at  various\\nlevels.  See  this  chapter’s  notebook  for  a  basic  implementation,  or  https://homl.info/\\nddpmcode for the official implementation: it’s based on TF 1.x, which is deprecated,\\nbut it’s quite readable.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.999,181.650,432.002,204.750 'WE can now train the model normally. The authors noted that using the MAE loss\\nworked better than the MSE. You can also use the Huber loss:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.000,145.763,382.250,174.663 'model = build_diffusion_model()\\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\\nhistory = model.fit(train_set, validation_data=valid_set, epochs=100)\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.780,78.954,431.664,86.954 '22 Olaf Ronneberger et al., “U-Net: Convolutional Networks for Biomedical Image Segmentation”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.000,68.954,160.552,76.954 'arXiv:1505.04597 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.002,40.500,84.440,49.500 '678 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.908,40.500,263.198,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,532.036,432.005,605.537 'Once  the  model  is  trained,  you  can  use  it  to  generate  new  images.  Unfortunately,\\nthere’s  no  shortcut  in  the  reverse  diffusion  process,  so  you  have  to  sample  xT\\nrandomly  from  a  Gaussian  distribution  with  mean  0  and  variance  1,  then  pass  it\\nto the model to predict the noise; subtract it from the image using Equation 17-8, and\\nyou get xT–1. Repeat the process 3,999 more times until you get x0: if all went well, it\\nshould look like a regular Fashion MNIST image!\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.002,505.749,346.583,516.249 'Equation 17-8. Going one step in reverse in the diffusion process\\n'>\n",
            "<LTTextBoxHorizontal(2) 87.003,476.751,116.097,489.297 'xt − 1 =\\n'>\n",
            "<LTTextBoxHorizontal(3) 124.967,469.660,134.501,493.681 '1\\nαt\\n'>\n",
            "<LTTextBoxHorizontal(4) 139.331,477.454,155.827,489.297 'xt −\\n'>\n",
            "<LTTextBoxHorizontal(5) 165.263,469.660,189.127,496.401 'βt\\n1 − αt\\n'>\n",
            "<LTTextBoxHorizontal(6) 190.497,477.454,255.171,489.297 'ϵθ xt, t + βtz\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.998,404.390,432.005,454.212 'In this equation, ϵθ(xt, t) represents the noise predicted by the model given the input\\nimage  xt  and  the  time  step  t.  The  θ  represents  the  model  parameters.  Moreover,\\nz  is  Gaussian  noise  with  mean  0  and  variance  1.  This  makes  the  reverse  process\\nstochastic: if you run it multiple times, you will get different images.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,373.190,432.002,396.290 'Let’s  write  a  function  that  implements  this  reverse  process,  and  call  it  to  generate  a\\nfew images:\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.003,255.703,420.503,366.203 'def generate(model, batch_size=32):\\n    X = tf.random.normal([batch_size, 28, 28, 1])\\n    for t in range(T, 0, -1):\\n        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\\n        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\\n        X = (\\n            1 / alpha[t] ** 0.5\\n            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\\n            + (1 - alpha[t]) ** 0.5 * noise\\n        )\\n    return X\\n'>\n",
            "<LTTextBoxHorizontal(10) 89.003,235.303,271.753,243.803 'X_gen = generate(model)  # generated images\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.995,152.990,432.003,226.490 'This  may  take  a  minute  or  two.  That’s  the  main  drawback  of  diffusion  models:\\ngenerating images is slow since the model needs to be called many times. It’s possible\\nto make this faster by using a smaller T value, or by using the same model prediction\\nfor several steps at a time, but the resulting images may not look as nice. That said,\\ndespite  this  speed  limitation,  diffusion  models  do  produce  high-quality  and  diverse\\nimages, as you can see in Figure 17-22.\\n'>\n",
            "<LTTextBoxHorizontal(12) 351.774,40.500,402.516,49.500 'Diffusion Models \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.984,40.500,432.000,49.500 '679\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 119.425,483.717,135.312,483.717>\n",
            "<LTCurve 119.981,469.653,134.756,482.884>\n",
            "<LTCurve 136.567,469.653,138.867,498.925>\n",
            "<LTLine 158.606,483.717,189.937,483.717>\n",
            "<LTCurve 159.161,469.653,189.381,482.884>\n",
            "<LTCurve 181.833,479.836,186.362,480.386>\n",
            "<LTCurve 200.187,469.653,202.488,498.925>\n",
            "<LTCurve 219.597,469.653,221.897,498.925>\n",
            "<LTCurve 223.057,469.653,225.357,498.925>\n",
            "<LTCurve 236.312,477.447,250.945,490.678>\n",
            "<LTTextBoxHorizontal(0) 72.003,416.625,258.031,427.125 'Figure 17-22. Images generated by the DDPM\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,291.225,432.005,402.525 'Diffusion  models  have  made  tremendous  progress  recently.  In  particular,  a  paper\\npublished in December 2021 by Robin Rombach, Andreas Blattmann, et al.,23 intro‐\\nduced latent diffusion models, where the diffusion process takes place in latent space,\\nrather  than  in  pixel  space.  To  achieve  this,  a  powerful  autoencoder  is  used  to  com‐\\npress  each  training  image  into  a  much  smaller  latent  space,  where  the  diffusion\\nprocess  takes  place,  then  the  autoencoder  is  used  to  decompress  the  final  latent\\nrepresentation,  generating  the  output  image.  This  considerably  speeds  up  image\\ngeneration, and reduces training time and cost dramatically. Importantly, the quality\\nof the generated images is outstanding.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,197.025,432.005,283.125 'Moreover,  the  researchers  also  adapted  various  conditioning  techniques  to  guide\\nthe  diffusion  process  using  text  prompts,  images,  or  any  other  inputs.  This  makes\\nit  possible  to  quickly  produce  a  beautiful,  high-resolution  image  of  a  salamander\\nreading a book, or anything else you might fancy. You can also condition the image\\ngeneration  process  using  an  input  image.  This  enables  many  applications,  such  as\\noutpainting—where an input image is extended beyond its borders—or inpainting—\\nwhere holes in an image are filled in.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,128.025,432.005,188.925 'Lastly,  a  powerful  pretrained  latent  diffusion  model  named  Stable  Diffusion  was\\nopen  sourced  in  August  2022  by  a  collaboration  between  LMU  Munich  and  a  few\\ncompanies,  including  StabilityAI,  and  Runway,  with  support  from  EleutherAI  and\\nLAION. In September 2022, it was ported to TensorFlow and included in KerasCV,\\na computer vision library built by the Keras team. Now anyone can generate mind‐\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,78.954,430.480,86.954 '23 Robin Rombach, Andreas Blattmann, et al., “High-Resolution Image Synthesis with Latent Diffusion Models”,\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,68.954,208.272,76.954 'arXiv preprint arXiv:2112.10752 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.002,40.500,84.440,49.500 '680 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.908,40.500,263.198,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,432.689,432.375,607.500>\n",
            "<LTLine 72.000,432.814,432.500,432.814>\n",
            "<LTLine 72.125,432.689,72.125,607.500>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 79.454,438.669,425.046,602.250 matrix=[345.59,0.00,0.00,163.58, (79.45,438.67)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'blowing images in seconds, for free, even on a regular laptop (see the last exercise in\\nthis chapter). The possibilities are endless!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,551.236,431.997,574.337 'In the next chapter we will move on to an entirely different branch of deep learning:\\ndeep reinforcement learning.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.004,519.977,127.647,538.897 'Exercises\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.313,497.236,325.068,507.736 '1. What are the main tasks that autoencoders are used for?\\n1.\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.313,455.436,432.005,491.136 '2. Suppose you want to train a classifier, and you have plenty of unlabeled training\\n2.\\ndata  but  only  a  few  thousand  labeled  instances.  How  can  autoencoders  help?\\nHow would you proceed?\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.312,438.836,432.004,449.336 '3.\\n3. If  an  autoencoder  perfectly  reconstructs  the  inputs,  is  it  necessarily  a  good\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.998,426.236,395.579,436.736 'autoencoder? How can you evaluate the performance of an autoencoder?\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.317,384.436,432.002,420.136 '4.\\n4. What are undercomplete and overcomplete autoencoders? What is the main risk\\nof  an  excessively  undercomplete  autoencoder?  What  about  the  main  risk  of  an\\novercomplete autoencoder?\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.315,367.836,427.760,378.336 '5.\\n5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\\n'>\n",
            "<LTTextBoxHorizontal(9) 77.315,351.236,414.331,361.736 '6.\\n6. What is a generative model? Can you name a type of generative autoencoder?\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.315,334.636,371.827,345.136 '7.\\n7. What is a GAN? Can you name a few tasks where GANs can shine?\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.315,318.036,309.352,328.536 '8.\\n8. What are the main difficulties when training GANs?\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.315,301.436,368.782,311.936 '9.\\n9. What are diffusion models good at? What is their main limitation?\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.275,247.036,432.001,295.336 '10. Try  using  a  denoising  autoencoder  to  pretrain  an  image  classifier.  You  can  use\\n10.\\nMNIST (the simplest option), or a more complex image dataset such as CIFAR10\\nif you want a bigger challenge. Regardless of the dataset you’re using, follow these\\nsteps:\\n'>\n",
            "<LTTextBoxHorizontal(14) 90.742,230.436,431.998,240.936 'a.\\na. Split  the  dataset  into  a  training  set  and  a  test  set.  Train  a  deep  denoising\\n'>\n",
            "<LTTextBoxHorizontal(15) 103.002,217.836,253.383,228.336 'autoencoder on the full training set.\\n'>\n",
            "<LTTextBoxHorizontal(16) 90.322,201.236,432.002,211.736 'b.\\nb. Check that the images are fairly well reconstructed. Visualize the images that\\n'>\n",
            "<LTTextBoxHorizontal(17) 102.995,188.636,295.218,199.136 'most activate each neuron in the coding layer.\\n'>\n",
            "<LTTextBoxHorizontal(18) 90.913,146.836,431.999,182.536 'c. Build a classification DNN, reusing the lower layers of the autoencoder. Train\\nc.\\nit using only 500 images from the training set. Does it perform better with or\\nwithout pretraining?\\n'>\n",
            "<LTTextBoxHorizontal(19) 72.272,105.036,432.003,140.736 '11. Train a variational autoencoder on the image dataset of your choice, and use it to\\n11.\\ngenerate images. Alternatively, you can try to find an unlabeled dataset that you\\nare interested in and see if you can generate new samples.\\n'>\n",
            "<LTTextBoxHorizontal(20) 374.622,40.500,402.513,49.500 'Exercises \\n'>\n",
            "<LTTextBoxHorizontal(21) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(22) 420.981,40.500,431.997,49.500 '681\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.276,569.837,432.002,605.537 '12. Train a DCGAN to tackle the image dataset of your choice, and use it to generate\\n12.\\nimages.  Add  experience  replay  and  see  if  this  helps.  Turn  it  into  a  conditional\\nGAN where you can control the generated class.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.275,528.037,432.000,563.737 '13.\\n13. Go through KerasCV’s excellent Stable Diffusion tutorial, and generate a beauti‐\\nful  drawing  of  a  salamander  reading  a  book.  If  you  post  your  best  drawing  on\\nTwitter, please tag me at @aureliengeron. I’d love to see your creations!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,492.837,431.997,515.937 'Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\\nhttps://homl.info/colab3.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.003,40.500,84.441,49.500 '682 \\n'>\n",
            "<LTTextBoxHorizontal(4) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 102.909,40.500,263.199,49.500 'Chapter 17: Autoencoders, GANs, and Diffusion Models\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 230.848,533.502,432.002,582.331 'CHAPTER 18\\nReinforcement Learning\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,242.668,432.004,416.969 'Reinforcement  learning  (RL)  is  one  of  the  most  exciting  fields  of  machine  learning\\ntoday, and also one of the oldest. It has been around since the 1950s, producing many\\ninteresting  applications  over  the  years,1  particularly  in  games  (e.g.,  TD-Gammon,\\na  Backgammon-playing  program)  and  in  machine  control,  but  seldom  making  the\\nheadline  news.  However,  a  revolution  took  place  in  2013,  when  researchers  from  a\\nBritish startup called DeepMind demonstrated a system that could learn to play just\\nabout  any  Atari  game  from  scratch,2  eventually  outperforming  humans3  in  most  of\\nthem, using only raw pixels as inputs and without any prior knowledge of the rules\\nof  the  games.4  This  was  the  first  of  a  series  of  amazing  feats,  culminating  with  the\\nvictory of their system AlphaGo against Lee Sedol, a legendary professional player of\\nthe game of Go, in March 2016 and against Ke Jie, the world champion, in May 2017.\\nNo program had ever come close to beating a master of this game, let alone the world\\nchampion. Today the whole field of RL is boiling with new ideas, with a wide range of\\napplications.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,186.268,432.003,234.568 'So how did DeepMind (bought by Google for over $500 million in 2014) achieve all\\nthis? With hindsight it seems rather simple: they applied the power of deep learning\\nto the field of reinforcement learning, and it worked beyond their wildest dreams. In\\nthis chapter I will first explain what reinforcement learning is and what it’s good at,\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,147.954,403.104,155.954 '1 For more details, be sure to check out Richard Sutton and Andrew Barto’s book on RL, Reinforcement\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,137.954,204.664,145.954 'Learning: An Introduction (MIT Press).\\n'>\n",
            "<LTTextBoxHorizontal(5) 73.140,124.954,421.552,132.954 '2 Volodymyr Mnih et al., “Playing Atari with Deep Reinforcement Learning”, arXiv preprint arXiv:1312.5602\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,114.954,102.720,122.954 '(2013).\\n'>\n",
            "<LTTextBoxHorizontal(7) 73.137,101.954,424.605,109.954 '3 Volodymyr Mnih et al., “Human-Level Control Through Deep Reinforcement Learning”, Nature 518 (2015):\\n'>\n",
            "<LTTextBoxHorizontal(8) 79.997,91.954,109.021,99.954 '529–533.\\n'>\n",
            "<LTTextBoxHorizontal(9) 73.138,78.954,427.062,86.954 '4 Check out the videos of DeepMind’s system learning to play Space Invaders, Breakout, and other video games\\n'>\n",
            "<LTTextBoxHorizontal(10) 79.998,68.954,160.174,76.954 'at https://homl.info/dqn3.\\n'>\n",
            "<LTTextBoxHorizontal(11) 420.984,40.500,432.000,49.500 '683\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTLine 72.000,163.900,162.000,163.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'then present two of the most important techniques in deep reinforcement learning:\\npolicy  gradients  and  deep  Q-networks,  including  a  discussion  of  Markov  decision\\nprocesses. Let’s get started!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,444.236,432.005,557.497 'Learning to Optimize Rewards\\nIn  reinforcement  learning,  a  software  agent  makes  observations  and  takes  actions\\nwithin  an  environment,  and  in  return  it  receives  rewards  from  the  environment.  Its\\nobjective is to learn to act in a way that will maximize its expected rewards over time.\\nIf you don’t mind a bit of anthropomorphism, you can think of positive rewards as\\npleasure, and negative rewards as pain (the term “reward” is a bit misleading in this\\ncase).  In  short,  the  agent  acts  in  the  environment  and  learns  by  trial  and  error  to\\nmaximize its pleasure and minimize its pain.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.002,413.036,431.901,436.136 'This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few\\nexamples (see Figure 18-1):\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.743,327.436,432.004,400.936 'a. The agent can be the program controlling a robot. In this case, the environment\\na.\\nis  the  real  world,  the  agent  observes  the  environment  through  a  set  of  sensors\\nsuch  as  cameras  and  touch  sensors,  and  its  actions  consist  of  sending  signals\\nto  activate  motors.  It  may  be  programmed  to  get  positive  rewards  whenever  it\\napproaches the target destination, and negative rewards whenever it wastes time\\nor goes in the wrong direction.\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.325,273.036,432.002,321.336 'b.\\nb. The agent can be the program controlling Ms. Pac-Man. In this case, the environ‐\\nment is a simulation of the Atari game, the actions are the nine possible joystick\\npositions (upper left, down, center, and so on), the observations are screenshots,\\nand the rewards are just the game points.\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.912,256.436,432.005,266.936 'c.\\nc. Similarly, the agent can be the program playing a board game such as Go. It only\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.999,243.836,186.494,254.336 'gets a reward if it wins.\\n'>\n",
            "<LTTextBoxHorizontal(7) 76.808,176.836,432.003,237.736 'd. The agent does not have to control a physically (or virtually) moving thing. For\\nd.\\nexample,  it  can  be  a  smart  thermostat,  getting  positive  rewards  whenever  it  is\\nclose  to  the  target  temperature  and  saves  energy,  and  negative  rewards  when\\nhumans  need  to  tweak  the  temperature,  so  the  agent  must  learn  to  anticipate\\nhuman needs.\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.892,160.236,431.996,170.736 'e.\\ne. The agent can observe stock market prices and decide how much to buy or sell\\n'>\n",
            "<LTTextBoxHorizontal(9) 90.001,147.636,374.246,158.136 'every second. Rewards are obviously the monetary gains and losses.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.998,87.236,432.005,135.536 'Note  that  there  may  not  be  any  positive  rewards  at  all;  for  example,  the  agent  may\\nmove  around  in  a  maze,  getting  a  negative  reward  at  every  time  step,  so  it  had\\nbetter find the exit as quickly as possible! There are many other examples of tasks to\\nwhich reinforcement learning is well suited, such as self-driving cars, recommender\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.996,40.500,84.434,49.500 '684 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.002,582.437,432.005,605.537 'systems,  placing  ads  on  a  web  page,  or  controlling  where  an  image  classification\\nsystem should focus its attention.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,254.409,415.058,277.509 'Figure 18-1. Reinforcement learning examples: (a) robotics, (b) Ms. Pac-Man, (c) Go\\nplayer, (d) thermostat, (e) automatic trader5\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,179.209,431.999,242.069 'Policy Search\\nThe algorithm a software agent uses to determine its actions is called its policy. The\\npolicy  could  be  a  neural  network  taking  observations  as  inputs  and  outputting  the\\naction to take (see Figure 18-2).\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,68.954,412.104,96.954 '5 Images (a), (d), and (e) are in the public domain. Image (b) is a screenshot from the Ms. Pac-Man game,\\ncopyright Atari (fair use in this chapter). Image (c) is reproduced from Wikipedia; it was created by user\\nStevertigo and released under Creative Commons BY-SA 2.0.\\n'>\n",
            "<LTTextBoxHorizontal(4) 362.517,40.500,402.513,49.500 'Policy Search \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.981,40.500,431.997,49.500 '685\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,576.175,432.500,576.175>\n",
            "<LTLine 432.375,283.073,432.375,576.300>\n",
            "<LTLine 72.000,283.198,432.500,283.198>\n",
            "<LTLine 72.125,283.073,72.125,576.300>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTFigure(I1) 79.456,289.053,425.044,571.050 matrix=[345.59,0.00,0.00,282.00, (79.46,289.05)]>\n",
            "<LTTextBoxHorizontal(0) 72.001,488.286,341.368,498.786 'Figure 18-2. Reinforcement learning using a neural network policy\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,362.886,432.004,474.186 'The policy can be any algorithm you can think of, and it does not have to be deter‐\\nministic. In fact, in some cases it does not even have to observe the environment! For\\nexample,  consider  a  robotic  vacuum  cleaner  whose  reward  is  the  amount  of  dust  it\\npicks  up  in  30  minutes.  Its  policy  could  be  to  move  forward  with  some  probability\\np every second, or randomly rotate left or right with probability 1 – p. The rotation\\nangle  would  be  a  random  angle  between  –r  and  +r.  Since  this  policy  involves  some\\nrandomness, it is called a stochastic policy. The robot will have an erratic trajectory,\\nwhich guarantees that it will eventually get to any place it can reach and pick up all\\nthe dust. The question is, how much dust will it pick up in 30 minutes?\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,268.686,432.003,354.786 'How  would  you  train  such  a  robot?  There  are  just  two  policy  parameters  you  can\\ntweak: the probability p and the angle range r. One possible learning algorithm could\\nbe  to  try  out  many  different  values  for  these  parameters,  and  pick  the  combination\\nthat performs best (see Figure 18-3). This is an example of policy search, in this case\\nusing a brute-force approach. When the policy space is too large (which is generally\\nthe case), finding a good set of parameters this way is like searching for a needle in a\\ngigantic haystack.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,187.086,432.005,260.586 'Another  way  to  explore  the  policy  space  is  to  use  genetic  algorithms.  For  example,\\nyou could randomly create a first generation of 100 policies and try them out, then\\n“kill” the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An\\noffspring is a copy of its parent7 plus some random variation. The surviving policies\\nplus  their  offspring  together  constitute  the  second  generation.  You  can  continue  to\\niterate through generations this way until you find a good policy.8\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.140,134.954,431.824,142.954 '6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,124.954,97.920,132.954 'pool”.\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.139,111.954,418.191,119.954 '7 If there is a single parent, this is called asexual reproduction. With two (or more) parents, it is called sexual\\n'>\n",
            "<LTTextBoxHorizontal(7) 79.999,91.954,430.463,109.954 'reproduction. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of\\nits parents’ genomes.\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.139,78.954,411.439,86.954 '8 One interesting example of a genetic algorithm used for reinforcement learning is the NeuroEvolution of\\n'>\n",
            "<LTTextBoxHorizontal(9) 79.999,68.954,215.767,76.954 'Augmenting Topologies (NEAT) algorithm.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,40.500,84.434,49.500 '686 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,504.350,432.375,607.500>\n",
            "<LTLine 72.000,504.475,432.500,504.475>\n",
            "<LTLine 72.125,504.350,72.125,607.500>\n",
            "<LTLine 72.000,150.900,162.000,150.900>\n",
            "<LTFigure(I1) 114.970,510.330,389.530,602.250 matrix=[274.56,0.00,0.00,91.92, (114.97,510.33)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,404.167,428.543,427.267 'Figure 18-3. Four points in the policy space (left) and the agent’s corresponding behavior\\n(right)\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,278.766,432.004,390.066 'Yet another approach is to use optimization techniques, by evaluating the gradients of\\nthe rewards with regard to the policy parameters, then tweaking these parameters by\\nfollowing the gradients toward higher rewards.9 We will discuss this approach, called\\npolicy gradients (PG), in more detail later in this chapter. Going back to the vacuum\\ncleaner robot, you could slightly increase p and evaluate whether doing so increases\\nthe  amount  of  dust  picked  up  by  the  robot  in  30  minutes;  if  it  does,  then  increase\\np  some  more,  or  else  reduce  p.  We  will  implement  a  popular  PG  algorithm  using\\nTensorFlow, but before we do, we need to create an environment for the agent to live\\nin—so it’s time to introduce OpenAI Gym.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,127.966,432.004,266.427 'Introduction to OpenAI Gym\\nOne  of  the  challenges  of  reinforcement  learning  is  that  in  order  to  train  an  agent,\\nyou first need to have a working environment. If you want to program an agent that\\nwill learn to play an Atari game, you will need an Atari game simulator. If you want\\nto  program  a  walking  robot,  then  the  environment  is  the  real  world,  and  you  can\\ndirectly train your robot in that environment. However, this has its limits: if the robot\\nfalls  off  a  cliff,  you  can’t  just  click  Undo.  You  can’t  speed  up  time  either—adding\\nmore  computing  power  won’t  make  the  robot  move  any  faster—and  it’s  generally\\ntoo expensive to train 1,000 robots in parallel. In short, training is hard and slow in\\nthe  real  world,  so  you  generally  need  a  simulated  environment  at  least  for  bootstrap\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,78.954,425.568,86.954 '9 This is called gradient ascent. It’s just like gradient descent, but in the opposite direction: maximizing instead\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,68.954,127.624,76.954 'of minimizing.\\n'>\n",
            "<LTTextBoxHorizontal(5) 318.553,40.500,402.514,49.500 'Introduction to OpenAI Gym \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.982,40.500,431.998,49.500 '687\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,432.830,432.375,607.500>\n",
            "<LTLine 72.000,432.955,432.500,432.955>\n",
            "<LTLine 72.125,432.830,72.125,607.500>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 79.450,438.810,425.050,602.250 matrix=[345.60,0.00,0.00,163.44, (79.45,438.81)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,582.437,432.002,605.537 'training.  For  example,  you  might  use  a  library  like  PyBullet  or  MuJoCo  for  3D\\nphysics simulation.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,538.637,432.000,574.957 'OpenAI  Gym10  is  a  toolkit  that  provides  a  wide  variety  of  simulated  environments\\n(Atari games, board games, 2D and 3D physical simulations, and so on), that you can\\nuse to train agents, compare them, or develop new RL algorithms.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,469.637,432.004,530.537 'OpenAI  Gym  is  preinstalled  on  Colab,  but  it’s  an  older  version,  so  you’ll  need  to\\nreplace it with the latest one. You also need to install a few of its dependencies. If you\\nare coding on your own machine instead of Colab, and you followed the installation\\ninstructions  at  https://homl.info/install,  then  you  can  skip  this  step;  otherwise,  enter\\nthese commands:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,433.750,386.497,462.650 '# Only run these commands on Colab or Kaggle!\\n%pip install -q -U gym\\n%pip install -q -U gym[classic_control,box2d,atari,accept-rom-license]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,311.857,432.005,425.903 'The first %pip command upgrades Gym to the latest version. The -q option stands for\\nquiet: it makes the output less verbose. The -U option stands for upgrade. The second\\n%pip command installs the libraries required to run various kinds of environments.\\nThis  includes  classic  environments  from  control  theory–the  science  of  controlling\\ndynamical systems–such as balancing a pole on a cart. It also includes environments\\nbased on the Box2D library—a 2D physics engine for games. Lastly, it includes envi‐\\nronments  based  on  the  Arcade  Learning  Environment  (ALE),  which  is  an  emulator\\nfor Atari 2600 games. Several Atari game ROMs are downloaded automatically, and\\nby running this code you agree with Atari’s ROM licenses.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.001,280.657,432.004,303.757 'With  that,  you’re  ready  to  use  OpenAI  Gym.  Let’s  import  it  and  make  an\\nenvironment:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.998,265.170,131.498,273.670 'import gym\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,244.770,318.498,253.270 'env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,200.257,432.000,235.957 'Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart\\ncan  be  accelerated  left  or  right  in  order  to  balance  a  pole  placed  on  top  of  it  (see\\nFigure 18-4). This is a classic control task.\\n'>\n",
            "<LTTextBoxHorizontal(9) 136.788,161.662,395.998,184.208 'The gym.envs.registry dictionary contains the names and speci‐\\nfications of all the available environments.\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.780,78.954,407.520,86.954 '10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.000,68.954,364.480,76.954 'promote and develop friendly AIs that will benefit humanity (rather than exterminate it).\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.999,40.500,84.437,49.500 '688 \\n'>\n",
            "<LTTextBoxHorizontal(13) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 102.905,40.500,210.158,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 85.000,133.343,126.760,189.120 matrix=[41.76,0.00,0.00,55.78, (85.00,133.34)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,469.916,231.642,480.416 'Figure 18-4. The CartPole environment\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,330.137,432.005,456.782 'After  the  environment  is  created,  you  must  initialize  it  using  the  reset()  method,\\noptionally specifying a random seed. This returns the first observation. Observations\\ndepend on the type of environment. For the CartPole environment, each observation\\nis a 1D NumPy array containing four floats representing the cart’s horizontal position\\n(0.0  =  center),  its  velocity  (positive  means  right),  the  angle  of  the  pole  (0.0  =\\nvertical),  and  its  angular  velocity  (positive  means  clockwise).  The  reset()  method\\nalso  returns  a  dictionary  that  may  contain  extra  environment-specific  information.\\nThis can be useful for debugging or for training. For example, in many Atari environ‐\\nments,  it  contains  the  number  of  lives  left.  However,  in  the  CartPole  environment,\\nthis dictionary is empty.\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.996,273.850,403.496,323.150 '>>> obs, info = env.reset(seed=42)\\n>>> obs\\narray([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)\\n>>> info\\n{}\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,228.150,432.002,266.002 'Let’s  call  the  render()  method  to  render  this  environment  as  an  image.  Since  we\\nset  render_mode=\"rgb_array\"  when  creating  the  environment,  the  image  will  be\\nreturned as a NumPy array:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,192.263,356.750,221.163 '>>> img = env.render()\\n>>> img.shape  # height, width, channels (3 = Red, Green, Blue)\\n(400, 600, 3)\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,172.357,403.799,184.416 'You can then use Matplotlib’s imshow() function to display this image, as usual.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.999,153.757,307.399,164.257 'Now let’s ask the environment what actions are possible:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.000,128.070,174.000,146.770 '>>> env.action_space\\nDiscrete(2)\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,69.770,432.004,120.223 'Discrete(2)  means  that  the  possible  actions  are  integers  0  and  1,  which  represent\\naccelerating  left  or  right.  Other  environments  may  have  additional  discrete  actions,\\nor other kinds of actions (e.g., continuous). Since the pole is leaning toward the right\\n(obs[2] > 0), let’s accelerate the cart toward the right:\\n'>\n",
            "<LTTextBoxHorizontal(9) 318.556,40.500,402.517,49.500 'Introduction to OpenAI Gym \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.985,40.500,432.001,49.500 '689\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,485.980,432.375,607.500>\n",
            "<LTLine 72.000,486.105,432.500,486.105>\n",
            "<LTLine 72.125,485.980,72.125,607.500>\n",
            "<LTFigure(I1) 162.250,491.960,342.250,602.250 matrix=[180.00,0.00,0.00,110.29, (162.25,491.96)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,485.950,403.500,606.650 '>>> action = 1  # accelerate right\\n>>> obs, reward, done, truncated, info = env.step(action)\\n>>> obs\\narray([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)\\n>>> reward\\n1.0\\n>>> done\\nFalse\\n>>> truncated\\nFalse\\n>>> info\\n{}\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,466.043,370.689,478.102 'The step() method executes the desired action and returns five values:\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.998,447.934,86.960,457.909 'obs\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.998,393.670,432.004,444.716 'This is the new observation. The cart is now moving toward the right (obs[1] >\\n0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is\\nnow negative (obs[3] < 0), so it will likely be tilted toward the left after the next\\nstep.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.001,375.561,101.926,385.536 'reward\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.998,348.277,431.998,371.377 'In this environment, you get a reward of 1.0 at every step, no matter what you do,\\nso the goal is to keep the episode running for as long as possible.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.998,330.168,91.948,340.143 'done\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.995,289.690,432.000,326.950 'This value will be True when the episode is over. This will happen when the pole\\ntilts too much, or goes off the screen, or after 200 steps (in this last case, you have\\nwon). After that, the environment must be reset before it can be used again.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,271.581,116.887,281.556 'truncated\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.995,205.311,432.001,268.363 'This  value  will  be  True  when  an  episode  is  interrupted  early,  for  example  by\\nan environment wrapper that imposes a maximum number of steps per episode\\n(see Gym’s documentation for more details on environment wrappers). Some RL\\nalgorithms  treat  truncated  episodes  differently  from  episodes  finished  normally\\n(i.e., when done is True), but in this chapter we will treat them identically.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,187.202,91.946,197.177 'info\\n'>\n",
            "<LTTextBoxHorizontal(11) 90.003,159.324,431.936,183.017 'This environment-specific dictionary may provide extra information, just like the\\none returned by the reset() method.\\n'>\n",
            "<LTTextBoxHorizontal(12) 136.790,120.730,396.000,142.393 'Once you have finished using an environment, you should call its\\nclose() method to free resources.\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.996,40.500,84.434,49.500 '690 \\n'>\n",
            "<LTTextBoxHorizontal(14) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,92.410,126.760,148.188 matrix=[41.76,0.00,0.00,55.78, (85.00,92.41)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the\\nleft and accelerates right when the pole is leaning toward the right. We will run this\\npolicy to see the average rewards it gets over 500 episodes:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,533.950,225.000,562.850 'def basic_policy(obs):\\n    angle = obs[2]\\n    return 0 if angle < 0 else 1\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,421.750,348.250,522.050 'totals = []\\nfor episode in range(500):\\n    episode_rewards = 0\\n    obs, info = env.reset(seed=episode)\\n    for step in range(200):\\n        action = basic_policy(obs)\\n        obs, reward, done, truncated, info = env.step(action)\\n        episode_rewards += reward\\n        if done or truncated:\\n            break\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,401.350,233.500,409.850 '    totals.append(episode_rewards)\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,382.037,289.381,392.537 'This code is self-explanatory. Let’s look at the result:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,346.150,348.252,375.050 '>>> import numpy as np\\n>>> np.mean(totals), np.std(totals), min(totals), max(totals)\\n(41.698, 8.389445512070509, 24.0, 63.0)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,276.436,432.004,337.336 'Even  with  500  tries,  this  policy  never  managed  to  keep  the  pole  upright  for  more\\nthan  63  consecutive  steps.  Not  great.  If  you  look  at  the  simulation  in  this  chapter’s\\nnotebook, you will see that the cart oscillates left and right more and more strongly\\nuntil the pole tilts too much. Let’s see if a neural network can come up with a better\\npolicy.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,125.636,432.004,264.097 'Neural Network Policies\\nLet’s create a neural network policy. This neural network will take an observation as\\ninput, and it will output the action to be executed, just like the policy we hardcoded\\nearlier. More precisely, it will estimate a probability for each action, and then we will\\nselect an action randomly, according to the estimated probabilities (see Figure 18-5).\\nIn  the  case  of  the  CartPole  environment,  there  are  just  two  possible  actions  (left  or\\nright), so we only need one output neuron. It will output the probability p of action\\n0 (left), and of course the probability of action 1 (right) will be 1 – p. For example, if\\nit outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with 30%\\nprobability.\\n'>\n",
            "<LTTextBoxHorizontal(8) 330.698,40.500,402.518,49.500 'Neural Network Policies \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.986,40.500,432.002,49.500 '691\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,326.497,213.918,336.997 'Figure 18-5. Neural network policy\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,188.496,432.004,312.397 'You  may  wonder  why  we  are  picking  a  random  action  based  on  the  probabilities\\ngiven  by  the  neural  network,  rather  than  just  picking  the  action  with  the  highest\\nscore.  This  approach  lets  the  agent  find  the  right  balance  between  exploring  new\\nactions  and  exploiting  the  actions  that  are  known  to  work  well.  Here’s  an  analogy:\\nsuppose  you  go  to  a  restaurant  for  the  first  time,  and  all  the  dishes  look  equally\\nappealing, so you randomly pick one. If it turns out to be good, you can increase the\\nprobability that you’ll order it next time, but you shouldn’t increase that probability\\nup  to  100%,  or  else  you  will  never  try  out  the  other  dishes,  some  of  which  may  be\\neven better than the one you tried. This exploration/exploitation dilemma is central in\\nreinforcement learning.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,81.696,432.004,180.396 'Also note that in this particular environment, the past actions and observations can\\nsafely be ignored, since each observation contains the environment’s full state. If there\\nwere some hidden state, then you might need to consider past actions and observa‐\\ntions as well. For example, if the environment only revealed the position of the cart\\nbut  not  its  velocity,  you  would  have  to  consider  not  only  the  current  observation\\nbut also the previous observation in order to estimate the current velocity. Another\\nexample  is  when  the  observations  are  noisy;  in  that  case,  you  generally  want  to  use\\nthe  past  few  observations  to  estimate  the  most  likely  current  state.  The  CartPole\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,40.500,84.434,49.500 '692 \\n'>\n",
            "<LTTextBoxHorizontal(4) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,342.560,432.375,607.500>\n",
            "<LTLine 72.000,342.685,432.500,342.685>\n",
            "<LTLine 72.125,342.560,72.125,607.500>\n",
            "<LTFigure(I1) 144.250,348.540,360.250,602.250 matrix=[216.00,0.00,0.00,253.71, (144.25,348.54)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'problem is thus as simple as can be; the observations are noise-free, and they contain\\nthe environment’s full state.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,563.837,353.607,574.337 'Here is the code to build a basic neural network policy using Keras:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,548.350,186.752,556.850 'import tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.002,497.350,305.752,536.450 'model = tf.keras.Sequential([\\n    tf.keras.layers.Dense(5, activation=\"relu\"),\\n    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\\n])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,401.843,432.005,489.502 'We  use  a  Sequential  model  to  define  the  policy  network.  The  number  of  inputs\\nis  the  size  of  the  observation  space—which  in  the  case  of  CartPole  is  4—and  we\\nhave  just  five  hidden  units  because  it’s  a  fairly  simple  task.  Finally,  we  want  to\\noutput a single probability—the probability of going left—so we have a single output\\nneuron using the sigmoid activation function. If there were more than two possible\\nactions, there would be one output neuron per action, and we would use the softmax\\nactivation function instead.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,370.643,432.001,393.743 'OK,  we  now  have  a  neural  network  policy  that  will  take  observations  and  output\\naction probabilities. But how do we train it?\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,194.643,432.004,358.303 'Evaluating Actions: The Credit Assignment Problem\\nIf we knew what the best action was at each step, we could train the neural network\\nas  usual,  by  minimizing  the  cross  entropy  between  the  estimated  probability  distri‐\\nbution  and  the  target  probability  distribution.  It  would  just  be  regular  supervised\\nlearning.  However,  in  reinforcement  learning  the  only  guidance  the  agent  gets  is\\nthrough  rewards,  and  rewards  are  typically  sparse  and  delayed.  For  example,  if  the\\nagent manages to balance the pole for 100 steps, how can it know which of the 100\\nactions it took were good, and which of them were bad? All it knows is that the pole\\nfell after the last action, but surely this last action is not entirely responsible. This is\\ncalled the credit assignment problem: when the agent gets a reward, it is hard for it to\\nknow which actions should get credited (or blamed) for it. Think of a dog that gets\\nrewarded hours after it behaved well; will it understand what it is being rewarded for?\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,75.243,432.004,186.543 'To tackle this problem, a common strategy is to evaluate an action based on the sum\\nof  all  the  rewards  that  come  after  it,  usually  applying  a  discount  factor,  γ  (gamma),\\nat  each  step.  This  sum  of  discounted  rewards  is  called  the  action’s  return.  Consider\\nthe example in Figure 18-6. If an agent decides to go right three times in a row and\\ngets +10 reward after the first step, 0 after the second step, and finally –50 after the\\nthird step, then assuming we use a discount factor γ = 0.8, the first action will have\\na  return  of  10  +  γ  ×  0  +  γ2  ×  (–50)  =  –22.  If  the  discount  factor  is  close  to  0,  then\\nfuture rewards won’t count for much compared to immediate rewards. Conversely, if\\nthe  discount  factor  is  close  to  1,  then  rewards  far  into  the  future  will  count  almost\\n'>\n",
            "<LTTextBoxHorizontal(8) 249.871,40.500,402.520,49.500 'Evaluating Actions: The Credit Assignment Problem \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.988,40.500,432.004,49.500 '693\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.995,532.036,432.003,605.537 'as much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With\\na  discount  factor  of  0.95,  rewards  13  steps  into  the  future  count  roughly  for  half  as\\nmuch as immediate rewards (since 0.9513 ≈ 0.5), while with a discount factor of 0.99,\\nrewards 69 steps into the future count for half as much as immediate rewards. In the\\nCartPole environment, actions have fairly short-term effects, so choosing a discount\\nfactor of 0.95 seems reasonable.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,348.366,397.972,358.866 'Figure 18-6. Computing an action’s return: the sum of discounted future rewards\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,197.766,432.005,334.266 'Of course, a good action may be followed by several bad actions that cause the pole to\\nfall quickly, resulting in the good action getting a low return. Similarly, a good actor\\nmay sometimes star in a terrible movie. However, if we play the game enough times,\\non average good actions will get a higher return than bad ones. We want to estimate\\nhow  much  better  or  worse  an  action  is,  compared  to  the  other  possible  actions,  on\\naverage.  This  is  called  the  action  advantage.  For  this,  we  must  run  many  episodes\\nand  normalize  all  the  action  returns,  by  subtracting  the  mean  and  dividing  by  the\\nstandard deviation. After that, we can reasonably assume that actions with a negative\\nadvantage were bad while actions with a positive advantage were good. OK, now that\\nwe  have  a  way  to  evaluate  each  action,  we  are  ready  to  train  our  first  agent  using\\npolicy gradients. Let’s see how.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,135.165,432.004,185.426 'Policy Gradients\\nAs discussed earlier, PG algorithms optimize the parameters of a policy by following\\nthe  gradients  toward  higher  rewards.  One  popular  class  of  PG  algorithms,  called\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,40.500,84.442,49.500 '694 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.910,40.500,210.163,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,525.775,432.500,525.775>\n",
            "<LTLine 432.375,364.429,432.375,525.900>\n",
            "<LTLine 72.000,364.554,432.500,364.554>\n",
            "<LTLine 72.125,364.429,72.125,525.900>\n",
            "<LTFigure(I1) 148.210,370.409,356.290,520.650 matrix=[208.08,0.00,0.00,150.24, (148.21,370.41)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,582.437,431.999,606.157 'REINFORCE algorithms, was introduced back in 199211 by Ronald Williams. Here is\\none common variant:\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.312,534.636,432.003,570.337 '1.\\n1. First, let the neural network policy play the game several times, and at each step,\\ncompute the gradients that would make the chosen action even more likely—but\\ndon’t apply these gradients yet.\\n'>\n",
            "<LTTextBoxHorizontal(2) 77.315,518.036,432.002,528.537 '2. Once you have run several episodes, compute each action’s advantage, using the\\n2.\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.996,505.437,265.451,515.937 'method described in the previous section.\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.315,425.836,432.005,499.337 '3. If an action’s advantage is positive, it means that the action was probably good,\\n3.\\nand  you  want  to  apply  the  gradients  computed  earlier  to  make  the  action  even\\nmore likely to be chosen in the future. However, if the action’s advantage is nega‐\\ntive, it means the action was probably bad, and you want to apply the opposite\\ngradients to make this action slightly less likely in the future. The solution is to\\nmultiply each gradient vector by the corresponding action’s advantage.\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.317,409.236,431.998,419.736 '4.\\n4. Finally,  compute  the  mean  of  all  the  resulting  gradient  vectors,  and  use  it  to\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.002,396.636,224.570,407.136 'perform a gradient descent step.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,311.036,432.005,384.536 'Let’s use Keras to implement this algorithm. We will train the neural network policy\\nwe  built  earlier  so  that  it  learns  to  balance  the  pole  on  the  cart.  First,  we  need  a\\nfunction that will play one step. We will pretend for now that whatever action it takes\\nis the right one so that we can compute the loss and its gradients. These gradients will\\njust be saved for a while, and we will modify them later depending on how good or\\nbad the action turned out to be:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.000,244.550,378.000,304.050 'def play_one_step(env, obs, model, loss_fn):\\n    with tf.GradientTape() as tape:\\n        left_proba = model(obs[np.newaxis])\\n        action = (tf.random.uniform([1, 1]) > left_proba)\\n        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\\n        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.000,203.750,352.500,232.650 '    grads = tape.gradient(loss, model.trainable_variables)\\n    obs, reward, done, truncated, info = env.step(int(action))\\n    return obs, reward, done, truncated, grads\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.001,184.436,203.545,194.936 'Let’s walk though this function:\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.650,123.443,432.003,173.302 '• Within the GradientTape block (see Chapter 12), we start by calling the model,\\n•\\ngiving it a single observation. We reshape the observation so it becomes a batch\\ncontaining  a  single  instance,  as  the  model  expects  a  batch.  This  outputs  the\\nprobability of going left.\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.780,78.954,412.600,86.954 '11 Ronald J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement\\n'>\n",
            "<LTTextBoxHorizontal(13) 80.000,68.954,230.912,76.954 'Leaning”, Machine Learning 8 (1992) : 229–256.\\n'>\n",
            "<LTTextBoxHorizontal(14) 353.632,40.500,402.520,49.500 'Policy Gradients \\n'>\n",
            "<LTTextBoxHorizontal(15) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 420.988,40.500,432.004,49.500 '695\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 80.655,556.050,432.002,605.537 '• Next,  we  sample  a  random  float  between  0  and  1,  and  we  check  whether  it  is\\n•\\ngreater than left_proba. The action will be False with probability left_proba,\\nor  True  with  probability  1  –  left_proba.  Once  we  cast  this  Boolean  to  an\\ninteger, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.652,514.250,432.005,549.950 '• We now define the target probability of going left: it is 1 minus the action (cast to\\n•\\na float). If the action is 0 (left), then the target probability of going left will be 1. If\\nthe action is 1 (right), then the target probability will be 0.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.651,459.850,432.003,508.150 '• Then we compute the loss using the given loss function, and we use the tape to\\n•\\ncompute  the  gradient  of  the  loss  with  regard  to  the  model’s  trainable  variables.\\nAgain, these gradients will be tweaked later, before we apply them, depending on\\nhow good or bad the action turned out to be.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.654,418.050,432.002,453.750 '• Finally,  we  play  the  selected  action,  and  we  return  the  new  observation,  the\\n•\\nreward, whether the episode is ended or not, whether it is truncated or not, and\\nof course the gradients that we just computed.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,369.657,432.003,406.916 'Now let’s create another function that will rely on the play_one_step() function to\\nplay multiple episodes, returning all the rewards and gradients for each episode and\\neach step:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,221.570,399.247,362.670 'def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\\n    all_rewards = []\\n    all_grads = []\\n    for episode in range(n_episodes):\\n        current_rewards = []\\n        current_grads = []\\n        obs, info = env.reset()\\n        for step in range(n_max_steps):\\n            obs, reward, done, truncated, grads = play_one_step(\\n                env, obs, model, loss_fn)\\n            current_rewards.append(reward)\\n            current_grads.append(grads)\\n            if done or truncated:\\n                break\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.997,190.970,271.747,209.670 '        all_rewards.append(current_rewards)\\n        all_grads.append(current_grads)\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.997,170.570,229.247,179.070 '    return all_rewards, all_grads\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,113.456,432.003,161.757 'This  code  returns  a  list  of  reward  lists:  one  reward  list  per  episode,  containing  one\\nreward  per  step.  It  also  returns  a  list  of  gradient  lists:  one  gradient  list  per  episode,\\neach containing one tuple of gradients per step and each tuple containing one gradi‐\\nent tensor per trainable variable.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.997,69.063,432.005,106.322 'The  algorithm  will  use  the  play_multiple_episodes()  function  to  play  the  game\\nseveral times (e.g., 10 times), then it will go back and look at all the rewards, discount\\nthem,  and  normalize  them.  To  do  that,  we  need  a  couple  more  functions;  the  first\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,40.500,84.434,49.500 '696 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'will compute the sum of future discounted rewards at each step, and the second will\\nnormalize  all  these  discounted  rewards  (i.e.,  the  returns)  across  many  episodes  by\\nsubtracting the mean and dividing by the standard deviation:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.003,513.550,369.503,562.850 'def discount_rewards(rewards, discount_factor):\\n    discounted = np.array(rewards)\\n    for step in range(len(rewards) - 2, -1, -1):\\n        discounted[step] += discounted[step + 1] * discount_factor\\n    return discounted\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.003,421.750,395.003,501.650 'def discount_and_normalize_rewards(all_rewards, discount_factor):\\n    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\\n                              for rewards in all_rewards]\\n    flat_rewards = np.concatenate(all_discounted_rewards)\\n    reward_mean = flat_rewards.mean()\\n    reward_std = flat_rewards.std()\\n    return [(discounted_rewards - reward_mean) / reward_std\\n            for discounted_rewards in all_discounted_rewards]\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,402.437,183.718,412.937 'Let’s check that this works:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,325.750,344.002,395.450 '>>> discount_rewards([10, 0, -50], discount_factor=0.8)\\narray([-22, -40, -50])\\n>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\\n...                                discount_factor=0.8)\\n...\\n[array([-0.28435071, -0.86597718, -1.18910299]),\\n array([1.26665318, 1.0727777 ])]\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,242.250,432.004,317.902 'The  call  to  discount_rewards()  returns  exactly  what  we  expect  (see  Figure  18-6).\\nYou  can  verify  that  the  function  discount_and_normalize_rewards()  does  indeed\\nreturn  the  normalized  action  advantages  for  each  action  in  both  episodes.  Notice\\nthat the first episode was much worse than the second, so its normalized advantages\\nare  all  negative;  all  actions  from  the  first  episode  would  be  considered  bad,  and\\nconversely all actions from the second episode would be considered good.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,198.450,432.000,234.150 'We are almost ready to run the algorithm! Now let’s define the hyperparameters. We\\nwill  run  150  training  iterations,  playing  10  episodes  per  iteration,  and  each  episode\\nwill last at most 200 steps. We will use a discount factor of 0.95:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.997,152.363,199.497,191.463 'n_iterations = 150\\nn_episodes_per_update = 10\\nn_max_steps = 200\\ndiscount_factor = 0.95\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,95.250,432.003,143.550 'We  also  need  an  optimizer  and  the  loss  function.  A  regular  Nadam  optimizer  with\\nlearning  rate  0.01  will  do  just  fine,  and  we  will  use  the  binary  cross-entropy  loss\\nfunction because we are training a binary classifier (there are two possible actions—\\nleft or right):\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.001,69.563,331.251,88.263 'optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\\nloss_fn = tf.keras.losses.binary_crossentropy\\n'>\n",
            "<LTTextBoxHorizontal(10) 353.628,40.500,402.516,49.500 'Policy Gradients \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.984,40.500,432.000,49.500 '697\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,294.432,605.537 'We are now ready to build and run the training loop!\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.004,467.350,416.254,588.050 'for iteration in range(n_iterations):\\n    all_rewards, all_grads = play_multiple_episodes(\\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\\n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\\n                                                       discount_factor)\\n    all_mean_grads = []\\n    for var_index in range(len(model.trainable_variables)):\\n        mean_grads = tf.reduce_mean(\\n            [final_reward * all_grads[episode_index][step][var_index]\\n             for episode_index, final_rewards in enumerate(all_final_rewards)\\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\\n        all_mean_grads.append(mean_grads)\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.004,446.950,416.254,455.450 '    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,427.637,191.260,438.137 'Let’s walk through this code:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.650,379.243,432.003,416.503 '• At  each  training  iteration,  this  loop  calls  the  play_multiple_episodes()  func‐\\n•\\ntion, which plays 10 episodes and returns the rewards and gradients for each step\\nin each episode.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.652,336.257,432.005,374.109 '• Then we call the discount_and_normalize_rewards() function to compute each\\n•\\naction’s  normalized  advantage,  called  the  final_reward  in  this  code.  This  pro‐\\nvides a measure of how good or bad each action actually was, in hindsight.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.651,293.863,432.003,330.157 '• Next,  we  go  through  each  trainable  variable,  and  for  each  of  them  we  compute\\n•\\nthe  weighted  mean  of  the  gradients  for  that  variable  over  all  episodes  and  all\\nsteps, weighted by the final_reward.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.655,277.263,431.997,287.763 '•\\n• Finally, we apply these mean gradients using the optimizer: the model’s trainable\\n'>\n",
            "<LTTextBoxHorizontal(8) 90.002,264.663,379.760,275.163 'variables will be tweaked, and hopefully the policy will be a bit better.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.999,216.863,432.005,252.563 'And  we’re  done!  This  code  will  train  the  neural  network  policy,  and  it  will  success‐\\nfully learn to balance the pole on the cart. The mean reward per episode will get very\\nclose to 200. By default, that’s the maximum for this environment. Success!\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,122.663,432.005,208.763 'The  simple  policy  gradients  algorithm  we  just  trained  solved  the  CartPole  task,  but\\nit would not scale well to larger and more complex tasks. Indeed, it is highly sample\\ninefficient,  meaning  it  needs  to  explore  the  game  for  a  very  long  time  before  it  can\\nmake significant progress. This is due to the fact that it must run multiple episodes to\\nestimate the advantage of each action, as we have seen. However, it is the foundation\\nof  more  powerful  algorithms,  such  as  actor-critic  algorithms  (which  we  will  discuss\\nbriefly at the end of this chapter).\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.997,40.500,84.435,49.500 '698 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.903,40.500,210.156,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 136.790,492.425,396.003,605.705 'Researchers  try  to  find  algorithms  that  work  well  even  when  the\\nagent  initially  knows  nothing  about  the  environment.  However,\\nunless  you  are  writing  a  paper,  you  should  not  hesitate  to  inject\\nprior knowledge into the agent, as it will speed up training dramat‐\\nically.  For  example,  since  you  know  that  the  pole  should  be  as\\nvertical  as  possible,  you  could  add  negative  rewards  proportional\\nto  the  pole’s  angle.  This  will  make  the  rewards  much  less  sparse\\nand speed up training. Also, if you already have a reasonably good\\npolicy (e.g., hardcoded), you may want to train the neural network\\nto imitate it before using policy gradients to improve it.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,401.836,432.003,475.336 'We  will  now  look  at  another  popular  family  of  algorithms.  Whereas  PG  algorithms\\ndirectly try to optimize the policy to increase rewards, the algorithms we will explore\\nnow  are  less  direct:  the  agent  learns  to  estimate  the  expected  return  for  each  state,\\nor  for  each  action  in  each  state,  then  it  uses  this  knowledge  to  decide  how  to  act.\\nTo  understand  these  algorithms,  we  must  first  consider  Markov  decision  processes\\n(MDPs).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,301.436,432.003,389.497 'Markov Decision Processes\\nIn the early 20th century, the mathematician Andrey Markov studied stochastic pro‐\\ncesses with no memory, called Markov chains. Such a process has a fixed number of\\nstates, and it randomly evolves from one state to another at each step. The probability\\nfor it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s,\\ns′), not on past states. This is why we say that the system has no memory.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.003,282.836,347.082,293.336 'Figure 18-7 shows an example of a Markov chain with four states.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,144.102,236.507,154.602 'Figure 18-7. Example of a Markov chain\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,69.102,432.005,130.002 'Suppose  that  the  process  starts  in  state  s0,  and  there  is  a  70%  chance  that  it  will\\nremain  in  that  state  at  the  next  step.  Eventually  it  is  bound  to  leave  that  state  and\\nnever come back, because no other state points back to s0. If it goes to state s1, it will\\nthen  most  likely  go  to  state  s2  (90%  probability),  then  immediately  back  to  state  s1\\n(with 100% probability). It may alternate a number of times between these two states,\\n'>\n",
            "<LTTextBoxHorizontal(6) 323.213,40.500,402.512,49.500 'Markov Decision Processes \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.980,40.500,431.996,49.500 '699\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,276.575,432.500,276.575>\n",
            "<LTLine 432.375,160.166,432.375,276.700>\n",
            "<LTLine 72.000,160.291,432.500,160.291>\n",
            "<LTLine 72.125,160.166,72.125,276.700>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTFigure(I2) 144.250,166.146,360.250,271.450 matrix=[216.00,0.00,0.00,105.30, (144.25,166.15)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,569.837,432.001,605.537 'but  eventually  it  will  fall  into  state  s3  and  remain  there  forever,  since  there’s  no  way\\nout: this is called a terminal state. Markov chains can have very different dynamics,\\nand they are heavily used in thermodynamics, chemistry, statistics, and much more.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,500.837,432.003,562.356 'Markov  decision  processes  were  first  described  in  the  1950s  by  Richard  Bellman.12\\nThey  resemble  Markov  chains,  but  with  a  twist:  at  each  step,  an  agent  can  choose\\none of several possible actions, and the transition probabilities depend on the chosen\\naction.  Moreover,  some  state  transitions  return  some  reward  (positive  or  negative),\\nand the agent’s goal is to find a policy that will maximize reward over time.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,457.036,432.000,492.737 'For  example,  the  MDP  represented  in  Figure  18-8  has  three  states  (represented\\nby  circles)  and  up  to  three  possible  discrete  actions  at  each  step  (represented  by\\ndiamonds).\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.004,299.526,278.025,310.026 'Figure 18-8. Example of a Markov decision process\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,123.138,432.005,285.426 'If it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses\\naction a1, it just remains in state s0 with certainty, and without any reward. It can thus\\ndecide  to  stay  there  forever  if  it  wants  to.  But  if  it  chooses  action  a0,  it  has  a  70%\\nprobability of gaining a reward of +10 and remaining in state s0. It can then try again\\nand again to gain as much reward as possible, but at one point it is going to end up\\ninstead in state s1. In state s1 it has only two possible actions: a0 or a2. It can choose\\nto stay put by repeatedly choosing action a0, or it can choose to move on to state s2\\nand get a negative reward of –50 (ouch). In state s2 it has no choice but to take action\\na1, which will most likely lead it back to state s0, gaining a reward of +40 on the way.\\nYou get the picture. By looking at this MDP, can you guess which strategy will gain\\nthe most reward over time? In state s0 it is clear that action a0 is the best option, and\\nin state s2 the agent has no choice but to take action a1, but in state s1 it is not obvious\\nwhether the agent should stay put (a0) or go through the fire (a2).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,78.954,414.776,86.954 '12 Richard Bellman, “A Markovian Decision Process”, Journal of Mathematics and Mechanics 6, no. 5 (1957):\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,109.024,76.954 '679–684.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.998,40.500,84.436,49.500 '700 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.904,40.500,210.157,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,450.775,432.500,450.775>\n",
            "<LTLine 432.375,315.590,432.375,450.900>\n",
            "<LTLine 72.000,315.715,432.500,315.715>\n",
            "<LTLine 72.125,315.590,72.125,450.900>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 106.929,321.570,397.570,445.650 matrix=[290.64,0.00,0.00,124.08, (106.93,321.57)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,506.836,432.004,605.537 'Bellman  found  a  way  to  estimate  the  optimal  state  value  of  any  state  s,  noted  V*(s),\\nwhich  is  the  sum  of  all  discounted  future  rewards  the  agent  can  expect  on  average\\nafter  it  reaches  the  state,  assuming  it  acts  optimally.  He  showed  that  if  the  agent\\nacts optimally, then the Bellman optimality equation applies (see Equation 18-1). This\\nrecursive equation says that if the agent acts optimally, then the optimal value of the\\ncurrent  state  is  equal  to  the  reward  it  will  get  on  average  after  taking  one  optimal\\naction, plus the expected optimal value of all possible next states that this action can\\nlead to.\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.005,480.549,267.048,491.049 'Equation 18-1. Bellman optimality equation\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.998,461.874,142.588,473.324 'V * s = max\\n'>\n",
            "<LTTextBoxHorizontal(3) 131.764,457.360,153.535,473.584 'a ∑\\n'>\n",
            "<LTTextBoxHorizontal(4) 146.919,455.409,152.575,463.409 's′\\n'>\n",
            "<LTTextBoxHorizontal(5) 154.645,461.874,283.345,473.324 'T s, a, s′ R s, a, s′ + γ · V * s′\\n'>\n",
            "<LTTextBoxHorizontal(6) 300.255,461.874,334.065,473.324 'for all s\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,427.939,140.891,438.439 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.655,405.339,431.996,417.362 '• T(s, a, s′) is the transition probability from state s to state s′, given that the agent\\n•\\n'>\n",
            "<LTTextBoxHorizontal(9) 90.000,392.151,345.738,403.239 'chose action a. For example, in Figure 18-8, T(s2, a1, s0) = 0.8.\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.652,350.351,432.000,388.162 '• R(s,  a,  s′)  is  the  reward  that  the  agent  gets  when  it  goes  from  state  s  to  state\\n•\\ns′,  given  that  the  agent  chose  action  a.  For  example,  in  Figure  18-8,  R(s2,  a1,\\ns0) = +40.\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.660,334.339,187.127,344.839 '•\\n• γ is the discount factor.\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.996,261.339,432.005,322.239 'This  equation  leads  directly  to  an  algorithm  that  can  precisely  estimate  the  optimal\\nstate value of every possible state: first initialize all the state value estimates to zero,\\nand  then  iteratively  update  them  using  the  value  iteration  algorithm  (see  Equation\\n18-2). A remarkable result is that, given enough time, these estimates are guaranteed\\nto converge to the optimal state values, corresponding to the optimal policy.\\n'>\n",
            "<LTTextBoxHorizontal(13) 86.997,235.052,253.601,245.552 'Equation 18-2. Value iteration algorithm\\n'>\n",
            "<LTTextBoxHorizontal(14) 87.001,215.280,119.633,227.826 'V k + 1 s\\n'>\n",
            "<LTTextBoxHorizontal(15) 140.203,211.863,168.450,228.086 'a ∑\\nmax\\n'>\n",
            "<LTTextBoxHorizontal(16) 161.826,209.912,167.482,217.912 's′\\n'>\n",
            "<LTTextBoxHorizontal(17) 169.562,215.983,298.166,227.826 'T s, a, s′ R s, a, s′ + γ · V k s′\\n'>\n",
            "<LTTextBoxHorizontal(18) 315.076,216.376,348.886,227.826 'for all s\\n'>\n",
            "<LTTextBoxHorizontal(19) 71.995,169.842,432.001,193.754 'In  this  equation,  Vk(s)  is  the  estimated  value  of  state  s  at  the  kth  iteration  of  the\\nalgorithm.\\n'>\n",
            "<LTTextBoxHorizontal(20) 136.791,120.270,396.002,152.910 'This  algorithm  is  an  example  of  dynamic  programming,  which\\nbreaks  down  a  complex  problem  into  tractable  subproblems  that\\ncan be tackled iteratively.\\n'>\n",
            "<LTTextBoxHorizontal(21) 323.221,40.500,402.520,49.500 'Markov Decision Processes \\n'>\n",
            "<LTTextBoxHorizontal(22) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(23) 420.988,40.500,432.004,49.500 '701\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,105.205,126.944,154.706 matrix=[37.94,0.00,0.00,49.50, (89.00,105.20)]>\n",
            "<LTCurve 100.339,463.316,102.639,473.316>\n",
            "<LTCurve 108.148,463.316,110.448,473.316>\n",
            "<LTCurve 163.182,463.316,165.482,473.316>\n",
            "<LTCurve 191.434,463.316,193.734,473.316>\n",
            "<LTCurve 195.644,463.316,197.344,473.316>\n",
            "<LTCurve 205.576,463.316,207.876,473.316>\n",
            "<LTCurve 233.828,463.316,236.128,473.316>\n",
            "<LTCurve 273.517,463.316,275.817,473.316>\n",
            "<LTCurve 283.797,463.316,286.097,473.316>\n",
            "<LTCurve 287.357,463.316,289.067,473.316>\n",
            "<LTCurve 112.282,217.818,114.582,227.818>\n",
            "<LTCurve 120.091,217.818,122.391,227.818>\n",
            "<LTCurve 125.879,220.006,134.649,225.146>\n",
            "<LTCurve 178.095,217.818,180.395,227.818>\n",
            "<LTCurve 206.347,217.818,208.647,227.818>\n",
            "<LTCurve 210.557,217.609,212.257,228.027>\n",
            "<LTCurve 220.489,217.818,222.789,227.818>\n",
            "<LTCurve 248.741,217.818,251.041,227.818>\n",
            "<LTCurve 288.338,217.818,290.638,227.818>\n",
            "<LTCurve 298.618,217.818,300.918,227.818>\n",
            "<LTCurve 302.178,217.609,303.888,228.027>\n",
            "<LTTextBoxHorizontal(0) 71.996,519.436,432.003,605.537 'Knowing the optimal state values can be useful, in particular to evaluate a policy, but\\nit  does  not  give  us  the  optimal  policy  for  the  agent.  Luckily,  Bellman  found  a  very\\nsimilar algorithm to estimate the optimal state-action values, generally called Q-values\\n(quality  values).  The  optimal  Q-value  of  the  state-action  pair  (s,  a),  noted  Q*(s,  a),\\nis  the  sum  of  discounted  future  rewards  the  agent  can  expect  on  average  after  it\\nreaches the state s and chooses action a, but before it sees the outcome of this action,\\nassuming it acts optimally after that action.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,475.637,432.005,511.336 'Let’s  look  at  how  it  works.  Once  again,  you  start  by  initializing  all  the  Q-value\\nestimates  to  zero,  then  you  update  them  using  the  Q-value  iteration  algorithm  (see\\nEquation 18-3).\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.996,449.349,263.091,459.849 'Equation 18-3. Q-value iteration algorithm\\n'>\n",
            "<LTTextBoxHorizontal(3) 87.001,425.658,127.889,438.204 'Qk + 1 s, a\\n'>\n",
            "<LTTextBoxHorizontal(4) 146.356,420.290,153.935,438.464 '∑\\ns′\\n'>\n",
            "<LTTextBoxHorizontal(5) 155.045,422.241,280.655,438.204 'T s, a, s′ R s, a, s′ + γ · max\\na′\\n'>\n",
            "<LTTextBoxHorizontal(6) 285.137,426.361,319.057,438.204 'Qk s′, a′\\n'>\n",
            "<LTTextBoxHorizontal(7) 335.967,426.754,382.547,438.204 'for all s, a\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.000,380.220,432.003,403.320 'Once  you  have  the  optimal  Q-values,  defining  the  optimal  policy,  noted  π*(s),  is\\ntrivial;  when  the  agent  is  in  state  s,  it  should  choose  the  action  with  the  highest\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.997,361.270,234.842,373.292 'Q-value for that state: π* s = argmax\\n'>\n",
            "<LTTextBoxHorizontal(10) 216.874,354.358,221.154,362.358 'a\\n'>\n",
            "<LTTextBoxHorizontal(11) 238.846,361.270,275.397,373.292 'Q* s, a .\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.998,323.288,432.001,346.388 'Let’s  apply  this  algorithm  to  the  MDP  represented  in  Figure  18-8.  First,  we  need  to\\ndefine the MDP:\\n'>\n",
            "<LTTextBoxHorizontal(13) 89.003,205.801,327.003,316.301 \"transition_probabilities = [  # shape=[s, a, s']\\n    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\\n    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\\n    [None, [0.8, 0.1, 0.1], None]\\n]\\nrewards = [  # shape=[s, a, s']\\n    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\\n    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\\n    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\\n]\\npossible_actions = [[0, 1, 2], [0, 2], [1]]\\n\">\n",
            "<LTTextBoxHorizontal(14) 71.995,108.515,432.001,196.988 'For  example,  to  know  the  transition  probability  of  going  from  s2  to  s0  after  play‐\\ning  action  a1,  we  will  look  up  transition_probabilities[2][1][0]  (which  is\\n0.8).  Similarly,  to  get  the  corresponding  reward,  we  will  look  up  rewards[2][1]\\n[0]  (which  is  +40).  And  to  get  the  list  of  possible  actions  in  s2,  we  will  look\\nup  possible_actions[2]  (in  this  case,  only  action  a1  is  possible).  Next,  we  must\\ninitialize all the Q-values to zero (except for the impossible actions, for which we set\\nthe Q-values to –∞):\\n'>\n",
            "<LTTextBoxHorizontal(15) 89.002,72.628,382.252,101.528 'Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\\nfor state, actions in enumerate(possible_actions):\\n    Q_values[state, actions] = 0.0  # for all possible actions\\n'>\n",
            "<LTTextBoxHorizontal(16) 71.999,40.500,84.437,49.500 '702 \\n'>\n",
            "<LTTextBoxHorizontal(17) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 102.905,40.500,210.158,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 111.232,428.196,113.532,438.196>\n",
            "<LTCurve 128.467,428.196,130.767,438.196>\n",
            "<LTCurve 134.255,430.384,143.025,435.524>\n",
            "<LTCurve 163.585,428.196,165.885,438.196>\n",
            "<LTCurve 191.837,428.196,194.137,438.196>\n",
            "<LTCurve 196.047,423.867,197.747,442.525>\n",
            "<LTCurve 205.979,428.196,208.279,438.196>\n",
            "<LTCurve 234.231,428.196,236.531,438.196>\n",
            "<LTCurve 297.330,428.196,299.630,438.196>\n",
            "<LTCurve 319.506,428.196,321.806,438.196>\n",
            "<LTCurve 323.066,423.867,324.776,442.525>\n",
            "<LTCurve 365.886,428.196,368.186,438.196>\n",
            "<LTCurve 383.121,428.196,385.421,438.196>\n",
            "<LTCurve 177.059,355.984,179.474,380.083>\n",
            "<LTCurve 185.259,355.984,187.674,380.083>\n",
            "<LTCurve 251.745,355.984,254.160,380.083>\n",
            "<LTCurve 269.841,355.984,272.256,380.083>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'Now let’s run the Q-value iteration algorithm. It applies Equation 18-3 repeatedly, to\\nall Q-values, for every state and every possible action:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,566.950,237.752,575.450 'gamma = 0.90  # the discount factor\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,475.150,378.002,555.050 'for iteration in range(50):\\n    Q_prev = Q_values.copy()\\n    for s in range(3):\\n        for a in possible_actions[s]:\\n            Q_values[s, a] = np.sum([\\n                    transition_probabilities[s][a][sp]\\n                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\\n                for sp in range(3)])\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,455.837,265.039,466.337 'That’s it! The resulting Q-values look like this:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,409.750,293.004,448.850 '>>> Q_values\\narray([[18.91891892, 17.02702702, 13.62162162],\\n       [ 0.        ,        -inf, -4.87971488],\\n       [       -inf, 50.13365013,        -inf]])\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,377.836,432.000,400.937 'For example, when the agent is in state s0 and it chooses action a1, the expected sum\\nof discounted future rewards is approximately 17.0.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,359.236,349.291,369.736 'For each state, we can find the action that has the highest Q-value:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.004,333.550,344.004,352.250 '>>> Q_values.argmax(axis=1)  # optimal action for each state\\narray([0, 0, 1])\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,251.236,432.002,324.736 'This gives us the optimal policy for this MDP when using a discount factor of 0.90:\\nin state s0 choose action a0, in state s1 choose action a0 (i.e., stay put), and in state s2\\nchoose action a1 (the only possible action). Interestingly, if we increase the discount\\nfactor to 0.95, the optimal policy changes: in state s1 the best action becomes a2 (go\\nthrough the fire!). This makes sense because the more you value future rewards, the\\nmore you are willing to put up with some pain now for the promise of future bliss.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,138.236,432.004,238.896 'Temporal Difference Learning\\nReinforcement  learning  problems  with  discrete  actions  can  often  be  modeled  as\\nMarkov  decision  processes,  but  the  agent  initially  has  no  idea  what  the  transition\\nprobabilities are (it does not know T(s, a, s′)), and it does not know what the rewards\\nare  going  to  be  either  (it  does  not  know  R(s,  a,  s′)).  It  must  experience  each  state\\nand each transition at least once to know the rewards, and it must experience them\\nmultiple times if it is to have a reasonable estimate of the transition probabilities.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,69.236,432.004,130.136 'The temporal difference (TD) learning algorithm is very similar to the Q-value itera‐\\ntion  algorithm,  but  tweaked  to  take  into  account  the  fact  that  the  agent  has  only\\npartial  knowledge  of  the  MDP.  In  general  we  assume  that  the  agent  initially  knows\\nonly  the  possible  states  and  actions,  and  nothing  more.  The  agent  uses  an  explora‐\\ntion  policy—for  example,  a  purely  random  policy—to  explore  the  MDP,  and  as  it\\n'>\n",
            "<LTTextBoxHorizontal(11) 313.343,40.500,402.515,49.500 'Temporal Difference Learning \\n'>\n",
            "<LTTextBoxHorizontal(12) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 420.983,40.500,431.999,49.500 '703\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'progresses, the TD learning algorithm updates the estimates of the state values based\\non the transitions and rewards that are actually observed (see Equation 18-4).\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.002,556.149,242.654,566.649 'Equation 18-4. TD learning algorithm\\n'>\n",
            "<LTTextBoxHorizontal(2) 140.886,537.078,260.534,548.921 '1 − α V k s + α r + γ · V k s′\\n'>\n",
            "<LTTextBoxHorizontal(3) 86.996,502.571,245.002,548.921 'V k + 1 s\\nor,\\xa0equivalently:\\xa0\\nV k + 1 s\\nwith\\xa0δk s, r, s′ = r + γ · V k s′ − Vk s\\n'>\n",
            "<LTTextBoxHorizontal(4) 137.426,514.824,221.916,526.668 'V k s + α · δk s, r, s′\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,475.101,140.888,485.601 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.653,418.713,236.295,463.001 '•\\n• α is the learning rate (e.g., 0.01).\\n• r + γ · Vk(s′) is called the TD target.\\n•\\n• δk(s, r, s′) is called the TD error.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,383.513,431.998,407.201 'A more concise way of writing the first form of this equation is to use the notation\\nb, which means ak+1 ← (1 – α) · ak + α ·bk. So, the first line of Equation 18-4 can\\na\\n'>\n",
            "<LTTextBoxHorizontal(8) 82.853,379.919,87.549,387.919 'α\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.005,363.829,180.690,375.851 'be rewritten like this: V s\\n'>\n",
            "<LTTextBoxHorizontal(10) 199.378,363.829,255.091,375.851 'r + γ · V s′ .\\n'>\n",
            "<LTTextBoxHorizontal(11) 189.442,359.647,194.138,367.647 'α\\n'>\n",
            "<LTTextBoxHorizontal(12) 136.790,287.165,396.001,342.845 'TD learning has many similarities with stochastic gradient descent,\\nincluding  the  fact  that  it  handles  one  sample  at  a  time.  Moreover,\\njust like SGD, it can only truly converge if you gradually reduce the\\nlearning rate; otherwise, it will keep bouncing around the optimum\\nQ-values.\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.997,234.377,432.003,270.077 'For  each  state  s,  this  algorithm  keeps  track  of  a  running  average  of  the  immediate\\nrewards the agent gets upon leaving that state, plus the rewards it expects to get later,\\nassuming it acts optimally.\\n'>\n",
            "<LTTextBoxHorizontal(14) 71.995,121.376,432.004,222.037 'Q-Learning\\nSimilarly,  the  Q-learning  algorithm  is  an  adaptation  of  the  Q-value  iteration  algo‐\\nrithm to the situation where the transition probabilities and the rewards are initially\\nunknown (see Equation 18-5). Q-learning works by watching an agent play (e.g., ran‐\\ndomly)  and  gradually  improving  its  estimates  of  the  Q-values.  Once  it  has  accurate\\nQ-value  estimates  (or  close  enough),  then  the  optimal  policy  is  just  choosing  the\\naction that has the highest Q-value (i.e., the greedy policy).\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.004,40.500,84.442,49.500 '704 \\n'>\n",
            "<LTTextBoxHorizontal(16) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(17) 102.910,40.500,210.163,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,292.863,126.760,348.640 matrix=[41.76,0.00,0.00,55.78, (85.00,292.86)]>\n",
            "<LTCurve 112.282,538.501,114.582,549.325>\n",
            "<LTCurve 120.091,538.501,122.391,549.325>\n",
            "<LTCurve 125.879,541.101,134.649,546.241>\n",
            "<LTCurve 138.127,538.501,140.427,549.325>\n",
            "<LTCurve 162.431,538.501,164.731,549.325>\n",
            "<LTCurve 178.688,538.501,180.988,549.325>\n",
            "<LTCurve 186.497,538.501,188.797,549.325>\n",
            "<LTCurve 206.501,538.704,208.801,549.122>\n",
            "<LTCurve 250.702,538.704,253.002,549.122>\n",
            "<LTCurve 260.982,538.704,263.282,549.122>\n",
            "<LTCurve 264.442,538.704,266.742,549.122>\n",
            "<LTCurve 112.282,516.248,114.582,527.071>\n",
            "<LTCurve 120.091,516.248,122.391,527.071>\n",
            "<LTCurve 125.879,518.848,134.649,523.988>\n",
            "<LTCurve 150.673,516.248,152.973,527.071>\n",
            "<LTCurve 158.483,516.248,160.783,527.071>\n",
            "<LTCurve 194.735,516.248,197.035,527.071>\n",
            "<LTCurve 222.362,516.248,224.662,527.071>\n",
            "<LTCurve 117.433,503.994,119.733,514.818>\n",
            "<LTCurve 145.060,503.994,147.360,514.818>\n",
            "<LTCurve 200.866,503.994,203.166,514.818>\n",
            "<LTCurve 211.146,503.994,213.446,514.818>\n",
            "<LTCurve 237.647,503.994,239.947,514.818>\n",
            "<LTCurve 245.457,503.994,247.757,514.818>\n",
            "<LTCurve 80.669,387.912,89.878,393.309>\n",
            "<LTCurve 172.977,361.273,175.392,379.912>\n",
            "<LTCurve 181.177,361.273,183.592,379.912>\n",
            "<LTCurve 187.254,367.640,196.463,373.037>\n",
            "<LTCurve 238.747,361.273,241.162,379.912>\n",
            "<LTCurve 249.541,361.273,251.956,379.912>\n",
            "<LTTextBoxHorizontal(0) 87.005,596.349,237.438,606.849 'Equation 18-5. Q-learning algorithm\\n'>\n",
            "<LTTextBoxHorizontal(1) 86.998,571.144,111.908,582.594 'Q s, a\\n'>\n",
            "<LTTextBoxHorizontal(2) 129.822,564.021,175.312,582.594 'r + γ · max\\na′\\n'>\n",
            "<LTTextBoxHorizontal(3) 120.236,566.781,124.932,574.781 'α\\n'>\n",
            "<LTTextBoxHorizontal(4) 178.116,571.144,213.146,582.594 '\\xa0 Q s′, a′\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,486.151,432.004,547.051 'For  each  state-action  pair  (s,  a),  this  algorithm  keeps  track  of  a  running  average  of\\nthe  rewards  r  the  agent  gets  upon  leaving  the  state  s  with  action  a,  plus  the  sum\\nof  discounted  future  rewards  it  expects  to  get.  To  estimate  this  sum,  we  take  the\\nmaximum  of  the  Q-value  estimates  for  the  next  state  s′,  since  we  assume  that  the\\ntarget policy will act optimally from then on.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,442.351,432.004,478.051 'Let’s  implement  the  Q-learning  algorithm.  First,  we  will  need  to  make  an  agent\\nexplore  the  environment.  For  this,  we  need  a  step  function  so  that  the  agent  can\\nexecute one action and get the resulting state and reward:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.003,386.064,318.503,435.364 'def step(state, action):\\n    probas = transition_probabilities[state][action]\\n    next_state = np.random.choice([0, 1, 2], p=probas)\\n    reward = rewards[state][action][next_state]\\n    return next_state, reward\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,328.951,432.005,377.251 'Now  let’s  implement  the  agent’s  exploration  policy.  Since  the  state  space  is  pretty\\nsmall,  a  simple  random  policy  will  be  sufficient.  If  we  run  the  algorithm  for  long\\nenough, the agent will visit every state many times, and it will also try every possible\\naction many times:\\n'>\n",
            "<LTTextBoxHorizontal(9) 88.996,303.264,309.996,321.964 'def exploration_policy(state):\\n    return np.random.choice(possible_actions[state])\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,258.751,432.003,294.451 'Next,  after  we  initialize  the  Q-values  just  like  earlier,  we  are  ready  to  run  the  Q-\\nlearning algorithm with learning rate decay (using power scheduling, introduced in\\nChapter 11):\\n'>\n",
            "<LTTextBoxHorizontal(11) 89.003,212.664,250.503,251.764 'alpha0 = 0.05  # initial learning rate\\ndecay = 0.005  # learning rate decay\\ngamma = 0.90  # discount factor\\nstate = 0  # initial state\\n'>\n",
            "<LTTextBoxHorizontal(12) 89.003,120.864,416.253,200.764 'for iteration in range(10_000):\\n    action = exploration_policy(state)\\n    next_state, reward = step(state, action)\\n    next_value = Q_values[next_state].max()  # greedy policy at the next step\\n    alpha = alpha0 / (1 + iteration * decay)\\n    Q_values[state, action] *= 1 - alpha\\n    Q_values[state, action] += alpha * (reward + gamma * next_value)\\n    state = next_state\\n'>\n",
            "<LTTextBoxHorizontal(13) 368.173,40.500,402.517,49.500 'Q-Learning \\n'>\n",
            "<LTTextBoxHorizontal(14) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 420.985,40.500,432.001,49.500 '705\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 95.249,565.647,97.549,589.525>\n",
            "<LTCurve 112.484,565.647,114.784,589.525>\n",
            "<LTCurve 118.271,574.774,127.041,579.914>\n",
            "<LTCurve 191.418,565.647,193.718,589.525>\n",
            "<LTCurve 213.594,565.647,215.894,589.525>\n",
            "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,605.537 'This algorithm will converge to the optimal Q-values, but it will take many iterations,\\nand possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the\\nQ-value iteration algorithm (left) converges very quickly, in fewer than 20 iterations,\\nwhile  the  Q-learning  algorithm  (right)  takes  about  8,000  iterations  to  converge.\\nObviously, not knowing the transition probabilities or the rewards makes finding the\\noptimal policy significantly harder!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,357.859,414.999,380.959 'Figure 18-9. Learning curve of the Q-value iteration algorithm versus the Q-learning\\nalgorithm\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,232.458,432.005,343.759 'The  Q-learning  algorithm  is  called  an  off-policy  algorithm  because  the  policy  being\\ntrained is not necessarily the one used during training. For example, in the code we\\njust ran, the policy being executed (the exploration policy) was completely random,\\nwhile the policy being trained was never used. After training, the optimal policy cor‐\\nresponds to systematically choosing the action with the highest Q-value. Conversely,\\nthe policy gradients algorithm is an on-policy algorithm: it explores the world using\\nthe  policy  being  trained.  It  is  somewhat  surprising  that  Q-learning  is  capable  of\\nlearning the optimal policy by just watching an agent act randomly. Imagine learning\\nto play golf when your teacher is a blindfolded monkey. Can we do better?\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,86.249,432.005,220.889 'Exploration Policies\\nOf  course,  Q-learning  can  work  only  if  the  exploration  policy  explores  the  MDP\\nthoroughly  enough.  Although  a  purely  random  policy  is  guaranteed  to  eventually\\nvisit every state and every transition many times, it may take an extremely long time\\nto  do  so.  Therefore,  a  better  option  is  to  use  the  ε-greedy  policy  (ε  is  epsilon):  at\\neach  step  it  acts  randomly  with  probability  ε,  or  greedily  with  probability  1–ε  (i.e.,\\nchoosing the action with the highest Q-value). The advantage of the ε-greedy policy\\n(compared to a completely random policy) is that it will spend more and more time\\nexploring the interesting parts of the environment, as the Q-value estimates get better\\nand better, while still spending some time visiting unknown regions of the MDP. It is\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,40.500,84.436,49.500 '706 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.904,40.500,210.157,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,525.775,432.500,525.775>\n",
            "<LTLine 432.375,386.522,432.375,525.900>\n",
            "<LTLine 72.000,386.647,432.500,386.647>\n",
            "<LTLine 72.125,386.522,72.125,525.900>\n",
            "<LTFigure(I1) 79.450,392.502,425.050,520.650 matrix=[345.60,0.00,0.00,128.15, (79.45,392.50)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'quite common to start with a high value for ε (e.g., 1.0) and then gradually reduce it\\n(e.g., down to 0.05).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,526.036,432.005,574.337 'Alternatively, rather than relying only on chance for exploration, another approach is\\nto encourage the exploration policy to try actions that it has not tried much before.\\nThis  can  be  implemented  as  a  bonus  added  to  the  Q-value  estimates,  as  shown  in\\nEquation 18-6.\\n'>\n",
            "<LTTextBoxHorizontal(2) 87.003,499.749,317.678,510.249 'Equation 18-6. Q-learning using an exploration function\\n'>\n",
            "<LTTextBoxHorizontal(3) 86.998,474.544,111.908,485.994 'Q s, a\\n'>\n",
            "<LTTextBoxHorizontal(4) 129.822,467.421,175.312,485.994 'r + γ · max\\na′\\n'>\n",
            "<LTTextBoxHorizontal(5) 120.236,470.181,124.932,478.181 'α\\n'>\n",
            "<LTTextBoxHorizontal(6) 178.126,474.544,259.816,485.994 '\\xa0 f Q s′, a′ , N s′, a′\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,439.951,140.887,450.451 'In this equation:\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.651,417.351,388.865,429.373 '• N(s′, a′) counts the number of times the action a′ was chosen in state s′.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.651,375.551,432.004,411.251 '•\\n• f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a\\ncuriosity  hyperparameter  that  measures  how  much  the  agent  is  attracted  to  the\\nunknown.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,229.342,432.005,363.981 'Approximate Q-Learning and Deep Q-Learning\\nThe  main  problem  with  Q-learning  is  that  it  does  not  scale  well  to  large  (or  even\\nmedium)  MDPs  with  many  states  and  actions.  For  example,  suppose  you  wanted\\nto  use  Q-learning  to  train  an  agent  to  play  Ms.  Pac-Man  (see  Figure  18-1).  There\\nare  about  150  pellets  that  Ms.  Pac-Man  can  eat,  each  of  which  can  be  present  or\\nabsent (i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045.\\nAnd if you add all the possible combinations of positions for all the ghosts and Ms.\\nPac-Man, the number of possible states becomes larger than the number of atoms in\\nour planet, so there’s absolutely no way you can keep track of an estimate for every\\nsingle Q-value.\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.995,109.942,432.005,221.242 'The solution is to find a function Qθ(s, a) that approximates the Q-value of any state-\\naction pair (s, a) using a manageable number of parameters (given by the parameter\\nvector  θ).  This  is  called  approximate  Q-learning.  For  years  it  was  recommended  to\\nuse  linear  combinations  of  handcrafted  features  extracted  from  the  state  (e.g.,  the\\ndistances of the closest ghosts, their directions, and so on) to estimate Q-values, but\\nin 2013, DeepMind showed that using deep neural networks can work much better,\\nespecially for complex problems, and it does not require any feature engineering. A\\nDNN used to estimate Q-values is called a deep Q-network (DQN), and using a DQN\\nfor approximate Q-learning is called deep Q-learning.\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.997,78.742,432.001,101.842 'Now,  how  can  we  train  a  DQN?  Well,  consider  the  approximate  Q-value  computed\\nby the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want\\n'>\n",
            "<LTTextBoxHorizontal(13) 368.173,40.500,402.517,49.500 'Q-Learning \\n'>\n",
            "<LTTextBoxHorizontal(14) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 420.985,40.500,432.001,49.500 '707\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 95.249,469.047,97.549,492.925>\n",
            "<LTCurve 112.484,469.047,114.784,492.925>\n",
            "<LTCurve 118.271,478.174,127.041,483.314>\n",
            "<LTCurve 189.132,475.986,191.432,485.986>\n",
            "<LTCurve 200.141,475.986,202.441,485.986>\n",
            "<LTCurve 222.317,475.986,224.617,485.986>\n",
            "<LTCurve 238.094,475.986,240.394,485.986>\n",
            "<LTCurve 260.270,475.986,262.570,485.986>\n",
            "<LTCurve 263.730,475.986,266.030,485.986>\n",
            "<LTTextBoxHorizontal(0) 71.995,494.236,432.003,605.537 'this  approximate  Q-value  to  be  as  close  as  possible  to  the  reward  r  that  we  actually\\nobserve after playing action a in state s, plus the discounted value of playing optimally\\nfrom then on. To estimate this sum of future discounted rewards, we can just execute\\nthe DQN on the next state s′, for all possible actions a′. We get an approximate future\\nQ-value for each possible action. We then pick the highest (since we assume we will\\nbe  playing  optimally)  and  discount  it,  and  this  gives  us  an  estimate  of  the  sum  of\\nfuture discounted rewards. By summing the reward r and the future discounted value\\nestimate,  we  get  a  target  Q-value  y(s,  a)  for  the  state-action  pair  (s,  a),  as  shown  in\\nEquation 18-7.\\n'>\n",
            "<LTTextBoxHorizontal(1) 86.996,467.949,211.526,478.449 'Equation 18-7. Target Q-value\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.996,435.621,170.196,454.194 'y s, a = r + γ · max\\na′\\n'>\n",
            "<LTTextBoxHorizontal(3) 173.010,442.351,211.944,454.194 '\\xa0 Qθ s′, a′\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,357.751,432.005,418.651 'With  this  target  Q-value,  we  can  run  a  training  step  using  any  gradient  descent\\nalgorithm.  Specifically,  we  generally  try  to  minimize  the  squared  error  between  the\\nestimated Q-value Qθ(s, a) and the target Q-value y(s, a), or the Huber loss to reduce\\nthe algorithm’s sensitivity to large errors. And that’s the deep Q-learning algorithm!\\nLet’s see how to implement it to solve the CartPole environment.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.993,244.750,432.004,345.411 'Implementing Deep Q-Learning\\nThe  first  thing  we  need  is  a  deep  Q-network.  In  theory,  we  need  a  neural  net  that\\ntakes a state-action pair as input, and outputs an approximate Q-value. However, in\\npractice it’s much more efficient to use a neural net that takes only a state as input,\\nand outputs one approximate Q-value for each possible action. To solve the CartPole\\nenvironment, we do not need a very complicated neural net; a couple of hidden layers\\nwill do:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.000,219.064,305.750,237.764 'input_shape = [4]  # == env.observation_space.shape\\nn_outputs = 2  # == env.action_space.n\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.000,157.864,399.250,207.164 'model = tf.keras.Sequential([\\n    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\\n    tf.keras.layers.Dense(32, activation=\"elu\"),\\n    tf.keras.layers.Dense(n_outputs)\\n])\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,113.350,432.003,149.050 'To  select  an  action  using  this  DQN,  we  pick  the  action  with  the  largest  predicted\\nQ-value. To ensure that the agent explores the environment, we will use an ε-greedy\\npolicy (i.e., we will choose a random action with probability ε):\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.997,40.500,84.435,49.500 '708 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.903,40.500,210.156,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTCurve 93.105,437.247,95.405,461.125>\n",
            "<LTCurve 110.340,437.247,112.640,461.125>\n",
            "<LTCurve 190.216,437.247,192.517,461.125>\n",
            "<LTCurve 212.393,437.247,214.693,461.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,547.150,390.750,606.650 'def epsilon_greedy_policy(state, epsilon=0):\\n    if np.random.rand() < epsilon:\\n        return np.random.randint(n_outputs)  # random action\\n    else:\\n        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\\n        return Q_values.argmax()  # optimal action according to the DQN\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,476.843,432.003,538.337 'Instead  of  training  the  DQN  based  only  on  the  latest  experiences,  we  will  store  all\\nexperiences  in  a  replay  buffer  (or  replay  memory),  and  we  will  sample  a  random\\ntraining  batch  from  it  at  each  training  iteration.  This  helps  reduce  the  correlations\\nbetween the experiences in a training batch, which tremendously helps training. For\\nthis, we will just use a double-ended queue (deque):\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,461.357,212.248,469.857 'from collections import deque\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.998,440.957,233.498,449.457 'replay_buffer = deque(maxlen=2000)\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.787,356.112,396.003,423.312 'A deque is a queue elements can be efficiently added to or removed\\nfrom on both ends. Inserting and deleting items from the ends of\\nthe  queue  is  very  fast,  but  random  access  can  be  slow  when  the\\nqueue gets long. If you need a very large replay buffer, you should\\nuse a circular buffer instead (see the notebook for an implementa‐\\ntion), or check out DeepMind’s Reverb library.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,264.930,432.005,339.023 'Each  experience  will  be  composed  of  six  elements:  a  state  s,  the  action  a  that  the\\nagent  took,  the  resulting  reward  r,  the  next  state  s′  it  reached,  a  Boolean  indicating\\nwhether the episode ended at that point (done), and finally another Boolean indicat‐\\ning  whether  the  episode  was  truncated  at  that  point.  We  will  need  a  small  function\\nto  sample  a  random  batch  of  experiences  from  the  replay  buffer.  It  will  return  six\\nNumPy arrays corresponding to the six experience elements:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.996,188.243,377.996,257.943 'def sample_experiences(batch_size):\\n    indices = np.random.randint(len(replay_buffer), size=batch_size)\\n    batch = [replay_buffer[index] for index in indices]\\n    return [\\n        np.array([experience[field_index] for experience in batch])\\n        for field_index in range(6)\\n    ]  # [states, actions, rewards, next_states, dones, truncateds]\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,156.330,431.997,179.430 'Let’s also create a function that will play a single step using the ε-greedy policy, then\\nstore the resulting experience in the replay buffer:\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.997,100.043,420.497,149.343 'def play_one_step(env, state, epsilon):\\n    action = epsilon_greedy_policy(state, epsilon)\\n    next_state, reward, done, truncated, info = env.step(action)\\n    replay_buffer.append((state, action, reward, next_state, done, truncated))\\n    return next_state, reward, done, truncated, info\\n'>\n",
            "<LTTextBoxHorizontal(9) 307.531,40.500,402.517,49.500 'Implementing Deep Q-Learning \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.985,40.500,432.001,49.500 '709\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,373.329,126.760,429.107 matrix=[41.76,0.00,0.00,55.78, (85.00,373.33)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'Finally, let’s create one last function that will sample a batch of experiences from the\\nreplay buffer and train the DQN by performing a single gradient descent step on this\\nbatch:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.004,523.750,331.254,562.850 'batch_size = 32\\ndiscount_factor = 0.95\\noptimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\\nloss_fn = tf.keras.losses.mean_squared_error\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.004,380.950,412.004,511.850 'def training_step(batch_size):\\n    experiences = sample_experiences(batch_size)\\n    states, actions, rewards, next_states, dones, truncateds = experiences\\n    next_Q_values = model.predict(next_states, verbose=0)\\n    max_next_Q_values = next_Q_values.max(axis=1)\\n    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\\n    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\\n    target_Q_values = target_Q_values.reshape(-1, 1)\\n    mask = tf.one_hot(actions, n_outputs)\\n    with tf.GradientTape() as tape:\\n        all_Q_values = model(states)\\n        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\\n        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.004,350.350,378.004,369.050 '    grads = tape.gradient(loss, model.trainable_variables)\\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,331.037,225.366,341.537 'Here’s what’s happening in this code:\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.652,308.437,432.000,318.937 '•\\n• First we define some hyperparameters, and we create the optimizer and the loss\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.004,295.837,128.382,306.337 'function.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.652,215.643,432.004,290.702 '• Then  we  create  the  training_step()  function.  It  starts  by  sampling  a  batch  of\\n•\\nexperiences, then it uses the DQN to predict the Q-value for each possible action\\nin  each  experience’s  next  state.  Since  we  assume  that  the  agent  will  be  playing\\noptimally, we only keep the maximum Q-value for each next state. Next, we use\\nEquation  18-7  to  compute  the  target  Q-value  for  each  experience’s  state-action\\npair.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.653,71.263,432.005,209.543 '• We  want  to  use  the  DQN  to  compute  the  Q-value  for  each  experienced  state-\\n•\\naction  pair,  but  the  DQN  will  also  output  the  Q-values  for  the  other  possible\\nactions, not just for the action that was actually chosen by the agent. So, we need\\nto mask out all the Q-values we do not need. The tf.one_hot() function makes\\nit possible to convert an array of action indices into such a mask. For example,\\nif  the  first  three  experiences  contain  actions  1,  1,  0,  respectively,  then  the  mask\\nwill start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply the DQN’s\\noutput with this mask, and this will zero out all the Q-values we do not want. We\\nthen sum over axis 1 to get rid of all the zeros, keeping only the Q-values of the\\nexperienced state-action pairs. This gives us the Q_values tensor, containing one\\npredicted Q-value for each experience in the batch.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,40.500,84.434,49.500 '710 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.902,40.500,210.155,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 80.655,595.037,431.997,605.537 '•\\n• Next, we compute the loss: it is the mean squared error between the target and\\n'>\n",
            "<LTTextBoxHorizontal(1) 90.002,582.437,330.231,592.937 'predicted Q-values for the experienced state-action pairs.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.654,565.837,432.002,576.337 '•\\n• Finally, we perform a gradient descent step to minimize the loss with regard to\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.996,553.236,218.778,563.736 'the model’s trainable variables.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,530.636,362.870,541.136 'This was the hardest part. Now training the model is straightforward:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,453.950,416.250,523.650 'for episode in range(600):\\n    obs, info = env.reset()\\n    for step in range(200):\\n        epsilon = max(1 - episode / 500, 0.01)\\n        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\\n        if done or truncated:\\n            break\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.000,423.350,229.250,442.050 '    if episode > 50:\\n        training_step(batch_size)\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,288.857,432.005,414.537 'We  run  600  episodes,  each  for  a  maximum  of  200  steps.  At  each  step,  we  first\\ncompute  the  epsilon  value  for  the  ε-greedy  policy:  it  will  go  from  1  down  to  0.01,\\nlinearly,  in  a  bit  under  500  episodes.  Then  we  call  the  play_one_step()  function,\\nwhich  will  use  the  ε-greedy  policy  to  pick  an  action,  then  execute  it  and  record  the\\nexperience in the replay buffer. If the episode is done or truncated, we exit the loop.\\nFinally,  if  we  are  past  episode  50,  we  call  the  training_step()  function  to  train\\nthe  model  on  one  batch  sampled  from  the  replay  buffer.  The  reason  we  play  many\\nepisodes without training is to give the replay buffer some time to fill up (if we don’t\\nwait enough, then there will not be enough diversity in the replay buffer). And that’s\\nit: we just implemented the Deep Q-learning algorithm!\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,270.257,372.411,280.757 'Figure 18-10 shows the total rewards the agent got during each episode.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,73.897,325.770,84.397 'Figure 18-10. Learning curve of the deep Q-learning algorithm\\n'>\n",
            "<LTTextBoxHorizontal(10) 307.527,40.500,402.513,49.500 'Implementing Deep Q-Learning \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.981,40.500,431.997,49.500 '711\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,263.995,432.500,263.995>\n",
            "<LTLine 432.375,89.960,432.375,264.120>\n",
            "<LTLine 72.000,90.085,432.500,90.085>\n",
            "<LTLine 72.125,89.960,72.125,264.120>\n",
            "<LTFigure(I1) 79.450,95.940,425.050,258.870 matrix=[345.60,0.00,0.00,162.93, (79.45,95.94)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,405.443,432.004,605.537 'As you can see, the algorithm took a while to start learning anything, in part because\\nε  was  very  high  at  the  beginning.  Then  its  progress  was  erratic:  it  first  reached  the\\nmax reward around episode 220, but it immediately dropped, then bounced up and\\ndown  a  few  times,  and  soon  after  it  looked  like  it  had  finally  stabilized  near  the\\nmax reward, at around episode 320, its score again dropped down dramatically. This\\nis  called  catastrophic  forgetting,  and  it  is  one  of  the  big  problems  facing  virtually\\nall  RL  algorithms:  as  the  agent  explores  the  environment,  it  updates  its  policy,  but\\nwhat  it  learns  in  one  part  of  the  environment  may  break  what  it  learned  earlier  in\\nother parts of the environment. The experiences are quite correlated, and the learning\\nenvironment  keeps  changing—this  is  not  ideal  for  gradient  descent!  If  you  increase\\nthe size of the replay buffer, the algorithm will be less subject to this problem. Tuning\\nthe  learning  rate  may  also  help.  But  the  truth  is,  reinforcement  learning  is  hard:\\ntraining is often unstable, and you may need to try many hyperparameter values and\\nrandom  seeds  before  you  find  a  combination  that  works  well.  For  example,  if  you\\ntry changing the activation function from \"elu\" to \"relu\", the performance will be\\nmuch lower.\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.787,263.712,396.000,388.512 'Reinforcement  learning  is  notoriously  difficult,  largely  because  of\\nthe  training  instabilities  and  the  huge  sensitivity  to  the  choice\\nof  hyperparameter  values  and  random  seeds.13  As  the  researcher\\nAndrej Karpathy put it, “[Supervised learning] wants to work. […]\\nRL must be forced to work”. You will need time, patience, persever‐\\nance,  and  perhaps  a  bit  of  luck  too.  This  is  a  major  reason  RL  is\\nnot as widely adopted as regular deep learning (e.g., convolutional\\nnets). But there are a few real-world applications, beyond AlphaGo\\nand  Atari  games:  for  example,  Google  uses  RL  to  optimize  its\\ndatacenter  costs,  and  it  is  used  in  some  robotics  applications,  for\\nhyperparameter tuning, and in recommender systems.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,147.923,432.005,246.623 'You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator\\nof  the  model’s  performance.  The  loss  might  go  down,  yet  the  agent  might  perform\\nworse  (e.g.,  this  can  happen  when  the  agent  gets  stuck  in  one  small  region  of  the\\nenvironment, and the DQN starts overfitting this region). Conversely, the loss could\\ngo up, yet the agent might perform better (e.g., if the DQN was underestimating the\\nQ-values and it starts correctly increasing its predictions, the agent will likely perform\\nbetter, getting more rewards, but the loss might increase because the DQN also sets\\nthe targets, which will be larger too). So, it’s preferable to plot the rewards.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,104.123,432.002,139.823 'The basic deep Q-learning algorithm we’ve been using so far would be too unstable\\nto  learn  to  play  Atari  games.  So  how  did  DeepMind  do  it?  Well,  they  tweaked  the\\nalgorithm!\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,68.954,354.904,76.954 '13 A great 2018 post by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,40.500,84.435,49.500 '712 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.903,40.500,210.156,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 89.000,340.806,126.944,390.307 matrix=[37.94,0.00,0.00,49.50, (89.00,340.81)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,557.712,432.004,607.973 'Deep Q-Learning Variants\\nLet’s  look  at  a  few  variants  of  the  deep  Q-learning  algorithm  that  can  stabilize  and\\nspeed up training.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.994,436.703,432.004,546.143 'Fixed Q-value Targets\\nIn the basic deep Q-learning algorithm, the model is used both to make predictions\\nand  to  set  its  own  targets.  This  can  lead  to  a  situation  analogous  to  a  dog  chasing\\nits  own  tail.  This  feedback  loop  can  make  the  network  unstable:  it  can  diverge,\\noscillate, freeze, and so on. To solve this problem, in their 2013 paper the DeepMind\\nresearchers used two DQNs instead of one: the first is the online model, which learns\\nat each step and is used to move the agent around, and the other is the target model\\nused only to define the targets. The target model is just a clone of the online model:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,411.017,416.251,429.717 \"target = tf.keras.models.clone_model(model)  # clone the model's architecture\\ntarget.set_weights(model.get_weights())  # copy the weights\\n\">\n",
            "<LTTextBoxHorizontal(3) 71.996,365.910,432.001,403.169 'Then, in the  training_step() function, we just need to change one line to use the\\ntarget model instead of the online model when computing the Q-values of the next\\nstates:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,350.424,318.502,358.924 'next_Q_values = target.predict(next_states, verbose=0)\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.001,318.510,432.004,341.610 'Finally,  in  the  training  loop,  we  must  copy  the  weights  of  the  online  model  to  the\\ntarget model, at regular intervals (e.g., every 50 episodes):\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.999,292.824,271.749,311.524 'if episode % 50 == 0:\\n    target.set_weights(model.get_weights())\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,172.117,432.004,284.010 'Since the target model is updated much less often than the online model, the Q-value\\ntargets are more stable, the feedback loop we discussed earlier is dampened, and its\\neffects  are  less  severe.  This  approach  was  one  of  the  DeepMind  researchers’  main\\ncontributions in their 2013 paper, allowing agents to learn to play Atari games from\\nraw  pixels.  To  stabilize  training,  they  used  a  tiny  learning  rate  of  0.00025,  they\\nupdated  the  target  model  only  every  10,000  steps  (instead  of  50),  and  they  used  a\\nvery large replay buffer of 1 million experiences. They decreased epsilon very slowly,\\nfrom 1 to 0.1 in 1 million steps, and they let the algorithm run for 50 million steps.\\nMoreover, their DQN was a deep convolutional net.\\n'>\n",
            "<LTTextBoxHorizontal(8) 325.560,40.500,402.519,49.500 'Deep Q-Learning Variants \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.987,40.500,432.003,49.500 '713\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.001,582.437,432.004,605.537 'Now let’s take a look at another DQN variant that managed to beat the state of the art\\nonce more.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,397.834,432.004,570.867 'Double DQN\\nIn  a  2015  paper,14  DeepMind  researchers  tweaked  their  DQN  algorithm,  increasing\\nits  performance  and  somewhat  stabilizing  training.  They  called  this  variant  double\\nDQN. The update was based on the observation that the target network is prone to\\noverestimating Q-values. Indeed, suppose all actions are equally good: the Q-values\\nestimated  by  the  target  model  should  be  identical,  but  since  they  are  approxima‐\\ntions,  some  may  be  slightly  greater  than  others,  by  pure  chance.  The  target  model\\nwill  always  select  the  largest  Q-value,  which  will  be  slightly  greater  than  the  mean\\nQ-value,  most  likely  overestimating  the  true  Q-value  (a  bit  like  counting  the  height\\nof  the  tallest  random  wave  when  measuring  the  depth  of  a  pool).  To  fix  this,  the\\nresearchers proposed using the online model instead of the target model when select‐\\ning the best actions for the next states, and using the target model only to estimate the\\nQ-values for these best actions. Here is the updated training_step() function:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,300.748,424.749,390.848 'def training_step(batch_size):\\n    experiences = sample_experiences(batch_size)\\n    states, actions, rewards, next_states, dones, truncateds = experiences\\n    next_Q_values = model.predict(next_states, verbose=0)  # ≠ target.predict()\\n    best_next_actions = next_Q_values.argmax(axis=1)\\n    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\\n    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\\n                        ).sum(axis=1)\\n    [...]  # the rest is the same as earlier\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.002,268.834,432.005,291.934 'Just  a  few  months  later,  another  improvement  to  the  DQN  algorithm  was  propose;\\nwe’ll look at that next.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,185.625,432.003,257.264 'Prioritized Experience Replay\\nInstead  of  sampling  experiences  uniformly  from  the  replay  buffer,  why  not  sample\\nimportant experiences more frequently? This idea is called importance sampling (IS)\\nor  prioritized  experience  replay  (PER),  and  it  was  introduced  in  a  2015  paper15  by\\nDeepMind researchers (once again!).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,91.954,414.920,99.954 '14 Hado van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning”, Proceedings of the 30th\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,81.954,274.504,89.954 'AAAI Conference on Artificial Intelligence (2015): 2094–2100.\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.781,68.954,370.393,76.954 '15 Tom Schaul et al., “Prioritized Experience Replay”, arXiv preprint arXiv:1511.05952 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,40.500,84.437,49.500 '714 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.905,40.500,210.158,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,107.900,162.000,107.900>\n",
            "<LTTextBoxHorizontal(0) 71.995,443.836,432.005,605.537 'More  specifically,  experiences  are  considered  “important”  if  they  are  likely  to  lead\\nto  fast  learning  progress.  But  how  can  we  estimate  this?  One  reasonable  approach\\nis  to  measure  the  magnitude  of  the  TD  error  δ  =  r  +  γ·V(s′)  –  V(s).  A  large  TD\\nerror indicates that a transition (s, a, s′) is very surprising, and thus probably worth\\nlearning  from.16  When  an  experience  is  recorded  in  the  replay  buffer,  its  priority  is\\nset to a very large value, to ensure that it gets sampled at least once. However, once\\nit  is  sampled  (and  every  time  it  is  sampled),  the  TD  error  δ  is  computed,  and  this\\nexperience’s priority is set to p = |δ| (plus a small constant to ensure that every expe‐\\nrience  has  a  nonzero  probability  of  being  sampled).  The  probability  P  of  sampling\\nan experience with priority p is proportional to pζ, where ζ is a hyperparameter that\\ncontrols  how  greedy  we  want  importance  sampling  to  be:  when  ζ  =  0,  we  just  get\\nuniform  sampling,  and  when  ζ  =  1,  we  get  full-blown  importance  sampling.  In  the\\npaper, the authors used ζ = 0.6, but the optimal value will depend on the task.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,286.636,432.004,435.736 'There’s  one  catch,  though:  since  the  samples  will  be  biased  toward  important  expe‐\\nriences,  we  must  compensate  for  this  bias  during  training  by  downweighting  the\\nexperiences  according  to  their  importance,  or  else  the  model  will  just  overfit  the\\nimportant  experiences.  To  be  clear,  we  want  important  experiences  to  be  sampled\\nmore often, but this also means we must give them a lower weight during training.\\nTo do this, we define each experience’s training weight as w = (n P)–β, where n is the\\nnumber of experiences in the replay buffer, and β is a hyperparameter that controls\\nhow much we want to compensate for the importance sampling bias (0 means not at\\nall, while 1 means entirely). In the paper, the authors used β = 0.4 at the beginning of\\ntraining and linearly increased it to β = 1 by the end of training. Again, the optimal\\nvalue  will  depend  on  the  task,  but  if  you  increase  one,  you  will  usually  want  to\\nincrease the other as well.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.998,268.036,350.983,278.536 'Now let’s look at one last important variant of the DQN algorithm.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,147.027,432.005,256.467 'Dueling DQN\\nThe dueling DQN algorithm (DDQN, not to be confused with double DQN, although\\nboth techniques can easily be combined) was introduced in yet another 2015 paper17\\nby  DeepMind  researchers.  To  understand  how  it  works,  we  must  first  note  that  the\\nQ-value of a state-action pair (s, a) can be expressed as Q(s, a) = V(s) + A(s, a), where\\nV(s) is the value of state s and A(s, a) is the advantage of taking the action a in state\\ns, compared to all other possible actions in that state. Moreover, the value of a state is\\nequal to the Q-value of the best action a* for that state (since we assume the optimal\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,101.954,408.568,109.954 '16 It could also just be that the rewards are noisy, in which case there are better methods for estimating an\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,91.954,268.888,99.954 'experience’s importance (see the paper for some examples).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.779,78.954,401.407,86.954 '17 Ziyu Wang et al., “Dueling Network Architectures for Deep Reinforcement Learning”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(7) 79.999,68.954,160.551,76.954 'arXiv:1511.06581 (2015).\\n'>\n",
            "<LTTextBoxHorizontal(8) 325.559,40.500,402.518,49.500 'Deep Q-Learning Variants \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.986,40.500,432.002,49.500 '715\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,544.636,432.004,606.349 'policy will pick the best action), so V(s) = Q(s, a*), which implies that A(s, a*) = 0. In\\na dueling DQN, the model estimates both the value of the state and the advantage of\\neach possible action. Since the best action should have an advantage of 0, the model\\nsubtracts the maximum predicted advantage from all predicted advantages. Here is a\\nsimple DDQN model, implemented using the functional API:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,447.550,373.752,537.650 'input_states = tf.keras.layers.Input(shape=[4])\\nhidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\\nhidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\\nstate_values = tf.keras.layers.Dense(1)(hidden2)\\nraw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\\nadvantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\\n                                            keepdims=True)\\nQ_values = state_values + advantages\\nmodel = tf.keras.Model(inputs=[input_states], outputs=[Q_values])\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,377.836,432.000,438.736 'The rest of the algorithm is just the same as earlier. In fact, you can build a double\\ndueling  DQN  and  combine  it  with  prioritized  experience  replay!  More  generally,\\nmany RL techniques can be combined, as DeepMind demonstrated in a 2017 paper:18\\nthe paper’s authors combined six different techniques into an agent called Rainbow,\\nwhich largely outperformed the state of the art.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.002,346.636,432.005,369.736 'As you can see, deep reinforcement learning is a fast-growing field and there’s much\\nmore to discover!\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,296.636,422.594,334.297 'Overview of Some Popular RL Algorithms\\nBefore we close this chapter, let’s take a brief look at a few other popular algorithms:\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,277.036,114.767,288.156 'AlphaGo19\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.996,138.436,432.005,274.936 'AlphaGo uses a variant of Monte Carlo tree search (MCTS) based on deep neural\\nnetworks  to  beat  human  champions  at  the  game  of  Go.  MCTS  was  invented  in\\n1949  by  Nicholas  Metropolis  and  Stanislaw  Ulam.  It  selects  the  best  move  after\\nrunning  many  simulations,  repeatedly  exploring  the  search  tree  starting  from\\nthe current position, and spending more time on the most promising branches.\\nWhen  it  reaches  a  node  that  it  hasn’t  visited  before,  it  plays  randomly  until\\nthe  game  ends,  and  updates  its  estimates  for  each  visited  node  (excluding  the\\nrandom  moves),  increasing  or  decreasing  each  estimate  depending  on  the  final\\noutcome. AlphaGo is based on the same principle, but it uses a policy network\\nto  select  moves,  rather  than  playing  randomly.  This  policy  net  is  trained  using\\npolicy  gradients.  The  original  algorithm  involved  three  more  neural  networks,\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,101.954,425.608,109.954 '18 Matteo Hessel et al., “Rainbow: Combining Improvements in Deep Reinforcement Learning”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.000,91.954,199.072,99.954 'arXiv:1710.02298 (2017): 3215–3222.\\n'>\n",
            "<LTTextBoxHorizontal(9) 69.781,78.954,412.177,86.954 '19 David Silver et al., “Mastering the Game of Go with Deep Neural Networks and Tree Search”, Nature 529\\n'>\n",
            "<LTTextBoxHorizontal(10) 80.001,68.954,133.561,76.954 '(2016): 484–489.\\n'>\n",
            "<LTTextBoxHorizontal(11) 72.001,40.500,84.439,49.500 '716 \\n'>\n",
            "<LTTextBoxHorizontal(12) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 102.907,40.500,210.160,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTTextBoxHorizontal(0) 89.996,519.436,432.004,606.157 'and  was  more  complicated,  but  it  was  simplified  in  the  AlphaGo  Zero  paper,20\\nwhich uses a single neural network to both select moves and evaluate game states.\\nThe AlphaZero paper21 generalized this algorithm, making it capable of tackling\\nnot  only  the  game  of  Go,  but  also  chess  and  shogi  (Japanese  chess).  Lastly,  the\\nMuZero  paper22  continued  to  improve  upon  this  algorithm,  outperforming  the\\nprevious  iterations  even  though  the  agent  starts  out  without  even  knowing  the\\nrules of the game!\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,499.836,164.704,510.336 'Actor-critic algorithms\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.995,386.436,432.004,497.736 'Actor-critics  are  a  family  of  RL  algorithms  that  combine  policy  gradients  with\\ndeep  Q-networks.  An  actor-critic  agent  contains  two  neural  networks:  a  policy\\nnet  and  a  DQN.  The  DQN  is  trained  normally,  by  learning  from  the  agent’s\\nexperiences.  The  policy  net  learns  differently  (and  much  faster)  than  in  regular\\nPG:  instead  of  estimating  the  value  of  each  action  by  going  through  multiple\\nepisodes,  then  summing  the  future  discounted  rewards  for  each  action,  and\\nfinally normalizing them, the agent (actor) relies on the action values estimated\\nby the DQN (critic). It’s a bit like an athlete (the agent) learning with the help of a\\ncoach (the DQN).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,366.836,255.198,377.956 'Asynchronous advantage actor-critic (A3C)23\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.995,266.036,432.004,364.736 'This  is  an  important  actor-critic  variant  introduced  by  DeepMind  researchers\\nin 2016 where multiple agents learn in parallel, exploring different copies of the\\nenvironment.  At  regular  intervals,  but  asynchronously  (hence  the  name),  each\\nagent  pushes  some  weight  updates  to  a  master  network,  then  it  pulls  the  latest\\nweights from that network. Each agent thus contributes to improving the master\\nnetwork and benefits from what the other agents have learned. Moreover, instead\\nof  estimating  the  Q-values,  the  DQN  estimates  the  advantage  of  each  action\\n(hence the second A in the name), which stabilizes training.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.004,246.436,191.190,256.936 'Advantage actor-critic (A2C)\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.995,208.636,432.001,244.336 'A2C is a variant of the A3C algorithm that removes the asynchronicity. All model\\nupdates are synchronous, so gradient updates are performed over larger batches,\\nwhich allows the model to better utilize the power of the GPU.\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,137.954,416.560,145.954 '20 David Silver et al., “Mastering the Game of Go Without Human Knowledge”, Nature 550 (2017): 354–359.\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.779,124.954,417.855,132.954 '21 David Silver et al., “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algo‐\\n'>\n",
            "<LTTextBoxHorizontal(9) 79.999,114.954,208.847,122.954 'rithm”, arXiv preprint arXiv:1712.01815.\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.777,101.954,421.085,109.954 '22 Julian Schrittwieser et al., “Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model”, arXiv\\n'>\n",
            "<LTTextBoxHorizontal(11) 79.997,91.954,188.893,99.954 'preprint arXiv:1911.08265 (2019).\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.780,78.954,422.480,86.954 '23 Volodymyr Mnih et al., “Asynchronous Methods for Deep Reinforcement Learning”, Proceedings of the 33rd\\n'>\n",
            "<LTTextBoxHorizontal(13) 80.000,68.954,289.112,76.954 'International Conference on Machine Learning (2016): 1928–1937.\\n'>\n",
            "<LTTextBoxHorizontal(14) 280.488,40.500,402.519,49.500 'Overview of Some Popular RL Algorithms \\n'>\n",
            "<LTTextBoxHorizontal(15) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 420.987,40.500,432.003,49.500 '717\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,153.900,162.000,153.900>\n",
            "<LTTextBoxHorizontal(0) 72.005,595.037,168.726,606.157 'Soft actor-critic (SAC)24\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.996,494.236,432.005,592.937 'SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other\\nUC  Berkeley  researchers.  It  learns  not  only  rewards,  but  also  to  maximize  the\\nentropy of its actions. In other words, it tries to be as unpredictable as possible\\nwhile  still  getting  as  many  rewards  as  possible.  This  encourages  the  agent  to\\nexplore  the  environment,  which  speeds  up  training,  and  makes  it  less  likely  to\\nrepeatedly execute the same action when the DQN produces imperfect estimates.\\nThis  algorithm  has  demonstrated  an  amazing  sample  efficiency  (contrary  to  all\\nthe previous algorithms, which learn very slowly).\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,474.636,224.903,485.756 'Proximal policy optimization (PPO)25\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.996,399.036,432.003,472.536 'This  algorithm  by  John  Schulman  and  other  OpenAI  researchers  is  based  on\\nA2C,  but  it  clips  the  loss  function  to  avoid  excessively  large  weight  updates\\n(which often lead to training instabilities). PPO is a simplification of the previous\\ntrust  region  policy  optimization26  (TRPO)  algorithm,  also  by  OpenAI.  OpenAI\\nmade the news in April 2019 with its AI called OpenAI Five, based on the PPO\\nalgorithm, which defeated the world champions at the multiplayer game Dota 2.\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.003,379.436,191.006,390.556 'Curiosity-based exploration27\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.995,203.036,432.005,377.336 'A recurring problem in RL is the sparsity of the rewards, which makes learning\\nvery slow and inefficient. Deepak Pathak and other UC Berkeley researchers have\\nproposed  an  exciting  way  to  tackle  this  issue:  why  not  ignore  the  rewards,  and\\njust make the agent extremely curious to explore the environment? The rewards\\nthus  become  intrinsic  to  the  agent,  rather  than  coming  from  the  environment.\\nSimilarly,  stimulating  curiosity  in  a  child  is  more  likely  to  give  good  results\\nthan  purely  rewarding  the  child  for  getting  good  grades.  How  does  this  work?\\nThe  agent  continuously  tries  to  predict  the  outcome  of  its  actions,  and  it  seeks\\nsituations where the outcome does not match its predictions. In other words, it\\nwants to be surprised. If the outcome is predictable (boring), it goes elsewhere.\\nHowever,  if  the  outcome  is  unpredictable  but  the  agent  notices  that  it  has  no\\ncontrol  over  it,  it  also  gets  bored  after  a  while.  With  only  curiosity,  the  authors\\nsucceeded in training an agent at many video games: even though the agent gets\\nno penalty for losing, the game starts over, which is boring so it learns to avoid it.\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,127.954,429.560,145.954 '24 Tuomas Haarnoja et al., “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with\\na Stochastic Actor”, Proceedings of the 35th International Conference on Machine Learning (2018): 1856–1865.\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.783,114.954,420.643,122.954 '25 John Schulman et al., “Proximal Policy Optimization Algorithms”, arXiv preprint arXiv:1707.06347 (2017).\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.781,101.954,428.169,109.954 '26 John Schulman et al., “Trust Region Policy Optimization”, Proceedings of the 32nd International Conference on\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.001,91.954,199.849,99.954 'Machine Learning (2015): 1889–1897.\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.781,78.954,419.465,86.954 '27 Deepak Pathak et al., “Curiosity-Driven Exploration by Self-Supervised Prediction”, Proceedings of the 34th\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.001,68.954,289.113,76.954 'International Conference on Machine Learning (2017): 2778–2787.\\n'>\n",
            "<LTTextBoxHorizontal(12) 72.004,40.500,84.442,49.500 '718 \\n'>\n",
            "<LTTextBoxHorizontal(13) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 102.910,40.500,210.163,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,153.900,162.000,153.900>\n",
            "<LTTextBoxHorizontal(0) 71.998,595.037,186.574,605.537 'Open-ended learning (OEL)\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.995,380.836,432.004,592.937 'The  objective  of  OEL  is  to  train  agents  capable  of  endlessly  learning  new  and\\ninteresting tasks, typically generated procedurally. We’re not there yet, but there\\nhas  been  some  amazing  progress  over  the  last  few  years.  For  example,  a  2019\\npaper28 by a team of researchers from Uber AI introduced the POET algorithm,\\nwhich  generates  multiple  simulated  2D  environments  with  bumps  and  holes\\nand  trains  one  agent  per  environment:  the  agent’s  goal  is  to  walk  as  fast  as\\npossible while avoiding the obstacles. The algorithm starts out with simple envi‐\\nronments,  but  they  gradually  get  harder  over  time:  this  is  called  curriculum\\nlearning. Moreover, although each agent is only trained within one environment,\\nit must regularly compete against other agents, across all environments. In each\\nenvironment, the winner is copied over and it replaces the agent that was there\\nbefore.  This  way,  knowledge  is  regularly  transferred  across  environments,  and\\nthe  most  adaptable  agents  are  selected.  In  the  end,  the  agents  are  much  better\\nwalkers  than  agents  trained  on  a  single  task,  and  they  can  tackle  much  harder\\nenvironments. Of course, this principle can be applied to other environments and\\ntasks as well. If you’re interested in OEL, make sure to check out the Enhanced\\nPOET paper,29 as well as DeepMind’s 2021 paper30 on this topic.\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.793,342.785,396.003,363.905 'If you’d like to learn more about reinforcement learning, check out\\nthe book Reinforcement Learning by Phil Winder (O’Reilly).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,212.236,432.003,298.336 'We  covered  many  topics  in  this  chapter:  policy  gradients,  Markov  chains,  Markov\\ndecision  processes,  Q-learning,  approximate  Q-learning,  and  deep  Q-learning  and\\nits main variants (fixed Q-value targets, double DQN, dueling DQN, and prioritized\\nexperience  replay),  and  finally  we  took  a  quick  look  at  a  few  other  popular  algo‐\\nrithms.  Reinforcement  learning  is  a  huge  and  exciting  field,  with  new  ideas  and\\nalgorithms popping out every day, so I hope this chapter sparked your curiosity: there\\nis a whole world to explore!\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,124.954,420.136,132.954 '28 Rui Wang et al., “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.000,114.954,381.360,122.954 'Diverse Learning Environments and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.782,101.954,426.298,109.954 '29 Rui Wang et al., “Enhanced POET: Open-Ended Reinforcement Learning Through Unbounded Invention of\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.002,91.954,344.562,99.954 'Learning Challenges and Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.783,78.954,431.563,86.954 '30 Open-Ended Learning Team et al., “Open-Ended Learning Leads to Generally Capable Agents”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.003,68.954,160.555,76.954 'arXiv:2107.12808 (2021).\\n'>\n",
            "<LTTextBoxHorizontal(10) 280.481,40.500,402.512,49.500 'Overview of Some Popular RL Algorithms \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.980,40.500,431.996,49.500 '719\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,140.900,162.000,140.900>\n",
            "<LTFigure(I1) 85.000,313.923,126.760,369.700 matrix=[41.76,0.00,0.00,55.78, (85.00,313.92)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,589.053,127.644,607.973 'Exercises\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.314,566.312,432.001,576.812 '1.\\n1. How would you define reinforcement learning? How is it different from regular\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.995,553.712,246.792,564.212 'supervised or unsupervised learning?\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.314,511.913,432.000,547.613 '2. Can  you  think  of  three  possible  applications  of  RL  that  were  not  mentioned  in\\n2.\\nthis  chapter?  For  each  of  them,  what  is  the  environment?  What  is  the  agent?\\nWhat are some possible actions? What are the rewards?\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.313,495.312,432.004,505.812 '3. What  is  the  discount  factor?  Can  the  optimal  policy  change  if  you  modify  the\\n3.\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.998,482.713,157.303,493.213 'discount factor?\\n'>\n",
            "<LTTextBoxHorizontal(6) 77.317,466.113,398.226,476.613 '4.\\n4. How do you measure the performance of a reinforcement learning agent?\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.311,449.513,431.999,460.013 '5.\\n5. What  is  the  credit  assignment  problem?  When  does  it  occur?  How  can  you\\n'>\n",
            "<LTTextBoxHorizontal(8) 90.003,436.912,136.518,447.412 'alleviate it?\\n'>\n",
            "<LTTextBoxHorizontal(9) 77.311,420.312,266.035,430.812 '6.\\n6. What is the point of using a replay buffer?\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.311,403.712,241.056,414.212 '7.\\n7. What is an off-policy RL algorithm?\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.312,387.112,403.764,397.612 '8.\\n8. Use policy gradients to solve OpenAI Gym’s LunarLander-v2 environment.\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.312,281.719,432.005,381.012 '9. Use  a  double  dueling  DQN  to  train  an  agent  that  can  achieve  a  superhuman\\n9.\\nlevel at the famous Atari Breakout game (\"ALE/Breakout-v5\"). The observations\\nare  images.  To  simplify  the  task,  you  should  convert  them  to  grayscale  (i.e.,\\naverage over the channels axis) then crop them and downsample them, so they’re\\njust  large  enough  to  play,  but  not  more.  An  individual  image  does  not  tell  you\\nwhich way the ball and the paddles are going, so you should merge two or three\\nconsecutive  images  to  form  each  state.  Lastly,  the  DQN  should  be  composed\\nmostly of convolutional layers.\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.272,151.719,432.005,275.619 '10. If  you  have  about  $100  to  spare,  you  can  purchase  a  Raspberry  Pi  3  plus  some\\n10.\\ncheap  robotics  components,  install  TensorFlow  on  the  Pi,  and  go  wild!  For  an\\nexample, check out this fun post by Lukas Biewald, or take a look at GoPiGo or\\nBrickPi.  Start  with  simple  goals,  like  making  the  robot  turn  around  to  find  the\\nbrightest  angle  (if  it  has  a  light  sensor)  or  the  closest  object  (if  it  has  a  sonar\\nsensor), and move in that direction. Then you can start using deep learning: for\\nexample, if the robot has a camera, you can try to implement an object detection\\nalgorithm so it detects people and moves toward them. You can also try to use RL\\nto make the agent learn on its own how to use the motors to achieve that goal.\\nHave fun!\\n'>\n",
            "<LTTextBoxHorizontal(14) 72.002,116.519,432.005,139.619 'Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\\nhttps://homl.info/colab3.\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.001,40.500,84.439,49.500 '720 \\n'>\n",
            "<LTTextBoxHorizontal(16) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(17) 102.907,40.500,210.160,49.500 'Chapter 18: Reinforcement Learning\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 205.141,509.542,431.995,582.331 'CHAPTER 19\\nTraining and Deploying\\nTensorFlow Models at Scale\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,168.309,432.005,393.009 'Once  you  have  a  beautiful  model  that  makes  amazing  predictions,  what  do  you  do\\nwith  it?  Well,  you  need  to  put  it  in  production!  This  could  be  as  simple  as  running\\nthe model on a batch of data, and perhaps writing a script that runs this model every\\nnight. However, it is often much more involved. Various parts of your infrastructure\\nmay need to use this model on live data, in which case you will probably want to wrap\\nyour model in a web service: this way, any part of your infrastructure can query the\\nmodel at any time using a simple REST API (or some other protocol), as we discussed\\nin Chapter 2. But as time passes, you’ll need to regularly retrain your model on fresh\\ndata and push the updated version to production. You must handle model versioning,\\ngracefully  transition  from  one  model  to  the  next,  possibly  roll  back  to  the  previous\\nmodel in case of problems, and perhaps run multiple different models in parallel to\\nperform A/B experiments.1 If your product becomes successful, your service may start\\nto get a large number of of queries per second (QPS), and it must scale up to support\\nthe load. A great solution to scale up your service, as you will see in this chapter, is\\nto use TF Serving, either on your own hardware infrastructure or via a cloud service\\nsuch as Google Vertex AI.2 It will take care of efficiently serving your model, handle\\ngraceful model transitions, and more. If you use the cloud platform you will also get\\nmany extra features, such as powerful monitoring tools.\\n'>\n",
            "<LTTextBoxHorizontal(2) 73.140,101.954,424.824,109.954 '1 An A/B experiment consists in testing two different versions of your product on different subsets of users in\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.000,91.954,281.680,99.954 'order to check which version works best and get other insights.\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.142,78.954,419.234,86.954 '2 Google AI Platform (formerly known as Google ML Engine) and Google AutoML merged in 2021 to form\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.002,68.954,137.594,76.954 'Google Vertex AI.\\n'>\n",
            "<LTTextBoxHorizontal(6) 420.986,40.500,432.002,49.500 '721\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTLine 72.000,117.900,162.000,117.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,469.037,432.005,605.537 'Moreover,  if  you  have  a  lot  of  training  data  and  compute-intensive  models,  then\\ntraining  time  may  be  prohibitively  long.  If  your  product  needs  to  adapt  to  changes\\nquickly,  then  a  long  training  time  can  be  a  showstopper  (e.g.,  think  of  a  news\\nrecommendation system promoting news from last week). Perhaps even more impor‐\\ntantly, a long training time will prevent you from experimenting with new ideas. In\\nmachine learning (as in many other fields), it is hard to know in advance which ideas\\nwill work, so you should try out as many as possible, as fast as possible. One way to\\nspeed up training is to use hardware accelerators such as GPUs or TPUs. To go even\\nfaster, you can train a model across multiple machines, each equipped with multiple\\nhardware  accelerators.  TensorFlow’s  simple  yet  powerful  distribution  strategies  API\\nmakes this easy, as you will see.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,374.836,432.004,460.936 'In  this  chapter  we  will  look  at  how  to  deploy  models,  first  using  TF  Serving,  then\\nusing  Vertex  AI.  We  will  also  take  a  quick  look  at  deploying  models  to  mobile\\napps, embedded devices, and web apps. Then we will discuss how to speed up com‐\\nputations  using  GPUs  and  how  to  train  models  across  multiple  devices  and  servers\\nusing the distribution strategies API. Lastly, we will explore how to train models and\\nfine-tune  their  hyperparameters  at  scale  using  Vertex  AI.  That’s  a  lot  of  topics  to\\ndiscuss, so let’s dive in!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.993,198.243,432.005,362.497 'Serving a TensorFlow Model\\nOnce you have trained a TensorFlow model, you can easily use it in any Python code:\\nif it’s a Keras model, just call its predict() method! But as your infrastructure grows,\\nthere  comes  a  point  where  it  is  preferable  to  wrap  your  model  in  a  small  service\\nwhose sole role is to make predictions and have the rest of the infrastructure query\\nit  (e.g.,  via  a  REST  or  gRPC  API).3  This  decouples  your  model  from  the  rest  of  the\\ninfrastructure, making it possible to easily switch model versions or scale the service\\nup  as  needed  (independently  from  the  rest  of  your  infrastructure),  perform  A/B\\nexperiments, and ensure that all your software components rely on the same model\\nversions. It also simplifies testing and development, and more. You could create your\\nown microservice using any technology you want (e.g., using the Flask library), but\\nwhy reinvent the wheel when you can just use TF Serving?\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,127.634,432.005,186.673 'Using TensorFlow Serving\\nTF  Serving  is  a  very  efficient,  battle-tested  model  server,  written  in  C++.  It  can\\nsustain  a  high  load,  serve  multiple  versions  of  your  models  and  watch  a  model\\nrepository to automatically deploy the latest versions, and more (see Figure 19-1).\\n'>\n",
            "<LTTextBoxHorizontal(4) 73.140,68.954,429.432,96.954 '3 A REST (or RESTful) API is an API that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE,\\nand uses JSON inputs and outputs. The gRPC protocol is more complex but more efficient; data is exchanged\\nusing protocol buffers (see Chapter 13).\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.002,40.500,84.440,49.500 '722 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.908,40.500,289.712,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTTextBoxHorizontal(0) 72.000,394.013,420.170,417.113 'Figure 19-1. TF Serving can serve multiple models and automatically deploy the latest\\nversion of each model\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,344.213,432.000,379.913 'So  let’s  suppose  you  have  trained  an  MNIST  model  using  Keras,  and  you  want  to\\ndeploy  it  to  TF  Serving.  The  first  thing  you  have  to  do  is  export  this  model  to  the\\nSavedModel format, introduced in Chapter 10.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.002,319.846,160.979,331.406 'Exporting SavedModels\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,288.728,432.002,313.387 'You already know how to save the model: just call model.save(). Now to version the\\nmodel, you just need to create a subdirectory for each model version. Easy!\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.996,263.041,190.996,281.741 'from pathlib import Path\\nimport tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.996,232.441,428.996,251.141 'X_train, X_valid, X_test = [...]  # load and split the MNIST dataset\\nmodel = [...]  # build & train an MNIST model (also handles image preprocessing)\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.997,181.441,280.247,220.541 'model_name = \"my_mnist_model\"\\nmodel_version = \"0001\"\\nmodel_path = Path(model_name) / model_version\\nmodel.save(model_path, save_format=\"tf\")\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,99.128,432.004,172.628 'It’s usually a good idea to include all the preprocessing layers in the final model you\\nexport so that it can ingest data in its natural form once it is deployed to production.\\nThis  avoids  having  to  take  care  of  preprocessing  separately  within  the  application\\nthat uses the model. Bundling the preprocessing steps within the model also makes it\\nsimpler to update them later on and limits the risk of mismatch between a model and\\nthe preprocessing steps it requires.\\n'>\n",
            "<LTTextBoxHorizontal(8) 318.358,40.500,402.517,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.985,40.500,432.001,49.500 '723\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,422.677,432.375,607.500>\n",
            "<LTLine 72.000,422.802,432.500,422.802>\n",
            "<LTLine 72.125,422.677,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,428.657,378.250,602.250 matrix=[252.00,0.00,0.00,173.59, (126.25,428.66)]>\n",
            "<LTTextBoxHorizontal(0) 136.790,561.002,396.001,605.705 'Since  a  SavedModel  saves  the  computation  graph,  it  can  only  be\\nused  with  models  that  are  based  exclusively  on  TensorFlow  oper‐\\nations,  excluding  the  tf.py_function()  operation,  which  wraps\\narbitrary Python code.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,516.443,432.003,541.103 'TensorFlow comes with a small saved_model_cli command-line interface to inspect\\nSavedModels. Let use it to inspect our exported model:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,480.557,314.252,509.457 \"$ saved_model_cli show --dir my_mnist_model/0001\\nThe given SavedModel contains the following tag-sets:\\n'serve'\\n\">\n",
            "<LTTextBoxHorizontal(3) 71.996,332.870,432.044,471.743 'What does this output mean? Well, a SavedModel contains one or more metagraphs.\\nA  metagraph  is  a  computation  graph  plus  some  function  signature  definitions,\\nincluding their input and output names, types, and shapes. Each metagraph is identi‐\\nfied by a set of tags. For example, you may want to have a metagraph containing the\\nfull  computation  graph,  including  the  training  operations:  you  would  typically  tag\\nthis  one  as  \"train\".  And  you  might  have  another  metagraph  containing  a  pruned\\ncomputation graph with only the prediction operations, including some GPU-specific\\noperations:  this  one  might  be  tagged  as  \"serve\",  \"gpu\".  You  might  want  to  have\\nother metagraphs as well. This can be done using TensorFlow’s low-level SavedModel\\nAPI.  However,  when  you  save  a  Keras  model  using  its  save()  method,  it  saves  a\\nsingle metagraph tagged as \"serve\". Let’s inspect this \"serve\" tag set:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.003,286.784,399.253,325.884 '$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve\\nThe given SavedModel MetaGraphDef contains SignatureDefs with these keys:\\nSignatureDef key: \"__saved_model_init_op\"\\nSignatureDef key: \"serving_default\"\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,215.290,432.002,277.970 'This  metagraph  contains  two  signature  definitions:  an  initialization  function  called\\n\"__saved_model_init_op\",  which  you  do  not  need  to  worry  about,  and  a  default\\nserving function called \"serving_default\". When saving a Keras model, the default\\nserving  function  is  the  model’s  call()  method,  which  makes  predictions,  as  you\\nalready know. Let’s get more details about this serving function:\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.001,40.500,84.439,49.500 '724 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.907,40.500,289.711,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,559.190,132.680,606.500 matrix=[49.68,0.00,0.00,47.31, (83.00,559.19)]>\n",
            "<LTTextBoxHorizontal(0) 88.996,475.750,373.746,606.650 \"$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve \\\\\\n                       --signature_def serving_default\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['flatten_input'] tensor_info:\\n      dtype: DT_UINT8\\n      shape: (-1, 28, 28)\\n      name: serving_default_flatten_input:0\\nThe given SavedModel SignatureDef contains the following output(s):\\n  outputs['dense_1'] tensor_info:\\n      dtype: DT_FLOAT\\n      shape: (-1, 10)\\n      name: StatefulPartitionedCall:0\\nMethod name is: tensorflow/serving/predict\\n\">\n",
            "<LTTextBoxHorizontal(1) 71.996,430.050,431.999,467.902 'Note that the function’s input is named  \"flatten_input\", and the output is named\\n\"dense_1\". These correspond to the Keras model’s input and output layer names. You\\ncan also see the type and shape of the input and output data. Looks good!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,411.450,368.778,421.950 'Now that you have a SavedModel, the next step is to install TF Serving.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,387.082,232.659,398.642 'Installing and starting TensorFlow Serving\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,343.365,432.002,379.658 'There are many ways to install TF Serving: using the system’s package manager, using\\na Docker image,4 installing from source, and more. Since Colab runs on Ubuntu, we\\ncan use Ubuntu’s apt package manager like this:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,276.878,399.252,336.378 'url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\\nsrc = \"stable tensorflow-model-server tensorflow-model-server-universal\"\\n!echo \\'deb {url} {src}\\' > /etc/apt/sources.list.d/tensorflow-serving.list\\n!curl \\'{url}/tensorflow-serving.release.pub.gpg\\' | apt-key add -\\n!apt update -q && apt-get install -y tensorflow-model-server\\n%pip install -q -U tensorflow-serving-api\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,193.378,432.005,268.065 'This  code  starts  by  adding  TensorFlow’s  package  repository  to  Ubuntu’s  list  of\\npackage  sources.  Then  it  downloads  TensorFlow’s  public  GPG  key  and  adds  it  to\\nthe  package  manager’s  key  list  so  it  can  verify  TensorFlow’s  package  signatures.\\nNext,  it  uses  apt  to  install  the  tensorflow-model-server  package.  Lastly,  it  installs\\nthe  tensorflow-serving-api  library,  which  we  will  need  to  communicate  with  the\\nserver.\\n'>\n",
            "<LTTextBoxHorizontal(7) 73.140,68.954,431.176,126.954 '4 If you are not familiar with Docker, it allows you to easily download a set of applications packaged in a Docker\\nimage (including all their dependencies and usually some good default configuration) and then run them on\\nyour system using a Docker engine. When you run an image, the engine creates a Docker container that keeps\\nthe applications well isolated from your own system—but you can give it some limited access if you want. It\\nis similar to a virtual machine, but much faster and lighter, as the container relies directly on the host’s kernel.\\nThis means that the image does not need to include or run its own kernel.\\n'>\n",
            "<LTTextBoxHorizontal(8) 318.358,40.500,402.517,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.985,40.500,432.001,49.500 '725\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,134.900,162.000,134.900>\n",
            "<LTTextBoxHorizontal(0) 71.996,568.650,431.999,605.537 'Now we want to start the server. The command will require the absolute path of the\\nbase model directory (i.e., the path to my_mnist_model, not 0001), so let’s save that to\\nthe MODEL_DIR environment variable:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.996,553.163,127.246,561.663 'import os\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.996,532.763,339.746,541.263 'os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,513.450,191.780,523.950 'We can then start the server:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.998,446.964,331.248,506.464 '%%bash --bg\\ntensorflow_model_server \\\\\\n     --port=8500 \\\\\\n     --rest_api_port=8501 \\\\\\n     --model_name=my_mnist_model \\\\\\n     --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,363.463,432.003,439.116 'In  Jupyter  or  Colab,  the  %%bash  --bg  magic  command  executes  the  cell  as  a  bash\\nscript,  running  it  in  the  background.  The  >my_server.log  2>&1  part  redirects  the\\nstandard output and standard error to the my_server.log file. And that’s it! TF Serving\\nis now running in the background, and its logs are saved to my_server.log. It loaded\\nour  MNIST  model  (version  1),  and  it  is  now  waiting  for  gRPC  and  REST  requests,\\nrespectively, on ports 8500 and 8501.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.251,251.392,423.751,343.452 'Running TF Serving in a Docker Container\\nIf  you  are  running  the  notebook  on  your  own  machine  and  you  have  installed\\nDocker, you can run docker pull tensorflow/serving in a terminal to download\\nthe  TF  Serving  image.  The  TensorFlow  team  highly  recommends  this  installation\\nmethod  because  it  is  simple,  it  will  not  mess  with  your  system,  and  it  offers  high\\nperformance.5 To start the server inside a Docker container, you can run the following\\ncommand in a terminal:\\n'>\n",
            "<LTTextBoxHorizontal(7) 97.254,225.712,424.504,244.412 '$ docker run -it --rm -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\\\\n    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.254,206.992,285.794,216.992 'Here is what all these command-line options mean:\\n'>\n",
            "<LTTextBoxHorizontal(9) 80.252,189.412,94.502,198.912 '-it\\n'>\n",
            "<LTTextBoxHorizontal(10) 98.252,163.427,423.752,185.427 'Makes the container interactive (so you can press Ctrl-C to stop it) and displays\\nthe server’s output.\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.248,145.847,99.248,155.347 '--rm\\n'>\n",
            "<LTTextBoxHorizontal(12) 98.248,119.862,423.748,141.862 'Deletes  the  container  when  you  stop  it:  no  need  to  clutter  your  machine  with\\ninterrupted containers. However, it does not delete the image.\\n'>\n",
            "<LTTextBoxHorizontal(13) 73.140,78.954,419.024,86.954 '5 There are also GPU images available, and other installation options. For more details, please check out the\\n'>\n",
            "<LTTextBoxHorizontal(14) 80.000,68.954,181.808,76.954 'official installation instructions.\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.001,40.500,84.439,49.500 '726 \\n'>\n",
            "<LTTextBoxHorizontal(16) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(17) 102.907,40.500,289.711,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,353.202,432.000,353.202>\n",
            "<LTLine 431.875,119.732,431.875,353.327>\n",
            "<LTLine 72.125,119.732,72.125,353.327>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 80.246,597.050,322.496,606.550 '-v \"/path/to/my_mnist_model:/models/my_mnist_model\"\\n'>\n",
            "<LTTextBoxHorizontal(1) 98.246,546.500,423.746,593.065 'Makes  the  host’s  my_mnist_model  directory  available  to  the  container  at  the\\npath  /models/mnist_model.  You  must  replace  /path/to/my_mnist_model  with  the\\nabsolute path of this directory. On Windows, remember to use \\\\ instead of / in\\nthe host path, but not in the container path (since the container runs on Linux).\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.246,528.920,137.246,538.420 '-p 8500:8500\\n'>\n",
            "<LTTextBoxHorizontal(3) 98.246,502.935,423.746,524.935 'Makes  the  Docker  engine  forward  the  host’s  TCP  port  8500  to  the  container’s\\nTCP port 8500. By default, TF Serving uses this port to serve the gRPC API.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.249,485.355,137.249,494.855 '-p 8501:8501\\n'>\n",
            "<LTTextBoxHorizontal(5) 98.249,459.370,423.749,481.370 'Forwards the host’s TCP port 8501 to the container’s TCP port 8501. The Docker\\nimage is configured to use this port by default to serve the REST API.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.246,441.790,213.246,451.290 '-e MODEL_NAME=my_mnist_model\\n'>\n",
            "<LTTextBoxHorizontal(7) 98.246,403.240,423.749,438.725 'Sets  the  container’s  MODEL_NAME  environment  variable,  so  TF  Serving  knows\\nwhich model to serve. By default, it will look for models in the /models directory,\\nand it will automatically serve the latest version it finds.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.245,385.660,165.745,395.160 'tensorflow/serving\\n'>\n",
            "<LTTextBoxHorizontal(9) 98.245,371.675,245.365,381.675 'This is the name of the image to run.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,325.231,432.000,348.332 'Now that the server is up and running, let’s query it, first using the REST API, then\\nthe gRPC API.\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.995,300.864,229.858,312.424 'Querying TF Serving through the REST API\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.996,257.739,432.001,293.440 'Let’s start by creating the query. It must contain the name of the function signature\\nyou want to call, and of course the input data. Since the request must use the JSON\\nformat, we have to convert the input images from a NumPy array to a Python list:\\n'>\n",
            "<LTTextBoxHorizontal(13) 88.998,242.253,135.748,250.753 'import json\\n'>\n",
            "<LTTextBoxHorizontal(14) 88.998,181.053,377.998,230.353 'X_new = X_test[:3]  # pretend we have 3 new digit images to classify\\nrequest_json = json.dumps({\\n    \"signature_name\": \"serving_default\",\\n    \"instances\": X_new.tolist(),\\n})\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.004,161.739,408.603,172.239 'Note that the JSON format is 100% text-based. The request string looks like this:\\n'>\n",
            "<LTTextBoxHorizontal(16) 89.003,136.053,412.003,154.753 '>>> request_json\\n\\'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, ... ]]]}\\'\\n'>\n",
            "<LTTextBoxHorizontal(17) 71.996,90.946,432.002,127.239 'Now  let’s  send  this  request  to  TF  Serving  via  an  HTTP  POST  request.  This  can  be\\ndone using the requests library (it is not part of Python’s standard library, but it is\\npreinstalled on Colab):\\n'>\n",
            "<LTTextBoxHorizontal(18) 318.358,40.500,402.517,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(19) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 420.985,40.500,432.001,49.500 '727\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 431.875,360.295,431.875,607.500>\n",
            "<LTLine 72.000,360.420,432.000,360.420>\n",
            "<LTLine 72.125,360.295,72.125,607.500>\n",
            "<LTTextBoxHorizontal(0) 89.001,598.150,152.751,606.650 'import requests\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.001,547.150,382.251,586.250 'server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\\nresponse = requests.post(server_url, data=request_json)\\nresponse.raise_for_status()  # raise an exception in case of error\\nresponse = response.json()\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,489.443,432.002,538.337 'If  all  goes  well,  the  response  should  be  a  dictionary  containing  a  single\\n\"predictions\"  key.  The  corresponding  value  is  the  list  of  predictions.  This  list  is\\na Python list, so let’s convert it to a NumPy array and round the floats it contains to\\nthe second decimal:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.996,422.957,382.246,482.457 '>>> import numpy as np\\n>>> y_proba = np.array(response[\"predictions\"])\\n>>> y_proba.round(2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,378.443,432.003,414.143 'Hurray, we have the predictions! The model is close to 100% confident that the first\\nimage is a 7, 99% confident that the second image is a 2, and 97% confident that the\\nthird image is a 1. That’s correct.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,233.843,432.005,370.343 'The  REST  API  is  nice  and  simple,  and  it  works  well  when  the  input  and  output\\ndata  are  not  too  large.  Moreover,  just  about  any  client  application  can  make  REST\\nqueries  without  additional  dependencies,  whereas  other  protocols  are  not  always\\nso  readily  available.  However,  it  is  based  on  JSON,  which  is  text-based  and  fairly\\nverbose.  For  example,  we  had  to  convert  the  NumPy  array  to  a  Python  list,  and\\nevery  float  ended  up  represented  as  a  string.  This  is  very  inefficient,  both  in  terms\\nof serialization/deserialization time—we have to convert all the floats to strings and\\nback—and in terms of payload size: many floats end up being represented using over\\n15  characters,  which  translates  to  over  120  bits  for  32-bit  floats!  This  will  result  in\\nhigh latency and bandwidth usage when transferring large NumPy arrays.6 So, let’s see\\nhow to use gRPC instead.\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.790,172.751,396.000,216.911 'When  transferring  large  amounts  of  data,  or  when  latency  is\\nimportant,  it  is  much  better  to  use  the  gRPC  API,  if  the  client\\nsupports  it,  as  it  uses  a  compact  binary  format  and  an  efficient\\ncommunication protocol based on HTTP/2 framing.\\n'>\n",
            "<LTTextBoxHorizontal(7) 73.140,68.954,431.920,86.954 '6 To be fair, this can be mitigated by serializing the data first and encoding it to Base64 before creating the REST\\nrequest. Moreover, REST requests can be compressed using gzip, which reduces the payload size significantly.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,40.500,84.436,49.500 '728 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.904,40.500,289.708,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 85.000,166.929,126.760,222.707 matrix=[41.76,0.00,0.00,55.78, (85.00,166.93)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.229,231.135,607.789 'Querying TF Serving through the gRPC API\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,538.725,432.004,589.771 'The gRPC API expects a serialized  PredictRequest protocol buffer as input, and it\\noutputs  a  serialized  PredictResponse  protocol  buffer.  These  protobufs  are  part  of\\nthe tensorflow-serving-api library, which we installed earlier. First, let’s create the\\nrequest:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,523.238,352.501,531.738 'from tensorflow_serving.apis.predict_pb2 import PredictRequest\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,462.038,361.001,511.338 'request = PredictRequest()\\nrequest.model_spec.name = model_name\\nrequest.model_spec.signature_name = \"serving_default\"\\ninput_name = model.input_names[0]  # == \"flatten_input\"\\nrequest.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,389.952,432.004,454.191 'This  code  creates  a  PredictRequest  protocol  buffer  and  fills  in  the  required  fields,\\nincluding  the  model  name  (defined  earlier),  the  signature  name  of  the  function  we\\nwant  to  call,  and  finally  the  input  data,  in  the  form  of  a  Tensor  protocol  buffer.\\nThe tf.make_tensor_proto() function creates a Tensor protocol buffer based on the\\ngiven tensor or NumPy array, in this case X_new.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,358.158,432.001,381.852 'Next, we’ll send the request to the server and get its response. For this, we will need\\nthe grpcio library, which is preinstalled in Colab:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.998,332.472,356.748,351.172 'import grpc\\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.998,291.672,411.998,320.572 \"channel = grpc.insecure_channel('localhost:8500')\\npredict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\\nresponse = predict_service.Predict(request, timeout=10.0)\\n\">\n",
            "<LTTextBoxHorizontal(8) 71.997,209.358,432.004,282.858 'The code is quite straightforward: after the imports, we create a gRPC communica‐\\ntion channel to localhost on TCP port 8500, then we create a gRPC service over this\\nchannel and use it to send a request, with a 10-second timeout. Note that the call is\\nsynchronous: it will block until it receives the response or when the timeout period\\nexpires.  In  this  example  the  channel  is  insecure  (no  encryption,  no  authentication),\\nbut gRPC and TF Serving also support secure channels over SSL/TLS.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.999,190.165,354.617,202.224 'Next, let’s convert the PredictResponse protocol buffer to a tensor:\\n'>\n",
            "<LTTextBoxHorizontal(10) 89.000,154.279,305.750,183.179 'output_name = model.output_names[0]  # == \"dense_1\"\\noutputs_proto = response.outputs[output_name]\\ny_proba = tf.make_ndarray(outputs_proto)\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.995,109.172,432.003,146.431 'If you run this code and print y_proba.round(2), you will get the exact same estima‐\\nted class probabilities as earlier. And that’s all there is to it: in just a few lines of code,\\nyou can now access your TensorFlow model remotely, using either REST or gRPC.\\n'>\n",
            "<LTTextBoxHorizontal(12) 318.354,40.500,402.513,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.981,40.500,431.997,49.500 '729\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.229,190.871,607.789 'Deploying a new model version\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.998,565.705,432.001,588.805 'Now  let’s  create  a  new  model  version  and  export  a  SavedModel,  this  time  to  the\\nmy_mnist_model/0002 directory:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,550.218,335.498,558.718 'model = [...]  # build and train a new MNIST model version\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,509.418,280.249,538.318 'model_version = \"0002\"\\nmodel_path = Path(model_name) / model_version\\nmodel.save(model_path, save_format=\"tf\")\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,427.104,432.004,500.605 'At regular intervals (the delay is configurable), TF Serving checks the model directory\\nfor new model versions. If it finds one, it automatically handles the transition grace‐\\nfully: by default, it answers pending requests (if any) with the previous model version,\\nwhile handling new requests with the new version. As soon as every pending request\\nhas been answered, the previous model version is unloaded. You can see this at work\\nin the TF Serving logs (in my_server.log):\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.996,340.218,386.496,420.118 '[...]\\nReading SavedModel from: /models/my_mnist_model/0002\\nReading meta graph with tags { serve }\\n[...]\\nSuccessfully loaded servable version {name: my_mnist_model version: 2}\\nQuiescing servable version {name: my_mnist_model version: 1}\\nDone quiescing servable version {name: my_mnist_model version: 1}\\nUnloading servable version {name: my_mnist_model version: 1}\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.790,266.893,396.001,322.573 'If  the  SavedModel  contains  some  example  instances  in  the  assets/\\nextra  directory,  you  can  configure  TF  Serving  to  run  the  new\\nmodel on these instances before starting to use it to serve requests.\\nThis is called model warmup: it will ensure that everything is prop‐\\nerly loaded, avoiding long response times for the first requests.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,176.304,432.005,249.805 'This approach offers a smooth transition, but it may use too much RAM—especially\\nGPU  RAM,  which  is  generally  the  most  limited.  In  this  case,  you  can  configure  TF\\nServing so that it handles all pending requests with the previous model version and\\nunloads  it  before  loading  and  using  the  new  model  version.  This  configuration  will\\navoid  having  two  model  versions  loaded  at  the  same  time,  but  the  service  will  be\\nunavailable for a short period.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,132.504,432.004,168.204 'As  you  can  see,  TF  Serving  makes  it  straightforward  to  deploy  new  models.  More‐\\nover, if you discover that version 2 does not work as well as you expected, then rolling\\nback to version 1 is as simple as removing the my_mnist_model/0002 directory.\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.002,40.500,84.440,49.500 '730 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.908,40.500,289.712,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,272.591,126.760,328.368 matrix=[41.76,0.00,0.00,55.78, (85.00,272.59)]>\n",
            "<LTTextBoxHorizontal(0) 136.786,491.340,396.003,605.705 'Another great feature of TF Serving is its automatic batching capa‐\\nbility, which you can activate using the --enable_batching option\\nupon  startup.  When  TF  Serving  receives  multiple  requests  within\\na short period of time (the delay is configurable), it will automat‐\\nically  batch  them  together  before  using  the  model.  This  offers  a\\nsignificant performance boost by leveraging the power of the GPU.\\nOnce  the  model  returns  the  predictions,  TF  Serving  dispatches\\neach  prediction  to  the  right  client.  You  can  trade  a  bit  of  latency\\nfor a greater throughput by increasing the batching delay (see the\\n--batching_parameters_file option).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,262.152,432.005,474.252 'If  you  expect  to  get  many  queries  per  second,  you  will  want  to  deploy  TF  Serving\\non multiple servers and load-balance the queries (see Figure 19-2). This will require\\ndeploying and managing many TF Serving containers across these servers. One way\\nto  handle  that  is  to  use  a  tool  such  as  Kubernetes,  which  is  an  open  source  system\\nfor  simplifying  container  orchestration  across  many  servers.  If  you  do  not  want  to\\npurchase,  maintain,  and  upgrade  all  the  hardware  infrastructure,  you  will  want  to\\nuse  virtual  machines  on  a  cloud  platform  such  as  Amazon  AWS,  Microsoft  Azure,\\nGoogle  Cloud  Platform,  IBM  Cloud,  Alibaba  Cloud,  Oracle  Cloud,  or  some  other\\nplatform  as  a  service  (PaaS)  offering.  Managing  all  the  virtual  machines,  handling\\ncontainer orchestration (even with the help of Kubernetes), taking care of TF Serving\\nconfiguration, tuning and monitoring—all of this can be a full-time job. Fortunately,\\nsome  service  providers  can  take  care  of  all  this  for  you.  In  this  chapter  we  will\\nuse  Vertex  AI:  it’s  the  only  platform  with  TPUs  today;  it  supports  TensorFlow  2,\\nScikit-Learn, and XGBoost; and it offers a nice suite of AI services. There are several\\nother providers in this space that are capable of serving TensorFlow models as well,\\nthough, such as Amazon AWS SageMaker and Microsoft AI Platform, so make sure\\nto check them out too.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,151.787,296.290,162.287 'Figure 19-2. Scaling up TF Serving with load balancing\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,127.187,365.265,137.687 'Now let’s see how to serve our wonderful MNIST model on the cloud!\\n'>\n",
            "<LTTextBoxHorizontal(4) 318.357,40.500,402.516,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.984,40.500,432.000,49.500 '731\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,255.890,432.500,255.890>\n",
            "<LTLine 432.375,167.850,432.375,256.015>\n",
            "<LTLine 72.000,167.975,432.500,167.975>\n",
            "<LTLine 72.125,167.850,72.125,256.015>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTFigure(I2) 126.250,173.831,378.250,250.765 matrix=[252.00,0.00,0.00,76.93, (126.25,173.83)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,435.455,432.005,607.894 'Creating a Prediction Service on Vertex AI\\nVertex  AI  is  a  platform  within  Google  Cloud  Platform  (GCP)  that  offers  a  wide\\nrange of AI-related tools and services. You can upload datasets, get humans to label\\nthem, store commonly used features in a feature store and use them for training or\\nin  production,  and  train  models  across  many  GPU  or  TPU  servers  with  automatic\\nhyperparameter tuning or model architecture search (AutoML). You can also manage\\nyour trained models, use them to make batch predictions on large amounts of data,\\nschedule  multiple  jobs  for  your  data  workflows,  serve  your  models  via  REST  or\\ngRPC  at  scale,  and  experiment  with  your  data  and  models  within  a  hosted  Jupyter\\nenvironment  called  the  Workbench.  There’s  even  a  Matching  Engine  service  that  lets\\nyou compare vectors very efficiently (i.e., approximate nearest neighbors). GCP also\\nincludes other AI services, such as APIs for computer vision, translation, speech-to-\\ntext, and more.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,416.855,307.622,427.355 'Before we start, there’s a little bit of setup to take care of:\\n'>\n",
            "<LTTextBoxHorizontal(2) 77.312,369.055,432.003,404.755 '1. Log  in  to  your  Google  account,  and  then  go  to  the  Google  Cloud  Platform\\n1.\\nconsole  (see  Figure  19-3).  If  you  don’t  have  a  Google  account,  you’ll  have  to\\ncreate one.\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.316,251.655,432.005,362.955 '2.\\n2. If  it’s  your  first  time  using  GCP,  you’ll  have  to  read  and  accept  the  terms  and\\nconditions. New users are offered a free trial, including $300 worth of GCP credit\\nthat  you  can  use  over  the  course  of  90  days  (as  of  May  2022).  You’ll  only  need\\na  small  portion  of  that  to  pay  for  the  services  you’ll  use  in  this  chapter.  Upon\\nsigning up for a free trial, you’ll still need to create a payment profile and enter\\nyour  credit  card  number:  it’s  used  for  verification  purposes—probably  to  avoid\\npeople  using  the  free  trial  multiple  times—but  you  won’t  be  billed  for  the  first\\n$300, and after that you’ll only be charged if you opt in by upgrading to a paid\\naccount.\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.002,74.781,267.851,85.281 'Figure 19-3. Google Cloud Platform console\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.002,40.500,84.440,49.500 '732 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.908,40.500,289.712,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 90.000,251.393,432.500,251.393>\n",
            "<LTLine 432.375,90.844,432.375,251.518>\n",
            "<LTLine 90.000,90.969,432.500,90.969>\n",
            "<LTLine 90.125,90.844,90.125,251.518>\n",
            "<LTFigure(I1) 107.350,97.011,415.150,246.268 matrix=[307.80,0.00,0.00,149.26, (107.35,97.01)]>\n",
            "<LTTextBoxHorizontal(0) 77.316,506.836,432.005,605.537 '3. If  you  have  used  GCP  before  and  your  free  trial  has  expired,  then  the  services\\n3.\\nyou will use in this chapter will cost you some money. It shouldn’t be too much,\\nespecially  if  you  remember  to  turn  off  the  services  when  you  don’t  need  them\\nanymore. Make sure you understand and agree to the pricing conditions before\\nyou run any service. I hereby decline any responsibility if services end up costing\\nmore than you expected! Also make sure your billing account is active. To check,\\nopen the ☰ navigation menu at the top left and click Billing, then make sure you\\nhave set up a payment method and that the billing account is active.\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.316,351.636,432.005,500.737 '4. Every resource in GCP belongs to a project. This includes all the virtual machines\\n4.\\nyou may use, the files you store, and the training jobs you run. When you create\\nan account, GCP automatically creates a project for you, called “My First Project”.\\nIf you want, you can change its display name by going to the project settings: in\\nthe ☰ navigation menu, select “IAM and admin → Settings”, change the project’s\\ndisplay  name,  and  click  SAVE.  Note  that  the  project  also  has  a  unique  ID  and\\nnumber. You can choose the project ID when you create a project, but you cannot\\nchange  it  later.  The  project  number  is  automatically  generated  and  cannot  be\\nchanged.  If  you  want  to  create  a  new  project,  click  the  project  name  at  the  top\\nof the page, then click NEW PROJECT and enter the project name. You can also\\nclick EDIT to set the project ID. Make sure billing is active for this new project so\\nthat service fees can be billed (to your free credits, if any).\\n'>\n",
            "<LTTextBoxHorizontal(2) 154.786,290.545,395.997,334.705 'Always  set  an  alarm  to  remind  yourself  to  turn  services  off\\nwhen  you  know  you  will  only  need  them  for  a  few  hours,\\nor  else  you  might  leave  them  running  for  days  or  months,\\nincurring potentially significant costs.\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.315,208.236,432.005,269.136 '5.\\n5. Now  that  you  have  a  GCP  account  and  a  project,  and  billing  is  activated,  you\\nmust  activate  the  APIs  you  need.  In  the  ☰  navigation  menu,  select  “APIs  and\\nservices”,  and  make  sure  the  Cloud  Storage  API  is  enabled.  If  needed,  click  +\\nENABLE APIS AND SERVICES, find Cloud Storage, and enable it. Also enable\\nthe Vertex AI API.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,147.836,432.002,196.136 'You  could  continue  to  do  everything  via  the  GCP  console,  but  I  recommend  using\\nPython  instead:  this  way  you  can  write  scripts  to  automate  just  about  anything  you\\nwant with GCP, and it’s often more convenient than clicking your way through menus\\nand forms, especially for common tasks.\\n'>\n",
            "<LTTextBoxHorizontal(5) 318.356,40.500,402.515,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.983,40.500,431.999,49.500 '733\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 101.000,288.190,150.680,335.500 matrix=[49.68,0.00,0.00,47.31, (101.00,288.19)]>\n",
            "<LTTextBoxHorizontal(0) 80.250,515.870,423.750,597.625 'Google Cloud CLI and Shell\\nGoogle Cloud’s command-line interface (CLI) includes the gcloud command, which\\nlets  you  control  almost  everything  in  GCP,  and  gsutil,  which  lets  you  interact\\nwith  Google  Cloud  Storage.  This  CLI  is  preinstalled  in  Colab:  all  you  need  to  do  is\\nauthenticate  using  google.auth.authenticate_user(),  and  you’re  good  to  go.  For\\nexample, !gcloud config list will display the configuration.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.250,449.870,423.750,507.870 'GCP  also  offers  a  preconfigured  shell  environment  called  the  Google  Cloud  Shell,\\nwhich you can use directly in your web browser; it runs on a free Linux VM (Debian)\\nwith  the  Google  Cloud  SDK  already  preinstalled  and  configured  for  you,  so  there’s\\nno need to authenticate. The Cloud Shell is available anywhere in GCP: just click the\\nActivate Cloud Shell icon at the top right of the page (see Figure 19-4).\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.250,375.641,259.900,385.641 'Figure 19-4. Activating the Google Cloud Shell\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.250,303.076,423.750,361.641 'If you prefer to install the CLI on your machine, then after installation you need to\\ninitialize  it  by  running  gcloud  init:  follow  the  instructions  to  log  in  to  GCP  and\\ngrant access to your GCP resources, then select the default GCP project you want to\\nuse (if you have more than one) and the default region where you want your jobs to\\nrun.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,256.633,431.999,279.733 'The first thing you need to do before you can use any GCP service is to authenticate.\\nThe simplest solution when using Colab is to execute the following code:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.003,241.146,212.253,249.646 'from google.colab import auth\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.003,220.746,191.003,229.246 'auth.authenticate_user()\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,125.833,432.004,211.933 'The authentication process is based on OAuth 2.0: a pop-up window will ask you to\\nconfirm that you want the Colab notebook to access your Google credentials. If you\\naccept, you must select the same Google account you used for GCP. Then you will be\\nasked to confirm that you agree to give Colab full access to all your data on Google\\nDrive and in GCP. If you allow access, only the current notebook will have access, and\\nonly  until  the  Colab  runtime  expires.  Obviously,  you  should  only  accept  this  if  you\\ntrust the code in the notebook.\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.002,40.500,84.440,49.500 '734 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.908,40.500,289.712,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.000,607.375>\n",
            "<LTLine 431.875,291.696,431.875,607.500>\n",
            "<LTLine 72.000,291.821,432.000,291.821>\n",
            "<LTLine 72.125,291.696,72.125,607.500>\n",
            "<LTFigure(I1) 140.363,396.841,363.637,438.740 matrix=[223.27,0.00,0.00,41.90, (140.36,396.84)]>\n",
            "<LTTextBoxHorizontal(0) 136.790,561.545,396.001,605.705 'If  you  are  not  working  with  the  official  notebooks  from  https://\\ngithub.com/ageron/handson-ml3,  then  you  should  be  extra  careful:\\nif the notebook’s author is mischievous, they could include code to\\ndo whatever they want with your data.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.245,404.730,423.755,532.225 'Authentication and Authorization on GCP\\nIn general, using OAuth 2.0 authentication is only recommended when an application\\nmust  access  the  user’s  personal  data  or  resources  from  another  application,  on  the\\nuser’s  behalf.  For  example,  some  applications  allow  the  user  to  save  data  to  their\\nGoogle  Drive,  but  for  that  the  application  first  needs  the  user  to  authenticate  with\\nGoogle  and  allow  access  to  Google  Drive.  In  general,  the  application  will  only  ask\\nfor  the  level  of  access  it  needs;  it  won’t  be  an  unlimited  access:  for  example,  the\\napplication will only request access to Google Drive, not Gmail or any other Google\\nservice. Moreover, the authorization usually expires after a while, and it can always be\\nrevoked.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.245,206.730,423.755,396.730 'When  an  application  needs  to  access  a  service  on  GCP  on  its  own  behalf,  not  on\\nbehalf  of  the  user,  then  it  should  generally  use  a  service  account.  For  example,  if\\nyou build  a website that needs to send prediction requests  to a Vertex AI endpoint,\\nthen  the  website  will  be  accessing  the  service  on  its  own  behalf.  There’s  no  data  or\\nresource  that  it  needs  to  access  in  the  user’s  Google  account.  In  fact,  many  users  of\\nthe website will not even have a Google account. For this scenario, you first need to\\ncreate  a  service  account.  Select  “IAM  and  admin  →  Service  accounts”  in  the  GCP\\nconsole’s ☰ navigation menu (or use the search box), then click + CREATE SERVICE\\nACCOUNT, fill in the first page of the form (service account name, ID, description),\\nand click CREATE AND CONTINUE. Next, you must give this account some access\\nrights.  Select  the  “Vertex  AI  user”  role:  this  will  allow  the  service  account  to  make\\npredictions  and  use  other  Vertex  AI  services,  but  nothing  else.  Click  CONTINUE.\\nYou can now optionally grant some users access to the service account: this is useful\\nwhen  your  GCP  user  account  is  part  of  an  organization  and  you  wish  to  authorize\\nother  users  in  the  organization  to  deploy  applications  that  will  be  based  on  this\\nservice account, or to manage the service account itself. Next, click DONE.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.251,80.730,423.751,198.730 'Once you have created a service account, your application must authenticate as that\\nservice  account.  There  are  several  ways  to  do  that.  If  your  application  is  hosted\\non  GCP—for  example,  if  you  are  coding  a  website  hosted  on  Google  Compute\\nEngine—then  the  simplest  and  safest  solution  is  to  attach  the  service  account  to\\nthe  GCP  resource  that  hosts  your  website,  such  as  a  VM  instance  or  a  Google  App\\nEngine  service.  This  can  be  done  when  creating  the  GCP  resource,  by  selecting  the\\nservice account in the “Identity and API access” section. Some resources, such as VM\\ninstances, also let you attach the service account after the VM instance is created: you\\nmust  stop  it  and  edit  its  settings.  In  any  case,  once  a  service  account  is  attached  to\\na VM instance, or any other GCP resource running your code, GCP’s client libraries\\n'>\n",
            "<LTTextBoxHorizontal(4) 318.361,40.500,402.520,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.098,40.500,413.410,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.988,40.500,432.004,49.500 '735\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,541.975,432.000,541.975>\n",
            "<LTLine 431.875,68.650,431.875,542.100>\n",
            "<LTLine 72.125,68.650,72.125,542.100>\n",
            "<LTFigure(I1) 83.000,559.190,132.680,606.500 matrix=[49.68,0.00,0.00,47.31, (83.00,559.19)]>\n",
            "<LTTextBoxHorizontal(0) 80.250,583.630,423.750,605.630 '(discussed shortly) will automatically authenticate as the chosen service account, with\\nno extra step needed.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.249,457.065,423.750,575.630 'If your application is hosted using Kubernetes, then you should use Google’s Work‐\\nload  Identity  service  to  map  the  right  service  account  to  each  Kubernetes  service\\naccount.  If  your  application  is  not  hosted  on  GCP—for  example,  if  you  are  just\\nrunning  the  Jupyter  notebook  on  your  own  machine—then  you  can  either  use  the\\nWorkload  Identity  Federation  service  (that’s  the  safest  but  hardest  option),  or  just\\ngenerate an access key for your service account, save it to a JSON file, and point the\\nGOOGLE_APPLICATION_CREDENTIALS environment variable to it so your client applica‐\\ntion can access it. You can manage access keys by clicking the service account you just\\ncreated, and then opening the KEYS tab. Make sure to keep the key file secret: it’s like\\na password for the service account.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.249,427.065,423.749,449.065 'For more details on setting up authentication and authorization so your application\\ncan access GCP services, check out the documentation.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,354.235,432.003,403.722 'Now  let’s  create  a  Google  Cloud  Storage  bucket  to  store  our  SavedModels  (a  GCS\\nbucket is a container for your data). For this we will use the google-cloud-storage\\nlibrary,  which  is  preinstalled  in  Colab.  We  first  create  a  Client  object,  which  will\\nserve as the interface with GCS, then we use it to create the bucket:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,338.748,224.999,347.248 'from google.cloud import storage\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.999,297.948,360.999,326.848 'project_id = \"my_project\"  # change this to your project ID\\nbucket_name = \"my_bucket\"  # change this to a unique bucket name\\nlocation = \"us-central1\"\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.999,267.348,382.249,286.048 'storage_client = storage.Client(project=project_id)\\nbucket = storage_client.create_bucket(bucket_name, location=location)\\n'>\n",
            "<LTTextBoxHorizontal(7) 136.787,215.978,395.997,249.703 'If  you  want  to  reuse  an  existing  bucket,  replace  the  last  line\\nwith  bucket  =  storage_client.bucket(bucket_name).  Make\\nsure location is set to the bucket’s region.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,110.635,432.004,184.135 'GCS uses a single worldwide namespace for buckets, so simple names like “machine-\\nlearning”  will  most  likely  not  be  available.  Make  sure  the  bucket  name  conforms\\nto  DNS  naming  conventions,  as  it  may  be  used  in  DNS  records.  Moreover,  bucket\\nnames are public, so do not put anything private in the name. It is common to use\\nyour  domain  name,  your  company  name,  or  your  project  ID  as  a  prefix  to  ensure\\nuniqueness, or simply use a random number as part of the name.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.998,79.435,432.001,102.535 'You can change the region if you want, but be sure to choose one that supports GPUs.\\nAlso,  you  may  want  to  consider  the  fact  that  prices  vary  greatly  between  regions,\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.998,40.500,84.436,49.500 '736 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.904,40.500,289.708,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 431.875,415.685,431.875,607.500>\n",
            "<LTLine 72.000,415.810,432.000,415.810>\n",
            "<LTLine 72.125,415.685,72.125,607.500>\n",
            "<LTFigure(I1) 85.000,199.721,126.760,255.498 matrix=[41.76,0.00,0.00,55.78, (85.00,199.72)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,556.643,432.000,605.537 'some regions produce much more CO₂ than others, some regions do not support all\\nservices, and using a single-region bucket improves performance. See Google Cloud’s\\nlist of regions and Vertex AI’s documentation on locations for more details. If you are\\nunsure, it might be best to stick with \"us-central1\".\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,474.450,432.005,548.543 'Next, let’s upload the my_mnist_model directory to the new bucket. Files in GCS are\\ncalled  blobs  (or  objects),  and  under  the  hood  they  are  all  just  placed  in  the  bucket\\nwithout  any  directory  structure.  Blob  names  can  be  arbitrary  Unicode  strings,  and\\nthey  can  even  contain  forward  slashes  (/).  The  GCP  console  and  other  tools  use\\nthese  slashes  to  give  the  illusion  that  there  are  directories.  So,  when  we  upload  the\\nmy_mnist_model directory, we only care about the files, not the directories:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,407.963,424.747,467.463 'def upload_directory(bucket, dirpath):\\n    dirpath = Path(dirpath)\\n    for filepath in dirpath.glob(\"**/*\"):\\n        if filepath.is_file():\\n            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\\n            blob.upload_from_filename(filepath)\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,387.564,267.497,396.064 'upload_directory(bucket, \"my_mnist_model\")\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,330.450,432.005,378.750 'This function works fine now, but it would be very slow if there were many files to\\nupload.  It’s  not  too  hard  to  speed  it  up  tremendously  by  multithreading  it  (see  the\\nnotebook  for  an  implementation).  Alternatively,  if  you  have  the  Google  Cloud  CLI,\\nthen you can use following command instead:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,314.963,305.752,323.463 '!gsutil -m cp -r my_mnist_model gs://{bucket_name}/\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,194.257,432.004,306.150 'Next, let’s tell Vertex AI about our MNIST model. To communicate with Vertex AI,\\nwe  can  use  the  google-cloud-aiplatform  library  (it  still  uses  the  old  AI  Platform\\nname  instead  of  Vertex  AI).  It’s  not  preinstalled  in  Colab,  so  we  need  to  install  it.\\nAfter  that,  we  can  import  the  library  and  initialize  it—just  to  specify  some  default\\nvalues  for  the  project  ID  and  the  location—then  we  can  create  a  new  Vertex  AI\\nmodel: we specify a display name, the GCS path to our model (in this case the version\\n0001),  and  the  URL  of  the  Docker  container  we  want  Vertex  AI  to  use  to  run  this\\nmodel. If you visit that URL and navigate up one level, you will find other containers\\nyou can use. This one supports TensorFlow 2.8 with a GPU:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.002,178.770,237.752,187.270 'from google.cloud import aiplatform\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.002,158.370,386.502,166.870 'server_image = \"gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\"\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.002,86.970,339.752,146.470 'aiplatform.init(project=project_id, location=location)\\nmnist_model = aiplatform.Model.upload(\\n    display_name=\"mnist\",\\n    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\\n    serving_container_image_uri=server_image,\\n)\\n'>\n",
            "<LTTextBoxHorizontal(10) 318.360,40.500,402.519,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.987,40.500,432.003,49.500 '737\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'Now let’s deploy this model so we can query it via a gRPC or REST API to make pre‐\\ndictions. For this we first need to create an endpoint. This is what client applications\\nconnect to when they want to access a service. Then we need to deploy our model to\\nthis endpoint:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.003,541.750,378.003,550.250 'endpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.003,449.950,259.003,529.850 'endpoint.deploy(\\n    mnist_model,\\n    min_replica_count=1,\\n    max_replica_count=5,\\n    machine_type=\"n1-standard-4\",\\n    accelerator_type=\"NVIDIA_TESLA_K80\",\\n    accelerator_count=1\\n)\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,353.257,432.005,441.136 'This code may take a few minutes to run, because Vertex AI needs to set up a virtual\\nmachine. In this example, we use a fairly basic machine of type n1-standard-4 (see\\nhttps://homl.info/machinetypes  for  other  types).  We  also  use  a  basic  GPU  of  type\\nNVIDIA_TESLA_K80 (see https://homl.info/accelerators for other types). If you selected\\nanother region than \"us-central1\", then you may need to change the machine type\\nor the accelerator type to values that are supported in that region (e.g., not all regions\\nhave Nvidia Tesla K80 GPUs).\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.789,257.605,396.001,336.325 'Google Cloud Platform enforces various GPU quotas, both world‐\\nwide  and  per  region:  you  cannot  create  thousands  of  GPU  nodes\\nwithout  prior  authorization  from  Google.  To  check  your  quotas,\\nopen  “IAM  and  admin  →  Quotas”  in  the  GCP  console.  If  some\\nquotas  are  too  low  (e.g.,  if  you  need  more  GPUs  in  a  particular\\nregion), you can ask for them to be increased; it often takes about\\n48 hours.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,129.217,432.004,240.517 'Vertex  AI  will  initially  spawn  the  minimum  number  of  compute  nodes  (just  one  in\\nthis case), and whenever the number of queries per second becomes too high, it will\\nspawn more nodes (up to the maximum number you defined, five in this case) and\\nwill  load-balance  the  queries  between  them.  If  the  QPS  rate  goes  down  for  a  while,\\nVertex  AI  will  stop  the  extra  compute  nodes  automatically.  The  cost  is  therefore\\ndirectly linked to the load, as well as the machine and accelerator types you selected\\nand the amount of data you store on GCS. This pricing model is great for occasional\\nusers and for services with important usage spikes. It’s also ideal for startups: the price\\nremains low until the startup actually starts up.\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.001,40.500,84.439,49.500 '738 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.907,40.500,289.711,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,288.620,126.944,338.120 matrix=[37.94,0.00,0.00,49.50, (89.00,288.62)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'Congratulations,  you  have  deployed  your  first  model  to  the  cloud!  Now  let’s  query\\nthis prediction service:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.999,566.950,314.249,575.450 'response = endpoint.predict(instances=X_new.tolist())\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,509.837,432.003,558.137 'We  first  need  to  convert  the  images  we  want  to  classify  to  a  Python  list,  as  we  did\\nearlier when we sent requests to TF Serving using the REST API. The response object\\ncontains  the  predictions,  represented  as  a  Python  list  of  lists  of  floats.  Let’s  round\\nthem to two decimal places and convert them to a NumPy array:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,453.550,382.251,502.850 '>>> import numpy as np\\n>>> np.round(response.predictions, 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\\n       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\\n       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,396.436,432.005,444.737 'Yes!  We  get  the  exact  same  predictions  as  earlier.  We  now  have  a  nice  prediction\\nservice running on the cloud that we can query from anywhere securely, and which\\ncan automatically scale up or down depending on the number of QPS. When you are\\ndone using the endpoint, don’t forget to delete it, to avoid paying for nothing:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.000,370.750,361.000,389.450 'endpoint.undeploy_all()  # undeploy all models from the endpoint\\nendpoint.delete()\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,338.836,432.000,361.936 'Now let’s see how to run a job on Vertex AI to make predictions on a potentially very\\nlarge batch of data.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,205.227,432.005,327.267 'Running Batch Prediction Jobs on Vertex AI\\nIf we have a large number of predictions to make, then instead of calling our predic‐\\ntion  service  repeatedly,  we  can  ask  Vertex  AI  to  run  a  prediction  job  for  us.  This\\ndoes  not  require  an  endpoint,  only  a  model.  For  example,  let’s  run  a  prediction  job\\non  the  first  100  images  of  the  test  set,  using  our  MNIST  model.  For  this,  we  first\\nneed  to  prepare  the  batch  and  upload  it  to  GCS.  One  way  to  do  this  is  to  create  a\\nfile containing one instance per line, each formatted as a JSON value—this format is\\ncalled JSON Lines—then pass this file to Vertex AI. So let’s create a JSON Lines file in\\na new directory, then upload this directory to GCS:\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.999,138.741,369.499,198.241 'batch_path = Path(\"my_mnist_batch\")\\nbatch_path.mkdir(exist_ok=True)\\nwith open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\\n    for image in X_test[:100].tolist():\\n        jsonl_file.write(json.dumps(image))\\n        jsonl_file.write(\"\\\\n\")\\n'>\n",
            "<LTTextBoxHorizontal(9) 88.999,118.341,241.999,126.841 'upload_directory(bucket, batch_path)\\n'>\n",
            "<LTTextBoxHorizontal(10) 318.355,40.500,402.514,49.500 'Serving a TensorFlow Model \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.982,40.500,431.998,49.500 '739\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.004,605.537 'Now we’re ready to launch the prediction job, specifying the job’s name, the type and\\nnumber of machines and accelerators to use, the GCS path to the JSON Lines file we\\njust created, and the path to the GCS directory where Vertex AI will save the model’s\\npredictions:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.001,439.750,420.501,550.250 'batch_prediction_job = mnist_model.batch_predict(\\n    job_display_name=\"my_batch_prediction_job\",\\n    machine_type=\"n1-standard-4\",\\n    starting_replica_count=1,\\n    max_replica_count=5,\\n    accelerator_type=\"NVIDIA_TESLA_K80\",\\n    accelerator_count=1,\\n    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\\n    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\\n    sync=True  # set to False if you don\\'t want to wait for completion\\n)\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.787,400.442,395.997,422.105 'For large batches, you can split the inputs into multiple JSON Lines\\nfiles and list them all via the gcs_source argument.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,244.050,432.004,356.537 'This  will  take  a  few  minutes,  mostly  to  spawn  the  compute  nodes  on  Vertex  AI.\\nOnce  this  command  completes,  the  predictions  will  be  available  in  a  set  of  files\\nnamed  something  like  prediction.results-00001-of-00002.  These  files  use  the  JSON\\nLines format by default, and each value is a dictionary containing an instance and its\\ncorresponding prediction (i.e., 10 probabilities). The instances are listed in the same\\norder as the inputs. The job also outputs prediction-errors* files, which can be useful\\nfor debugging if something goes wrong. We can iterate through all these output files\\nusing batch_prediction_job.iter_outputs(), so let’s go through all the predictions\\nand store them in a y_probas array:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.003,177.563,331.253,237.063 'y_probas = []\\nfor blob in batch_prediction_job.iter_outputs():\\n    if \"prediction.results\" in blob.name:\\n        for line in blob.download_as_text().splitlines():\\n            y_proba = json.loads(line)[\"prediction\"]\\n            y_probas.append(y_proba)\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.998,158.250,260.683,168.750 'Now let’s see how good these predictions are:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.000,122.363,305.750,151.263 '>>> y_pred = np.argmax(y_probas, axis=1)\\n>>> accuracy = np.sum(y_pred == y_test[:100]) / 100\\n0.98\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.001,103.050,156.211,113.550 'Nice, 98% accuracy!\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,40.500,84.436,49.500 '740 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.904,40.500,289.708,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,372.123,126.760,427.900 matrix=[41.76,0.00,0.00,55.78, (85.00,372.12)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,451.691,432.005,605.537 'The  JSON  Lines  format  is  the  default,  but  when  dealing  with  large  instances\\nsuch  as  images,  it  is  too  verbose.  Luckily,  the  batch_predict()  method  accepts\\nan  instances_format  argument  that  lets  you  choose  another  format  if  you  want.\\nIt  defaults  to  \"jsonl\",  but  you  can  change  it  to  \"csv\",  \"tf-record\",  \"tf-record-\\ngzip\",  \"bigquery\",  or  \"file-list\".  If  you  set  it  to  \"file-list\",  then  the\\ngcs_source  argument  should  point  to  a  text  file  containing  one  input  filepath  per\\nline;  for  instance,  pointing  to  PNG  image  files.  Vertex  AI  will  read  these  files\\nas  binary,  encode  them  using  Base64,  and  pass  the  resulting  byte  strings  to  the\\nmodel.  This  means  that  you  must  add  a  preprocessing  layer  in  your  model  to\\nparse  the  Base64  strings,  using  tf.io.decode_base64().  If  the  files  are  images,\\nyou  must  then  parse  the  result  using  a  function  like  tf.io.decode_image()  or\\ntf.io.decode_png(), as discussed in Chapter 13.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,407.297,432.004,443.590 'When  you’re  finished  using  the  model,  you  can  delete  it  if  you  want,  by  running\\nmnist_model.delete(). You can also delete the directories you created in your GCS\\nbucket, optionally the bucket itself (if it’s empty), and the batch prediction job:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,361.211,420.497,400.311 'for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\\n    blobs = bucket.list_blobs(prefix=prefix)\\n    for blob in blobs:\\n        blob.delete()\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,330.611,263.247,349.311 'bucket.delete()  # if the bucket is empty\\nbatch_prediction_job.delete()\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,273.497,432.003,321.797 'You now know how to deploy a model to Vertex AI, create a prediction service, and\\nrun batch prediction jobs. But what if you want to deploy your model to a mobile app\\ninstead? Or to an embedded device, such as a heating control system, a fitness tracker,\\nor a self-driving car?\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.991,147.897,432.005,261.157 'Deploying a Model to a Mobile or Embedded Device\\nMachine learning models are not limited to running on big centralized servers with\\nmultiple GPUs: they can run closer to the source of data (this is called edge comput‐\\ning),  for  example  in  the  user’s  mobile  device  or  in  an  embedded  device.  There  are\\nmany benefits to decentralizing the computations and moving them toward the edge:\\nit allows the device to be smart even when it’s not connected to the internet, it reduces\\nlatency  by  not  having  to  send  data  to  a  remote  server  and  reduces  the  load  on  the\\nservers, and it may improve privacy, since the user’s data can stay on the device.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,78.897,432.005,139.797 'However, deploying models to the edge has its downsides too. The device’s computing\\nresources  are  generally  tiny  compared  to  a  beefy  multi-GPU  server.  A  large  model\\nmay not fit in the device, it may use too much RAM and CPU, and it may take too\\nlong  to  download.  As  a  result,  the  application  may  become  unresponsive,  and  the\\ndevice may heat up and quickly run out of battery. To avoid all this, you need to make\\n'>\n",
            "<LTTextBoxHorizontal(7) 250.184,40.500,402.518,49.500 'Deploying a Model to a Mobile or Embedded Device \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.986,40.500,432.002,49.500 '741\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.002,605.537 'a  lightweight  and  efficient  model,  without  sacrificing  too  much  of  its  accuracy.  The\\nTFLite  library  provides  several  tools7  to  help  you  deploy  your  models  to  the  edge,\\nwith three main objectives:\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.652,547.236,400.027,557.737 '•\\n• Reduce the model size, to shorten download time and reduce RAM usage.\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.652,530.637,432.004,541.137 '•\\n• Reduce  the  amount  of  computations  needed  for  each  prediction,  to  reduce\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.998,518.037,235.780,528.537 'latency, battery usage, and heating.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.656,501.437,286.674,511.937 '•\\n• Adapt the model to device-specific constraints.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,390.636,432.004,489.337 'To  reduce  the  model  size,  TFLite’s  model  converter  can  take  a  SavedModel  and\\ncompress it to a much lighter format based on FlatBuffers. This is an efficient cross-\\nplatform serialization library (a bit like protocol buffers) initially created by Google\\nfor gaming. It is designed so you can load FlatBuffers straight to RAM without any\\npreprocessing: this reduces the loading time and memory footprint. Once the model\\nis loaded into a mobile or embedded device, the TFLite interpreter will execute it to\\nmake predictions. Here is how you can convert a SavedModel to a FlatBuffer and save\\nit to a .tflite file:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.999,344.550,382.249,383.650 'converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\\ntflite_model = converter.convert()\\nwith open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\\n    f.write(tflite_model)\\n'>\n",
            "<LTTextBoxHorizontal(7) 136.787,305.242,396.004,326.905 'You  can  also  save  a  Keras  model  directly  to  a  FlatBuffer  using\\ntf.lite.TFLiteConverter.from_keras_model(model).\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,124.836,432.005,261.336 'The converter also optimizes the model, both to shrink it and to reduce its latency. It\\nprunes  all  the  operations  that  are  not  needed  to  make  predictions  (such  as  training\\noperations), and it optimizes computations whenever possible; for example, 3 × a +\\n4  ×_  a_  +  5  ×  a  will  be  converted  to  12  ×  a.  Addtionally,  it  tries  to  fuse  operations\\nwhenever possible. For example, if possible, batch normalization layers end up folded\\ninto  the  previous  layer’s  addition  and  multiplication  operations.  To  get  a  good  idea\\nof how much TFLite can optimize a model, download one of the pretrained TFLite\\nmodels,  such  as  Inception_V1_quant  (click  tflite&pb),  unzip  the  archive,  then  open\\nthe  excellent  Netron  graph  visualization  tool  and  upload  the  .pb  file  to  view  the\\noriginal model. It’s a big, complex graph, right? Next, open the optimized .tflite model\\nand marvel at its beauty!\\n'>\n",
            "<LTTextBoxHorizontal(9) 73.140,68.954,416.224,76.954 '7 Also check out TensorFlow’s Graph Transform Tool for modifying and optimizing computational graphs.\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,40.500,84.434,49.500 '742 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.902,40.500,289.706,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 85.000,276.923,126.760,332.700 matrix=[41.76,0.00,0.00,55.78, (85.00,276.92)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Another way you can reduce the model size—other than simply using smaller neural\\nnetwork  architectures—is  by  using  smaller  bit-widths:  for  example,  if  you  use  half-\\nfloats  (16  bits)  rather  than  regular  floats  (32  bits),  the  model  size  will  shrink  by  a\\nfactor of 2, at the cost of a (generally small) accuracy drop. Moreover, training will be\\nfaster, and you will use roughly half the amount of GPU RAM.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,400.036,432.005,536.536 'TFLite’s  converter  can  go  further  than  that,  by  quantizing  the  model  weights  down\\nto  fixed-point,  8-bit  integers!  This  leads  to  a  fourfold  size  reduction  compared  to\\nusing 32-bit floats. The simplest approach is called post-training quantization: it just\\nquantizes  the  weights  after  training,  using  a  fairly  basic  but  efficient  symmetrical\\nquantization technique. It finds the maximum absolute weight value, m, then it maps\\nthe  floating-point  range  –m  to  +m  to  the  fixed-point  (integer)  range  –127  to  +127.\\nFor example, if the weights range from –1.5 to +0.8, then the bytes –127, 0, and +127\\nwill correspond to the floats –1.5, 0.0, and +1.5, respectively (see Figure 19-5). Note\\nthat  0.0  always  maps  to  0  when  using  symmetrical  quantization.  Also  note  that  the\\nbyte  values  +68  to  +127  will  not  be  used  in  this  example,  since  they  map  to  floats\\ngreater than +0.8.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,274.446,397.049,284.946 'Figure 19-5. From 32-bit floats to 8-bit integers, using symmetrical quantization\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,236.060,432.001,261.312 'To  perform  this  post-training  quantization,  simply  add  DEFAULT  to  the  list  of  con‐\\nverter optimizations before calling the convert() method:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,220.573,310.004,229.073 'converter.optimizations = [tf.lite.Optimize.DEFAULT]\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,113.060,432.005,211.760 'This  technique  dramatically  reduces  the  model’s  size,  which  makes  it  much  faster\\nto  download,  and  uses  less  storage  space.  At  runtime  the  quantized  weights  get\\nconverted back to floats before they are used. These recovered floats are not perfectly\\nidentical  to  the  original  floats,  but  they’re  not  too  far  off,  so  the  accuracy  loss  is\\nusually  acceptable.  To  avoid  recomputing  the  float  values  all  the  time,  which  would\\nseverely  slow  down  the  model,  TFLite  caches  them:  unfortunately,  this  means  that\\nthis technique does not reduce RAM usage, and it doesn’t speed up the model either.\\nIt’s mostly useful to reduce the application’s size.\\n'>\n",
            "<LTTextBoxHorizontal(6) 250.181,40.500,402.515,49.500 'Deploying a Model to a Mobile or Embedded Device \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.983,40.500,431.999,49.500 '743\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,393.775,432.500,393.775>\n",
            "<LTLine 432.375,290.510,432.375,393.900>\n",
            "<LTLine 72.000,290.635,432.500,290.635>\n",
            "<LTLine 72.125,290.510,72.125,393.900>\n",
            "<LTFigure(I1) 91.329,296.490,413.171,388.650 matrix=[321.84,0.00,0.00,92.16, (91.33,296.49)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,456.436,432.005,605.537 'The most effective way to reduce latency and power consumption is to also quantize\\nthe activations so that the computations can be done entirely with integers, without\\nthe need for any floating-point operations. Even when using the same bit-width (e.g.,\\n32-bit  integers  instead  of  32-bit  floats),  integer  computations  use  less  CPU  cycles,\\nconsume  less  energy,  and  produce  less  heat.  And  if  you  also  reduce  the  bit-width\\n(e.g.,  down  to  8-bit  integers),  you  can  get  huge  speedups.  Moreover,  some  neural\\nnetwork accelerator devices—such as Google’s Edge TPU—can only process integers,\\nso full quantization of both weights and activations is compulsory. This can be done\\npost-training; it requires a calibration step to find the maximum absolute value of the\\nactivations, so you need to provide a representative sample of training data to TFLite\\n(it  does  not  need  to  be  huge),  and  it  will  process  the  data  through  the  model  and\\nmeasure the activation statistics required for quantization. This step is typically fast.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,362.236,432.005,448.336 'The main problem with quantization is that it loses a bit of accuracy: it is similar to\\nadding noise to the weights and activations. If the accuracy drop is too severe, then\\nyou may need to use quantization-aware training. This means adding fake quantiza‐\\ntion operations to the model so it can learn to ignore the quantization noise during\\ntraining;  the  final  weights  will  then  be  more  robust  to  quantization.  Moreover,  the\\ncalibration step can be taken care of automatically during training, which simplifies\\nthe whole process.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,268.036,432.004,354.136 'I  have  explained  the  core  concepts  of  TFLite,  but  going  all  the  way  to  coding  a\\nmobile  or  embedded  application  woud  require  a  dedicated  book.  Fortunately,  some\\nexist:  if  you  want  to  learn  more  about  building  TensorFlow  applications  for  mobile\\nand  embedded  devices,  check  out  the  O’Reilly  books  TinyML:  Machine  Learning\\nwith TensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden\\n(former lead of the TFLite team) and Daniel Situnayake and AI and Machine Learning\\nfor On-Device Development, by Laurence Moroney.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,236.836,431.998,259.936 'Now what if you want to use your model in a website, running directly in the user’s\\nbrowser?\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,174.236,432.000,224.496 'Running a Model in a Web Page\\nRunning your machine learning model on the client side, in the user’s browser, rather\\nthan on the server side can be useful in many scenarios, such as:\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.652,126.436,432.005,162.136 '• When your web application is often used in situations where the user’s connec‐\\n•\\ntivity  is  intermittent  or  slow  (e.g.,  a  website  for  hikers),  so  running  the  model\\ndirectly on the client side is the only way to make your website reliable.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.651,84.636,432.003,120.336 '•\\n• When you need the model’s responses to be as fast as possible (e.g., for an online\\ngame). Removing the need to query the server to make predictions will definitely\\nreduce the latency and make the website much more responsive.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.999,40.500,84.437,49.500 '744 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 80.654,569.837,432.002,605.537 '• When your web service makes predictions based on some private user data, and\\n•\\nyou  want  to  protect  the  user’s  privacy  by  making  the  predictions  on  the  client\\nside so that the private data never has to leave the user’s machine.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.998,471.636,432.005,557.736 'For all these scenarios, you can use the TensorFlow.js (TFJS) JavaScript library. This\\nlibrary can load a TFLite model and make predictions directly in the user’s browser.\\nFor  example,  the  following  JavaScript  module  imports  the  TFJS  library,  downloads\\na pretrained MobileNet model, and uses this model to classify an image and log the\\npredictions. You can play with the code at https://homl.info/tfjscode, using Glitch.com,\\na website that lets you build web apps in your browser for free; click the PREVIEW\\nbutton in the lower-right corner of the page to see the code in action:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,445.950,399.250,464.650 'import \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\\nimport \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,425.550,288.750,434.050 'const image = document.getElementById(\"image\");\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,323.550,382.250,413.650 'mobilenet.load().then(model => {\\n    model.classify(image).then(predictions => {\\n        for (var i = 0; i < predictions.length; i++) {\\n            let className = predictions[i].className\\n            let proba = (predictions[i].probability * 100).toFixed(1)\\n            console.log(className + \" : \" + proba + \"%\");\\n        }\\n    });\\n});\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,140.436,432.004,314.736 'It’s  even  possible  to  turn  this  website  into  a  progressive  web  app  (PWA):  this  is  a\\nwebsite that respects a number of criteria8 that allow it to be viewed in any browser,\\nand even installed as a standalone app on a mobile device. For example, try visiting\\nhttps://homl.info/tfjswpa  on  a  mobile  device:  most  modern  browsers  will  ask  you\\nwhether you would like to add TFJS Demo to your home screen. If you accept, you\\nwill see a new icon in your list of applications. Clicking this icon will load the TFJS\\nDemo website inside its own window, just like a regular mobile app. A PWA can even\\nbe configured to work offline, by using a service worker: this is a JavaScript module\\nthat runs in its own separate thread in the browser and intercepts network requests,\\nallowing it to cache resources so the PWA can run faster, or even entirely offline. It\\ncan also deliver push messages, run tasks in the background, and more. PWAs allow\\nyou to manage a single code base for the web and for mobile devices. They also make\\nit easier to ensure that all users run the same version of your application. You can play\\nwith this TFJS Demo’s PWA code on Glitch.com at https://homl.info/wpacode.\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,88.954,416.760,96.954 '8 For example, a PWA must include icons of various sizes for different mobile devices, it must be served via\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,426.808,86.954 'HTTPS, it must include a manifest file containing metadata such as the name of the app and the background\\ncolor.\\n'>\n",
            "<LTTextBoxHorizontal(8) 307.731,40.500,402.519,49.500 'Running a Model in a Web Page \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.987,40.500,432.003,49.500 '745\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,104.900,162.000,104.900>\n",
            "<LTTextBoxHorizontal(0) 136.790,584.585,396.000,605.705 'Check out many more demos of machine learning models running\\nin your browser at https://tensorflow.org/js/demos.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,466.637,432.005,540.137 'TFJS  also  supports  training  a  model  directly  in  your  web  browser!  And  it’s  actually\\npretty  fast.  If  your  computer  has  a  GPU  card,  then  TFJS  can  generally  use  it,  even\\nif it’s not an Nvidia card. Indeed, TFJS will use WebGL when it’s available, and since\\nmodern  web  browsers  generally  support  a  wide  range  of  GPU  cards,  TFJS  actually\\nsupports  more  GPU  cards  than  regular  TensorFlow  (which  only  supports  Nvidia\\ncards).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,410.236,432.002,458.536 'Training  a  model  in  a  user’s  web  browser  can  be  especially  useful  to  guarantee  that\\nthis user’s data remains private. A model can be trained centrally, and then fine-tuned\\nlocally,  in  the  browser,  based  on  that  user’s  data.  If  you’re  interested  in  this  topic,\\ncheck out federated learning.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,353.836,432.003,402.136 'Once  again,  doing  justice  to  this  topic  would  require  a  whole  book.  If  you  want  to\\nlearn more about TensorFlow.js, check out the O’reilly books Practical Deep Learning\\nfor  Cloud,  Mobile,  and  Edge,  by  Anirudh  Koul  et  al.,  or  Learning  TensorFlow.js,  by\\nGant Laborde.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,310.036,432.005,345.736 'Now  that  you’ve  seen  how  to  deploy  TensorFlow  models  to  TF  Serving,  or  to  the\\ncloud with Vertex AI, or to mobile and embedded devices using TFLite, or to a web\\nbrowser using TFJS, let’s discuss how to use GPUs to speed up computations.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.992,184.436,432.004,297.696 'Using GPUs to Speed Up Computations\\nIn Chapter 11 we looked at several techniques that can considerably speed up train‐\\ning: better weight initialization, sophisticated optimizers, and so on. But even with all\\nof these techniques, training a large neural network on a single machine with a single\\nCPU can take hours, days, or even weeks, depending on the task. Thanks to GPUs,\\nthis training time can be reduced down to minutes or hours. Not only does this save\\nan enormous amount of time, but it also means that you can experiment with various\\nmodels much more easily, and frequently retrain your models on fresh data.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,77.043,432.004,176.336 'In the previous chapters, we used GPU-enabled runtimes on Google Colab. All you\\nhave  to  do  for  this  is  select  “Change  runtime  type”  from  the  Runtime  menu,  and\\nchoose the GPU accelerator type; TensorFlow automatically detects the GPU and uses\\nit  to  speed  up  computations,  and  the  code  is  exactly  the  same  as  without  a  GPU.\\nThen, in this chapter you saw how to deploy your models to Vertex AI on multiple\\nGPU-enabled  compute  nodes:  it’s  just  a  matter  of  selecting  the  right  GPU-enabled\\nDocker  image  when  creating  the  Vertex  AI  model,  and  selecting  the  desired  GPU\\ntype when calling endpoint.deploy(). But what if you want to buy your own GPU?\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,40.500,84.435,49.500 '746 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.903,40.500,289.707,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'And  what  if  you  want  to  distribute  the  computations  across  the  CPU  and  multiple\\nGPU  devices  on  a  single  machine  (see  Figure  19-6)?  This  is  what  we  will  discuss\\nnow, then later in this chapter we will discuss how to distribute computations across\\nmultiple servers.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,307.391,385.164,317.891 'Figure 19-6. Executing a TensorFlow graph across multiple devices in parallel\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,224.009,432.003,295.649 'Getting Your Own GPU\\nIf you know that you’ll be using a GPU heavily and for a long period of time, then\\nbuying your own can make financial sense. You may also want to train your models\\nlocally because you do not want to upload your data to the cloud. Or perhaps you just\\nwant to buy a GPU card for gaming, and you’d like to use it for deep learning as well.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,104.609,432.005,215.909 'If you decide to purchase a GPU card, then take some time to make the right choice.\\nYou  will  need  to  consider  the  amount  of  RAM  you  will  need  for  your  tasks  (e.g.,\\ntypically at least 10 GB for image processing or NLP), the bandwidth (i.e., how fast\\nyou can send data into and out of the GPU), the number of cores, the cooling system,\\netc. Tim Dettmers wrote an excellent blog post to help you choose: I encourage you\\nto read it carefully. At the time of this writing, TensorFlow only supports Nvidia cards\\nwith  CUDA  Compute  Capability  3.5+  (as  well  as  Google’s  TPUs,  of  course),  but  it\\nmay extend its support to other manufacturers, so make sure to check TensorFlow’s\\ndocumentation to see what devices are supported today.\\n'>\n",
            "<LTTextBoxHorizontal(4) 288.020,40.500,402.518,49.500 'Using GPUs to Speed Up Computations \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.986,40.500,432.002,49.500 '747\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,550.975,432.500,550.975>\n",
            "<LTLine 432.375,323.454,432.375,551.100>\n",
            "<LTLine 72.000,323.579,432.500,323.579>\n",
            "<LTLine 72.125,323.454,72.125,551.100>\n",
            "<LTFigure(I1) 126.250,329.434,378.250,545.850 matrix=[252.00,0.00,0.00,216.42, (126.25,329.43)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,481.637,432.003,605.537 'If  you  go  for  an  Nvidia  GPU  card,  you  will  need  to  install  the  appropriate  Nvidia\\ndrivers  and  several  Nvidia  libraries.9  These  include  the  Compute  Unified  Device\\nArchitecture library (CUDA) Toolkit, which allows developers to use CUDA-enabled\\nGPUs  for  all  sorts  of  computations  (not  just  graphics  acceleration),  and  the  CUDA\\nDeep Neural Network library (cuDNN), a GPU-accelerated library of common DNN\\ncomputations such as activation layers, normalization, forward and backward convo‐\\nlutions, and pooling (see Chapter 14). cuDNN is part of Nvidia’s Deep Learning SDK.\\nNote that you will need to create an Nvidia developer account in order to download\\nit.  TensorFlow  uses  CUDA  and  cuDNN  to  control  the  GPU  cards  and  accelerate\\ncomputations (see Figure 19-7).\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,262.467,410.391,272.967 'Figure 19-7. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,186.874,432.005,248.367 'Once  you  have  installed  the  GPU  card(s)  and  all  the  required  drivers  and  libraries,\\nyou can use the nvidia-smi command to check that everything is properly installed.\\nThis command lists the available GPU cards, as well as all the processes running on\\neach  card.  In  this  example,  it’s  an  Nvidia  Tesla  T4  GPU  card  with  about  15  GB  of\\navailable RAM, and there are no processes currently running on it:\\n'>\n",
            "<LTTextBoxHorizontal(3) 73.140,78.954,422.440,86.954 '9 Please check the TensorFlow docs for detailed and up-to-date installation instructions, as they change quite\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.000,68.954,98.488,76.954 'often.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,40.500,84.437,49.500 '748 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,475.375,432.500,475.375>\n",
            "<LTLine 432.375,278.531,432.375,475.500>\n",
            "<LTLine 72.000,278.656,432.500,278.656>\n",
            "<LTLine 72.125,278.531,72.125,475.500>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 180.250,284.511,324.250,470.250 matrix=[144.00,0.00,0.00,185.74, (180.25,284.51)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,475.750,424.750,606.650 '$ nvidia-smi\\nSun Apr 10 04:52:10 2022\\n+-----------------------------------------------------------------------------+\\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\\n|-------------------------------+----------------------+----------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|                               |                      |               MIG M. |\\n|===============================+======================+======================|\\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\\n| N/A   34C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,394.150,424.750,463.850 '+-----------------------------------------------------------------------------+\\n| Processes:                                                                  |\\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\\n|        ID   ID                                                   Usage      |\\n|=============================================================================|\\n|  No running processes found                                                 |\\n+-----------------------------------------------------------------------------+\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,362.237,432.003,385.337 'To check that TensorFlow actually sees your GPU, run the following commands and\\nmake sure the result is not empty:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.001,326.350,369.501,355.250 '>>> physical_gpus = tf.config.list_physical_devices(\"GPU\")\\n>>> physical_gpus\\n[PhysicalDevice(name=\\'/physical_device:GPU:0\\', device_type=\\'GPU\\')]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,179.427,432.005,314.067 'Managing the GPU RAM\\nBy default TensorFlow automatically grabs almost all the RAM in all available GPUs\\nthe first time you run a computation. It does this to limit GPU RAM fragmentation.\\nThis  means  that  if  you  try  to  start  a  second  TensorFlow  program  (or  any  program\\nthat requires the GPU), it will quickly run out of RAM. This does not happen as often\\nas you might think, as you will most often have a single TensorFlow program running\\non  a  machine:  usually  a  training  script,  a  TF  Serving  node,  or  a  Jupyter  notebook.\\nIf  you  need  to  run  multiple  programs  for  some  reason  (e.g.,  to  train  two  different\\nmodels in parallel on the same machine), then you will need to split the GPU RAM\\nbetween these processes more evenly.\\n'>\n",
            "<LTTextBoxHorizontal(5) 288.021,40.500,402.519,49.500 'Using GPUs to Speed Up Computations \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.987,40.500,432.003,49.500 '749\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,518.250,432.005,605.537 'If  you  have  multiple  GPU  cards  on  your  machine,  a  simple  solution  is  to  assign\\neach of them to a single process. To do this, you can set the CUDA_VISIBLE_DEVICES\\nenvironment  variable  so  that  each  process  only  sees  the  appropriate  GPU  card(s).\\nAlso set the CUDA_DEVICE_ORDER environment variable to PCI_BUS_ID to ensure that\\neach  ID  always  refers  to  the  same  GPU  card.  For  example,  if  you  have  four  GPU\\ncards, you could start two programs, assigning two GPUs to each of them, by execut‐\\ning commands like the following in two separate terminal windows:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,482.363,412.002,511.264 '$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\\n# and in another terminal:\\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,397.677,432.003,474.516 'Program  1  will  then  only  see  GPU  cards  0  and  1,  named  \"/gpu:0\"  and  \"/gpu:1\",\\nrespectively,  in  TensorFlow,  and  program  2  will  only  see  GPU  cards  2  and  3,\\nnamed  \"/gpu:1\"  and  \"/gpu:0\",  respectively  (note  the  order).  Everything  will  work\\nfine  (see  Figure  19-8).  Of  course,  you  can  also  define  these  environment  variables\\nin  Python  by  setting  os.environ[\"CUDA_DEVICE_ORDER\"]  and  os.environ[\"CUDA_\\nVISIBLE_DEVICES\"], as long as you do so before using TensorFlow.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.002,263.002,242.448,273.502 'Figure 19-8. Each program gets two GPUs\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,188.002,432.003,248.902 'Another  option  is  to  tell  TensorFlow  to  grab  only  a  specific  amount  of  GPU  RAM.\\nThis  must  be  done  immediately  after  importing  TensorFlow.  For  example,  to  make\\nTensorFlow  grab  only  2  GiB  of  RAM  on  each  GPU,  you  must  create  a  logical  GPU\\ndevice (sometimes called a virtual GPU device) for each physical GPU device and set\\nits memory limit to 2 GiB (i.e., 2,048 MiB):\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.004,131.715,365.254,181.015 'for gpu in physical_gpus:\\n    tf.config.set_logical_device_configuration(\\n        gpu,\\n        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\\n    )\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,74.009,432.003,122.902 'Let’s  suppose  you  have  four  GPUs,  each  with  at  least  4  GiB  of  RAM:  in  this  case,\\ntwo  programs  like  this  one  can  run  in  parallel,  each  using  all  four  GPU  cards  (see\\nFigure 19-9). If you run the nvidia-smi command while both programs are running,\\nyou should see that each process holds 2 GiB of RAM on each card.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.999,40.500,84.437,49.500 '750 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,391.415,432.500,391.415>\n",
            "<LTLine 432.375,279.065,432.375,391.540>\n",
            "<LTLine 72.000,279.190,432.500,279.190>\n",
            "<LTLine 72.125,279.065,72.125,391.540>\n",
            "<LTFigure(I1) 126.250,285.045,378.250,386.290 matrix=[252.00,0.00,0.00,101.25, (126.25,285.05)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,437.533,410.499,460.633 'Figure 19-9. Each program gets all four GPUs, but with only 2 GiB of RAM on each\\nGPU\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,400.333,432.000,423.433 'Yet another option is to tell TensorFlow to grab memory only when it needs it. Again,\\nthis must be done immediately after importing TensorFlow:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,374.646,322.751,393.346 'for gpu in physical_gpus:\\n    tf.config.experimental.set_memory_growth(gpu, True)\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,240.153,432.005,366.799 'Another  way  to  do  this  is  to  set  the  TF_FORCE_GPU_ALLOW_GROWTH  environment\\nvariable  to  true.  With  this  option,  TensorFlow  will  never  release  memory  once\\nit  has  grabbed  it  (again,  to  avoid  memory  fragmentation),  except  of  course  when\\nthe  program  ends.  It  can  be  harder  to  guarantee  deterministic  behavior  using  this\\noption (e.g., one program may crash because another program’s memory usage went\\nthrough  the  roof),  so  in  production  you’ll  probably  want  to  stick  with  one  of  the\\nprevious options. However, there are some cases where it is very useful: for example,\\nwhen  you  use  a  machine  to  run  multiple  Jupyter  notebooks,  several  of  which  use\\nTensorFlow. The TF_FORCE_GPU_ALLOW_GROWTH environment variable is set to true in\\nColab runtimes.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,171.153,432.002,232.053 'Lastly, in some cases you may want to split a GPU into two or more logical devices.\\nFor  example,  this  is  useful  if  you  only  have  one  physical  GPU—like  in  a  Colab\\nruntime—but  you  want  to  test  a  multi-GPU  algorithm.  The  following  code  splits\\nGPU #0 into two logical devices, with 2 GiB of RAM each (again, this must be done\\nimmediately after importing TensorFlow):\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.998,114.866,348.248,164.166 'tf.config.set_logical_device_configuration(\\n    physical_gpus[0],\\n    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\\n     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\\n)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,82.359,432.003,107.019 'These two logical devices are called \"/gpu:0\" and \"/gpu:1\", and you can use them as\\nif they were two normal GPUs. You can list all logical devices like this:\\n'>\n",
            "<LTTextBoxHorizontal(7) 288.021,40.500,402.519,49.500 'Using GPUs to Speed Up Computations \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.987,40.500,432.003,49.500 '751\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,466.196,432.375,607.500>\n",
            "<LTLine 72.000,466.321,432.500,466.321>\n",
            "<LTLine 72.125,466.196,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,472.176,378.250,602.250 matrix=[252.00,0.00,0.00,130.07, (126.25,472.18)]>\n",
            "<LTTextBoxHorizontal(0) 88.998,567.550,326.998,606.650 '>>> logical_gpus = tf.config.list_logical_devices(\"GPU\")\\n>>> logical_gpus\\n[LogicalDevice(name=\\'/device:GPU:0\\', device_type=\\'GPU\\'),\\n LogicalDevice(name=\\'/device:GPU:1\\', device_type=\\'GPU\\')]\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,535.637,431.995,558.736 'Now let’s see how TensorFlow decides which devices it should use to place variables\\nand execute operations.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.992,465.027,432.004,524.067 'Placing Operations and Variables on Devices\\nKeras and tf.data generally do a good job of placing operations and variables where\\nthey belong, but you can also place operations and variables manually on each device,\\nif you want more control:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.651,442.427,431.998,452.927 '•\\n• You generally want to place the data preprocessing operations on the CPU, and\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.003,429.827,299.436,440.327 'place the neural network operations on the GPUs.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.651,413.227,432.003,423.727 '•\\n• GPUs usually have a fairly limited communication bandwidth, so it is important\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.997,400.627,347.058,411.127 'to avoid unnecessary data transfers into and out of the GPUs.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.655,333.627,432.002,394.527 '• Adding  more  CPU  RAM  to  a  machine  is  simple  and  fairly  cheap,  so  there’s\\n•\\nusually  plenty  of  it,  whereas  the  GPU  RAM  is  baked  into  the  GPU:  it  is  an\\nexpensive and thus limited resource, so if a variable is not needed in the next few\\ntraining steps, it should probably be placed on the CPU (e.g., datasets generally\\nbelong on the CPU).\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,272.041,431.999,321.527 'By  default,  all  variables  and  all  operations  will  be  placed  on  the  first  GPU  (the  one\\nnamed \"/gpu:0\"), except for variables and operations that don’t have a GPU kernel:10\\nthese are placed on the CPU (always named \"/cpu:0\"). A tensor or variable’s device\\nattribute tells you which device it was placed on:11\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.004,205.554,382.254,265.054 \">>> a = tf.Variable([1., 2., 3.])  # float32 variable goes to the GPU\\n>>> a.device\\n'/job:localhost/replica:0/task:0/device:GPU:0'\\n>>> b = tf.Variable([1, 2, 3])  # int32 variable goes to the CPU\\n>>> b.device\\n'/job:localhost/replica:0/task:0/device:CPU:0'\\n\">\n",
            "<LTTextBoxHorizontal(10) 71.995,160.448,432.001,197.707 'You can safely ignore the prefix /job:localhost/replica:0/task:0 for now; we will\\ndiscuss jobs, replicas, and tasks later in this chapter. As you can see, the first variable\\nwas placed on GPU #0, which is the default device. However, the second variable was\\n'>\n",
            "<LTTextBoxHorizontal(11) 69.780,82.646,420.888,112.030 '10 As we saw in Chapter 12, a kernel is an operation’s implementation for a specific data type and device type.\\nFor example, there is a GPU kernel for the float32 tf.matmul() operation, but there is no GPU kernel for\\nint32 tf.matmul(), only a CPU kernel.\\n'>\n",
            "<LTTextBoxHorizontal(12) 69.781,68.954,388.649,78.142 '11 You can also use tf.debugging.set_log_device_placement(True) to log all device placements.\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.004,40.500,84.442,49.500 '752 \\n'>\n",
            "<LTTextBoxHorizontal(14) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 102.910,40.500,289.714,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,119.976,162.000,119.976>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'placed on the CPU: this is because there are no GPU kernels for integer variables, or\\nfor operations involving integer tensors, so TensorFlow fell back to the CPU.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,550.643,431.997,574.337 'If  you  want  to  place  an  operation  on  a  different  device  than  the  default  one,  use  a\\ntf.device() context:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,494.357,284.498,543.657 '>>> with tf.device(\"/cpu:0\"):\\n...     c = tf.Variable([1., 2., 3.])\\n...\\n>>> c.device\\n\\'/job:localhost/replica:0/task:0/device:CPU:0\\'\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.792,432.009,396.003,477.595 'The  CPU  is  always  treated  as  a  single  device  (\"/cpu:0\"),  even\\nif  your  machine  has  multiple  CPU  cores.  Any  operation  placed\\non  the  CPU  may  run  in  parallel  across  multiple  cores  if  it  has  a\\nmultithreaded kernel.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,337.050,432.005,411.143 'If you explicitly try to place an operation or variable on a device that does not exist\\nor for which there is no kernel, then TensorFlow will silently fall back to the device\\nit would have chosen by default. This is useful when you want to be able to run the\\nsame code on different machines that don’t have the same number of GPUs. However,\\nyou can run tf.config.set_soft_device_placement(False) if you prefer to get an\\nexception.\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.004,318.450,403.058,328.950 'Now, how exactly does TensorFlow execute operations across multiple devices?\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,172.241,432.005,306.880 'Parallel Execution Across Multiple Devices\\nAs  we  saw  in  Chapter  12,  one  of  the  benefits  of  using  TF  functions  is  parallelism.\\nLet’s  look  at  this  a  bit  more  closely.  When  TensorFlow  runs  a  TF  function,  it  starts\\nby analyzing its graph to find the list of operations that need to be evaluated, and it\\ncounts how many dependencies each of them has. TensorFlow then adds each opera‐\\ntion with zero dependencies (i.e., each source operation) to the evaluation queue of\\nthis operation’s device (see Figure 19-10). Once an operation has been evaluated, the\\ndependency  counter  of  each  operation  that  depends  on  it  is  decremented.  Once  an\\noperation’s dependency counter reaches zero, it is pushed to the evaluation queue of\\nits device. And once all the outputs have been computed, they are returned.\\n'>\n",
            "<LTTextBoxHorizontal(7) 288.020,40.500,402.518,49.500 'Using GPUs to Speed Up Computations \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.986,40.500,432.002,49.500 '753\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,429.006,126.944,478.507 matrix=[37.94,0.00,0.00,49.50, (89.00,429.01)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,358.703,311.148,369.203 'Figure 19-10. Parallelized execution of a TensorFlow graph\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,258.503,432.005,344.603 'Operations  in  the  CPU’s  evaluation  queue  are  dispatched  to  a  thread  pool  called\\nthe  inter-op  thread  pool.  If  the  CPU  has  multiple  cores,  then  these  operations  will\\neffectively  be  evaluated  in  parallel.  Some  operations  have  multithreaded  CPU  ker‐\\nnels:  these  kernels  split  their  tasks  into  multiple  suboperations,  which  are  placed  in\\nanother evaluation queue and dispatched to a second thread pool called the intra-op\\nthread pool (shared by all multithreaded CPU kernels). In short, multiple operations\\nand suboperations may be evaluated in parallel on different CPU cores.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,164.303,432.005,250.403 'For  the  GPU,  things  are  a  bit  simpler.  Operations  in  a  GPU’s  evaluation  queue  are\\nevaluated  sequentially.  However,  most  operations  have  multithreaded  GPU  kernels,\\ntypically implemented by libraries that TensorFlow depends on, such as CUDA and\\ncuDNN.  These  implementations  have  their  own  thread  pools,  and  they  typically\\nexploit  as  many  GPU  threads  as  they  can  (which  is  the  reason  why  there  is  no\\nneed for an inter-op thread pool in GPUs: each operation already floods most GPU\\nthreads).\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,70.103,432.005,156.203 'For  example,  in  Figure  19-10,  operations  A,  B,  and  C  are  source  ops,  so  they  can\\nimmediately  be  evaluated.  Operations  A  and  B  are  placed  on  the  CPU,  so  they  are\\nsent  to  the  CPU’s  evaluation  queue,  then  they  are  dispatched  to  the  inter-op  thread\\npool  and  immediately  evaluated  in  parallel.  Operation  A  happens  to  have  a  multi‐\\nthreaded  kernel;  its  computations  are  split  into  three  parts,  which  are  executed  in\\nparallel by the intra-op thread pool. Operation C goes to GPU #0’s evaluation queue,\\nand in this example its GPU kernel happens to use cuDNN, which manages its own\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,40.500,84.437,49.500 '754 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,374.767,432.375,607.500>\n",
            "<LTLine 72.000,374.892,432.500,374.892>\n",
            "<LTLine 72.125,374.767,72.125,607.500>\n",
            "<LTFigure(I1) 126.250,380.747,378.250,602.250 matrix=[252.00,0.00,0.00,221.50, (126.25,380.75)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,506.836,432.005,605.537 'intra-op  thread  pool  and  runs  the  operation  across  many  GPU  threads  in  parallel.\\nSuppose C finishes first. The dependency counters of D and E are decremented and\\nthey reach 0, so both operations are pushed to GPU #0’s evaluation queue, and they\\nare  executed  sequentially.  Note  that  C  only  gets  evaluated  once,  even  though  both\\nD  and  E  depend  on  it.  Suppose  B  finishes  next.  Then  F’s  dependency  counter  is\\ndecremented from 4 to 3, and since that’s not 0, it does not run yet. Once A, D, and\\nE are finished, then F’s dependency counter reaches 0, and it is pushed to the CPU’s\\nevaluation queue and evaluated. Finally, TensorFlow returns the requested outputs.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,436.650,432.005,498.736 'An extra bit of magic that TensorFlow performs is when the TF function modifies a\\nstateful resource, such as a variable: it ensures that the order of execution matches the\\norder in the code, even if there is no explicit dependency between the statements. For\\nexample,  if  your  TF  function  contains  v.assign_add(1)  followed  by  v.assign(v  *\\n2), TensorFlow will ensure that these operations are executed in that order.\\n'>\n",
            "<LTTextBoxHorizontal(2) 136.786,350.891,396.004,419.718 'You  can  control  the  number  of  threads  in  the  inter-op  thread\\npool  by  calling  tf.config.threading.set_inter_op_parallel\\nism_threads().  To  set  the  number  of  intra-op  threads,  use\\ntf.config.threading.set_intra_op_parallelism_threads().\\nThis  is  useful  if  you  do  not  want  TensorFlow  to  use  all  the  CPU\\ncores or if you want it to be single-threaded.12\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,310.703,432.001,333.803 'With that, you have all you need to run any operation on any device, and exploit the\\npower of your GPUs! Here are some of the things you could do:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.653,199.309,432.004,298.603 '• You  could  train  several  models  in  parallel,  each  on  its  own  GPU:  just\\n•\\nwrite  a  training  script  for  each  model  and  run  them  in  parallel,  setting\\nCUDA_DEVICE_ORDER  and  CUDA_VISIBLE_DEVICES  so  that  each  script  only  sees\\na  single  GPU  device.  This  is  great  for  hyperparameter  tuning,  as  you  can  train\\nin parallel multiple models with different hyperparameters. If you have a single\\nmachine with two GPUs, and it takes one hour to train one model on one GPU,\\nthen  training  two  models  in  parallel,  each  on  its  own  dedicated  GPU,  will  take\\njust one hour. Simple!\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.650,144.316,432.003,193.210 '• You  could  train  a  model  on  a  single  GPU  and  perform  all  the  preprocessing  in\\n•\\nparallel on the CPU, using the dataset’s prefetch() method13 to prepare the next\\nfew  batches  in  advance  so  that  they  are  ready  when  the  GPU  needs  them  (see\\nChapter 13).\\n'>\n",
            "<LTTextBoxHorizontal(6) 69.780,68.954,431.386,111.338 '12 This can be useful if you want to guarantee perfect reproducibility, as I explain in this video, based on TF 1.\\n13 At the time of writing, it only prefetches the data to the CPU RAM, but use tf.data.experimental.prefetch\\n_to_device() to make it prefetch the data and push it to the device of your choice so that the GPU does not\\nwaste time waiting for the data to be transferred.\\n'>\n",
            "<LTTextBoxHorizontal(7) 288.021,40.500,402.519,49.500 'Using GPUs to Speed Up Computations \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.987,40.500,432.003,49.500 '755\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,119.284,162.000,119.284>\n",
            "<LTFigure(I1) 85.000,369.736,126.760,425.513 matrix=[41.76,0.00,0.00,55.78, (85.00,369.74)]>\n",
            "<LTTextBoxHorizontal(0) 80.651,569.837,431.999,605.537 '• If  your  model  takes  two  images  as  input  and  processes  them  using  two  CNNs\\n•\\nbefore joining their outputs,14 then it will probably run much faster if you place\\neach CNN on a different GPU.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.654,528.037,432.013,563.737 '•\\n• You can create an efficient ensemble: just place a different trained model on each\\nGPU so that you can get all the predictions much faster to produce the ensemble’s\\nfinal prediction.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.999,505.437,352.128,515.937 'But what if you want to speed up training by using multiple GPUs?\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,417.636,432.002,493.097 'Training Models Across Multiple Devices\\nThere  are  two  main  approaches  to  training  a  single  model  across  multiple  devices:\\nmodel  parallelism,  where  the  model  is  split  across  the  devices,  and  data  parallelism,\\nwhere  the  model  is  replicated  across  every  device,  and  each  replica  is  trained  on  a\\ndifferent subset of the data. Let’s look at these two options.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.993,183.227,432.005,406.066 'Model Parallelism\\nSo  far  we  have  trained  each  neural  network  on  a  single  device.  What  if  we  want\\nto  train  a  single  neural  network  across  multiple  devices?  This  requires  chopping\\nthe  model  into  separate  chunks  and  running  each  chunk  on  a  different  device.\\nUnfortunately, such model parallelism turns out to be pretty tricky, and its effective‐\\nness really depends on the architecture of your neural network. For fully connected\\nnetworks,  there  is  generally  not  much  to  be  gained  from  this  approach  (see  Fig‐\\nure 19-11). Intuitively, it may seem that an easy way to split the model is to place each\\nlayer  on  a  different  device,  but  this  does  not  work  because  each  layer  needs  to  wait\\nfor  the  output  of  the  previous  layer  before  it  can  do  anything.  So  perhaps  you  can\\nslice it vertically—for example, with the left half of each layer on one device, and the\\nright part on another device? This is slightly better, since both halves of each layer can\\nindeed  work  in  parallel,  but  the  problem  is  that  each  half  of  the  next  layer  requires\\nthe  output  of  both  halves,  so  there  will  be  a  lot  of  cross-device  communication\\n(represented by the dashed arrows). This is likely to completely cancel out the benefit\\nof  the  parallel  computation,  since  cross-device  communication  is  slow  (and  even\\nmore so when the devices are located on different machines).\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,68.954,308.504,76.954 '14 If the two CNNs are identical, then it is called a Siamese neural network.\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.003,40.500,84.441,49.500 '756 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 102.909,40.500,289.713,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 72.000,377.406,298.254,387.906 'Figure 19-11. Splitting a fully connected neural network\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,327.606,432.000,363.306 'Some  neural  network  architectures,  such  as  convolutional  neural  networks  (see\\nChapter 14), contain layers that are only partially connected to the lower layers, so it\\nis much easier to distribute chunks across devices in an efficient way (Figure 19-12).\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.005,80.747,314.849,91.247 'Figure 19-12. Splitting a partially connected neural network\\n'>\n",
            "<LTTextBoxHorizontal(3) 283.709,40.500,402.518,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(4) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 420.986,40.500,432.002,49.500 '757\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,393.469,432.375,607.500>\n",
            "<LTLine 72.000,393.594,432.500,393.594>\n",
            "<LTLine 72.125,393.469,72.125,607.500>\n",
            "<LTLine 72.000,321.344,432.500,321.344>\n",
            "<LTLine 432.375,96.811,432.375,321.469>\n",
            "<LTLine 72.000,96.936,432.500,96.936>\n",
            "<LTLine 72.125,96.811,72.125,321.469>\n",
            "<LTFigure(I1) 79.689,399.449,424.811,602.250 matrix=[345.12,0.00,0.00,202.80, (79.69,399.45)]>\n",
            "<LTFigure(I2) 117.250,102.791,387.250,316.219 matrix=[270.00,0.00,0.00,213.43, (117.25,102.79)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,455.843,432.005,605.537 'Deep  recurrent  neural  networks  (see  Chapter  15)  can  be  split  a  bit  more  efficiently\\nacross multiple GPUs. If you split the network horizontally by placing each layer on a\\ndifferent device, and feed the network with an input sequence to process, then at the\\nfirst time step only one device will be active (working on the sequence’s first value), at\\nthe second step two will be active (the second layer will be handling the output of the\\nfirst layer for the first value, while the first layer will be handling the second value),\\nand  by  the  time  the  signal  propagates  to  the  output  layer,  all  devices  will  be  active\\nsimultaneously  (Figure  19-13).  There  is  still  a  lot  of  cross-device  communication\\ngoing on, but since each cell may be fairly complex, the benefit of running multiple\\ncells  in  parallel  may  (in  theory)  outweigh  the  communication  penalty.  However,  in\\npractice a regular stack of LSTM layers running on a single GPU actually runs much\\nfaster.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,196.332,295.952,206.832 'Figure 19-13. Splitting a deep recurrent neural network\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,133.932,432.005,182.232 'In short, model parallelism may speed up running or training some types of neural\\nnetworks,  but  not  all,  and  it  requires  special  care  and  tuning,  such  as  making  sure\\nthat  devices  that  need  to  communicate  the  most  run  on  the  same  machine.15  Next\\nwe’ll look at a much simpler and generally more efficient option: data parallelism.\\n'>\n",
            "<LTTextBoxHorizontal(3) 69.780,68.954,367.416,76.954 '15 If you are interested in going further with model parallelism, check out Mesh TensorFlow.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,40.500,84.434,49.500 '758 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.902,40.500,289.706,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,449.582,432.500,449.582>\n",
            "<LTLine 432.375,212.396,432.375,449.707>\n",
            "<LTLine 72.000,212.521,432.500,212.521>\n",
            "<LTLine 72.125,212.396,72.125,449.707>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 97.329,218.376,407.171,444.457 matrix=[309.84,0.00,0.00,226.08, (97.33,218.38)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,511.055,432.004,607.894 'Data Parallelism\\nAnother way to parallelize the training of a neural network is to replicate it on every\\ndevice  and  run  each  training  step  simultaneously  on  all  replicas,  using  a  different\\nmini-batch for each. The gradients computed by each replica are then averaged, and\\nthe result is used to update the model parameters. This is called data parallelism, or\\nsometimes  single  program,  multiple  data  (SPMD).  There  are  many  variants  of  this\\nidea, so let’s look at the most important ones.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.998,486.687,240.497,498.247 'Data parallelism using the mirrored strategy\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.998,418.363,432.005,479.263 'Arguably  the  simplest  approach  is  to  completely  mirror  all  the  model  parameters\\nacross  all  the  GPUs  and  always  apply  the  exact  same  parameter  updates  on  every\\nGPU.  This  way,  all  replicas  always  remain  perfectly  identical.  This  is  called  the\\nmirrored strategy, and it turns out to be quite efficient, especially when using a single\\nmachine (see Figure 19-14).\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.002,90.212,308.651,100.712 'Figure 19-14. Data parallelism using the mirrored strategy\\n'>\n",
            "<LTTextBoxHorizontal(4) 283.703,40.500,402.512,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(5) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 420.980,40.500,431.996,49.500 '759\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,412.101,432.500,412.101>\n",
            "<LTLine 432.375,106.275,432.375,412.226>\n",
            "<LTLine 72.000,106.400,432.500,106.400>\n",
            "<LTLine 72.125,106.275,72.125,412.226>\n",
            "<LTFigure(I1) 93.129,112.255,411.371,406.976 matrix=[318.24,0.00,0.00,294.72, (93.13,112.26)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,532.036,432.000,605.537 'The  tricky  part  when  using  this  approach  is  to  efficiently  compute  the  mean  of  all\\nthe  gradients  from  all  the  GPUs  and  distribute  the  result  across  all  the  GPUs.  This\\ncan be done using an AllReduce algorithm, a class of algorithms where multiple nodes\\ncollaborate  to  efficiently  perform  a  reduce  operation  (such  as  computing  the  mean,\\nsum, and max), while ensuring that all nodes obtain the same final result. Fortunately,\\nthere are off-the-shelf implementations of such algorithms, as you will see.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.005,507.669,243.567,519.229 'Data parallelism with centralized parameters\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,439.344,432.005,500.244 'Another  approach  is  to  store  the  model  parameters  outside  of  the  GPU  devices\\nperforming  the  computations  (called  workers);  for  example,  on  the  CPU  (see  Fig‐\\nure 19-15). In a distributed setup, you may place all the parameters on one or more\\nCPU-only servers called parameter servers, whose only role is to host and update the\\nparameters.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,174.340,312.551,184.840 'Figure 19-15. Data parallelism with centralized parameters\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,124.540,432.003,160.240 'Whereas the mirrored strategy imposes synchronous weight updates across all GPUs,\\nthis  centralized  approach  allows  either  synchronous  or  asynchronous  updates.  Let’s\\ntake a look at the pros and cons of both options.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,40.500,84.437,49.500 '760 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,433.083,432.500,433.083>\n",
            "<LTLine 432.375,190.404,432.375,433.208>\n",
            "<LTLine 72.000,190.529,432.500,190.529>\n",
            "<LTLine 72.125,190.404,72.125,433.208>\n",
            "<LTFigure(I1) 79.450,196.384,425.050,427.958 matrix=[345.60,0.00,0.00,231.57, (79.45,196.38)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,488.583,432.004,601.788 'Synchronous updates.    With synchronous updates, the aggregator waits until all gradi‐\\nents  are  available  before  it  computes  the  average  gradients  and  passes  them  to  the\\noptimizer,  which  will  update  the  model  parameters.  Once  a  replica  has  finished\\ncomputing its gradients, it must wait for the parameters to be updated before it can\\nproceed  to  the  next  mini-batch.  The  downside  is  that  some  devices  may  be  slower\\nthan others, so the fast devices will have to wait for the slow ones at every step, mak‐\\ning the whole process as slow as the slowest device. Moreover, the parameters will be\\ncopied to every device almost at the same time (immediately after the gradients are\\napplied), which may saturate the parameter servers’ bandwidth.\\n'>\n",
            "<LTTextBoxHorizontal(1) 136.789,381.411,396.001,471.651 'To reduce the waiting time at each step, you could ignore the gradi‐\\nents  from  the  slowest  few  replicas  (typically  ~10%).  For  example,\\nyou  could  run  20  replicas,  but  only  aggregate  the  gradients  from\\nthe  fastest  18  replicas  at  each  step,  and  just  ignore  the  gradients\\nfrom  the  last  2.  As  soon  as  the  parameters  are  updated,  the  first\\n18 replicas can start working again immediately, without having to\\nwait for the 2 slowest replicas. This setup is generally described as\\nhaving 18 replicas plus 2 spare replicas.16\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,259.968,432.005,360.573 'Asynchronous updates.     With  asynchronous  updates,  whenever  a  replica  has  finished\\ncomputing  the  gradients,  the  gradients  are  immediately  used  to  update  the  model\\nparameters.  There  is  no  aggregation  (it  removes  the  “mean”  step  in  Figure  19-15)\\nand  no  synchronization.  Replicas  work  independently  of  the  other  replicas.  Since\\nthere is no waiting for the other replicas, this approach runs more training steps per\\nminute. Moreover, although the parameters still need to be copied to every device at\\nevery step, this happens at different times for each replica, so the risk of bandwidth\\nsaturation is reduced.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,140.568,432.005,251.868 'Data  parallelism  with  asynchronous  updates  is  an  attractive  choice  because  of  its\\nsimplicity, the absence of synchronization delay, and its better use of the bandwidth.\\nHowever,  although  it  works  reasonably  well  in  practice,  it  is  almost  surprising  that\\nit  works  at  all!  Indeed,  by  the  time  a  replica  has  finished  computing  the  gradients\\nbased  on  some  parameter  values,  these  parameters  will  have  been  updated  several\\ntimes by other replicas (on average N – 1 times, if there are N replicas), and there is\\nno guarantee that the computed gradients will still be pointing in the right direction\\n(see  Figure  19-16).  When  gradients  are  severely  out  of  date,  they  are  called  stale\\ngradients:  they  can  slow  down  convergence,  introducing  noise  and  wobble  effects\\n'>\n",
            "<LTTextBoxHorizontal(4) 69.780,68.954,423.264,106.954 '16 This name is slightly confusing because it sounds like some replicas are special, doing nothing. In reality, all\\nreplicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary\\nat every step (unless some devices are really slower than others). However, it does mean that if one or two\\nservers crash, training will continue just fine.\\n'>\n",
            "<LTTextBoxHorizontal(5) 283.703,40.500,402.512,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(6) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 420.980,40.500,431.996,49.500 '761\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,114.900,162.000,114.900>\n",
            "<LTFigure(I1) 85.000,421.669,126.760,477.446 matrix=[41.76,0.00,0.00,55.78, (85.00,421.67)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,582.437,431.999,605.537 '(the learning curve may contain temporary oscillations), or they can even make the\\ntraining algorithm diverge.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,375.156,329.235,385.656 'Figure 19-16. Stale gradients when using asynchronous updates\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,350.556,341.594,361.056 'There are a few ways you can reduce the effect of stale gradients:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.651,327.956,194.636,338.456 '•\\n• Reduce the learning rate.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.651,311.356,261.773,321.856 '•\\n• Drop stale gradients or scale them down.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.651,294.756,202.910,305.256 '•\\n• Adjust the mini-batch size.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.651,227.756,432.003,288.656 '• Start the first few epochs using just one replica (this is called the warmup phase).\\n•\\nStale  gradients  tend  to  be  more  damaging  at  the  beginning  of  training,  when\\ngradients  are  typically  large  and  the  parameters  have  not  settled  into  a  valley\\nof  the  cost  function  yet,  so  different  replicas  may  push  the  parameters  in  quite\\ndifferent directions.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,154.756,432.005,216.276 'A  paper  published  by  the  Google  Brain  team  in  201617  benchmarked  various\\napproaches  and  found  that  using  synchronous  updates  with  a  few  spare  replicas\\nwas more efficient than using asynchronous updates, not only converging faster but\\nalso producing a better model. However, this is still an active area of research, so you\\nshould not rule out asynchronous updates just yet.\\n'>\n",
            "<LTTextBoxHorizontal(8) 69.780,68.954,413.784,76.954 '17 Jianmin Chen et al., “Revisiting Distributed Synchronous SGD”, arXiv preprint arXiv:1604.00981 (2016).\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.000,40.500,84.438,49.500 '762 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.906,40.500,289.710,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,576.175,432.500,576.175>\n",
            "<LTLine 432.375,391.219,432.375,576.300>\n",
            "<LTLine 72.000,391.344,432.500,391.344>\n",
            "<LTLine 72.125,391.219,72.125,576.300>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 108.250,397.199,396.250,571.050 matrix=[288.00,0.00,0.00,173.85, (108.25,397.20)]>\n",
            "<LTTextBoxHorizontal(0) 72.002,596.229,155.604,607.789 'Bandwidth saturation\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,452.304,432.005,588.805 'Whether  you  use  synchronous  or  asynchronous  updates,  data  parallelism  with  cen‐\\ntralized  parameters  still  requires  communicating  the  model  parameters  from  the\\nparameter  servers  to  every  replica  at  the  beginning  of  each  training  step,  and  the\\ngradients in the other direction at the end of each training step. Similarly, when using\\nthe  mirrored  strategy,  the  gradients  produced  by  each  GPU  will  need  to  be  shared\\nwith  every  other  GPU.  Unfortunately,  there  often  comes  a  point  where  adding  an\\nextra  GPU  will  not  improve  performance  at  all  because  the  time  spent  moving  the\\ndata  into  and  out  of  GPU  RAM  (and  across  the  network  in  a  distributed  setup)\\nwill outweigh the speedup obtained by splitting the computation load. At that point,\\nadding more GPUs will just worsen the bandwidth saturation and actually slow down\\ntraining.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,345.504,432.005,444.204 'Saturation is more severe for large dense models, since they have a lot of parameters\\nand gradients to transfer. It is less severe for small models (but the parallelization gain\\nis limited) and for large sparse models, where the gradients are typically mostly zeros\\nand  so  can  be  communicated  efficiently.  Jeff  Dean,  initiator  and  lead  of  the  Google\\nBrain  project,  reported  typical  speedups  of  25–40×  when  distributing  computations\\nacross  50  GPUs  for  dense  models,  and  a  300×  speedup  for  sparser  models  trained\\nacross 500 GPUs. As you can see, sparse models really do scale better. Here are a few\\nconcrete examples:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.650,322.904,306.197,333.404 '•\\n• Neural machine translation: 6× speedup on 8 GPUs\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.650,306.304,286.699,316.804 '•\\n• Inception/ImageNet: 32× speedup on 50 GPUs\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.650,289.704,256.595,300.204 '•\\n• RankBrain: 300× speedup on 500 GPUs\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,115.904,432.004,277.604 'There is plenty of research going on to alleviate the bandwidth saturation issue, with\\nthe goal of allowing training to scale linearly with the number of GPUs available. For\\nexample,  a  2018  paper18  by  a  team  of  researchers  from  Carnegie  Mellon  University,\\nStanford  University,  and  Microsoft  Research  proposed  a  system  called  PipeDream\\nthat managed to reduce network communications by over 90%, making it possible to\\ntrain large models across many machines. They achieved this using a new technique\\ncalled  pipeline  parallelism,  which  combines  model  parallelism  and  data  parallelism:\\nthe model is chopped into consecutive parts, called stages, each of which is trained on\\na different machine. This results in an asynchronous pipeline in which all machines\\nwork in parallel with very little idle time. During training, each stage alternates one\\nround of forward propagation and one round of backpropagation (see Figure 19-17):\\nit pulls a mini-batch from its input queue, processes it, and sends the outputs to the\\nnext  stage’s  input  queue,  then  it  pulls  one  mini-batch  of  gradients  from  its  gradient\\n'>\n",
            "<LTTextBoxHorizontal(7) 69.780,78.954,397.704,86.954 '18 Aaron Harlap et al., “PipeDream: Fast and Efficient Pipeline Parallel DNN Training”, arXiv preprint\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.000,68.954,160.552,76.954 'arXiv:1806.03377 (2018).\\n'>\n",
            "<LTTextBoxHorizontal(9) 283.703,40.500,402.512,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.980,40.500,431.996,49.500 '763\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'queue,  backpropagates  these  gradients  and  updates  its  own  model  parameters,  and\\npushes  the  backpropagated  gradients  to  the  previous  stage’s  gradient  queue.  It  then\\nrepeats  the  whole  process  again  and  again.  Each  stage  can  also  use  regular  data\\nparallelism (e.g., using the mirrored strategy), independently from the other stages.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.999,469.566,260.474,480.066 'Figure 19-17. PipeDream’s pipeline parallelism\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,306.366,432.005,455.466 'However,  as  it’s  presented  here,  PipeDream  would  not  work  so  well.  To  understand\\nwhy,  consider  mini-batch  #5  in  Figure  19-17:  when  it  went  through  stage  1  during\\nthe forward pass, the gradients from mini-batch #4 had not yet been backpropagated\\nthrough that stage, but by the time #5’s gradients flow back to stage 1, #4’s gradients\\nwill  have  been  used  to  update  the  model  parameters,  so  #5’s  gradients  will  be  a\\nbit  stale.  As  we  have  seen,  this  can  degrade  training  speed  and  accuracy,  and  even\\nmake  it  diverge:  the  more  stages  there  are,  the  worse  this  problem  becomes.  The\\npaper’s  authors  proposed  methods  to  mitigate  this  issue,  though:  for  example,  each\\nstage  saves  weights  during  forward  propagation  and  restores  them  during  backpro‐\\npagation,  to  ensure  that  the  same  weights  are  used  for  both  the  forward  pass  and\\nthe backward pass. This is called weight stashing. Thanks to this, PipeDream demon‐\\nstrates impressive scaling capability, well beyond simple data parallelism.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,186.966,432.005,298.886 'The latest breakthrough in this field of research was published in a 2022 paper19 by\\nGoogle  researchers:  they  developed  a  system  called  Pathways  that  uses  automated\\nmodel  parallelism,  asynchronous  gang  scheduling,  and  other  techniques  to  reach\\nclose  to  100%  hardware  utilization  across  thousands  of  TPUs!  Scheduling  means\\norganizing when and where each task must run, and gang scheduling means running\\nrelated tasks at the same time in parallel and close to each other to reduce the time\\ntasks have to wait for the others’ outputs. As we saw in Chapter 16, this system was\\nused to train a massive language model across over 6,000 TPUs, with close to 100%\\nhardware utilization: that’s a mindblowing engineering feat.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,117.966,432.005,178.866 'At  the  time  of  writing,  Pathways  is  not  public  yet,  but  it’s  likely  that  in  the  near\\nfuture  you  will  be  able  to  train  huge  models  on  Vertex  AI  using  Pathways  or  a\\nsimilar  system.  In  the  meantime,  to  reduce  the  saturation  problem,  you’ll  probably\\nwant to use a few powerful GPUs rather than plenty of weak GPUs, and if you need\\nto  train  a  model  across  multiple  servers,  you  should  group  your  GPUs  on  few  and\\n'>\n",
            "<LTTextBoxHorizontal(5) 69.780,78.954,427.488,86.954 '19 Paul Barham et al., “Pathways: Asynchronous Distributed Dataflow for ML”, arXiv preprint arXiv:2203.12533\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.000,68.954,102.720,76.954 '(2022).\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,40.500,84.442,49.500 '764 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.910,40.500,289.714,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,550.975,432.500,550.975>\n",
            "<LTLine 432.375,485.630,432.375,551.100>\n",
            "<LTLine 72.000,485.755,432.500,485.755>\n",
            "<LTLine 72.125,485.630,72.125,551.100>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 80.649,491.610,423.851,545.850 matrix=[343.20,0.00,0.00,54.24, (80.65,491.61)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,531.443,432.005,605.537 'very well interconnected servers. You can also try dropping the float precision from\\n32  bits  (tf.float32)  to  16  bits  (tf.bfloat16).  This  will  cut  in  half  the  amount  of\\ndata to transfer, often without much impact on the convergence rate or the model’s\\nperformance. Lastly, if you are using centralized parameters, you can shard (split) the\\nparameters  across  multiple  parameter  servers:  adding  more  parameter  servers  will\\nreduce the network load on each server and limit the risk of bandwidth saturation.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,500.243,431.996,523.343 'OK,  now  that  we’ve  gone  through  all  the  theory,  let’s  actually  train  a  model  across\\nmultiple GPUs!\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.994,377.454,432.005,488.673 'Training at Scale Using the Distribution Strategies API\\nLuckily, TensorFlow comes with a very nice API that takes care of all the complexity\\nof  distributing  your  model  across  multiple  devices  and  machines:  the  distribution\\nstrategies API. To train a Keras model across all available GPUs (on a single machine,\\nfor  now)  using  data  parallelism  with  the  mirrored  strategy,  just  create  a  Mirrored\\nStrategy object, call its scope() method to get a distribution context, and wrap the\\ncreation  and  compilation  of  your  model  inside  that  context.  Then  call  the  model’s\\nfit() method normally:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,361.968,271.747,370.468 'strategy = tf.distribute.MirroredStrategy()\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.997,321.168,390.747,350.068 'with strategy.scope():\\n    model = tf.keras.Sequential([...])  # create a Keras model normally\\n    model.compile([...])  # compile the model normally\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,280.368,377.997,309.268 'batch_size = 100  # preferably divisible by the number of replicas\\nmodel.fit(X_train, y_train, epochs=10,\\n          validation_data=(X_valid, y_valid), batch_size=batch_size)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,234.668,432.003,272.520 'Under  the  hood,  Keras  is  distribution-aware,  so  in  this  MirroredStrategy  context\\nit  knows  that  it  must  replicate  all  variables  and  operations  across  all  available  GPU\\ndevices. If you look at the model’s weights, they are of type MirroredVariable:\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.004,208.981,310.004,227.681 '>>> type(model.weights[0])\\ntensorflow.python.distribute.values.MirroredVariable\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,138.675,432.005,201.134 'Note that the fit() method will automatically split each training batch across all the\\nreplicas, so it’s preferable to ensure that the batch size is divisible by the number of\\nreplicas  (i.e.,  the  number  of  available  GPUs)  so  that  all  replicas  get  batches  of  the\\nsame  size.  And  that’s  all!  Training  will  generally  be  significantly  faster  than  using  a\\nsingle device, and the code change was really minimal.\\n'>\n",
            "<LTTextBoxHorizontal(9) 283.703,40.500,402.512,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.980,40.500,431.996,49.500 '765\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,505.057,432.004,605.537 'Once  you  have  finished  training  your  model,  you  can  use  it  to  make  predictions\\nefficiently: call the predict() method, and it will automatically split the batch across\\nall replicas, making predictions in parallel. Again, the batch size must be divisible by\\nthe  number  of  replicas.  If  you  call  the  model’s  save()  method,  it  will  be  saved  as  a\\nregular model, not as a mirrored model with multiple replicas. So when you load it, it\\nwill run like a regular model, on a single device: by default on GPU #0, or on the CPU\\nif there are no GPUs. If you want to load a model and run it on all available devices,\\nyou must call tf.keras.models.load_model() within a distribution context:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,479.370,339.752,498.070 'with strategy.scope():\\n    model = tf.keras.models.load_model(\"my_mirrored_model\")\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.999,446.864,432.002,470.557 'If you only want to use a subset of all the available GPU devices, you can pass the list\\nto the MirroredStrategy’s constructor:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.996,431.377,390.746,439.877 'strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,334.090,432.005,423.530 'By default, the MirroredStrategy class uses the NVIDIA Collective Communications\\nLibrary (NCCL) for the AllReduce mean operation, but you can change it by setting\\nthe cross_device_ops argument to an instance of the tf.distribute.Hierarchical\\nCopyAllReduce  class,  or  an  instance  of  the  tf.distribute.ReductionToOneDevice\\nclass. The default NCCL option is based on the tf.distribute.NcclAllReduce class,\\nwhich  is  usually  faster,  but  this  depends  on  the  number  and  types  of  GPUs,  so  you\\nmay want to give the alternatives a try.20\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,302.297,431.996,325.990 'If  you  want  to  try  using  data  parallelism  with  centralized  parameters,  replace  the\\nMirroredStrategy with the CentralStorageStrategy:\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.998,286.811,352.498,295.311 'strategy = tf.distribute.experimental.CentralStorageStrategy()\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,228.511,432.001,278.963 'You  can  optionally  set  the  compute_devices  argument  to  specify  the  list  of  devices\\nyou want to use as workers—by default it will use all available GPUs—and you can\\noptionally set the parameter_device argument to specify the device you want to store\\nthe parameters on. By default it will use the CPU, or the GPU if there is just one.\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.005,209.911,378.468,220.411 'Now let’s see how to train a model across a cluster of TensorFlow servers!\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.994,126.702,432.003,198.341 'Training a Model on a TensorFlow Cluster\\nA TensorFlow cluster is a group of TensorFlow processes running in parallel, usually\\non different machines, and talking to each other to complete some work—for exam‐\\nple, training or executing a neural network model. Each TF process in the cluster is\\ncalled  a  task,  or  a  TF  server.  It  has  an  IP  address,  a  port,  and  a  type  (also  called  its\\n'>\n",
            "<LTTextBoxHorizontal(10) 69.780,78.954,405.024,86.954 '20 For more details on AllReduce algorithms, read Yuichiro Ueno’s post on the technologies behind deep\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.000,68.954,371.536,76.954 'learning and Sylvain Jeaugey’s post on massively scaling deep learning training with NCCL.\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.997,40.500,84.435,49.500 '766 \\n'>\n",
            "<LTTextBoxHorizontal(13) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 102.903,40.500,289.707,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 72.000,581.250,432.001,606.503 'role or its job). The type can be either \"worker\", \"chief\", \"ps\" (parameter server), or\\n\"evaluator\":\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.652,558.650,432.000,569.150 '• Each  worker  performs  computations,  usually  on  a  machine  with  one  or  more\\n•\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.005,546.050,116.843,556.550 'GPUs.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.650,491.650,432.003,539.950 '• The chief performs computations as well (it is a worker), but it also handles extra\\n•\\nwork such as writing TensorBoard logs or saving checkpoints. There is a single\\nchief  in  a  cluster.  If  no  chief  is  specified  explicitly,  then  by  convention  the  first\\nworker is the chief.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.659,461.857,431.692,485.550 '• A parameter server only keeps track of variable values, and it is usually on a CPU-\\n•\\nonly machine. This type of task is only used with the ParameterServerStrategy.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.653,445.257,432.001,455.757 '•\\n• An evaluator obviously takes care of evaluation. This type is not used often, and\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.995,432.657,290.052,443.157 'when it’s used, there’s usually just one evaluator.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,359.657,432.001,420.557 'To  start  a  TensorFlow  cluster,  you  must  first  define  its  specification.  This  means\\ndefining each task’s IP address, TCP port, and type. For example, the following cluster\\nspecification defines a cluster with three tasks (two workers and one parameter server;\\nsee Figure 19-18). The cluster spec is a dictionary with one key per job, and the values\\nare lists of task addresses (IP:port):\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.999,282.970,352.499,352.670 'cluster_spec = {\\n    \"worker\": [\\n        \"machine-a.example.com:2222\",     # /job:worker/task:0\\n        \"machine-b.example.com:2222\"      # /job:worker/task:1\\n    ],\\n    \"ps\": [\"machine-a.example.com:2221\"]  # /job:ps/task:0\\n}\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.997,238.457,432.005,274.157 'In general there will be a single task per machine, but as this example shows, you can\\nconfigure multiple tasks on the same machine if you want. In this case, if they share\\nthe same GPUs, make sure the RAM is split appropriately, as discussed earlier.\\n'>\n",
            "<LTTextBoxHorizontal(10) 136.788,177.365,395.999,221.525 'By  default,  every  task  in  the  cluster  may  communicate  with  every\\nother  task,  so  make  sure  to  configure  your  firewall  to  authorize\\nall  communications  between  these  machines  on  these  ports  (it’s\\nusually simpler if you use the same port on every machine).\\n'>\n",
            "<LTTextBoxHorizontal(11) 283.708,40.500,402.517,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(12) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(13) 420.985,40.500,432.001,49.500 '767\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,175.011,132.680,222.320 matrix=[49.68,0.00,0.00,47.31, (83.00,175.01)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,391.640,256.422,402.140 'Figure 19-18. An example TensorFlow cluster\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,276.467,432.004,377.540 'When  you  start  a  task,  you  must  give  it  the  cluster  spec,  and  you  must  also  tell\\nit  what  its  type  and  index  are  (e.g.,  worker  #0).  The  simplest  way  to  specify  every‐\\nthing  at  once  (both  the  cluster  spec  and  the  current  task’s  type  and  index)  is  to\\nset  the  TF_CONFIG  environment  variable  before  starting  TensorFlow.  It  must  be  a\\nJSON-encoded  dictionary  containing  a  cluster  specification  (under  the  \"cluster\"\\nkey) and the type and index of the current task (under the \"task\" key). For example,\\nthe  following  TF_CONFIG  environment  variable  uses  the  cluster  we  just  defined  and\\nspecifies that the task to start is worker #0:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,230.381,267.498,269.481 'os.environ[\"TF_CONFIG\"] = json.dumps({\\n    \"cluster\": cluster_spec,\\n    \"task\": {\"type\": \"worker\", \"index\": 0}\\n})\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.791,168.033,396.002,213.619 'In general you want to define the TF_CONFIG environment variable\\noutside of Python, so the code does not need to include the current\\ntask’s  type  and  index  (this  makes  it  possible  to  use  the  same  code\\nacross all workers).\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,85.081,432.003,147.167 'Now  let’s  train  a  model  on  a  cluster!  We  will  start  with  the  mirrored  strategy.  First,\\nyou  need  to  set  the  TF_CONFIG  environment  variable  appropriately  for  each  task.\\nThere should be no parameter server (remove the \"ps\" key in the cluster spec), and\\nin  general  you  will  want  a  single  worker  per  machine.  Make  extra  sure  you  set  a\\ndifferent task index for each task. Finally, run the following script on every worker:\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.003,40.500,84.441,49.500 '768 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.909,40.500,289.713,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,407.704,432.375,607.500>\n",
            "<LTLine 72.000,407.829,432.500,407.829>\n",
            "<LTLine 72.125,407.704,72.125,607.500>\n",
            "<LTFigure(I1) 85.000,162.753,126.760,218.531 matrix=[41.76,0.00,0.00,55.78, (85.00,162.75)]>\n",
            "<LTFigure(I2) 108.250,413.684,396.250,602.250 matrix=[288.00,0.00,0.00,188.57, (108.25,413.68)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,587.950,186.750,606.650 'import tempfile\\nimport tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,536.950,390.750,576.050 'strategy = tf.distribute.MultiWorkerMirroredStrategy()  # at the start!\\nresolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\\nprint(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\\n[...] # load and split the MNIST dataset\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,496.150,356.750,525.050 'with strategy.scope():\\n    model = tf.keras.Sequential([...])  # build the Keras model\\n    model.compile([...])  # compile the model\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,475.750,403.500,484.250 'model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.000,404.350,420.500,463.850 'if resolver.task_id == 0:  # the chief saves the model to the right location\\n    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\\nelse:\\n    tmpdir = tempfile.mkdtemp()  # other workers save to a temporary directory\\n    model.save(tmpdir, save_format=\"tf\")\\n    tf.io.gfile.rmtree(tmpdir)  # and we can delete this directory at the end!\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,334.043,432.005,395.537 'That’s  almost  the  same  code  you  used  earlier,  except  this  time  you  are  using  the\\nMultiWorkerMirroredStrategy. When you start this script on the first workers, they\\nwill remain blocked at the AllReduce step, but training will begin as soon as the last\\nworker starts up, and you will see them all advancing at exactly the same rate since\\nthey synchronize at each step.\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.786,249.369,395.997,317.995 'When  using  the  MultiWorkerMirroredStrategy,  it’s  important  to\\nensure that all workers do the same thing, including saving model\\ncheckpoints  or  writing  TensorBoard  logs,  even  though  you  will\\nonly  keep  what  the  chief  writes.  This  is  because  these  operations\\nmay need to run the AllReduce operations, so all workers must be\\nin sync.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,158.781,432.005,232.281 'There  are  two  AllReduce  implementations  for  this  distribution  strategy:  a  ring  All‐\\nReduce  algorithm  based  on  gRPC  for  the  network  communications,  and  NCCL’s\\nimplementation.  The  best  algorithm  to  use  depends  on  the  number  of  workers,  the\\nnumber and types of GPUs, and the network. By default, TensorFlow will apply some\\nheuristics to select the right algorithm for you, but you can force NCCL (or RING)\\nlike this:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.002,122.894,429.002,151.794 'strategy = tf.distribute.MultiWorkerMirroredStrategy(\\n    communication_options=tf.distribute.experimental.CommunicationOptions(\\n        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))\\n'>\n",
            "<LTTextBoxHorizontal(9) 283.706,40.500,402.515,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.983,40.500,431.999,49.500 '769\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,270.597,132.680,317.907 matrix=[49.68,0.00,0.00,47.31, (83.00,270.60)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,543.450,432.005,605.537 'If  you  prefer  to  implement  asynchronous  data  parallelism  with  parameter  servers,\\nchange  the  strategy  to  ParameterServerStrategy,  add  one  or  more  parameter\\nservers,  and  configure  TF_CONFIG  appropriately  for  each  task.  Note  that  although\\nthe  workers  will  work  asynchronously,  the  replicas  on  each  worker  will  work\\nsynchronously.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,511.657,431.999,535.350 'Lastly, if you have access to TPUs on Google Cloud—for example, if you use Colab\\nand you set the accelerator type to TPU—then you can create a TPUStrategy like this:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,475.770,352.501,504.670 'resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\\ntf.tpu.experimental.initialize_tpu_system(resolver)\\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,443.857,432.001,466.957 'This needs to be run right after importing TensorFlow. You can then use this strategy\\nnormally.\\n'>\n",
            "<LTTextBoxHorizontal(4) 136.786,405.805,395.996,426.925 'If you are a researcher, you may be eligible to use TPUs for free; see\\nhttps://tensorflow.org/tfrc for more details.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,287.857,432.004,361.357 'You can now train models across multiple GPUs and multiple servers: give yourself\\na  pat  on  the  back!  If  you  want  to  train  a  very  large  model,  however,  you  will  need\\nmany GPUs, across many servers, which will require either buying a lot of hardware\\nor  managing  a  lot  of  cloud  virtual  machines.  In  many  cases,  it’s  less  hassle  and  less\\nexpensive to use a cloud service that takes care of provisioning and managing all this\\ninfrastructure for you, just when you need it. Let’s see how to do that using Vertex AI.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,100.881,432.005,276.287 'Running Large Training Jobs on Vertex AI\\nVertex  AI  allows  you  to  create  custom  training  jobs  with  your  own  training  code.\\nIn  fact,  you  can  use  almost  the  same  training  code  as  you  would  use  on  your  own\\nTF  cluster.  The  main  thing  you  must  change  is  where  the  chief  should  save  the\\nmodel,  the  checkpoints,  and  the  TensorBoard  logs.  Instead  of  saving  the  model  to\\na  local  directory,  the  chief  must  save  it  to  GCS,  using  the  path  provided  by  Vertex\\nAI  in  the  AIP_MODEL_DIR  environment  variable.  For  the  model  checkpoints  and\\nTensorBoard  logs,  you  should  use  the  paths  contained  in  the  AIP_CHECKPOINT_DIR\\nand  AIP_TENSORBOARD_LOG_DIR  environment  variables,  respectively.  Of  course,  you\\nmust also make sure that the training data can be accessed from the virtual machines,\\nsuch  as  on  GCS,  or  another  GCP  service  like  BigQuery,  or  directly  from  the  web.\\nLastly, Vertex AI sets the \"chief\" task type explicitly, so you should identify the chief\\nusing resolved.task_type == \"chief\" instead of resolved.task_id == 0:\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.001,40.500,84.439,49.500 '770 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.907,40.500,289.711,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,376.943,126.760,432.720 matrix=[41.76,0.00,0.00,55.78, (85.00,376.94)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,587.950,395.000,606.650 'import os\\n[...]  # other imports, create MultiWorkerMirroredStrategy, and resolver\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,485.950,403.500,576.050 'if resolver.task_type == \"chief\":\\n    model_dir = os.getenv(\"AIP_MODEL_DIR\")  # paths provided by Vertex AI\\n    tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\\n    checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\\nelse:\\n    tmp_dir = Path(tempfile.mkdtemp())  # other workers use temporary dirs\\n    model_dir = tmp_dir / \"model\"\\n    tensorboard_log_dir = tmp_dir / \"logs\"\\n    checkpoint_dir = tmp_dir / \"ckpt\"\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,414.550,403.500,474.050 'callbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\\n             tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\\n[...]  # build and  compile using the strategy scope, just like earlier\\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10,\\n          callbacks=callbacks)\\nmodel.save(model_dir, save_format=\"tf\")\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.785,340.140,396.003,396.905 'If  you  place  the  training  data  on  GCS,  you  can  create  a\\ntf.data.TextLineDataset or tf.data.TFRecordDataset to access\\nit:  just  use  the  GCS  paths  as  the  filenames  (e.g.,  gs://my_bucket/\\ndata/001.csv).  These  datasets  rely  on  the  tf.io.gfile  package  to\\naccess files: it supports both local files and GCS files.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,236.952,432.004,323.052 'Now you can create a custom training job on Vertex AI, based on this script. You’ll\\nneed to specify the job name, the path to your training script, the Docker image to\\nuse for training, the one to use for predictions (after training), any additional Python\\nlibraries you may need, and lastly the bucket that Vertex AI should use as a staging\\ndirectory to store the training script. By default, that’s also where the training script\\nwill save the trained model, as well as the TensorBoard logs and model checkpoints (if\\nany). Let’s create the job:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.001,150.065,412.001,229.965 'custom_training_job = aiplatform.CustomTrainingJob(\\n    display_name=\"my_custom_training_job\",\\n    script_path=\"my_vertex_ai_training_task.py\",\\n    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\\n    model_serving_container_image_uri=server_image,\\n    requirements=[\"gcsfs==2022.3.0\"],  # not needed, this is just an example\\n    staging_bucket=f\"gs://{bucket_name}/staging\"\\n)\\n'>\n",
            "<LTTextBoxHorizontal(6) 283.709,40.500,402.518,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.986,40.500,432.002,49.500 '771\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,346.923,126.760,402.700 matrix=[41.76,0.00,0.00,55.78, (85.00,346.92)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,315.148,605.537 'And now let’s run it on two workers, each with two GPUs:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.998,528.550,258.998,588.050 'mnist_model2 = custom_training_job.run(\\n    machine_type=\"n1-standard-4\",\\n    replica_count=2,\\n    accelerator_type=\"NVIDIA_TESLA_K80\",\\n    accelerator_count=2,\\n)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,407.250,432.004,519.736 'And that’s it: Vertex AI will provision the compute nodes you requested (within your\\nquotas), and it will run your training script across them. Once the job is complete, the\\nrun() method will return a trained model that you can use exactly like the one you\\ncreated earlier: you can deploy it to an endpoint, or use it to make batch predictions.\\nIf  anything  goes  wrong  during  training,  you  can  view  the  logs  in  the  GCP  console:\\nin  the  ☰  navigation  menu,  select  Vertex  AI  →  Training,  click  on  your  training  job,\\nand click VIEW LOGS. Alternatively, you can click the CUSTOM JOBS tab and copy\\nthe job’s ID (e.g., 1234), then select Logging from the ☰ navigation menu and query\\nresource.labels.job_id=1234.\\n'>\n",
            "<LTTextBoxHorizontal(3) 136.792,333.011,396.004,390.318 'To  visualize  the  training  progress,  just  start  TensorBoard  and\\npoint its --logdir to the GCS path of the logs. It will use applica‐\\ntion  default  credentials,  which  you  can  set  up  using  gcloud  auth\\napplication-default login. Vertex AI also offers hosted Tensor‐\\nBoard servers if you prefer.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,266.436,432.002,315.923 'If you want to try out a few hyperparameter values, one option is to run multiple jobs.\\nYou can pass the hyperparameter values to your script as command-line arguments\\nby setting the args parameter when calling the run() method, or you can pass them\\nas environment variables using the environment_variables parameter.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,235.236,431.996,258.336 'However, if you want to run a large hyperparameter tuning job on the cloud, a much\\nbetter option is to use Vertex AI’s hyperparameter tuning service. Let’s see how.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,138.834,432.004,223.666 'Hyperparameter Tuning on Vertex AI\\nVertex AI’s hyperparameter tuning service is based on a Bayesian optimization algo‐\\nrithm,  capable  of  quickly  finding  optimal  combinations  of  hyperparameters.  To  use\\nit,  you  first  need  to  create  a  training  script  that  accepts  hyperparameter  values  as\\ncommand-line arguments. For example, your script could use the argparse standard\\nlibrary like this:\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.004,40.500,84.442,49.500 '772 \\n'>\n",
            "<LTTextBoxHorizontal(8) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 102.910,40.500,289.714,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,340.336,126.760,396.113 matrix=[41.76,0.00,0.00,55.78, (85.00,340.34)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,598.150,152.750,606.650 'import argparse\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,526.750,361.000,586.250 'parser = argparse.ArgumentParser()\\nparser.add_argument(\"--n_hidden\", type=int, default=2)\\nparser.add_argument(\"--n_neurons\", type=int, default=256)\\nparser.add_argument(\"--learning_rate\", type=float, default=1e-2)\\nparser.add_argument(\"--optimizer\", default=\"adam\")\\nargs = parser.parse_args()\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,444.436,432.003,517.937 'The  hyperparameter  tuning  service  will  call  your  script  multiple  times,  each  time\\nwith different hyperparameter values: each run is called a trial, and the set of trials is\\ncalled a study. Your training script must then use the given hyperparameter values to\\nbuild and compile a model. You can use a mirrored distribution strategy if you want,\\nin case each trial runs on a multi-GPU machine. Then the script can load the dataset\\nand train the model. For example:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,428.950,186.749,437.450 'import tensorflow as tf\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,296.350,428.999,417.050 'def build_model(args):\\n    with tf.distribute.MirroredStrategy().scope():\\n        model = tf.keras.Sequential()\\n        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\\n        for _ in range(args.n_hidden):\\n            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\\n        opt = tf.keras.optimizers.get(args.optimizer)\\n        opt.learning_rate = args.learning_rate\\n        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\\n                      metrics=[\"accuracy\"])\\n        return model\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.999,255.550,199.499,284.450 '[...]  # load the dataset\\nmodel = build_model(args)\\nhistory = model.fit([...])\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.790,204.722,396.000,238.788 'You can use the AIP_* environment variables we mentioned earlier\\nto determine where to save the checkpoints, the TensorBoard logs,\\nand the final model.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,123.443,432.004,172.336 'Lastly, the script must report the model’s performance back to Vertex AI’s hyperpara‐\\nmeter  tuning  service,  so  it  can  decide  which  hyperparameters  to  try  next.  For  this,\\nyou  must  use  the  hypertune  library,  which  is  automatically  installed  on  Vertex  AI\\ntraining VMs:\\n'>\n",
            "<LTTextBoxHorizontal(8) 283.706,40.500,402.515,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.983,40.500,431.999,49.500 '773\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,187.923,126.760,243.700 matrix=[41.76,0.00,0.00,55.78, (85.00,187.92)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,598.150,157.000,606.650 'import hypertune\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,526.750,395.000,586.250 'hypertune = hypertune.HyperTune()\\nhypertune.report_hyperparameter_tuning_metric(\\n    hyperparameter_metric_tag=\"accuracy\",  # name of the reported metric\\n    metric_value=max(history.history[\"val_accuracy\"]),  # metric value\\n    global_step=model.optimizer.iterations.numpy(),\\n)\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,482.237,431.998,517.937 'Now  that  your  training  script  is  ready,  you  need  to  define  the  type  of  machine  you\\nwould like to run it on. For this, you must define a custom job, which Vertex AI will\\nuse as a template for each trial:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.004,395.350,395.004,475.250 'trial_job = aiplatform.CustomJob.from_local_script(\\n    display_name=\"my_search_trial_job\",\\n    script_path=\"my_vertex_ai_trial.py\",  # path to your training script\\n    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\\n    staging_bucket=f\"gs://{bucket_name}/staging\",\\n    accelerator_type=\"NVIDIA_TESLA_K80\",\\n    accelerator_count=2,  # in this example, each trial will have 2 GPUs\\n)\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.002,376.037,364.133,386.537 'Finally, you’re ready to create and run the hyperparameter tuning job:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.996,360.550,360.996,369.050 'from google.cloud.aiplatform import hyperparameter_tuning as hpt\\n'>\n",
            "<LTTextBoxHorizontal(6) 88.996,207.550,428.996,348.650 'hp_job = aiplatform.HyperparameterTuningJob(\\n    display_name=\"my_hp_search_job\",\\n    custom_job=trial_job,\\n    metric_spec={\"accuracy\": \"maximize\"},\\n    parameter_spec={\\n        \"learning_rate\": hpt.DoubleParameterSpec(min=1e-3, max=10, scale=\"log\"),\\n        \"n_neurons\": hpt.IntegerParameterSpec(min=1, max=300, scale=\"linear\"),\\n        \"n_hidden\": hpt.IntegerParameterSpec(min=1, max=10, scale=\"linear\"),\\n        \"optimizer\": hpt.CategoricalParameterSpec([\"sgd\", \"adam\"]),\\n    },\\n    max_trial_count=100,\\n    parallel_trial_count=20,\\n)\\nhp_job.run()\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,74.243,432.004,199.702 'Here, we tell Vertex AI to maximize the metric named \"accuracy\": this name must\\nmatch  the  name  of  the  metric  reported  by  the  training  script.  We  also  define  the\\nsearch  space,  using  a  log  scale  for  the  learning  rate  and  a  linear  (i.e.,  uniform)\\nscale  for  the  other  hyperparameters.  The  hyperparameter  names  must  match  the\\ncommand-line arguments of the training script. Then we set the maximum number\\nof trials to 100, and the maximum number of trials running in parallel to 20. If you\\nincrease the number of parallel trials to (say) 60, the total search time will be reduced\\nsignificantly, by a factor of up to 3. But the first 60 trials will be started in parallel, so\\nthey will not benefit from the other trials’ feedback. Therefore, you should increase\\nthe max number of trials to compensate—for example, up to about 140.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,40.500,84.437,49.500 '774 \\n'>\n",
            "<LTTextBoxHorizontal(9) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.243,432.003,605.537 'This will take quite a while. Once the job is completed, you can fetch the trial results\\nusing hp_job.trials. Each trial result is represented as a protobuf object, containing\\nthe hyperparameter values and the resulting metrics. Let’s find the best trial:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,523.157,301.502,562.257 'def get_final_metric(trial, metric_id):\\n    for metric in trial.final_measurement.metrics:\\n        if metric.metric_id == metric_id:\\n            return metric.value\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,482.357,412.002,511.257 'trials = hp_job.trials\\ntrial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\\nbest_trial = trials[np.argmax(trial_accuracies)]\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.998,463.043,354.847,473.543 'Now let’s look at this trial’s accuracy, and its hyperparameter values:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,355.757,348.252,456.057 '>>> max(trial_accuracies)\\n0.977400004863739\\n>>> best_trial.id\\n\\'98\\'\\n>>> best_trial.parameters\\n[parameter_id: \"learning_rate\" value { number_value: 0.001 },\\n parameter_id: \"n_hidden\" value { number_value: 8.0 },\\n parameter_id: \"n_neurons\" value { number_value: 216.0 },\\n parameter_id: \"optimizer\" value { string_value: \"adam\" }\\n]\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,323.843,432.000,346.943 'That’s it! Now you can get this trial’s SavedModel, optionally train it a bit more, and\\ndeploy it to production.\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.786,228.192,395.998,306.912 'Vertex AI also includes an AutoML service, which completely takes\\ncare of finding the right model architecture and training it for you.\\nAll  you  need  to  do  is  upload  your  dataset  to  Vertex  AI  using  a\\nspecial  format  that  depends  on  the  type  of  dataset  (images,  text,\\ntabular,  video,  etc.),  then  create  an  AutoML  training  job,  pointing\\nto  the  dataset  and  specifying  the  maximum  number  of  compute\\nhours you’re willing to spend. See the notebook for an example.\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.253,87.697,423.753,203.192 'Hyperparameter Tuning Using Keras Tuner on Vertex AI\\nInstead of using Vertex AI’s hyperparameter tuning service, you can use Keras Tuner\\n(introduced in Chapter 10) and run it on Vertex AI VMs. Keras Tuner provides a sim‐\\nple  way  to  scale  hyperparameter  search  by  distributing  it  across  multiple  machines:\\nit  only  requires  setting  three  environment  variables  on  each  machine,  then  running\\nyour regular Keras Tuner code on each machine. You can use the exact same script on\\nall machines. One of the machines acts as the chief (i.e., the oracle), and the others act\\nas workers. Each worker asks the chief which hyperparameter values to try, then the\\nworker trains the model using these hyperparameter values, and finally it reports the\\n'>\n",
            "<LTTextBoxHorizontal(8) 283.704,40.500,402.513,49.500 'Training Models Across Multiple Devices \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.981,40.500,431.997,49.500 '775\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,212.942,432.000,212.942>\n",
            "<LTLine 431.875,68.650,431.875,213.067>\n",
            "<LTLine 72.125,68.650,72.125,213.067>\n",
            "<LTFigure(I1) 85.000,256.929,126.760,312.707 matrix=[41.76,0.00,0.00,55.78, (85.00,256.93)]>\n",
            "<LTTextBoxHorizontal(0) 80.250,583.630,423.750,605.630 'model’s performance back to the chief, which can then decide which hyperparameter\\nvalues the worker should try next.\\n'>\n",
            "<LTTextBoxHorizontal(1) 80.250,565.630,360.090,575.630 'The three environment variables you need to set on each machine are:\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.248,548.050,170.498,557.550 'KERASTUNER_TUNER_ID\\n'>\n",
            "<LTTextBoxHorizontal(3) 98.248,520.935,423.748,544.985 'This  is  equal  to  \"chief\"  on  the  chief  machine,  or  a  unique  identifier  on  each\\nworker machine, such as \"worker0\", \"worker1\", etc.\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.253,503.355,175.253,512.855 'KERASTUNER_ORACLE_IP\\n'>\n",
            "<LTTextBoxHorizontal(5) 98.252,476.805,423.753,499.370 'This is the IP address or hostname of the chief machine. The chief itself should\\ngenerally use \"0.0.0.0\" to listen on every IP address on the machine.\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.252,459.225,184.752,468.725 'KERASTUNER_ORACLE_PORT\\n'>\n",
            "<LTTextBoxHorizontal(7) 98.252,445.240,314.612,455.240 'This is the TCP port that the chief will be listening on.\\n'>\n",
            "<LTTextBoxHorizontal(8) 80.252,391.240,423.752,437.240 'You can distribute Keras Tuner across any set of machines. If you want to run it on\\nVertex AI machines, then you can spawn a regular training job, and just modify the\\ntraining  script  to  set  the  environment  variables  properly  before  using  Keras  Tuner.\\nSee the notebook for an example.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.998,319.596,432.004,367.896 'Now you have all the tools and knowledge you need to create state-of-the-art neural\\nnet architectures and train them at scale using various distribution strategies, on your\\nown infrastructure or on the cloud, and then deploy them anywhere. In other words,\\nyou now have superpowers: use them well!\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.005,288.337,127.648,307.257 'Exercises\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.314,265.596,370.723,276.096 '1.\\n1. What does a SavedModel contain? How do you inspect its content?\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.314,248.996,431.995,259.496 '2.\\n2. When  should  you  use  TF  Serving?  What  are  its  main  features?  What  are  some\\n'>\n",
            "<LTTextBoxHorizontal(13) 90.000,236.396,216.052,246.896 'tools you can use to deploy it?\\n'>\n",
            "<LTTextBoxHorizontal(14) 77.319,219.796,368.365,230.296 '3. How do you deploy a model across multiple TF Serving instances?\\n3.\\n'>\n",
            "<LTTextBoxHorizontal(15) 77.313,203.196,432.000,213.696 '4.\\n4. When should you use the gRPC API rather than the REST API to query a model\\n'>\n",
            "<LTTextBoxHorizontal(16) 90.004,190.596,182.677,201.096 'served by TF Serving?\\n'>\n",
            "<LTTextBoxHorizontal(17) 77.313,173.996,432.004,184.496 '5. What  are  the  different  ways  TFLite  reduces  a  model’s  size  to  make  it  run  on  a\\n5.\\n'>\n",
            "<LTTextBoxHorizontal(18) 89.999,161.396,210.140,171.896 'mobile or embedded device?\\n'>\n",
            "<LTTextBoxHorizontal(19) 77.318,144.796,364.584,155.296 '6. What is quantization-aware training, and why would you need it?\\n6.\\n'>\n",
            "<LTTextBoxHorizontal(20) 77.312,128.196,431.999,138.696 '7.\\n7. What  are  model  parallelism  and  data  parallelism?  Why  is  the  latter  generally\\n'>\n",
            "<LTTextBoxHorizontal(21) 90.003,115.596,155.093,126.096 'recommended?\\n'>\n",
            "<LTTextBoxHorizontal(22) 77.312,98.996,432.003,109.496 '8. When training a model across multiple servers, what distribution strategies can\\n8.\\n'>\n",
            "<LTTextBoxHorizontal(23) 89.997,86.396,288.773,96.896 'you use? How do you choose which one to use?\\n'>\n",
            "<LTTextBoxHorizontal(24) 71.999,40.500,84.437,49.500 '776 \\n'>\n",
            "<LTTextBoxHorizontal(25) 92.015,40.500,95.327,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(26) 102.905,40.500,289.709,49.500 'Chapter 19: Training and Deploying TensorFlow Models at Scale\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 431.875,379.860,431.875,607.500>\n",
            "<LTLine 72.000,379.985,432.000,379.985>\n",
            "<LTLine 72.125,379.860,72.125,607.500>\n",
            "<LTTextBoxHorizontal(0) 77.316,557.236,432.002,605.537 '9. Train a model (any model you like) and deploy it to TF Serving or Google Vertex\\n9.\\nAI.  Write  the  client  code  to  query  it  using  the  REST  API  or  the  gRPC  API.\\nUpdate the model and deploy the new version. Your client code will now query\\nthe new version. Roll back to the first version.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.273,501.057,432.002,552.103 '10. Train any model across multiple GPUs on the same machine using the Mirrored\\n10.\\nStrategy  (if  you  do  not  have  access  to  GPUs,  you  can  use  Google  Colab  with\\na  GPU  runtime  and  create  two  logical  GPUs).  Train  the  model  again  using  the\\nCentralStorageStrategy and compare the training time.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.272,484.457,431.999,494.957 '11. Fine-tune  a  model  of  your  choice  on  Vertex  AI,  using  either  Keras  Tuner  or\\n11.\\n'>\n",
            "<LTTextBoxHorizontal(3) 90.004,471.857,269.680,482.357 'Vertex AI’s hyperparameter tuning service.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,436.657,431.996,459.757 'Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\\nhttps://homl.info/colab3.\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,361.456,432.001,424.317 'Thank You!\\nBefore we close the last chapter of this book, I would like to thank you for reading it\\nup to the last paragraph. I truly hope that you had as much fun reading this book as I\\nhad writing it, and that it will be useful for your projects, big or small.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,317.656,432.003,353.356 'If you find errors, please send feedback. More generally, I would love to know what\\nyou  think,  so  please  don’t  hesitate  to  contact  me  via  O’Reilly,  through  the  ageron/\\nhandson-ml3 GitHub project, or on Twitter at @aureliengeron.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,135.256,432.005,309.556 'Going forward, my best advice to you is to practice and practice: try going through all\\nthe exercises (if you have not done so already), play with the notebooks, join Kaggle\\nor  some  other  ML  community,  watch  ML  courses,  read  papers,  attend  conferences,\\nand  meet  experts.  Things  move  fast,  so  try  to  keep  up  to  date.  Several  YouTube\\nchannels regularly present deep learning papers in great detail, in a very approachable\\nway. I particularly recommend the channels by Yannic Kilcher, Letitia Parcalabescu,\\nand  Xander  Steenbrugge.  For  fascinating  ML  discussions  and  higher-level  insights,\\nmake  sure  to  check  out  ML  Street  Talk,  and  Lex  Fridman’s  channel.  It  also  helps\\ntremendously to have a concrete project to work on, whether it is for work or for fun\\n(ideally  for  both),  so  if  there’s  anything  you  have  always  dreamed  of  building,  give\\nit a shot! Work incrementally; don’t shoot for the moon right away, but stay focused\\non your project and build it piece by piece. It will require patience and perseverance,\\nbut when you have a walking robot, or a working chatbot, or whatever else you fancy\\nbuilding, it will be immensely rewarding!\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,104.056,431.996,127.156 'My greatest hope is that this book will inspire you to build a wonderful ML applica‐\\ntion that will benefit all of us. What will it be?\\n'>\n",
            "<LTTextBoxHorizontal(9) 357.079,85.456,431.996,95.956 '—Aurélien Géron\\n'>\n",
            "<LTTextBoxHorizontal(10) 368.312,40.500,402.512,49.500 'Thank You! \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.980,40.500,431.996,49.500 '777\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 143.709,533.502,431.998,582.331 'APPENDIX A\\nMachine Learning Project Checklist\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,393.868,431.996,416.969 'This checklist can guide you through your machine learning projects. There are eight\\nmain steps:\\n'>\n",
            "<LTTextBoxHorizontal(2) 77.319,371.268,285.972,381.768 '1.\\n1. Frame the problem and look at the big picture.\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.319,354.668,143.372,365.168 '2.\\n2. Get the data.\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.319,338.068,227.382,348.568 '3.\\n3. Explore the data to gain insights.\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.313,321.468,432.000,331.968 '4.\\n4. Prepare the data to better expose the underlying data patterns to machine learn‐\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.005,308.868,153.761,319.368 'ing algorithms.\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.313,292.268,334.277,302.768 '5.\\n5. Explore many different models and shortlist the best ones.\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.313,275.668,356.747,286.168 '6.\\n6. Fine-tune your models and combine them into a great solution.\\n'>\n",
            "<LTTextBoxHorizontal(9) 77.313,259.068,182.699,269.568 '7.\\n7. Present your solution.\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.313,242.468,277.104,252.968 '8.\\n8. Launch, monitor, and maintain your system.\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.997,219.868,356.568,230.368 'Obviously, you should feel free to adapt this checklist to your needs.\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.992,188.609,361.374,207.529 'Frame the Problem and Look at the Big Picture\\n'>\n",
            "<LTTextBoxHorizontal(13) 77.320,165.868,251.575,176.368 '1.\\n1. Define the objective in business terms.\\n'>\n",
            "<LTTextBoxHorizontal(14) 77.320,149.268,224.979,159.768 '2.\\n2. How will your solution be used?\\n'>\n",
            "<LTTextBoxHorizontal(15) 77.320,132.668,315.184,143.168 '3. What are the current solutions/workarounds (if any)?\\n3.\\n'>\n",
            "<LTTextBoxHorizontal(16) 77.314,116.068,432.001,126.568 '4. How  should  you  frame  this  problem  (supervised/unsupervised,  online/offline,\\n4.\\n'>\n",
            "<LTTextBoxHorizontal(17) 89.995,103.468,112.108,113.968 'etc.)?\\n'>\n",
            "<LTTextBoxHorizontal(18) 77.314,86.868,257.302,97.368 '5. How should performance be measured?\\n5.\\n'>\n",
            "<LTTextBoxHorizontal(19) 77.314,70.268,358.995,80.768 '6.\\n6. Is the performance measure aligned with the business objective?\\n'>\n",
            "<LTTextBoxHorizontal(20) 420.982,40.500,431.998,49.500 '779\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTTextBoxHorizontal(0) 77.316,595.037,431.997,605.537 '7.\\n7. What would be the minimum performance needed to reach the business objec‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 90.001,582.437,109.321,592.937 'tive?\\n'>\n",
            "<LTTextBoxHorizontal(2) 77.320,565.837,374.929,576.337 '8.\\n8. What are comparable problems? Can you reuse experience or tools?\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.321,549.236,212.305,559.737 '9.\\n9. Is human expertise available?\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.281,532.637,280.020,543.137 '10.\\n10. How would you solve the problem manually?\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.281,516.036,316.770,526.537 '11. List the assumptions you (or others) have made so far.\\n11.\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.281,499.437,217.703,509.937 '12.\\n12. Verify assumptions if possible.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.991,449.436,354.190,487.097 'Get the Data\\nNote: automate as much as possible so you can easily get fresh data.\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.318,426.836,290.402,437.336 '1.\\n1. List the data you need and how much you need.\\n'>\n",
            "<LTTextBoxHorizontal(9) 77.318,410.236,292.670,420.736 '2.\\n2. Find and document where you can get that data.\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.318,393.636,237.262,404.136 '3.\\n3. Check how much space it will take.\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.318,377.036,334.796,387.536 '4.\\n4. Check legal obligations, and get authorization if necessary.\\n'>\n",
            "<LTTextBoxHorizontal(12) 77.318,360.436,198.643,370.936 '5.\\n5. Get access authorizations.\\n'>\n",
            "<LTTextBoxHorizontal(13) 77.318,343.836,294.140,354.336 '6.\\n6. Create a workspace (with enough storage space).\\n'>\n",
            "<LTTextBoxHorizontal(14) 77.318,327.236,143.371,337.736 '7.\\n7. Get the data.\\n'>\n",
            "<LTTextBoxHorizontal(15) 77.312,310.636,431.999,321.136 '8.\\n8. Convert  the  data  to  a  format  you  can  easily  manipulate  (without  changing  the\\n'>\n",
            "<LTTextBoxHorizontal(16) 90.004,298.036,137.096,308.536 'data itself).\\n'>\n",
            "<LTTextBoxHorizontal(17) 77.312,281.436,390.073,291.936 '9.\\n9. Ensure sensitive information is deleted or protected (e.g., anonymized).\\n'>\n",
            "<LTTextBoxHorizontal(18) 72.272,264.836,388.676,275.336 '10.\\n10. Check the size and type of data (time series, sample, geographical, etc.).\\n'>\n",
            "<LTTextBoxHorizontal(19) 72.272,248.236,383.426,258.736 '11. Sample a test set, put it aside, and never look at it (no data snooping!).\\n11.\\n'>\n",
            "<LTTextBoxHorizontal(20) 72.002,198.236,314.984,235.896 'Explore the Data\\nNote: try to get insights from a field expert for these steps.\\n'>\n",
            "<LTTextBoxHorizontal(21) 77.314,175.636,432.001,186.136 '1.\\n1. Create a copy of the data for exploration (sampling it down to a manageable size\\n'>\n",
            "<LTTextBoxHorizontal(22) 89.995,163.036,144.773,173.536 'if necessary).\\n'>\n",
            "<LTTextBoxHorizontal(23) 77.314,146.436,375.847,156.936 '2. Create a Jupyter notebook to keep a record of your data exploration.\\n2.\\n'>\n",
            "<LTTextBoxHorizontal(24) 77.314,129.836,269.555,140.336 '3.\\n3. Study each attribute and its characteristics:\\n'>\n",
            "<LTTextBoxHorizontal(25) 90.653,113.236,125.201,123.736 '•\\n• Name\\n'>\n",
            "<LTTextBoxHorizontal(26) 90.653,96.636,396.889,107.136 '• Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\\n•\\n'>\n",
            "<LTTextBoxHorizontal(27) 90.653,80.036,182.164,90.536 '•\\n• % of missing values\\n'>\n",
            "<LTTextBoxHorizontal(28) 71.998,40.500,84.436,49.500 '780 \\n'>\n",
            "<LTTextBoxHorizontal(29) 92.014,40.500,242.737,49.500 '|  Appendix A: Machine Learning Project Checklist\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 90.655,595.037,393.405,605.537 '•\\n• Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\\n'>\n",
            "<LTTextBoxHorizontal(1) 90.655,578.437,194.398,588.937 '•\\n• Usefulness for the task\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.655,561.837,344.727,572.337 '•\\n• Type of distribution (Gaussian, uniform, logarithmic, etc.)\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.316,545.236,344.034,555.737 '4.\\n4. For supervised learning tasks, identify the target attribute(s).\\n'>\n",
            "<LTTextBoxHorizontal(4) 77.316,528.637,166.343,539.137 '5.\\n5. Visualize the data.\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.316,512.036,264.213,522.537 '6. Study the correlations between attributes.\\n6.\\n'>\n",
            "<LTTextBoxHorizontal(6) 77.316,495.437,301.992,505.937 '7.\\n7. Study how you would solve the problem manually.\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.316,478.837,353.673,489.337 '8.\\n8. Identify the promising transformations you may want to apply.\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.316,462.236,424.758,472.737 '9.\\n9. Identify extra data that would be useful (go back to “Get the Data” on page 780).\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.276,445.636,233.711,456.136 '10.\\n10. Document what you have learned.\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.004,395.636,177.277,433.297 'Prepare the Data\\nNotes:\\n'>\n",
            "<LTTextBoxHorizontal(11) 80.659,373.036,342.967,383.536 '•\\n• Work on copies of the data (keep the original dataset intact).\\n'>\n",
            "<LTTextBoxHorizontal(12) 80.659,356.436,387.067,366.936 '•\\n• Write functions for all data transformations you apply, for five reasons:\\n'>\n",
            "<LTTextBoxHorizontal(13) 91.073,339.836,397.693,350.336 '—\\n— So you can easily prepare the data the next time you get a fresh dataset\\n'>\n",
            "<LTTextBoxHorizontal(14) 91.073,323.236,342.841,333.736 '—\\n— So you can apply these transformations in future projects\\n'>\n",
            "<LTTextBoxHorizontal(15) 91.073,306.636,237.757,317.136 '—\\n— To clean and prepare the test set\\n'>\n",
            "<LTTextBoxHorizontal(16) 91.073,290.036,379.717,300.536 '—\\n— To clean and prepare new data instances once your solution is live\\n'>\n",
            "<LTTextBoxHorizontal(17) 91.073,273.436,391.624,283.936 '—\\n— To make it easy to treat your preparation choices as hyperparameters\\n'>\n",
            "<LTTextBoxHorizontal(18) 77.320,246.836,152.654,257.336 '1. Clean the data:\\n1.\\n'>\n",
            "<LTTextBoxHorizontal(19) 90.659,230.236,240.455,240.736 '• Fix or remove outliers (optional).\\n•\\n'>\n",
            "<LTTextBoxHorizontal(20) 90.659,213.636,431.997,224.136 '• Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or\\n•\\n'>\n",
            "<LTTextBoxHorizontal(21) 99.997,201.036,142.249,211.536 'columns).\\n'>\n",
            "<LTTextBoxHorizontal(22) 77.320,184.436,243.049,194.936 '2.\\n2. Perform feature selection (optional):\\n'>\n",
            "<LTTextBoxHorizontal(23) 90.659,167.836,380.652,178.336 '•\\n• Drop the attributes that provide no useful information for the task.\\n'>\n",
            "<LTTextBoxHorizontal(24) 77.320,151.236,292.872,161.736 '3.\\n3. Perform feature engineering, where appropriate:\\n'>\n",
            "<LTTextBoxHorizontal(25) 90.659,134.636,229.462,145.136 '• Discretize continuous features.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(26) 90.659,101.436,401.684,128.536 '• Decompose features (e.g., categorical, date/time, etc.).\\n•\\n• Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\\n•\\n'>\n",
            "<LTTextBoxHorizontal(27) 90.660,84.836,300.485,95.336 '•\\n• Aggregate features into promising new features.\\n'>\n",
            "<LTTextBoxHorizontal(28) 298.211,40.500,402.512,49.500 'Machine Learning Project Checklist \\n'>\n",
            "<LTTextBoxHorizontal(29) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(30) 420.980,40.500,431.996,49.500 '781\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 77.316,595.037,190.198,605.537 '4.\\n4. Perform feature scaling:\\n'>\n",
            "<LTTextBoxHorizontal(1) 90.655,578.437,244.127,588.937 '•\\n• Standardize or normalize features.\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,528.436,238.845,566.097 'Shortlist Promising Models\\nNotes:\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.655,480.636,432.001,516.336 '•\\n• If the data is huge, you may want to sample smaller training sets so you can train\\nmany different models in a reasonable time (be aware that this penalizes complex\\nmodels such as large neural nets or random forests).\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.653,464.036,341.040,474.536 '•\\n• Once again, try to automate these steps as much as possible.\\n'>\n",
            "<LTTextBoxHorizontal(5) 77.314,437.436,431.995,447.936 '1.\\n1. Train many quick-and-dirty models from different categories (e.g., linear, naive\\n'>\n",
            "<LTTextBoxHorizontal(6) 90.000,424.836,391.969,435.336 'Bayes, SVM, random forest, neural net, etc.) using standard parameters.\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.319,408.236,264.489,418.736 '2.\\n2. Measure and compare their performance:\\n'>\n",
            "<LTTextBoxHorizontal(8) 90.658,391.636,431.996,402.136 '•\\n• For each model, use N-fold cross-validation and compute the mean and stan‐\\n'>\n",
            "<LTTextBoxHorizontal(9) 99.996,379.036,346.200,389.536 'dard deviation of the performance measure on the N folds.\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.319,362.436,331.406,372.936 '3.\\n3. Analyze the most significant variables for each algorithm.\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.319,345.836,278.591,356.336 '4.\\n4. Analyze the types of errors the models make:\\n'>\n",
            "<LTTextBoxHorizontal(12) 90.658,329.236,347.145,339.736 '•\\n• What data would a human have used to avoid these errors?\\n'>\n",
            "<LTTextBoxHorizontal(13) 77.319,312.636,340.919,323.136 '5.\\n5. Perform a quick round of feature selection and engineering.\\n'>\n",
            "<LTTextBoxHorizontal(14) 77.319,296.036,373.710,306.536 '6.\\n6. Perform one or two more quick iterations of the five previous steps.\\n'>\n",
            "<LTTextBoxHorizontal(15) 77.314,279.436,432.000,289.936 '7.\\n7. Shortlist  the  top  three  to  five  most  promising  models,  preferring  models  that\\n'>\n",
            "<LTTextBoxHorizontal(16) 90.005,266.836,216.614,277.336 'make different types of errors.\\n'>\n",
            "<LTTextBoxHorizontal(17) 71.996,216.836,206.795,254.496 'Fine-Tune the System\\nNotes:\\n'>\n",
            "<LTTextBoxHorizontal(18) 80.651,194.236,432.003,204.736 '•\\n• You will want to use as much data as possible for this step, especially as you move\\n'>\n",
            "<LTTextBoxHorizontal(19) 89.997,181.636,217.068,192.136 'toward the end of fine-tuning.\\n'>\n",
            "<LTTextBoxHorizontal(20) 80.655,165.036,235.538,175.536 '• As always, automate what you can.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(21) 77.316,138.436,317.679,148.936 '1. Fine-tune the hyperparameters using cross-validation:\\n1.\\n'>\n",
            "<LTTextBoxHorizontal(22) 90.655,96.636,432.004,132.336 '• Treat  your  data  transformation  choices  as  hyperparameters,  especially  when\\n•\\nyou are not sure about them (e.g., if you’re not sure whether to replace missing\\nvalues with zeros or with the median value, or to just drop the rows).\\n'>\n",
            "<LTTextBoxHorizontal(23) 71.998,40.500,84.436,49.500 '782 \\n'>\n",
            "<LTTextBoxHorizontal(24) 92.014,40.500,242.737,49.500 '|  Appendix A: Machine Learning Project Checklist\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 90.660,557.236,431.999,605.537 '• Unless  there  are  very  few  hyperparameter  values  to  explore,  prefer  random\\n•\\nsearch  over  grid  search.  If  training  is  very  long,  you  may  prefer  a  Bayesian\\noptimization  approach  (e.g.,  using  Gaussian  process  priors,  as  described  by\\nJasper Snoek et al.1).\\n'>\n",
            "<LTTextBoxHorizontal(1) 77.315,540.637,431.996,551.137 '2.\\n2. Try  ensemble  methods.  Combining  your  best  models  will  often  produce  better\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.000,528.037,281.153,538.537 'performance than running them individually.\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.313,511.437,432.000,521.937 '3.\\n3. Once you are confident about your final model, measure its performance on the\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.005,498.837,270.111,509.337 'test set to estimate the generalization error.\\n'>\n",
            "<LTTextBoxHorizontal(5) 136.786,450.785,395.996,471.905 'Don’t  tweak  your  model  after  measuring  the  generalization  error:\\nyou would just start overfitting the test set.\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,389.853,206.464,408.773 'Present Your Solution\\n'>\n",
            "<LTTextBoxHorizontal(7) 77.317,367.113,223.443,377.613 '1.\\n1. Document what you have done.\\n'>\n",
            "<LTTextBoxHorizontal(8) 77.317,350.513,201.477,361.013 '2.\\n2. Create a nice presentation:\\n'>\n",
            "<LTTextBoxHorizontal(9) 90.656,333.913,287.115,344.413 '•\\n• Make sure you highlight the big picture first.\\n'>\n",
            "<LTTextBoxHorizontal(10) 77.317,317.312,334.396,327.813 '3.\\n3. Explain why your solution achieves the business objective.\\n'>\n",
            "<LTTextBoxHorizontal(11) 77.317,300.712,375.588,311.212 '4.\\n4. Don’t forget to present interesting points you noticed along the way:\\n'>\n",
            "<LTTextBoxHorizontal(12) 90.656,284.112,270.262,294.612 '•\\n• Describe what worked and what did not.\\n'>\n",
            "<LTTextBoxHorizontal(13) 90.656,267.512,318.762,278.012 '•\\n• List your assumptions and your system’s limitations.\\n'>\n",
            "<LTTextBoxHorizontal(14) 77.311,225.712,432.003,261.412 '5.\\n5. Ensure  your  key  findings  are  communicated  through  beautiful  visualizations\\nor  easy-to-remember  statements  (e.g.,  “the  median  income  is  the  number-one\\npredictor of housing prices”).\\n'>\n",
            "<LTTextBoxHorizontal(15) 71.991,194.453,121.013,213.373 'Launch!\\n'>\n",
            "<LTTextBoxHorizontal(16) 77.315,171.712,431.996,182.212 '1. Get your solution ready for production (plug into production data inputs, write\\n1.\\n'>\n",
            "<LTTextBoxHorizontal(17) 90.000,159.112,153.599,169.612 'unit tests, etc.).\\n'>\n",
            "<LTTextBoxHorizontal(18) 77.313,142.512,432.000,153.012 '2.\\n2. Write monitoring code to check your system’s live performance at regular inter‐\\n'>\n",
            "<LTTextBoxHorizontal(19) 90.005,129.912,242.528,140.412 'vals and trigger alerts when it drops:\\n'>\n",
            "<LTTextBoxHorizontal(20) 90.652,113.312,372.602,123.812 '• Beware of slow degradation: models tend to “rot” as data evolves.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(21) 73.140,78.954,431.416,86.954 '1 Jasper Snoek et al., “Practical Bayesian Optimization of Machine Learning Algorithms”, Proceedings of the 25th\\n'>\n",
            "<LTTextBoxHorizontal(22) 80.000,68.954,358.728,76.954 'International Conference on Neural Information Processing Systems 2 (2012): 2951–2959.\\n'>\n",
            "<LTTextBoxHorizontal(23) 298.212,40.500,402.513,49.500 'Machine Learning Project Checklist \\n'>\n",
            "<LTTextBoxHorizontal(24) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(25) 420.981,40.500,431.997,49.500 '783\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTFigure(I1) 83.000,425.390,132.680,472.700 matrix=[49.68,0.00,0.00,47.31, (83.00,425.39)]>\n",
            "<LTTextBoxHorizontal(0) 90.655,595.037,432.004,605.537 '•\\n• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 100.004,582.437,150.981,592.937 'ing service).\\n'>\n",
            "<LTTextBoxHorizontal(2) 90.655,540.636,432.005,576.337 '• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐\\n•\\ndom  values,  or  another  team’s  output  becoming  stale).  This  is  particularly\\nimportant for online learning systems.\\n'>\n",
            "<LTTextBoxHorizontal(3) 77.312,524.036,431.999,534.537 '3. Retrain  your  models  on  a  regular  basis  on  fresh  data  (automate  as  much  as\\n3.\\n'>\n",
            "<LTTextBoxHorizontal(4) 90.003,511.437,129.861,521.937 'possible).\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,40.500,84.438,49.500 '784 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.016,40.500,242.739,49.500 '|  Appendix A: Machine Learning Project Checklist\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 363.980,533.502,431.998,582.331 'APPENDIX B\\nAutodiff\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,393.868,431.995,416.969 'This  appendix  explains  how  TensorFlow’s  autodifferentiation  (autodiff)  feature\\nworks, and how it compares to other solutions.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,312.268,432.005,386.580 'Suppose you define a function f(x, y) = x2y + y + 2, and you need its partial derivatives\\n∂f/∂x  and  ∂f/∂y,  typically  to  perform  gradient  descent  (or  some  other  optimization\\nalgorithm). Your main options are manual differentiation, finite difference approxi‐\\nmation, forward-mode autodiff, and reverse-mode autodiff. TensorFlow implements\\nreverse-mode  autodiff,  but  to  understand  it,  it’s  useful  to  look  at  the  other  options\\nfirst. So let’s go through each of them, starting with manual differentiation.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,237.068,432.002,299.929 'Manual Differentiation\\nThe first approach to compute derivatives is to pick up a pencil and a piece of paper\\nand use your calculus knowledge to derive the appropriate equation. For the function\\nf(x, y) just defined, it is not too hard; you just need to use five rules:\\n'>\n",
            "<LTTextBoxHorizontal(4) 80.651,214.468,225.307,224.968 '•\\n• The derivative of a constant is 0.\\n'>\n",
            "<LTTextBoxHorizontal(5) 80.651,181.268,323.711,208.368 '• The derivative of λx is λ (where λ is a constant).\\n•\\n• The derivative of xλ is λxλ – 1, so the derivative of x2 is 2x.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(6) 80.660,164.668,415.607,175.168 '• The derivative of a sum of functions is the sum of these functions’ derivatives.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.660,148.068,339.503,158.568 '• The derivative of λ times a function is λ times its derivative.\\n•\\n'>\n",
            "<LTTextBoxHorizontal(8) 420.980,40.500,431.996,49.500 '785\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,268.875,605.537 'From these rules, you can derive Equation B-1.\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.005,568.749,257.798,579.249 'Equation B-1. Partial derivatives of f(x, y)\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.108,504.934,109.352,558.873 '∂ f\\n∂x =\\n∂ f\\n∂y =\\n'>\n",
            "<LTTextBoxHorizontal(3) 113.238,504.934,148.874,561.229 '∂ x2y\\n∂x +\\n∂ x2y\\n∂y +\\n'>\n",
            "<LTTextBoxHorizontal(4) 152.200,504.934,171.664,558.873 '∂y\\n∂x +\\n∂y\\n∂y +\\n'>\n",
            "<LTTextBoxHorizontal(5) 204.314,535.185,283.582,561.229 '∂ x2\\n∂x + 0 + 0 = 2xy\\n'>\n",
            "<LTTextBoxHorizontal(6) 174.970,504.934,272.774,557.423 '∂2\\n∂x = y\\n∂2\\n∂y = x2 + 1 + 0 = x2 + 1\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,453.141,432.005,488.841 'This  approach  can  become  very  tedious  for  more  complex  functions,  and  you  run\\nthe risk of making mistakes. Fortunately, there are other options. Let’s look at finite\\ndifference approximation now.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,364.752,432.002,440.801 'Finite Difference Approximation\\nRecall  that  the  derivative  h′(x0)  of  a  function  h(x)  at  a  point  x0  is  the  slope  of  the\\nfunction  at  that  point.  More  precisely,  the  derivative  is  defined  as  the  limit  of  the\\nslope  of  a  straight  line  going  through  this  point  x0  and  another  point  x  on  the\\nfunction, as x gets infinitely close to x0 (see Equation B-2).\\n'>\n",
            "<LTTextBoxHorizontal(9) 87.003,338.465,374.831,349.553 'Equation B-2. Definition of the derivative of a function h(x) at point x0\\n'>\n",
            "<LTTextBoxHorizontal(10) 87.001,312.587,120.859,325.133 'ℎ′ x0 =\\n'>\n",
            "<LTTextBoxHorizontal(11) 126.419,307.292,131.839,317.292 'x\\n'>\n",
            "<LTTextBoxHorizontal(12) 134.029,313.683,147.429,323.683 'lim\\n'>\n",
            "<LTTextBoxHorizontal(13) 146.169,304.746,155.045,317.292 'x0\\n'>\n",
            "<LTTextBoxHorizontal(14) 115.059,276.125,145.139,292.517 '= lim\\nε\\n'>\n",
            "<LTTextBoxHorizontal(15) 145.659,274.675,150.459,284.675 '0\\n'>\n",
            "<LTTextBoxHorizontal(16) 154.339,277.561,219.463,332.237 'ℎ x − ℎ x0\\nx − x0\\nℎ x0 + ε − ℎ x0\\nε\\n'>\n",
            "<LTTextBoxHorizontal(17) 71.996,197.681,432.025,258.581 'So, if we wanted to calculate the partial derivative of f(x, y) with regard to x at x = 3\\nand y = 4, we could compute f(3 + ε, 4) – f(3, 4) and divide the result by ε, using a\\nvery small value for ε. This type of numerical approximation of the derivative is called\\na finite difference approximation, and this specific equation is called Newton’s difference\\nquotient. That’s exactly what the following code does:\\n'>\n",
            "<LTTextBoxHorizontal(18) 89.002,171.995,195.252,190.695 'def f(x, y):\\n    return x**2*y + y + 2\\n'>\n",
            "<LTTextBoxHorizontal(19) 89.002,141.395,361.002,160.095 'def derivative(f, x, y, x_eps, y_eps):\\n    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)\\n'>\n",
            "<LTTextBoxHorizontal(20) 89.002,110.795,254.752,129.495 'df_dx = derivative(f, 3, 4, 0.00001, 0)\\ndf_dy = derivative(f, 3, 4, 0, 0.00001)\\n'>\n",
            "<LTTextBoxHorizontal(21) 71.997,78.881,432.000,101.981 'Unfortunately, the result is imprecise (and it gets worse for more complicated func‐\\ntions). The correct results are respectively 24 and 10, but instead we get:\\n'>\n",
            "<LTTextBoxHorizontal(22) 72.000,40.500,84.438,49.500 '786 \\n'>\n",
            "<LTTextBoxHorizontal(23) 92.016,40.500,164.178,49.500 '|  Appendix B: Autodiff\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 87.556,547.460,98.996,547.460>\n",
            "<LTLine 112.685,547.460,140.291,547.460>\n",
            "<LTCurve 118.841,548.293,121.141,561.189>\n",
            "<LTCurve 136.726,548.293,139.026,561.189>\n",
            "<LTLine 151.647,547.460,163.088,547.460>\n",
            "<LTLine 174.443,547.460,185.884,547.460>\n",
            "<LTLine 203.756,547.460,225.957,547.460>\n",
            "<LTCurve 209.912,548.293,212.212,561.189>\n",
            "<LTCurve 222.391,548.293,224.691,561.189>\n",
            "<LTLine 87.580,517.209,98.996,517.209>\n",
            "<LTLine 112.685,517.209,140.291,517.209>\n",
            "<LTCurve 118.841,518.043,121.141,530.939>\n",
            "<LTCurve 136.726,518.043,139.026,530.939>\n",
            "<LTLine 151.647,517.209,163.063,517.209>\n",
            "<LTLine 174.419,517.209,185.835,517.209>\n",
            "<LTCurve 95.971,315.125,98.271,325.125>\n",
            "<LTCurve 108.051,315.125,110.351,325.125>\n",
            "<LTCurve 134.624,309.472,143.394,314.612>\n",
            "<LTLine 158.375,319.553,209.471,319.553>\n",
            "<LTCurve 165.431,322.229,167.731,332.229>\n",
            "<LTCurve 174.071,322.229,176.371,332.229>\n",
            "<LTCurve 193.826,322.229,196.126,332.229>\n",
            "<LTCurve 205.906,322.229,208.206,332.229>\n",
            "<LTCurve 134.107,278.305,142.877,283.445>\n",
            "<LTLine 153.788,288.387,223.481,288.387>\n",
            "<LTCurve 160.844,291.062,163.144,301.062>\n",
            "<LTCurve 188.080,291.062,190.380,301.062>\n",
            "<LTCurve 207.836,291.062,210.136,301.062>\n",
            "<LTCurve 219.915,291.062,222.215,301.062>\n",
            "<LTTextBoxHorizontal(0) 89.000,567.550,165.500,606.650 '>>> df_dx\\n24.000039999805264\\n>>> df_dy\\n10.000000000331966\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,496.650,432.004,559.703 'Notice that to compute both partial derivatives, we have to call f() at least three times\\n(we  called  it  four  times  in  the  preceding  code,  but  it  could  be  optimized).  If  there\\nwere 1,000 parameters, we would need to call f() at least 1,001 times. When you are\\ndealing  with  large  neural  networks,  this  makes  finite  difference  approximation  way\\ntoo inefficient.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,452.850,431.998,488.550 'However, this method is so simple to implement that it is a great tool to check that\\nthe other methods are implemented correctly. For example, if it disagrees with your\\nmanually derived function, then your function probably contains a mistake.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,396.450,432.003,444.750 'So far, we have considered two ways to compute gradients: using manual differentia‐\\ntion and using finite difference approximation. Unfortunately, both are fatally flawed\\nfor  training  a  large-scale  neural  network.  So  let’s  turn  to  autodiff,  starting  with\\nforward mode.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,296.049,432.004,384.110 'Forward-Mode Autodiff\\nFigure  B-1  shows  how  forward-mode  autodiff  works  on  an  even  simpler  function,\\ng(x, y) = 5 + xy. The graph for that function is represented on the left. After forward-\\nmode autodiff, we get the graph on the right, which represents the partial derivative\\n∂g/∂x = 0 + (0 × x + y × 1) = y (we could similarly obtain the partial derivative with\\nregard to y).\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.995,214.449,432.002,287.949 'The algorithm will go through the computation graph from the inputs to the outputs\\n(hence  the  name  “forward  mode”).  It  starts  by  getting  the  partial  derivatives  of  the\\nleaf  nodes.  The  constant  node  (5)  returns  the  constant  0,  since  the  derivative  of  a\\nconstant  is  always  0.  The  variable  x  returns  the  constant  1  since  ∂x/∂x  =  1,  and  the\\nvariable  y  returns  the  constant  0  since  ∂y/∂x  =  0  (if  we  were  looking  for  the  partial\\nderivative with regard to y, it would be the reverse).\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,158.049,432.004,206.349 'Now we have all we need to move up the graph to the multiplication node in function\\ng.  Calculus  tells  us  that  the  derivative  of  the  product  of  two  functions  u  and  v  is\\n∂(u × v)/∂x = ∂v/∂x × u + v × ∂u/∂x. We can therefore construct a large part of the\\ngraph on the right, representing 0 × x + y × 1.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,101.649,432.003,149.949 'Finally, we can go up to the addition node in function g. As mentioned, the derivative\\nof  a  sum  of  functions  is  the  sum  of  these  functions’  derivatives,  so  we  just  need  to\\ncreate  an  addition  node  and  connect  it  to  the  parts  of  the  graph  we  have  already\\ncomputed. We get the correct partial derivative: ∂g/∂x = 0 + (0 × x + y × 1).\\n'>\n",
            "<LTTextBoxHorizontal(8) 376.821,40.500,402.516,49.500 'Autodiff \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.984,40.500,432.000,49.500 '787\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,323.886,215.251,334.386 'Figure B-1. Forward-mode autodiff\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,248.886,432.004,309.786 'However, this equation can be simplified (a lot). By applying a few pruning steps to\\nthe  computation  graph  to  get  rid  of  all  the  unnecessary  operations,  we  get  a  much\\nsmaller graph with just one node: ∂g/∂x = y. In this case simplification is fairly easy,\\nbut  for  a  more  complex  function  forward-mode  autodiff  can  produce  a  huge  graph\\nthat may be tough to simplify and lead to suboptimal performance.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,154.686,432.005,240.786 'Note that we started with a computation graph, and forward-mode autodiff produced\\nanother computation graph. This is called symbolic differentiation, and it has two nice\\nfeatures:  first,  once  the  computation  graph  of  the  derivative  has  been  produced,  we\\ncan use it as many times as we want to compute the derivatives of the given function\\nfor  any  value  of  x  and  y;  second,  we  can  run  forward-mode  autodiff  again  on  the\\nresulting graph to get second-order derivatives if we ever need to (i.e., derivatives of\\nderivatives). We could even compute third-order derivatives, and so on.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,73.085,432.004,146.585 'But  it  is  also  possible  to  run  forward-mode  autodiff  without  constructing  a  graph\\n(i.e.,  numerically,  not  symbolically),  just  by  computing  intermediate  results  on  the\\nfly.  One  way  to  do  this  is  to  use  dual  numbers,  which  are  weird  but  fascinating\\nnumbers of the form a + bε, where a and b are real numbers and ε is an infinitesimal\\nnumber such that ε2 = 0 (but ε ≠ 0). You can think of the dual number 42 + 24ε as\\nsomething akin to 42.0000⋯000024 with an infinite number of 0s (but of course this\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,40.500,84.437,49.500 '788 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.015,40.500,164.177,49.500 '|  Appendix B: Autodiff\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,339.949,432.375,607.500>\n",
            "<LTLine 72.000,340.074,432.500,340.074>\n",
            "<LTLine 72.125,339.949,72.125,607.500>\n",
            "<LTFigure(I1) 108.010,345.929,396.490,602.250 matrix=[288.48,0.00,0.00,256.32, (108.01,345.93)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,569.837,432.004,605.537 'is simplified just to give you some idea of what dual numbers are). A dual number is\\nrepresented in memory as a pair of floats. For example, 42 + 24ε is represented by the\\npair (42.0, 24.0).\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.001,551.236,398.666,561.736 'Dual numbers can be added, multiplied, and so on, as shown in Equation B-3.\\n'>\n",
            "<LTTextBoxHorizontal(2) 86.995,524.949,294.412,535.449 'Equation B-3. A few operations with dual numbers\\n'>\n",
            "<LTTextBoxHorizontal(3) 91.002,474.596,352.390,518.133 'λ a + bε = λa + λbε\\na + bε + c + dε = a + c + b + d ε\\na + bε × c + dε = ac + ad + bc ε + bd ε2 = ac + ad + bc ε\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.995,385.002,432.002,460.025 'Most importantly, it can be shown that h(a + bε) = h(a) + b × h′(a)ε, so computing\\nh(a  +  ε)  gives  you  both  h(a)  and  the  derivative  h′(a)  in  just  one  shot.  Figure  B-2\\nshows that the partial derivative of f(x, y) with regard to x at x = 3 and y = 4 (which\\nI will write ∂f/∂x (3, 4)) can be computed using dual numbers. All we need to do is\\ncompute f(3 + ε, 4); this will output a dual number whose first component is equal to\\nf(3, 4) and whose second component is equal to ∂f/∂x (3, 4).\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.002,70.964,298.067,81.464 'Figure B-2. Forward-mode autodiff using dual numbers\\n'>\n",
            "<LTTextBoxHorizontal(6) 376.817,40.500,402.512,49.500 'Autodiff \\n'>\n",
            "<LTTextBoxHorizontal(7) 410.090,40.500,413.402,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(8) 420.980,40.500,431.996,49.500 '789\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,378.741,432.500,378.741>\n",
            "<LTLine 432.375,87.028,432.375,378.866>\n",
            "<LTLine 72.000,87.153,432.500,87.153>\n",
            "<LTLine 72.125,87.028,72.125,378.866>\n",
            "<LTCurve 97.081,508.125,99.381,518.125>\n",
            "<LTCurve 125.848,508.125,128.148,518.125>\n",
            "<LTCurve 91.700,493.173,94.000,503.173>\n",
            "<LTCurve 120.467,493.173,122.767,503.173>\n",
            "<LTCurve 134.421,493.173,136.721,503.173>\n",
            "<LTCurve 163.257,493.173,165.557,503.173>\n",
            "<LTCurve 178.322,493.173,180.622,503.173>\n",
            "<LTCurve 202.021,493.173,204.321,503.173>\n",
            "<LTCurve 215.975,493.173,218.275,503.173>\n",
            "<LTCurve 240.055,493.173,242.355,503.173>\n",
            "<LTCurve 91.700,476.037,94.000,486.037>\n",
            "<LTCurve 120.467,476.037,122.767,486.037>\n",
            "<LTCurve 134.421,476.037,136.721,486.037>\n",
            "<LTCurve 163.257,476.037,165.557,486.037>\n",
            "<LTCurve 198.811,476.037,201.111,486.037>\n",
            "<LTCurve 233.134,476.037,235.434,486.037>\n",
            "<LTCurve 252.001,476.037,254.301,486.037>\n",
            "<LTCurve 265.836,476.037,268.136,486.037>\n",
            "<LTCurve 310.142,476.037,312.442,486.037>\n",
            "<LTCurve 344.466,476.037,346.766,486.037>\n",
            "<LTFigure(I1) 135.250,93.008,369.250,373.616 matrix=[234.00,0.00,0.00,280.61, (135.25,93.01)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'To compute ∂f/∂y (3, 4) we would have to go through the graph again, but this time\\nwith x = 3 and y = 4 + ε.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,500.837,432.005,574.337 'So, forward-mode autodiff is much more accurate than finite difference approxima‐\\ntion, but it suffers from the same major flaw, at least when there are many inputs and\\nfew  outputs  (as  is  the  case  when  dealing  with  neural  networks):  if  there  were  1,000\\nparameters, it would require 1,000 passes through the graph to compute all the partial\\nderivatives. This is where reverse-mode autodiff shines: it can compute all of them in\\njust two passes through the graph. Let’s see how.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.993,349.448,432.005,488.497 'Reverse-Mode Autodiff\\nReverse-mode  autodiff  is  the  solution  implemented  by  TensorFlow.  It  first  goes\\nthrough  the  graph  in  the  forward  direction  (i.e.,  from  the  inputs  to  the  output)  to\\ncompute the value of each node. Then it does a second pass, this time in the reverse\\ndirection (i.e., from the output to the inputs), to compute all the partial derivatives.\\nThe  name  “reverse  mode”  comes  from  this  second  pass  through  the  graph,  where\\ngradients flow in the reverse direction. Figure B-3 represents the second pass. During\\nthe first pass, all the node values were computed, starting from x = 3 and y = 4. You\\ncan see those values at the bottom right of each node (e.g., x × x = 9). The nodes are\\nlabeled n1 to n7 for clarity. The output node is n7: f(3, 4) = n7 = 42.\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.003,91.870,211.149,102.370 'Figure B-3. Reverse-mode autodiff\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,40.500,84.438,49.500 '790 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.016,40.500,164.178,49.500 '|  Appendix B: Autodiff\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,343.775,432.500,343.775>\n",
            "<LTLine 432.375,107.934,432.375,343.900>\n",
            "<LTLine 72.000,108.059,432.500,108.059>\n",
            "<LTLine 72.125,107.934,72.125,343.900>\n",
            "<LTFigure(I1) 79.450,113.914,425.050,338.650 matrix=[345.60,0.00,0.00,224.74, (79.45,113.91)]>\n",
            "<LTTextBoxHorizontal(0) 71.998,569.837,432.004,605.537 'The idea is to gradually go down the graph, computing the partial derivative of f(x, y)\\nwith  regard  to  each  consecutive  node,  until  we  reach  the  variable  nodes.  For  this,\\nreverse-mode autodiff relies heavily on the chain rule, shown in Equation B-4.\\n'>\n",
            "<LTTextBoxHorizontal(1) 87.002,543.549,190.417,554.049 'Equation B-4. Chain rule\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.107,511.775,108.127,535.463 '∂ f\\n∂x =\\n'>\n",
            "<LTTextBoxHorizontal(3) 112.017,511.381,125.313,535.463 '∂ f\\n∂ni\\n'>\n",
            "<LTTextBoxHorizontal(4) 128.753,518.180,134.553,528.180 '×\\n'>\n",
            "<LTTextBoxHorizontal(5) 137.883,511.775,151.171,536.733 '∂ni\\n∂x\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.003,483.323,273.765,494.411 'Since n7 is the output node, f = n7 so ∂f / ∂n7 = 1.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,426.923,431.998,475.811 'Let’s  continue  down  the  graph  to  n5:  how  much  does  f  vary  when  n5  varies?  The\\nanswer is ∂f / ∂n5 = ∂f / ∂n7 × ∂n7 / ∂n5. We already know that ∂f / ∂n7 = 1, so all we\\nneed is ∂n7 / ∂n5. Since n7 simply performs the sum n5 + n6, we find that ∂n7 / ∂n5 = 1,\\nso ∂f / ∂n5 = 1 × 1 = 1.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,383.123,432.004,419.411 'Now we can proceed to node n4: how much does f vary when n4 varies? The answer is\\n∂f / ∂n4 = ∂f / ∂n5 × ∂n5 / ∂n4. Since n5 = n4 × n2, we find that ∂n5 / ∂n4 = n2, so ∂f / ∂n4\\n= 1 × n2 = 4.\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,339.911,432.001,375.611 'The process continues until we reach the bottom of the graph. At that point we will\\nhave calculated all the partial derivatives of f(x, y) at the point x = 3 and y = 4. In this\\nexample, we find ∂f / ∂x = 24 and ∂f / ∂y = 10. Sounds about right!\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.995,233.111,432.005,331.811 'Reverse-mode  autodiff  is  a  very  powerful  and  accurate  technique,  especially  when\\nthere are many inputs and few outputs, since it requires only one forward pass plus\\none  reverse  pass  per  output  to  compute  all  the  partial  derivatives  for  all  outputs\\nwith  regard  to  all  the  inputs.  When  training  neural  networks,  we  generally  want  to\\nminimize  the  loss,  so  there  is  a  single  output  (the  loss),  and  hence  only  two  passes\\nthrough the graph are needed to compute the gradients. Reverse-mode autodiff can\\nalso  handle  functions  that  are  not  entirely  differentiable,  as  long  as  you  ask  it  to\\ncompute the partial derivatives at points that are differentiable.\\n'>\n",
            "<LTTextBoxHorizontal(11) 71.996,138.911,432.005,225.011 'In Figure B-3, the numerical results are computed on the fly, at each node. However,\\nthat’s not exactly what TensorFlow does: instead, it creates a new computation graph.\\nIn other words, it implements symbolic reverse-mode autodiff. This way, the compu‐\\ntation graph to compute the gradients of the loss with regard to all the parameters in\\nthe neural network only needs to be generated once, and then it can be executed over\\nand  over  again,  whenever  the  optimizer  needs  to  compute  the  gradients.  Moreover,\\nthis makes it possible to compute higher-order derivatives if needed.\\n'>\n",
            "<LTTextBoxHorizontal(12) 376.824,40.500,402.519,49.500 'Autodiff \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.987,40.500,432.003,49.500 '791\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 87.556,524.050,98.996,524.050>\n",
            "<LTLine 111.463,524.050,125.973,524.050>\n",
            "<LTLine 137.328,524.050,151.838,524.050>\n",
            "<LTTextBoxHorizontal(0) 136.790,526.985,396.002,605.705 'If you ever want to implement a new type of low-level TensorFlow\\noperation in C++, and you want to make it compatible with auto‐\\ndiff, then you will need to provide a function that returns the par‐\\ntial  derivatives  of  the  function’s  outputs  with  regard  to  its  inputs.\\nFor example, suppose you implement a function that computes the\\nsquare of its input: f(x) = x2. In that case you would need to provide\\nthe corresponding derivative function: f′(x) = 2x.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.003,40.500,84.441,49.500 '792 \\n'>\n",
            "<LTTextBoxHorizontal(2) 92.019,40.500,164.181,49.500 '|  Appendix B: Autodiff\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 85.000,555.723,126.760,611.500 matrix=[41.76,0.00,0.00,55.78, (85.00,555.72)]>\n",
            "<LTTextBoxHorizontal(0) 241.482,533.502,431.994,582.331 'APPENDIX C\\nSpecial Data Structures\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.996,381.268,432.002,416.969 'In  this  appendix  we  will  take  a  very  quick  look  at  the  data  structures  supported  by\\nTensorFlow,  beyond  regular  float  or  integer  tensors.  This  includes  strings,  ragged\\ntensors, sparse tensors, tensor arrays, sets, and queues.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.992,318.668,431.996,368.929 'Strings\\nTensors  can  hold  byte  strings,  which  is  useful  in  particular  for  natural  language\\nprocessing (see Chapter 16):\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.003,292.982,331.253,311.682 '>>> tf.constant(b\"hello world\")\\n<tf.Tensor: shape=(), dtype=string, numpy=b\\'hello world\\'>\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.998,261.068,431.998,284.168 'If you try to build a tensor with a Unicode string, TensorFlow automatically encodes\\nit to UTF-8:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.004,235.381,331.254,254.081 '>>> tf.constant(\"café\")\\n<tf.Tensor: shape=(), dtype=string, numpy=b\\'caf\\\\xc3\\\\xa9\\'>\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.996,203.468,431.999,226.568 'It is also possible to create tensors representing Unicode strings. Just create an array\\nof 32-bit integers, each representing a single Unicode code point:1\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.999,167.581,420.499,196.481 '>>> u = tf.constant([ord(c) for c in \"café\"])\\n>>> u\\n<tf.Tensor: shape=(4,), [...], numpy=array([ 99,  97, 102, 233], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(8) 73.140,68.954,373.040,76.954 '1 If you are not familiar with Unicode code points, please check out https://homl.info/unicode.\\n'>\n",
            "<LTTextBoxHorizontal(9) 420.985,40.500,432.001,49.500 '793\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTTextBoxHorizontal(0) 136.790,561.002,396.001,606.588 'In  tensors  of  type  tf.string,  the  string  length  is  not  part  of  the\\ntensor’s  shape.  In  other  words,  strings  are  considered  as  atomic\\nvalues. However, in a Unicode string tensor (i.e., an int32 tensor),\\nthe length of the string is part of the tensor’s shape.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,476.641,432.005,540.880 'The  tf.strings  package  contains  several  functions  to  manipulate  string  tensors,\\nsuch  as  length()  to  count  the  number  of  bytes  in  a  byte  string  (or  the  number  of\\ncode  points  if  you  set  unit=\"UTF8_CHAR\"),  unicode_encode()  to  convert  a  Unicode\\nstring tensor (i.e., int32 tensor) to a byte string tensor, and unicode_decode() to do\\nthe reverse:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.996,399.955,420.496,469.655 '>>> b = tf.strings.unicode_encode(u, \"UTF-8\")\\n>>> b\\n<tf.Tensor: shape=(), dtype=string, numpy=b\\'caf\\\\xc3\\\\xa9\\'>\\n>>> tf.strings.length(b, unit=\"UTF8_CHAR\")\\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\\n>>> tf.strings.unicode_decode(b, \"UTF-8\")\\n<tf.Tensor: shape=(4,), [...], numpy=array([ 99,  97, 102, 233], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,380.641,325.324,391.141 'You can also manipulate tensors containing multiple strings:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.999,303.955,416.249,373.655 '>>> p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])\\n>>> tf.strings.length(p, unit=\"UTF8_CHAR\")\\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>\\n>>> r = tf.strings.unicode_decode(p, \"UTF8\")\\n>>> r\\n<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97,\\n102, 102, 232], [21654, 21857]]>\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.003,284.048,386.919,296.107 'Notice that the decoded strings are stored in a RaggedTensor. What is that?\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,183.054,432.004,271.708 'Ragged Tensors\\nA ragged tensor is a special kind of tensor that represents a list of arrays of different\\nsizes.  More  generally,  it  is  a  tensor  with  one  or  more  ragged  dimensions,  meaning\\ndimensions  whose  slices  may  have  different  lengths.  In  the  ragged  tensor  r,  the\\nsecond dimension is a ragged dimension. In all ragged tensors, the first dimension is\\nalways a regular dimension (also called a uniform dimension).\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.995,151.261,431.995,175.920 'All the elements of the ragged tensor r are regular tensors. For example, let’s look at\\nthe second element of the ragged tensor:\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.998,125.574,411.998,144.274 '>>> r[1]\\n<tf.Tensor: [...], numpy=array([ 67, 111, 102, 102, 101, 101], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.995,79.874,431.998,117.727 'The  tf.ragged  package  contains  several  functions  to  create  and  manipulate  ragged\\ntensors. Let’s create a second ragged tensor using tf.ragged.constant() and concat‐\\nenate it with the first ragged tensor, along axis 0:\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.997,40.500,84.435,49.500 '794 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.013,40.500,207.276,49.500 '|  Appendix C: Special Data Structures\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 89.000,557.999,126.944,607.500 matrix=[37.94,0.00,0.00,49.50, (89.00,558.00)]>\n",
            "<LTTextBoxHorizontal(0) 89.000,567.550,416.250,606.650 '>>> r2 = tf.ragged.constant([[65, 66], [], [67]])\\n>>> tf.concat([r, r2], axis=0)\\n<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97,\\n102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,534.450,432.005,559.703 'The result is not too surprising: the tensors in r2 were appended after the tensors in r\\nalong axis 0. But what if we concatenate r and another ragged tensor along axis 1?\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.998,488.363,424.748,527.463 '>>> r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\\n>>> print(tf.concat([r, r3], axis=1))\\n<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101,\\n71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,455.857,432.000,480.516 'This  time,  notice  that  the  ith  tensor  in  r  and  the  ith  tensor  in  r3  were  concatenated.\\nNow that’s more unusual, since all of these tensors can have different lengths.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,410.870,432.005,448.723 'If  you  call  the  to_tensor()  method,  the  ragged  tensor  gets  converted  to  a  regular\\ntensor,  padding  shorter  tensors  with  zeros  to  get  tensors  of  equal  lengths  (you  can\\nchange the default value by setting the default_value argument):\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.999,344.384,365.249,403.884 '>>> r.to_tensor()\\n<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\\narray([[   67,    97,   102,   233,     0,     0],\\n       [   67,   111,   102,   102,   101,   101],\\n       [   99,    97,   102,   102,   232,     0],\\n       [21654, 21857,     0,     0,     0,     0]], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,311.877,432.005,335.570 'Many TF operations support ragged tensors. For the full list, see the documentation\\nof the tf.RaggedTensor class.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.993,197.097,432.004,299.537 'Sparse Tensors\\nTensorFlow  can  also  efficiently  represent  sparse  tensors  (i.e.,  tensors  containing\\nmostly  zeros).  Just  create  a  tf.SparseTensor,  specifying  the  indices  and  values  of\\nthe  nonzero  elements  and  the  tensor’s  shape.  The  indices  must  be  listed  in  “read‐\\ning  order”  (from  left  to  right,  and  top  to  bottom).  If  you  are  unsure,  just  use\\ntf.sparse.reorder().  You  can  convert  a  sparse  tensor  to  a  dense  tensor  (i.e.,  a\\nregular tensor) using tf.sparse.to_dense():\\n'>\n",
            "<LTTextBoxHorizontal(8) 88.999,100.010,331.249,190.110 '>>> s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\\n...                     values=[1., 2., 3.],\\n...                     dense_shape=[3, 4])\\n...\\n>>> tf.sparse.to_dense(s)\\n<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\\narray([[0., 1., 0., 0.],\\n       [2., 0., 0., 0.],\\n       [0., 0., 0., 3.]], dtype=float32)>\\n'>\n",
            "<LTTextBoxHorizontal(9) 333.107,40.500,402.515,49.500 'Special Data Structures \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.983,40.500,431.999,49.500 '795\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'Note  that  sparse  tensors  do  not  support  as  many  operations  as  dense  tensors.  For\\nexample,  you  can  multiply  a  sparse  tensor  by  any  scalar  value,  and  you  get  a  new\\nsparse tensor, but you cannot add a scalar value to a sparse tensor, as this would not\\nreturn a sparse tensor:\\n'>\n",
            "<LTTextBoxHorizontal(1) 88.998,511.150,420.498,550.250 \">>> s * 42.0\\n<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f84a6749f10>\\n>>> s + 42.0\\n[...] TypeError: unsupported operand type(s) for +: 'SparseTensor' and 'float'\\n\">\n",
            "<LTTextBoxHorizontal(2) 71.991,434.643,432.004,498.097 'Tensor Arrays\\nA tf.TensorArray represents a list of tensors. This can be handy in dynamic models\\ncontaining  loops,  to  accumulate  results  and  later  compute  some  statistics.  You  can\\nread or write tensors at any location in the array:\\n'>\n",
            "<LTTextBoxHorizontal(3) 89.000,378.356,416.250,427.656 'array = tf.TensorArray(dtype=tf.float32, size=3)\\narray = array.write(0, tf.constant([1., 2.]))\\narray = array.write(1, tf.constant([3., 10.]))\\narray = array.write(2, tf.constant([5., 7.]))\\ntensor1 = array.read(1)  # => returns (and zeros out!) tf.constant([3., 10.])\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,345.850,431.997,369.543 'By default, reading an item also replaces it with a tensor of the same shape but full of\\nzeros. You can set clear_after_read to False if you don’t want this.\\n'>\n",
            "<LTTextBoxHorizontal(5) 136.790,284.758,396.001,328.918 'When  you  write  to  the  array,  you  must  assign  the  output  back  to\\nthe  array,  as  shown  in  this  code  example.  If  you  don’t,  although\\nyour code will work fine in eager mode, it will break in graph mode\\n(these modes are discussed in Chapter 12).\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,200.077,432.003,264.316 'By  default,  a  TensorArray  has  a  fixed  size  that  is  set  upon  creation.  Alternatively,\\nyou can set size=0 and dynamic_size=True to let the array grow automatically when\\nneeded. However, this will hinder performance, so if you know the size in advance,\\nit’s better to use a fixed-size array. You must also specify the dtype, and all elements\\nmust have the same shape as the first one written to the array.\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,180.883,403.296,192.943 'You can stack all the items into a regular tensor by calling the stack() method:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.000,124.597,288.750,173.897 '>>> array.stack()\\n<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\\narray([[1., 2.],\\n       [0., 0.],\\n       [5., 7.]], dtype=float32)>\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.002,40.500,84.440,49.500 '796 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.018,40.500,207.281,49.500 '|  Appendix C: Special Data Structures\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,282.404,132.680,329.713 matrix=[49.68,0.00,0.00,47.31, (83.00,282.40)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,517.540,432.003,607.973 'Sets\\nTensorFlow  supports  sets  of  integers  or  strings  (but  not  floats).  It  represents  sets\\nusing regular tensors. For example, the set {1, 5, 9} is just represented as the tensor\\n[[1,  5,  9]].  Note  that  the  tensor  must  have  at  least  two  dimensions,  and  the  sets\\nmust be in the last dimension. For example, [[1, 5, 9], [2, 5, 11]] is a tensor\\nholding two independent sets: {1, 5, 9} and {2, 5, 11}.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,472.553,432.000,510.406 'The tf.sets package contains several functions to manipulate sets. For example, let’s\\ncreate  two  sets  and  compute  their  union  (the  result  is  a  sparse  tensor,  so  we  call\\nto_dense() to display it):\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,395.867,390.750,465.567 '>>> a = tf.constant([[1, 5, 9]])\\n>>> b = tf.constant([[5, 6, 9, 11]])\\n>>> u = tf.sets.union(a, b)\\n>>> u\\n<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x132b60d30>\\n>>> tf.sparse.to_dense(u)\\n<tf.Tensor: [...], numpy=array([[ 1,  5,  6,  9, 11]], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,363.953,431.999,387.053 'You can also compute the union of multiple pairs of sets simultaneously. If some sets\\nare shorter than others, you must pad them with a padding value, such as 0:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,297.466,373.754,356.967 '>>> a = tf.constant([[1, 5, 9], [10, 0, 0]])\\n>>> b = tf.constant([[5, 6, 9, 11], [13, 0, 0, 0]])\\n>>> u = tf.sets.union(a, b)\\n>>> tf.sparse.to_dense(u)\\n<tf.Tensor: [...] numpy=array([[ 1,  5,  6,  9, 11],\\n                               [ 0, 10, 13,  0,  0]], dtype=int32)>\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,264.960,432.005,288.653 'If  you  prefer  to  use  a  different  padding  value,  such  as  –1,  then  you  must  set\\ndefault_value=-1 (or your preferred value) when calling to_dense().\\n'>\n",
            "<LTTextBoxHorizontal(6) 136.789,226.366,395.999,248.911 'The  default  default_value  is  0,  so  when  dealing  with  string  sets,\\nyou must set this parameter (e.g., to an empty string).\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,132.973,432.005,183.426 'Other  functions  available  in  tf.sets  include  difference(),  intersection(),  and\\nsize(), which are self-explanatory. If you want to check whether or not a set contains\\nsome given values, you can compute the intersection of that set and the values. If you\\nwant to add some values to a set, you can compute the union of the set and the values.\\n'>\n",
            "<LTTextBoxHorizontal(8) 333.107,40.500,402.515,49.500 'Special Data Structures \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.093,40.500,413.405,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.983,40.500,431.999,49.500 '797\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,201.514,132.680,248.823 matrix=[49.68,0.00,0.00,47.31, (83.00,201.51)]>\n",
            "<LTTextBoxHorizontal(0) 71.995,494.119,432.004,607.973 'Queues\\nA  queue  is  a  data  structure  to  which  you  can  push  data  records,  and  later  pull\\nthem out. TensorFlow implements several types of queues in the tf.queue package.\\nThey used to be very important when implementing efficient data loading and pre‐\\nprocessing pipelines, but the tf.data API has essentially rendered them useless (except\\nperhaps  in  some  rare  cases)  because  it  is  much  simpler  to  use  and  provides  all  the\\ntools you need to build efficient pipelines. For the sake of completeness, though, let’s\\ntake a quick look at them.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,399.919,432.004,486.019 'The  simplest  kind  of  queue  is  the  first-in,  first-out  (FIFO)  queue.  To  build  it,  you\\nneed  to  specify  the  maximum  number  of  records  it  can  contain.  Moreover,  each\\nrecord is a tuple of tensors, so you must specify the type of each tensor, and option‐\\nally their shapes. For example, the following code example creates a FIFO queue with\\na  maximum  of  three  records,  each  containing  a  tuple  with  a  32-bit  integer  and  a\\nstring. Then it pushes two records to it, looks at the size (which is 2 at this point), and\\npulls a record out:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,313.033,382.247,392.933 '>>> q = tf.queue.FIFOQueue(3, [tf.int32, tf.string], shapes=[(), ()])\\n>>> q.enqueue([10, b\"windy\"])\\n>>> q.enqueue([15, b\"sunny\"])\\n>>> q.size()\\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\\n>>> q.dequeue()\\n[<tf.Tensor: shape=(), dtype=int32, numpy=10>,\\n <tf.Tensor: shape=(), dtype=string, numpy=b\\'windy\\'>]\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.996,267.333,432.004,304.219 'It  is  also  possible  to  enqueue  and  dequeue  multiple  records  at  once  using\\nenqueue_many() and dequeue_many() (to use dequeue_many(), you must specify the\\nshapes argument when you create the queue, as we did previously):\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.004,221.246,429.004,260.346 \">>> q.enqueue_many([[13, 16], [b'cloudy', b'rainy']])\\n>>> q.dequeue_many(3)\\n[<tf.Tensor: [...], numpy=array([15, 13, 16], dtype=int32)>,\\n <tf.Tensor: [...], numpy=array([b'sunny', b'cloudy', b'rainy'], dtype=object)>]\\n\">\n",
            "<LTTextBoxHorizontal(5) 72.004,201.933,185.079,212.433 'Other queue types include:\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.002,183.824,151.802,193.799 'PaddingFIFOQueue\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.998,143.346,432.004,180.605 'Same  as  FIFOQueue,  but  its  dequeue_many()  method  supports  dequeueing  mul‐\\ntiple  records  of  different  shapes.  It  automatically  pads  the  shortest  records  to\\nensure all the records in the batch have the same shape.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,125.237,136.833,135.212 'PriorityQueue\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.995,72.753,432.003,121.053 'A  queue  that  dequeues  records  in  a  prioritized  order.  The  priority  must  be  a\\n64-bit integer included as the first element of each record. Surprisingly, records\\nwith a lower priority will be dequeued first. Records with the same priority will\\nbe dequeued in FIFO order.\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.004,40.500,84.442,49.500 '798 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.020,40.500,207.283,49.500 '|  Appendix C: Special Data Structures\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.528,161.775,606.503 'RandomShuffleQueue\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.997,569.243,431.997,592.343 'A  queue  whose  records  are  dequeued  in  random  order.  This  was  useful  to\\nimplement a shuffle buffer before tf.data existed.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,511.657,432.005,562.109 'If  a  queue  is  already  full  and  you  try  to  enqueue  another  record,  the  enqueue*()\\nmethod will freeze until a record is dequeued by another thread. Similarly, if a queue\\nis  empty  and  you  try  to  dequeue  a  record,  the  dequeue*()  method  will  freeze  until\\nrecords are pushed to the queue by another thread.\\n'>\n",
            "<LTTextBoxHorizontal(3) 333.108,40.500,402.516,49.500 'Special Data Structures \\n'>\n",
            "<LTTextBoxHorizontal(4) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(5) 420.984,40.500,432.000,49.500 '799\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 274.878,533.502,431.998,582.331 'APPENDIX D\\nTensorFlow Graphs\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,393.868,431.996,416.969 'In  this  appendix,  we  will  explore  the  graphs  generated  by  TF  functions  (see\\nChapter 12).\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,330.675,432.001,381.529 'TF Functions and Concrete Functions\\nTF  functions  are  polymorphic,  meaning  they  support  inputs  of  different  types  (and\\nshapes). For example, consider the following tf_cube() function:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.998,294.788,161.248,323.688 '@tf.function\\ndef tf_cube(x):\\n    return x ** 3\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,108.115,432.005,285.975 'Every time you call a TF function with a new combination of input types or shapes,\\nit  generates  a  new  concrete  function,  with  its  own  graph  specialized  for  this  partic‐\\nular  combination.  Such  a  combination  of  argument  types  and  shapes  is  called  an\\ninput  signature.  If  you  call  the  TF  function  with  an  input  signature  it  has  already\\nseen  before,  it  will  reuse  the  concrete  function  it  generated  earlier.  For  example,  if\\nyou call tf_cube(tf.constant(3.0)), the TF function will reuse the same concrete\\nfunction  it  used  for  tf_cube(tf.constant(2.0))  (for  float32  scalar  tensors).  But  it\\nwill  generate  a  new  concrete  function  if  you  call  tf_cube(tf.constant([2.0]))  or\\ntf_cube(tf.constant([3.0])) (for float32 tensors of shape [1]), and yet another for\\ntf_cube(tf.constant([[1.0,  2.0],  [3.0,  4.0]]))  (for  float32  tensors  of  shape\\n[2,  2]).  You  can  get  the  concrete  function  for  a  particular  combination  of  inputs  by\\ncalling  the  TF  function’s  get_concrete_function()  method.  It  can  then  be  called\\nlike a regular function, but it will only support one input signature (in this example,\\nfloat32 scalar tensors):\\n'>\n",
            "<LTTextBoxHorizontal(5) 420.981,40.500,431.997,49.500 '801\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,561.816,432.000,561.816>\n",
            "<LTTextBoxHorizontal(0) 89.000,557.350,390.750,606.650 '>>> concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\\n>>> concrete_function\\n<ConcreteFunction tf_cube(x) at 0x7F84411F4250>\\n>>> concrete_function(tf.constant(2.0))\\n<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,368.304,432.005,549.503 'Figure  D-1  shows  the  tf_cube()  TF  function,  after  we  called  tf_cube(2)  and\\ntf_cube(tf.constant(2.0)):  two  concrete  functions  were  generated,  one  for  each\\nsignature,  each  with  its  own  optimized  function  graph  (FuncGraph)  and  its  own\\nfunction  definition  (FunctionDef).  A  function  definition  points  to  the  parts  of  the\\ngraph  that  correspond  to  the  function’s  inputs  and  outputs.  In  each  FuncGraph,  the\\nnodes  (ovals)  represent  operations  (e.g.,  power,  constants,  or  placeholders  for  argu‐\\nments  like  x),  while  the  edges  (the  solid  arrows  between  the  operations)  represent\\nthe  tensors  that  will  flow  through  the  graph.  The  concrete  function  on  the  left  is\\nspecialized  for  x=2,  so  TensorFlow  managed  to  simplify  it  to  just  output  8  all  the\\ntime  (note  that  the  function  definition  does  not  even  have  an  input).  The  concrete\\nfunction  on  the  right  is  specialized  for  float32  scalar  tensors,  and  it  could  not  be\\nsimplified. If we call tf_cube(tf.constant(5.0)), the second concrete function will\\nbe  called,  the  placeholder  operation  for  x  will  output  5.0,  then  the  power  operation\\nwill compute 5.0 ** 3, so the output will be 125.0.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,163.167,395.197,188.419 'Figure D-1. The tf_cube() TF function, with its ConcreteFunctions and their\\nFuncGraphs\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.995,74.974,432.004,149.067 'The tensors in these graphs are symbolic tensors, meaning they don’t have an actual\\nvalue,  just  a  data  type,  a  shape,  and  a  name.  They  represent  the  future  tensors  that\\nwill flow through the graph once an actual value is fed to the placeholder x and the\\ngraph is executed. Symbolic tensors make it possible to specify ahead of time how to\\nconnect operations, and they also allow TensorFlow to recursively infer the data types\\nand shapes of all tensors, given the data types and shapes of their inputs.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,40.500,84.437,49.500 '802 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.015,40.500,196.127,49.500 '|  Appendix D: TensorFlow Graphs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,362.042,432.500,362.042>\n",
            "<LTLine 432.375,193.017,432.375,362.167>\n",
            "<LTLine 72.000,193.142,432.500,193.142>\n",
            "<LTLine 72.125,193.017,72.125,362.167>\n",
            "<LTFigure(I1) 111.730,198.997,392.770,356.917 matrix=[281.04,0.00,0.00,157.92, (111.73,199.00)]>\n",
            "<LTTextBoxHorizontal(0) 71.999,582.437,432.002,605.537 'Now let’s continue to peek under the hood, and see how to access function definitions\\nand function graphs and how to explore a graph’s operations and tensors.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.992,518.650,432.003,570.097 'Exploring Function Definitions and Graphs\\nYou  can  access  a  concrete  function’s  computation  graph  using  the  graph  attribute,\\nand get the list of its operations by calling the graph’s get_operations() method:\\n'>\n",
            "<LTTextBoxHorizontal(2) 88.997,431.763,377.997,511.663 \">>> concrete_function.graph\\n<tensorflow.python.framework.func_graph.FuncGraph at 0x7f84411f4790>\\n>>> ops = concrete_function.graph.get_operations()\\n>>> ops\\n[<tf.Operation 'x' type=Placeholder>,\\n <tf.Operation 'pow/y' type=Const>,\\n <tf.Operation 'pow' type=Pow>,\\n <tf.Operation 'Identity' type=Identity>]\\n\">\n",
            "<LTTextBoxHorizontal(3) 71.995,334.477,432.004,423.916 'In  this  example,  the  first  operation  represents  the  input  argument  x  (it  is  called  a\\nplaceholder),  the  second  “operation”  represents  the  constant  3,  the  third  operation\\nrepresents the power operation (**), and the final operation represents the output of\\nthis function (it is an identity operation, meaning it will do nothing more than copy\\nthe  output  of  the  power  operation1).  Each  operation  has  a  list  of  input  and  output\\ntensors that you can easily access using the operation’s inputs and outputs attributes.\\nFor example, let’s get the list of inputs and outputs of the power operation:\\n'>\n",
            "<LTTextBoxHorizontal(4) 88.998,267.990,284.498,327.490 \">>> pow_op = ops[2]\\n>>> list(pow_op.inputs)\\n[<tf.Tensor 'x:0' shape=() dtype=float32>,\\n <tf.Tensor 'pow/y:0' shape=() dtype=float32>]\\n>>> pow_op.outputs\\n[<tf.Tensor 'pow:0' shape=() dtype=float32>]\\n\">\n",
            "<LTTextBoxHorizontal(5) 72.004,248.676,296.389,259.176 'This computation graph is represented in Figure D-2.\\n'>\n",
            "<LTTextBoxHorizontal(6) 73.140,78.954,423.056,86.954 '1 You can safely ignore it—it is only here for technical reasons, to ensure that TF functions don’t leak internal\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.000,68.954,113.792,76.954 'structures.\\n'>\n",
            "<LTTextBoxHorizontal(8) 345.025,40.500,402.517,49.500 'TensorFlow Graphs \\n'>\n",
            "<LTTextBoxHorizontal(9) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(10) 420.985,40.500,432.001,49.500 '803\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,94.900,162.000,94.900>\n",
            "<LTTextBoxHorizontal(0) 71.997,443.603,254.697,454.103 'Figure D-2. Example of a computation graph\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,326.650,432.003,429.503 'Note  that  each  operation  has  a  name.  It  defaults  to  the  name  of  the  operation  (e.g.,\\n\"pow\"),  but  you  can  define  it  manually  when  calling  the  operation  (e.g.,  tf.pow(x,\\n3,  name=\"other_name\")).  If  a  name  already  exists,  TensorFlow  automatically  adds\\na  unique  index  (e.g.,  \"pow_1\",  \"pow_2\",  etc.).  Each  tensor  also  has  a  unique  name:\\nit  is  always  the  name  of  the  operation  that  outputs  this  tensor,  plus  :0  if  it  is  the\\noperation’s  first  output,  or  :1  if  it  is  the  second  output,  and  so  on.  You  can  fetch\\nan  operation  or  a  tensor  by  name  using  the  graph’s  get_operation_by_name()  or\\nget_tensor_by_name() methods:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.002,280.564,344.002,319.664 \">>> concrete_function.graph.get_operation_by_name('x')\\n<tf.Operation 'x' type=Placeholder>\\n>>> concrete_function.graph.get_tensor_by_name('Identity:0')\\n<tf.Tensor 'Identity:0' shape=() dtype=float32>\\n\">\n",
            "<LTTextBoxHorizontal(3) 71.995,223.450,432.001,271.750 'The concrete function also contains the function definition (represented as a protocol\\nbuffer2),  which  includes  the  function’s  signature.  This  signature  allows  the  concrete\\nfunction to know which placeholders to feed with the input values, and which tensors\\nto return:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.001,116.164,276.001,216.464 '>>> concrete_function.function_def.signature\\nname: \"__inference_tf_cube_3515903\"\\ninput_arg {\\n  name: \"x\"\\n  type: DT_FLOAT\\n}\\noutput_arg {\\n  name: \"identity\"\\n  type: DT_FLOAT\\n}\\n'>\n",
            "<LTTextBoxHorizontal(5) 73.140,68.954,238.600,76.954 '2 A popular binary format discussed in Chapter 13.\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.002,40.500,84.440,49.500 '804 \\n'>\n",
            "<LTTextBoxHorizontal(7) 92.018,40.500,196.130,49.500 '|  Appendix D: TensorFlow Graphs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,607.375,432.500,607.375>\n",
            "<LTLine 432.375,459.667,432.375,607.500>\n",
            "<LTLine 72.000,459.792,432.500,459.792>\n",
            "<LTLine 72.125,459.667,72.125,607.500>\n",
            "<LTLine 72.000,84.900,162.000,84.900>\n",
            "<LTFigure(I1) 108.250,465.647,396.250,602.250 matrix=[288.00,0.00,0.00,136.60, (108.25,465.65)]>\n",
            "<LTTextBoxHorizontal(0) 72.000,595.037,230.382,605.537 'Now let’s look more closely at tracing.\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.002,544.443,292.533,582.697 'A Closer Look at Tracing\\nLet’s tweak the tf_cube() function to print its input:\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.001,498.356,178.251,537.456 '@tf.function\\ndef tf_cube(x):\\n    print(f\"x = {x}\")\\n    return x ** 3\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.001,479.043,137.248,489.543 'Now let’s call it:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,432.956,288.752,472.056 '>>> result = tf_cube(tf.constant(2.0))\\nx = Tensor(\"x:0\", shape=(), dtype=float32)\\n>>> result\\n<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,335.077,432.004,425.109 'The result looks good, but look at what was printed: x is a symbolic tensor! It has a\\nshape and a data type, but no value. Plus it has a name (\"x:0\"). This is because the\\nprint() function is not a TensorFlow operation, so it will only run when the Python\\nfunction  is  traced,  which  happens  in  graph  mode,  with  arguments  replaced  with\\nsymbolic  tensors  (same  type  and  shape,  but  no  value).  Since  the  print()  function\\nwas not captured into the graph, the next times we call tf_cube() with float32 scalar\\ntensors, nothing is printed:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.001,309.390,250.501,328.090 '>>> result = tf_cube(tf.constant(3.0))\\n>>> result = tf_cube(tf.constant(4.0))\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.998,276.290,432.001,301.543 'But  if  we  call  tf_cube()  with  a  tensor  of  a  different  type  or  shape,  or  with  a  new\\nPython value, the function will be traced again, so the print() function will be called:\\n'>\n",
            "<LTTextBoxHorizontal(8) 89.003,179.204,429.003,269.304 '>>> result = tf_cube(2)  # new Python value: trace!\\nx = 2\\n>>> result = tf_cube(3)  # new Python value: trace!\\nx = 3\\n>>> result = tf_cube(tf.constant([[1., 2.]]))  # new shape: trace!\\nx = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\\n>>> result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))  # new shape: trace!\\nx = Tensor(\"x:0\", shape=(None, 2), dtype=float32)\\n>>> result = tf_cube(tf.constant([[7., 8.], [9., 10.]]))  # same shape: no trace\\n'>\n",
            "<LTTextBoxHorizontal(9) 136.789,105.878,396.000,161.558 'If your function has Python side effects (e.g., it saves some logs to\\ndisk),  be  aware  that  this  code  will  only  run  when  the  function  is\\ntraced  (i.e.,  every  time  the  TF  function  is  called  with  a  new  input\\nsignature).  It’s  best  to  assume  that  the  function  may  be  traced  (or\\nnot) any time the TF function is called.\\n'>\n",
            "<LTTextBoxHorizontal(10) 345.024,40.500,402.516,49.500 'TensorFlow Graphs \\n'>\n",
            "<LTTextBoxHorizontal(11) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 420.984,40.500,432.000,49.500 '805\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,115.044,132.680,162.353 matrix=[49.68,0.00,0.00,47.31, (83.00,115.04)]>\n",
            "<LTTextBoxHorizontal(0) 71.996,531.443,432.005,605.537 'In some cases, you may want to restrict a TF function to a specific input signature.\\nFor  example,  suppose  you  know  that  you  will  only  ever  call  a  TF  function  with\\nbatches  of  28  ×  28–pixel  images,  but  the  batches  will  have  very  different  sizes.  You\\nmay  not  want  TensorFlow  to  generate  a  different  concrete  function  for  each  batch\\nsize, or count on it to figure out on its own when to use None. In this case, you can\\nspecify the input signature like this:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,495.557,399.250,524.457 '@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\\ndef shrink(images):\\n    return images[:, ::2, ::2]  # drop half the rows and columns\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.995,463.643,431.998,486.743 'This TF function will accept any float32 tensor of shape [*, 28, 28], and it will reuse\\nthe same concrete function every time:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.997,417.557,424.747,456.657 'img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\\nimg_batch_2 = tf.random.uniform(shape=[50, 28, 28])\\npreprocessed_images = shrink(img_batch_1)  # works fine, traces the function\\npreprocessed_images = shrink(img_batch_2)  # works fine, same concrete function\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.996,385.643,431.999,408.743 'However,  if  you  try  to  call  this  TF  function  with  a  Python  value,  or  a  tensor  of  an\\nunexpected data type or shape, you will get an exception:\\n'>\n",
            "<LTTextBoxHorizontal(5) 89.002,359.957,412.002,378.657 'img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\\npreprocessed_images = shrink(img_batch_3)  # ValueError! Incompatible inputs\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.991,296.050,431.998,346.903 'Using AutoGraph to Capture Control Flow\\nIf  your  function  contains  a  simple  for  loop,  what  do  you  expect  will  happen?  For\\nexample, let’s write a function that will add 10 to its input, by just adding 1 10 times:\\n'>\n",
            "<LTTextBoxHorizontal(7) 88.997,239.763,186.747,289.063 '@tf.function\\ndef add_10(x):\\n    for i in range(10):\\n        x += 1\\n    return x\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,207.850,431.999,230.950 'It works fine, but when we look at its graph, we find that it does not contain a loop: it\\njust contains 10 addition operations!\\n'>\n",
            "<LTTextBoxHorizontal(9) 88.999,100.563,390.749,200.863 \">>> add_10(tf.constant(0))\\n<tf.Tensor: shape=(), dtype=int32, numpy=15>\\n>>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\\n[<tf.Operation 'x' type=Placeholder>, [...],\\n <tf.Operation 'add' type=AddV2>, [...],\\n <tf.Operation 'add_1' type=AddV2>, [...],\\n <tf.Operation 'add_2' type=AddV2>, [...],\\n [...]\\n <tf.Operation 'add_9' type=AddV2>, [...],\\n <tf.Operation 'Identity' type=Identity>]\\n\">\n",
            "<LTTextBoxHorizontal(10) 71.999,40.500,84.437,49.500 '806 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.015,40.500,196.127,49.500 '|  Appendix D: TensorFlow Graphs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,556.050,432.002,605.537 'This actually makes sense: when the function got traced, the loop ran 10 times, so the\\nx += 1 operation was run 10 times, and since it was in graph mode, it recorded this\\noperation 10 times in the graph. You can think of this for loop as a “static” loop that\\ngets unrolled when the graph is created.\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.995,433.684,432.005,547.950 'If you want the graph to contain a “dynamic” loop instead (i.e., one that runs when\\nthe graph is executed), you can create one manually using the tf.while_loop() oper‐\\nation, but it is not very intuitive (see the “Using AutoGraph to Capture Control Flow”\\nsection  of  the  Chapter  12  notebook  for  an  example).  Instead,  it  is  much  simpler  to\\nuse TensorFlow’s AutoGraph feature, discussed in Chapter 12. AutoGraph is actually\\nactivated  by  default  (if  you  ever  need  to  turn  it  off,  you  can  pass  autograph=False\\nto tf.function()). So if it is on, why didn’t it capture the for loop in the add_10()\\nfunction?  It  only  captures  for  loops  that  iterate  over  tensors  of  tf.data.Dataset\\nobjects, so you should use tf.range(), not range(). This is to give you the choice:\\n'>\n",
            "<LTTextBoxHorizontal(2) 80.651,385.290,432.003,422.550 '• If you use range(), the for loop will be static, meaning it will only be executed\\n•\\nwhen the function is traced. The loop will be “unrolled” into a set of operations\\nfor each iteration, as we saw.\\n'>\n",
            "<LTTextBoxHorizontal(3) 80.654,368.097,432.002,380.156 '• If you use tf.range(), the loop will be dynamic, meaning that it will be included\\n•\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.996,355.497,312.816,365.997 'in the graph itself (but it will not run during tracing).\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,319.111,432.002,344.363 'Let’s look at the graph that gets generated if we just replace range() with tf.range()\\nin the add_10() function:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.003,283.224,390.753,312.124 \">>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\\n[<tf.Operation 'x' type=Placeholder>, [...],\\n <tf.Operation 'while' type=StatelessWhile>, [...]]\\n\">\n",
            "<LTTextBoxHorizontal(7) 71.997,250.124,431.915,275.377 'As you can see, the graph now contains a While loop operation, as if we had called the\\ntf.while_loop() function.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.995,124.524,432.004,237.784 'Handling Variables and Other Resources in TF Functions\\nIn  TensorFlow,  variables  and  other  stateful  objects,  such  as  queues  or  datasets,  are\\ncalled resources. TF functions treat them with special care: any operation that reads or\\nupdates a resource is considered stateful, and TF functions ensure that stateful opera‐\\ntions are executed in the order they appear (as opposed to stateless operations, which\\nmay be run in parallel, so their order of execution is not guaranteed). Moreover, when\\nyou pass a resource as an argument to a TF function, it gets passed by reference, so\\nthe function may modify it. For example:\\n'>\n",
            "<LTTextBoxHorizontal(9) 345.022,40.500,402.514,49.500 'TensorFlow Graphs \\n'>\n",
            "<LTTextBoxHorizontal(10) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 420.982,40.500,431.998,49.500 '807\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 89.000,598.150,191.000,606.650 'counter = tf.Variable(0)\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.000,557.350,225.000,586.250 '@tf.function\\ndef increment(counter, c=1):\\n    return counter.assign_add(c)\\n'>\n",
            "<LTTextBoxHorizontal(2) 89.000,526.750,288.750,545.450 'increment(counter)  # counter is now equal to 1\\nincrement(counter)  # counter is now equal to 2\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.999,507.437,409.647,517.937 'If you peek at the function definition, the first argument is marked as a resource:\\n'>\n",
            "<LTTextBoxHorizontal(4) 89.002,461.350,395.002,500.450 '>>> function_def = increment.get_concrete_function(counter).function_def\\n>>> function_def.signature.input_arg[0]\\nname: \"counter\"\\ntype: DT_RESOURCE\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.999,428.843,432.002,453.502 'It  is  also  possible  to  use  a  tf.Variable  defined  outside  of  the  function,  without\\nexplicitly passing it as an argument:\\n'>\n",
            "<LTTextBoxHorizontal(6) 89.003,413.357,191.003,421.857 'counter = tf.Variable(0)\\n'>\n",
            "<LTTextBoxHorizontal(7) 89.003,372.557,225.003,401.457 '@tf.function\\ndef increment(c=1):\\n    return counter.assign_add(c)\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,302.250,432.005,363.743 'The  TF  function  will  treat  this  as  an  implicit  first  argument,  so  it  will  actually  end\\nup  with  the  same  signature  (except  for  the  name  of  the  argument).  However,  using\\nglobal  variables  can  quickly  become  messy,  so  you  should  generally  wrap  variables\\n(and other resources) inside classes. The good news is @tf.function works fine with\\nmethods too:\\n'>\n",
            "<LTTextBoxHorizontal(9) 89.000,266.363,246.250,295.263 'class Counter:\\n    def __init__(self):\\n        self.counter = tf.Variable(0)\\n'>\n",
            "<LTTextBoxHorizontal(10) 89.000,225.563,263.250,254.463 '    @tf.function\\n    def increment(self, c=1):\\n        return self.counter.assign_add(c)\\n'>\n",
            "<LTTextBoxHorizontal(11) 136.787,162.131,395.998,208.801 'Do not use =, +=, -=, or any other Python assignment operator with\\nTF  variables.  Instead,  you  must  use  the  assign(),  assign_add(),\\nor  assign_sub()  methods.  If  you  try  to  use  a  Python  assignment\\noperator, you will get an exception when you call the method.\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.997,119.250,432.000,142.350 'A good example of this object-oriented approach is, of course, Keras. Let’s see how to\\nuse TF functions with Keras.\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.997,40.500,84.435,49.500 '808 \\n'>\n",
            "<LTTextBoxHorizontal(14) 92.013,40.500,196.125,49.500 '|  Appendix D: TensorFlow Graphs\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTFigure(I1) 83.000,161.404,132.680,208.713 matrix=[49.68,0.00,0.00,47.31, (83.00,161.40)]>\n",
            "<LTTextBoxHorizontal(0) 71.997,506.719,432.005,607.973 'Using TF Functions with Keras (or Not)\\nBy default, any custom function, layer, or model you use with Keras will automatically\\nbe  converted  to  a  TF  function;  you  do  not  need  to  do  anything  at  all!  However,  in\\nsome  cases  you  may  want  to  deactivate  this  automatic  conversion—for  example,  if\\nyour custom code cannot be turned into a TF function, or if you just want to debug\\nyour  code  (which  is  much  easier  in  eager  mode).  To  do  this,  you  can  simply  pass\\ndynamic=True when creating the model or any of its layers:\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.002,491.233,212.252,499.733 'model = MyModel(dynamic=True)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.001,458.726,432.004,482.419 'If your custom model or layer will always be dynamic, you can instead call the base\\nclass’s constructor with dynamic=True:\\n'>\n",
            "<LTTextBoxHorizontal(3) 88.999,412.640,292.999,451.740 'class MyDense(tf.keras.layers.Layer):\\n    def __init__(self, units, **kwargs):\\n        super().__init__(dynamic=True, **kwargs)\\n        [...]\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,392.733,421.848,404.792 'Alternatively, you can pass run_eagerly=True when calling the compile() method:\\n'>\n",
            "<LTTextBoxHorizontal(5) 88.997,367.046,356.747,385.746 'model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae],\\n              run_eagerly=True)\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.995,309.933,432.004,358.233 'Now  you  know  how  TF  functions  handle  polymorphism  (with  multiple  concrete\\nfunctions),  how  graphs  are  automatically  generated  using  AutoGraph  and  tracing,\\nwhat graphs look like, how to explore their symbolic operations and tensors, how to\\nhandle variables and resources, and how to use TF functions with Keras.\\n'>\n",
            "<LTTextBoxHorizontal(7) 345.022,40.500,402.514,49.500 'TensorFlow Graphs \\n'>\n",
            "<LTTextBoxHorizontal(8) 410.092,40.500,413.404,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(9) 420.982,40.500,431.998,49.500 '809\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 385.358,561.061,431.990,586.280 'Index\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.994,303.708,214.082,446.841 'Symbols\\n@ operator (matrix multiplication), 135\\nβ (momentum), 380\\nγ (gamma) value, 182\\nε (tolerance), 144, 183\\nε greedy policy, 706\\nε neighborhood, 279\\nε sensitive, 184\\nχ² test, 202\\nℓ₀ norm, 45\\nℓ₁ norm, 45\\nℓ₂ norm, 45\\nℓₖ norm, 45\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,192.008,237.249,291.941 'A\\nA/B experiments, 721\\naccelerated k-means, 268\\naccelerated linear algebra (XLA), 434\\naccuracy performance measure, 4, 107\\nACF (autocorrelation function), 551\\naction advantage, reinforcement learning, 694\\naction potentials (APs), 301\\nactions, in reinforcement learning, 684,\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,181.208,113.364,190.208 '693-694\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,170.408,193.995,179.408 'activation functions, 306, 312-313\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,73.208,210.339,168.608 'for Conv2D layer, 490\\ncustom models, 415\\nELU, 363-366\\nGELU, 365-367\\nhyperbolic tangent (htan), 312, 565\\nhyperparameters, 352\\ninitialization parameters, 360\\nleakyReLU, 361-363\\nMish, 366\\n'>\n",
            "<LTTextBoxHorizontal(6) 259.500,133.408,424.686,444.808 'PReLU, 362\\nReLU (see ReLU)\\nRReLU, 362\\nSELU, 364, 397\\nsigmoid, 164, 312, 358, 651\\nSiLU, 366\\nsoftmax, 170, 315, 320, 600\\nsoftplus, 314\\nSwish, 366\\nactive learning, 278\\nactor-critic, 717\\nactual class, 109\\nactual versus estimated probabilities, 118\\nAdaBoost, 222-226\\nAdaGrad, 382\\nAdam optimization, 384, 394\\nAdaMax, 385\\nAdamW, 386, 394\\nadaptive boosting (AdaBoost), 222-226\\nadaptive instance normalization (AdaIN), 671\\nadaptive learning rate algorithms, 382-387\\nadaptive moment estimation (Adam), 384\\nadditive attention, 606\\nadvantage actor-critic (A2C), 717\\nadversarial learning, 534, 636\\naffine transformations, 671\\naffinity function, 261\\naffinity propagation, 283\\nagents, reinforcement learning, 16, 684, 699,\\n'>\n",
            "<LTTextBoxHorizontal(7) 271.740,122.608,284.700,131.608 '705\\n'>\n",
            "<LTTextBoxHorizontal(8) 259.500,79.408,404.661,120.808 'agglomerative clustering, 282\\nAkaike information criterion (AIC), 289\\nAlexNet, 499\\nalgorithms, preparing data for, 67-88\\n'>\n",
            "<LTTextBoxHorizontal(9) 420.985,40.500,432.001,49.500 '811\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTLine 72.000,589.375,432.000,589.375>\n",
            "<LTTextBoxHorizontal(0) 72.000,424.017,226.647,605.817 'alignment model, 606\\nAllReduce algorithm, 760\\nalpha dropout, 397\\nAlphaGo, 17, 683, 716\\nanchor priors, 528\\nANNs (see artificial neural networks)\\nanomaly detection, 8, 13\\nclustering for, 262\\nGMM, 288-289\\nisolation forest, 293\\nAP (average precision), 529\\nAPs (action potentials), 301\\narea under the curve (AUC), 116\\nargmax(), 171\\nARIMA model, 550-551\\nARMA model family, 549-551\\nartificial neural networks (ANNs), 299-353\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,391.617,217.224,422.217 'autoencoders (see autoencoders)\\nbackpropagation, 309-312\\nbiological neurons as background to,\\n'>\n",
            "<LTTextBoxHorizontal(2) 96.480,380.817,125.604,389.817 '301-302\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,316.017,242.514,379.017 'evolution of, 300-301\\nhyperparameter fine-tuning, 344-353\\nimplementing MLPs with Keras, 317-344\\nlogical computations with neurons, 303-303\\nperceptrons (see multilayer perceptrons)\\nreinforcement learning policies, 691\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,283.617,234.981,314.217 'artificial neuron, 303\\nassociation rule learning, 14\\nassumptions, checking in model building, 37,\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,272.817,92.880,281.817 '46\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,262.017,229.626,271.017 'asynchronous advantage actor-critic (A3C),\\n'>\n",
            "<LTTextBoxHorizontal(7) 84.240,251.217,97.200,260.217 '717\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.000,229.617,242.694,249.417 'asynchronous gang scheduling, 764\\nasynchronous updates, with centralized param‐\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.240,218.817,118.332,227.817 'eters, 761\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,197.217,218.961,217.017 'à-trous convolutional layer, 533\\nattention mechanisms, 578, 604-619, 625\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.240,186.417,191.178,195.417 '(see also transformer models)\\n'>\n",
            "<LTTextBoxHorizontal(12) 72.000,175.617,130.824,184.617 'attributes, 52-55\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.000,78.417,231.768,173.817 'categorical, 71, 74\\ncombinations of, 66-67\\npreprocessed, 55\\ntarget, 55\\nunsupervised learning, 11\\nAUC (area under the curve), 116\\nautocorrelated time series, 545\\nautocorrelation function (ACF), 551\\nautodiff (automatic differentiation), 785-791\\n'>\n",
            "<LTTextBoxHorizontal(14) 72.004,40.500,84.442,49.500 '812 \\n'>\n",
            "<LTTextBoxHorizontal(15) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 102.910,40.500,119.551,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.740,553.617,401.520,605.817 'for computing gradients, 426-430\\nfinite difference approximation, 786\\nforward-mode, 787-790\\nmanual differentiation, 785\\nreverse-mode, 790-791\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.500,542.817,340.977,551.817 'autoencoders, 635-659\\n'>\n",
            "<LTTextBoxHorizontal(19) 271.740,488.817,416.487,541.017 'convolutional, 648-649\\ndenoising, 649-650\\nefficient data representations, 637-638\\novercomplete, 649\\nPCA with undercomplete linear autoen‐\\n'>\n",
            "<LTTextBoxHorizontal(20) 283.980,478.017,337.512,487.017 'coder, 639-640\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.740,424.017,382.215,476.217 'sparse, 651-654\\nstacked, 640-648\\ntraining one at a time, 646-648\\nundercomplete, 638\\nvariational, 654-658\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.500,380.817,408.243,422.217 'Autograph, 435\\nAutoGraph, 806\\nAutoML service, 349, 775\\nautoregressive integrated moving average\\n'>\n",
            "<LTTextBoxHorizontal(23) 271.740,370.017,365.160,379.017 '(ARIMA) model, 550-551\\n'>\n",
            "<LTTextBoxHorizontal(24) 259.500,348.417,428.988,368.217 'autoregressive model, 549\\nautoregressive moving average (ARMA) model\\n'>\n",
            "<LTTextBoxHorizontal(25) 271.740,337.617,327.765,346.617 'family, 549-551\\n'>\n",
            "<LTTextBoxHorizontal(26) 259.500,294.417,381.099,335.817 'auxiliary task, pretraining on, 378\\naverage absolute deviation, 45\\naverage pooling layer, 493\\naverage precision (AP), 529\\n'>\n",
            "<LTTextBoxHorizontal(27) 259.498,204.317,431.953,282.650 'B\\nbackbone, model, 504\\nbackpropagation, 309-312, 358, 429, 660\\nbackpropagation through time (BPTT), 542\\nbagging (bootstrap aggregating), 215-219\\nBahdanau attention, 606\\nbalanced iterative reducing and clustering using\\n'>\n",
            "<LTTextBoxHorizontal(28) 259.504,74.717,410.029,202.517 'hierarchies (BIRCH), 282\\nbandwidth saturation, 763-765\\nBaseEstimator, 80\\nbatch gradient descent, 142-144, 156\\nbatch learning, 18-19\\nbatch normalization (BN), 367-372, 565\\nbatch predictions, 731, 732, 739\\nBayesian Gaussian mixtures, 292\\nBayesian information criterion (BIC), 289\\nbeam search, 603-604\\nBellman optimality equation, 701\\nbias, 155\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,499.617,236.511,605.817 'bias and fairness, NLP transformers, 630\\nbias term constant, 132, 137\\nbias/variance trade-off, 155\\nBIC (Bayesian information criterion), 289\\nbidirectional recurrent layer, 601\\nbinary classifiers, 106, 164\\nbinary logarithm, 200\\nbinary trees, 198, 282\\nbiological neural networks (BNNs), 301-302\\nBIRCH (balanced iterative reducing and clus‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,488.817,189.405,497.817 'tering using hierarchies), 282\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,359.217,239.103,487.017 'black box models, 199\\nblending, in stacking, 232\\nBN (batch normalization), 367, 565\\nBNNs (biological neural networks), 301-302\\nboosting, 222-231\\nbootstrap aggregating (bagging), 215-219\\nbootstrapping, 215\\nbottleneck layers, 503\\nbounding boxes, image identification, 521-530\\nBPTT (backpropagation through time), 542\\nbucketizing a feature, 77\\nbyte pair encoding (BPE), 588\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.994,301.517,226.766,347.450 'C\\nCalifornia Housing Prices dataset, 40-46\\ncallbacks, 338-340\\nCART (Classification and Regression Tree)\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.002,117.917,211.808,299.717 'algorithm, 198, 200, 205\\ncatastrophic forgetting, 712\\ncategorical attributes, 71, 74\\ncategorical features, encoding, 466-471\\nCategoryEncoding layer, 463\\ncausal model, 563, 601\\ncentralized parameters, 760-762\\ncentroid, cluster, 262, 264, 265-268\\nchain rule, 311\\nchain-of-thought prompting, 623\\nChainClassifier, 126\\nchaining transformations, 443-445\\nchar-RNN model, 578-586\\nchatbot or personal assistant, 8\\ncheck_estimator(), 82\\nchi-squared (χ²) test, 202\\nclassification, 103-128\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.242,74.717,185.024,116.117 'application examples of, 8-9\\nbinary classifier, 106, 164\\nCNNs, 521-530\\nerror analysis, 122-125\\n'>\n",
            "<LTTextBoxHorizontal(6) 271.742,434.817,422.753,605.817 'hard margin, 176, 187\\nhard voting classifiers, 212\\nimage (see images)\\nlogistic regression (see logistic regression)\\nMLPs for, 315-316\\nMNIST dataset, 103-105\\nmulticlass, 119-121, 315-316\\nmultilabel, 125-127\\nmultioutput, 127-128\\nperformance measures, 107-119\\nand regression, 10, 127\\nsoft margin, 177-178\\nsoftmax regression, 170-173\\nSVMs (see support vector machines)\\ntext, 587-595\\nvoting classifiers, 212-215\\n'>\n",
            "<LTTextBoxHorizontal(7) 259.502,424.017,414.266,433.017 'Classification and Regression Tree (CART)\\n'>\n",
            "<LTTextBoxHorizontal(8) 271.742,413.217,358.610,422.217 'algorithm, 198, 200, 205\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.502,391.617,422.735,411.417 'clone(), 163\\nclosed-form equation/solution, 131, 134, 157,\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.742,380.817,284.702,389.817 '166\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.502,370.017,415.292,379.017 'cloud platform deployment with Vertex AI,\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.742,359.217,300.866,368.217 '732-739\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.502,348.417,390.542,357.417 'clustering algorithms, 9, 11, 260-283\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.742,218.817,429.755,346.617 'affinity propagation, 283\\nagglomerative clustering, 282\\napplications for, 261-262\\nBIRCH, 282\\nDBSCAN, 279-281\\nGMM, 283-294\\nimage segmentation, 262, 273-275\\nk-means (see k-means algorithm)\\nmean-shift, 282\\nresponsibilities of clusters for instances, 285\\nsemi-supervised learning with, 275-278\\nspectral clustering, 283\\n'>\n",
            "<LTTextBoxHorizontal(15) 259.502,110.817,425.228,217.017 'CNNs (see convolutional neural networks)\\nColab, 46-48\\ncolor channels, 486\\ncolor segmentation, images, 273\\ncolumn vectors, 133\\nColumnTransformer, 85\\ncomplex models with functional API, 329-335\\ncompound scaling, 513\\ncompressed TFRecord files, 454\\ncompression and decompression, PCA,\\n'>\n",
            "<LTTextBoxHorizontal(16) 271.742,100.017,300.866,109.017 '249-250\\n'>\n",
            "<LTTextBoxHorizontal(17) 259.502,78.417,354.011,98.217 'computation graphs, 404\\ncomputational complexity\\n'>\n",
            "<LTTextBoxHorizontal(18) 384.455,40.500,402.518,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(19) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 420.986,40.500,432.002,49.500 '813\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,370.017,227.052,605.817 'DBSCAN, 281\\ndecision trees, 200\\nGaussian mixture model, 288\\nhistogram-based gradient boosting, 230\\nk-means algorithm, 265\\nNormal equation, 138\\nand SVM classes, 183\\nconcatenative attention, 606\\nconcrete function, 801-805\\nconditional GAN, 668\\nconditional probability, 603\\nconfidence interval, 96\\nconfusion matrix (CM), 108-110, 122-125\\nConfusionMatrixDisplay, 122\\nconnectionism, 300\\nconstrained optimization, 187\\nconstraints, custom models, 415\\nconvergence rate, 144\\nconvex function, 141\\nconvex quadratic optimization, 188\\nconvolution kernels (kernels), 484, 496\\nconvolutional neural networks (CNNs),\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,316.017,242.316,368.217 '479-535\\narchitectures, 495-515\\nautoencoders, 648-649\\nclassification and localization, 521-530\\nconvolutional layers, 481-491, 531-535, 573,\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,197.217,225.756,314.217 '574-575, 591\\nevolution of, 479\\nGANs, 665-667\\nobject detection, 523-530\\nobject tracking, 530\\npooling layers, 491-495\\npretrained models from Keras, 516-518\\nResNet-34 CNN using Keras, 515\\nsemantic segmentation, 8, 273, 531-535\\nsplitting across devices, 757\\ntransfer learning pretrained models,\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,78.417,192.204,195.417 '518-521\\nU-Net, 678\\nand vision transformers, 624\\nvisual cortex architecture, 480\\nWaveNet, 574-575\\ncopy.deepcopy(), 163\\ncore instance, 279\\ncorrelation coefficient, 63-66\\ncost function, 24, 45\\nin AdaBoost, 223\\nin autoencoders, 639\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.004,40.500,84.442,49.500 '814 \\n'>\n",
            "<LTTextBoxHorizontal(5) 92.020,40.500,95.332,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(6) 102.910,40.500,119.551,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(7) 271.740,564.417,425.703,605.817 'in bounding box prediction model, 522\\nin CART training algorithm, 200, 205\\nin elastic net, 161\\nin gradient descent, 132, 138-142, 145-148,\\n'>\n",
            "<LTTextBoxHorizontal(8) 283.980,553.617,296.940,562.617 '358\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.500,445.617,404.436,551.817 'in lasso regression, 158-161\\nin Nesterov accelerated gradient, 381\\nin linear regression, 134\\nin logistic regression, 165-166\\nin momentum optimization, 379\\nin ridge regression, 156\\nin variational autoencoders, 656\\ncredit assignment problem, 693-694\\ncross entropy, 171\\ncross-validation, 35, 89-91, 97, 107-108,\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.740,434.817,300.864,443.817 '152-155\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.500,370.017,414.336,433.017 'cross_val_predict(), 108, 113, 117, 122, 232\\ncross_val_score(), 90, 107\\nCUDA library, 747\\ncuriosity-based exploration, 718\\ncurriculum learning, 719\\ncustom models and training algorithms,\\n'>\n",
            "<LTTextBoxHorizontal(12) 259.500,218.817,422.742,368.217 '412-433\\nactivation functions, 415\\nautodiff for computing gradients, 426-430\\nconstraints, 415\\ninitializers, 415\\nlayers, 419-422\\nloss functions, 412, 424-426\\nmetrics, 416-419, 424-426\\nmodels, 422-424\\nregularizers, 415\\nsaving and loading models, 413-415\\ntraining loops, 430-433\\ncustom transformers, 79-83\\ncustomer segmentation, 261\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.494,171.917,307.069,207.050 'D\\nDALL-E, 628\\ndata\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.744,85.517,408.805,170.117 'downloading, 48-49\\nefficient data representations, 637-638\\nenqueuing and dequeuing, 798\\nfinding correlations in, 63-66\\nmaking assumptions about, 37\\noverfitting (see overfitting of data)\\npreparing for ML algorithms, 67-88\\npreparing for ML models, 552-555\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,596.817,239.499,605.817 'preprocessing (see loading and preprocess‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,316.017,243.288,595.017 'ing data)\\nshuffling of, 445\\ntest data (see test set)\\ntime series (see time series data)\\ntraining data (see training set)\\nunderfitting of, 89, 151-155, 180\\nunreasonable effectiveness, 27\\nvisualizing (see visualization)\\nworking with real data, 39-40\\ndata analysis, clustering for, 261\\ndata augmentation, 129, 500, 519\\ndata cleaning, 68-71\\ndata drift, 18\\ndata mining, 7\\ndata mismatch, 35\\ndata parallelism, 759-765\\ndata pipeline, 42\\ndata snooping bias, 56\\ndata structure, 51-55, 793-799\\ndata-efficient image transformers (DeiT), 627\\nDataFrame, 68, 74, 78\\nDataquest, xxi\\nDatasets library, 475\\nDBSCAN, 279-281\\nDCGANs (deep convolutional GANS), 665-667\\nDDPM (denoising diffusion probabilistic\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,283.617,230.949,314.217 'model), 636, 674-681\\nDDQN (dueling DQN), 715\\ndecision boundaries, 167-169, 172, 198, 207,\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,272.817,97.200,281.817 '264\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,229.617,185.184,271.017 'decision function, 111, 186-189\\ndecision stumps, 226\\ndecision threshold, 111-115\\ndecision trees, 195-208, 211\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,78.417,230.841,227.817 '(see also ensemble learning)\\nbagging and pasting, 217\\nCART training algorithm, 200\\nclass probability estimates, 199\\ncomputational complexity, 200\\nand decision boundaries, 198\\nGINI impurity or entropy measures, 201\\nhigh variance with, 207\\npredictions, 197-200\\nregression tasks, 204-205\\nregularization hyperparameters, 201-203\\nsensitivity to axis orientation, 206\\ntraining and visualizing, 195-196\\nin training the model, 89-91\\n'>\n",
            "<LTTextBoxHorizontal(6) 259.500,521.217,430.788,605.817 'DecisionTreeClassifier, 195, 201, 202, 207, 220\\nDecisionTreeRegressor, 89, 195, 204, 226\\ndecision_function(), 112\\ndeconvolution layer, 532\\ndeep autoencoders (see stacked autoencoders)\\ndeep convolutional GANS (DCGANs), 665-667\\ndeep Gaussian process, 397\\ndeep learning, xv\\n'>\n",
            "<LTTextBoxHorizontal(7) 271.740,510.417,421.689,519.417 '(see also deep neural networks; reinforce‐\\n'>\n",
            "<LTTextBoxHorizontal(8) 283.980,499.617,337.692,508.617 'ment learning)\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.500,488.817,401.691,497.817 'deep neural networks (DNNs), 357-401\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.740,370.017,429.069,487.017 'CNNs (see convolutional neural networks)\\ndefault configuration, 400\\nfaster optimizers for, 379-387\\nlearning rate scheduling, 388-392\\nMLPs (see multilayer perceptrons)\\nregularization, 392-400\\nreusing pretrained layers, 373-379\\nRNNs (see recurrent neural networks)\\nand transfer learning, 16\\nunstable gradients, 358\\nvanishing and exploding gradients, 358-373\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.500,337.617,431.625,368.217 'deep neuroevolution, 349\\ndeep Q-learning, 707-716\\ndeep Q-networks (DQNs) (see Q-learning algo‐\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.740,326.817,295.527,335.817 'rithm)\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.500,262.017,423.318,325.017 'deepcopy(), 163\\nDeepMind, 17\\ndegrees of freedom, 32, 155\\nDeiT (data-efficient image transformers), 627\\ndenoising autoencoders, 649-650\\ndenoising diffusion probabilistic model\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.500,143.217,423.733,260.217 '(DDPM), 636, 674-681\\nDense layer, 305, 320, 321, 332\\ndense matrix, 76, 86\\ndensity estimation, 260, 279-281, 286\\ndensity threshold, 288\\ndepth concatenation layer, 502\\ndepthwise separable convolution layer, 509\\ndeque, 709\\ndescribe(), 53\\ndevelopment set (dev set), 35\\ndifferencing, time series forecasting, 545, 548,\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.741,132.417,284.701,141.417 '549\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.501,89.217,396.697,130.617 'diffusion models, 635, 673-681\\ndilated filter, 533\\ndilation rate, 533\\ndimensionality reduction, 12, 237-257\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.741,78.417,355.171,87.417 'approaches to, 239-242\\n'>\n",
            "<LTTextBoxHorizontal(18) 384.454,40.500,402.517,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(19) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 420.985,40.500,432.001,49.500 '815\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,586.017,235.575,605.817 'autoencoders, 635, 638-640\\nchoosing the right number of dimensions,\\n'>\n",
            "<LTTextBoxHorizontal(1) 259.496,586.017,422.702,605.817 'EllipticEnvelope, 293\\nELMo (Embeddings from Language Models),\\n'>\n",
            "<LTTextBoxHorizontal(2) 96.480,575.217,109.440,584.217 '247\\n'>\n",
            "<LTTextBoxHorizontal(3) 271.736,575.217,284.696,584.217 '593\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.240,456.417,231.651,573.417 'clustering, 261\\ncurse of dimensionality, 238-239\\nfor data visualization, 238\\ninformation loss from, 237\\nIsomap, 256\\nlinear discriminant analysis, 257\\nLLE, 254-256\\nmultidimensional scaling, 256\\nPCA (see principal component analysis)\\nrandom projection algorithm, 252-254\\nt-distributed stochastic neighbor embed‐\\n'>\n",
            "<LTTextBoxHorizontal(5) 96.480,445.617,146.889,454.617 'ding, 256, 643\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,305.217,244.125,443.817 'discount factor γ in reward system, 693\\ndiscounted rewards, 693, 696\\nDiscretization layer, 463\\ndiscriminator, GAN, 636, 660-665\\nDistilBERT model, 622, 631-633\\ndistillation, 622\\ndistribution strategies API, 765\\nDocker container, 726\\ndot product, 607\\nDota 2, 718\\nDouble DQN, 714\\ndownloading data, 48-49\\nDQNs (deep Q-networks) (see Q-learning algo‐\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,164.817,242.181,303.417 'rithm)\\ndrop(), 68\\ndropna(), 68\\nDropout, 649\\ndropout rate, 394\\ndropout regularization, 394-397\\ndual numbers, 788\\ndual problem, 189-193\\ndueling DQN (DDQN), 715\\ndummy attributes, 72\\ndying ReLU problem, 361\\ndynamic models with subclassing API, 336-337\\ndynamic programming, 701\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,96.317,242.915,153.050 'E\\neager execution (eager mode), 436\\nearly stopping regularization, 162-163, 229, 555\\nedge computing, 741\\nefficient data representations, autoencoders,\\n'>\n",
            "<LTTextBoxHorizontal(9) 71.996,74.717,124.907,94.517 '637-638\\nelastic net, 161\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.996,40.500,84.434,49.500 '816 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.012,40.500,95.324,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.902,40.500,119.543,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.496,499.617,429.389,573.417 'ELU (exponential linear unit), 363-366\\nEM (expectation-maximization), 284\\nembedded device, deploying model to, 741-744\\nembedded Reber grammars, 633\\nembedding matrix, 468, 600\\nembedding size, 599, 613\\nembeddings, 74\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.736,467.217,431.162,497.817 'encoding categorical features using, 466-471\\nreusing pretrained, 593-595\\nsentiment analysis, 589\\n'>\n",
            "<LTTextBoxHorizontal(15) 259.496,456.417,422.702,465.417 'Embeddings from Language Models (ELMo),\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.496,434.817,305.612,454.617 '593\\nencoder, 637\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.736,424.017,355.382,433.017 '(see also autoencoders)\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.496,413.217,417.401,422.217 'encoder–decoder models, 541, 578, 595-604\\n'>\n",
            "<LTTextBoxHorizontal(19) 259.496,391.617,402.038,411.417 '(see also attention mechanisms)\\nend-to-end ML project exercise, 39-100\\n'>\n",
            "<LTTextBoxHorizontal(20) 259.496,294.417,430.298,389.817 'building the model, 41-46\\ndiscovering and visualizing data, 60-67\\nfine-tuning your model, 91-97\\ngetting the data, 46-60\\npreparing data for ML algorithms, 67-88\\nreal data, advantages of working with, 39-40\\nselecting and training a model, 88-89\\nendpoint, deploying model on GCP, 738\\nensemble learning, 211-235\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.736,218.817,402.479,292.617 'bagging and pasting, 215-219\\nboosting, 222-231\\ncross-validation, 90\\nfine-tuning the system, 95\\nrandom forests (see random forests)\\nstacking, 232-235\\nvoting classifiers, 212-215\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.496,186.417,415.988,217.017 'entailment, 620\\nentropy impurity measure, 201\\nenvironments, reinforcement learning, 684,\\n'>\n",
            "<LTTextBoxHorizontal(23) 259.496,78.417,406.790,184.617 '687-691\\nepochs, 143\\nequalized learning rate, GAN, 670\\nequivariance, 493\\nerror analysis, classification, 122-125\\nestimated versus actual probabilities, 118\\nestimators, 70, 84, 214\\nEuclidean norm, 45\\nevent files, TensorBoard, 340\\nExample protobuf, 456\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,521.217,220.977,605.817 'exclusive or (XOR) problem, 307\\nexemplars, affinity propagation, 283\\nexpectation-maximization (EM), 284\\nexperience replay, 664\\nexplainability, attention mechanisms, 625\\nexplained variance ratio, 246\\nexplained variance, plotting, 247-249\\nexploding gradients, 358\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,510.417,243.738,519.417 '(see also vanishing and exploding gradients)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,488.817,233.712,508.617 'exploration policies, 703, 706\\nexploration/exploitation dilemma, reinforce‐\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,478.017,151.893,487.017 'ment learning, 692\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,424.017,234.837,476.217 'exponential linear unit (ELU), 363-366\\nexponential scheduling, 389, 392\\nexport_graphviz(), 196\\nextra-trees, random forest, 220\\nextremely randomized trees ensemble (extra-\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,413.217,121.446,422.217 'trees), 221\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.998,312.317,239.426,401.450 'F\\nface-recognition classifier, 125\\nfalse negatives, confusion matrix, 109\\nfalse positive rate (FPR) or fall-out, 115\\nfalse positives, confusion matrix, 109\\nfan-in/fan-out, 359\\nfast-MCD, 293\\nFCNs (fully convolutional networks), 525-526,\\n'>\n",
            "<LTTextBoxHorizontal(7) 84.239,301.517,97.199,310.517 '532\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.999,139.517,237.410,299.717 'feature engineering, 30, 262\\nfeature extraction, 12, 30\\nfeature maps, 485-487, 488, 508, 511\\nfeature scaling, 75-79, 141, 176\\nfeature selection, 30, 95, 159, 221\\nfeature vectors, 45, 133, 186\\nfeatures, 11\\nfederated learning, 746\\nfeedforward neural network (FNN), 309, 538\\nfetch_openml(), 104\\nfillna(), 68\\nfilters, convolutional layers, 484, 487, 497, 509\\nfirst moment (mean of gradient), 384\\nfirst-order partial derivatives (Jacobians), 386\\nfit()\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.239,96.317,200.708,137.717 'and custom transformers, 80, 84\\ndata cleaning, 69\\nversus partial_fit(), 148\\nusing only with training set, 75\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.999,74.717,177.713,94.517 'fitness function, 24\\nfit_transform(), 70, 75, 80, 84\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.499,456.417,426.926,605.817 'fixed Q-value targets, 713\\nflat dataset, 553\\nflowers dataset, 518-519\\nFNN (feedforward neural network), 309, 538\\nfolds, 90, 105, 107, 108\\nforecasting time series (see time series data)\\nforget gate, LSTM, 569\\nforward pass, in backpropagation, 311\\nforward process, diffusion model, 674-676\\nFPR (false positive rate) or fall-out, 115\\nfrom_predictions(), 122\\nfull gradient descent, 143\\nfully connected layer, 305, 481, 491\\nfully convolutional networks (FCNs), 525-526,\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.739,445.617,284.699,454.617 '532\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.499,391.617,427.277,443.817 'function definition (FuncDef), 802\\nfunction graph (FuncGraph), 802\\nfunctional API, complex models with, 329-335\\nFunctionTransformer, 79\\nF₁ score, 111\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.497,279.917,422.598,379.850 'G\\ngame play (see reinforcement learning)\\ngamma (γ) value, 182\\nGANs (see generative adversarial networks)\\ngate controllers, LSTM, 570\\ngated activation units, 574\\ngated recurrent unit (GRU) cell, 571-572, 591\\nGaussian distribution, 76, 166, 656-657\\nGaussian mixture model (GMM), 283-294\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.740,182.717,405.687,278.117 'anomaly detection, 288-289\\nBayesian Gaussian mixtures, 292\\nfast-MCD, 293\\ninverse_transform() with PCA, 294\\nisolation forest, 293\\nand k-means limitations, 272\\nlocal outlier factor, 293\\none-class SVM, 294\\nselecting number of clusters, 289-291\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.500,150.317,428.061,180.917 'Gaussian process, 348\\nGaussian RBF kernel, 80, 181-184, 192\\nGBRT (gradient boosted regression trees), 226,\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.740,139.517,300.864,148.517 '227-231\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.500,85.517,402.708,137.717 'GCP (Google Cloud Platform), 732-737\\nGCS (Google Cloud Storage), 736\\nGD (see gradient descent)\\nGELU, 365-367\\ngeneralization error, 34-35\\n'>\n",
            "<LTTextBoxHorizontal(19) 384.454,40.500,402.517,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(20) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(21) 420.985,40.500,432.001,49.500 '817\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,596.817,218.484,605.817 'generative adversarial networks (GANs),\\n'>\n",
            "<LTTextBoxHorizontal(1) 271.740,596.817,401.511,605.817 'stochastic gradient descent, 145-148\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,542.817,199.683,595.017 '659-673\\ndeep convolutional, 665-667\\nprogressive growing of, 668-671\\nStyleGANs, 671-673\\ntraining difficulties, 663-665\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,521.217,176.409,541.017 'generative autoencoders, 654\\ngenerative models, 635\\n'>\n",
            "<LTTextBoxHorizontal(4) 259.500,575.217,355.710,595.017 'gradient tree boosting, 226\\ngradients\\n'>\n",
            "<LTTextBoxHorizontal(5) 271.740,521.217,425.793,573.417 'autodiff for computing, 426-430\\nbandwidth saturation issue, 763\\nPG algorithm, 687, 694-698\\nstale, 761\\nunstable (see vanishing and exploding gra‐\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.240,510.417,208.134,519.417 '(see also Gaussian mixture model)\\n'>\n",
            "<LTTextBoxHorizontal(7) 283.980,510.417,308.883,519.417 'dients)\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.000,402.417,241.281,508.617 'generator, GAN, 636, 659-665\\ngenetic algorithm, 686\\ngeodesic distance, 256\\ngeographic data, visualizing, 61-62\\nget_dummies(), 73\\nget_feature_names_out(), 81\\nget_params(), 80\\nGINI impurity, 197, 201\\nglobal average pooling layer, 495\\nglobal versus local minimum, gradient descent,\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.240,391.617,97.200,400.617 '140\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,262.017,234.846,389.817 'Glorot initialization, 359-361\\nGMM (see Gaussian mixture model)\\nGoogle Cloud Platform (GCP), 732-737\\nGoogle Cloud Storage (GCS), 736\\nGoogle Colab, 46-48\\nGoogle Vertex AI (see Vertex AI)\\nGoogLeNet, 502-505, 509\\nGPU implementation, 148, 746-756, 765\\ngetting your own GPU, 747-749\\nmanaging RAM, 749-752\\noperations handling, 754\\nparallel execution across multiple devices,\\n'>\n",
            "<LTTextBoxHorizontal(11) 96.480,251.217,125.604,260.217 '753-756\\n'>\n",
            "<LTTextBoxHorizontal(12) 84.240,240.417,242.055,249.417 'placing operations and variables on devices,\\n'>\n",
            "<LTTextBoxHorizontal(13) 72.000,208.017,240.561,238.617 '752-753\\ngradient ascent, 687\\ngradient boosted regression trees (GBRT), 226,\\n'>\n",
            "<LTTextBoxHorizontal(14) 84.240,197.217,113.364,206.217 '227-231\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.000,78.417,218.574,195.417 'gradient boosting, 226-230\\ngradient clipping, 372\\ngradient descent (GD), 132, 138-149\\nalgorithm comparisons, 148\\nbatch gradient descent, 142-144, 156\\nlocal versus global minimum, 140\\nmini-batch gradient descent, 148-149\\nminimizing hinge loss, 188\\nversus momentum optimization, 380\\nwith optimizers, 379-387\\nshuffling data, 445\\n'>\n",
            "<LTTextBoxHorizontal(16) 72.002,40.500,84.440,49.500 '818 \\n'>\n",
            "<LTTextBoxHorizontal(17) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 102.908,40.500,119.549,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(19) 259.500,488.817,431.553,508.617 'graph mode, 436\\ngraphical processing units (see GPU implemen‐\\n'>\n",
            "<LTTextBoxHorizontal(20) 271.740,478.017,296.049,487.017 'tation)\\n'>\n",
            "<LTTextBoxHorizontal(21) 259.500,467.217,401.160,476.217 'graphs and functions, TensorFlow, 404,\\n'>\n",
            "<LTTextBoxHorizontal(22) 271.740,456.417,334.083,465.417 '433-438, 801-809\\n'>\n",
            "<LTTextBoxHorizontal(23) 259.500,370.017,422.598,454.617 'Graphviz, 196\\ngreedy algorithm, CART as, 200\\ngreedy decoding, 582\\ngreedy layer-wise pretraining, 377, 647, 668\\ngrid search, 91-93\\nGridSearchCV, 91-93\\ngRPC API, querying through, 729\\nGRU (gated recurrent unit) cell, 571-572, 591\\n'>\n",
            "<LTTextBoxHorizontal(24) 259.498,182.717,406.624,358.250 \"H\\nhard clustering, 264\\nhard margin classification, 176, 187\\nhard voting classifiers, 212\\nharmonic mean, 111\\nhashing collision, 466\\nHashing layer, 466\\nhashing trick, 466\\nHDBSCAN (hierarchical DBSCAN), 281\\nHe initialization, 359-361\\nHeaviside step function, 304\\nheavy tail, feature distribution, 76\\nHebb's rule, 306\\nHebbian learning, 306\\nHessians, 386\\nhidden layers\\n\">\n",
            "<LTTextBoxHorizontal(25) 271.741,150.317,382.108,180.917 'neurons per layer, 350\\nnumber of, 349\\nstacked autoencoders, 640-648\\n'>\n",
            "<LTTextBoxHorizontal(26) 259.501,96.317,414.202,148.517 'hierarchical clustering, 11\\nhierarchical DBSCAN (HDBSCAN), 281\\nhigh variance, with decision trees, 207\\nhinge loss function, 188\\nhistogram-based gradient boosting (HGB),\\n'>\n",
            "<LTTextBoxHorizontal(27) 259.501,74.717,312.268,94.517 '230-231\\nhistograms, 54\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,218.817,242.694,605.817 'hold-out sets, 34\\nholdout validation, 34\\nhousing dataset, 40-46\\nHuber loss, 314, 412, 416, 555\\nHugging Face, 629-633\\nHungarian algorithm, 531\\nHyperband tuner, 347\\nhyperbolic tangent (htan), 312, 565\\nhyperparameters, 32, 344-353\\nactivation function, 352\\nbatch size, 352\\nCART algorithm, 200\\nconvolutional layers, 490\\nin custom transformations, 80\\ndecision tree, 228\\ndimensionality reduction, 248\\ngamma (γ) value, 182\\nGAN challenges, 664\\nKeras Tuner, 775\\nlearning rate, 139, 351\\nmomentum β, 380\\nMonte Carlo samples, 399\\nneurons per hidden layer, 350\\nand normalization, 76\\nnumber of hidden layers, 349\\nnumber of iterations, 352\\noptimizer, 352\\nPG algorithms, 697\\npreprocessor and model interaction, 92\\nrandomized search, 344-346\\nsaving along with model, 416\\nSGDClassifier, 183\\nsubsample, 230\\nSVM classifiers with polynomial kernel, 180\\ntolerance (ε), 183\\ntuning of, 34-35, 91-93, 97, 327, 772-776\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,186.417,196.434,217.017 'hypothesis, 45\\nhypothesis boosting (see boosting)\\nhypothesis function, 133\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.999,139.517,219.122,174.650 'I\\nidentity matrix, 157\\nIID (see independent and identically dis‐\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.239,128.717,116.522,137.717 'tributed)\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.999,74.717,238.283,126.917 'image generation, 534, 671, 673\\nimage segmentation, 262, 273-275\\nimages, classifying and generating, 8\\nautoencoders (see autoencoders)\\nCNNs (see convolutional neural networks)\\n'>\n",
            "<LTTextBoxHorizontal(5) 271.739,521.217,401.204,605.817 'diffusion models, 673-681\\ngenerating with GANs, 659-673\\nimplementing MLPs, 318-328\\nlabels, 521\\nloading and preprocessing data, 474\\nrepresentative images, 276\\nsemantic segmentation, 273\\ntuning hyperparameters, 344\\n'>\n",
            "<LTTextBoxHorizontal(6) 259.499,456.417,424.757,519.417 'importance sampling (IS), 714\\nimpurity measures, 197, 201\\nimputation, 68\\nincremental learning, 20\\nincremental PCA (IPCA), 250-251\\nindependent and identically distributed (IID),\\n'>\n",
            "<LTTextBoxHorizontal(7) 271.739,445.617,362.243,454.617 'training instances as, 147\\n'>\n",
            "<LTTextBoxHorizontal(8) 259.499,348.417,418.538,443.817 'inductive bias, 626\\ninertia, model, 267, 269\\ninference, 26\\ninfo(), 52\\ninformation theory, 171, 201\\ninliers, 260\\ninput and output sequences, RNNs, 541-542\\ninput gate, LSTM, 569\\ninput layer, neural network, 305, 320, 331\\n'>\n",
            "<LTTextBoxHorizontal(9) 271.739,337.617,355.304,346.617 '(see also hidden layers)\\n'>\n",
            "<LTTextBoxHorizontal(10) 259.499,154.017,422.984,335.817 'input signature, 801\\ninstance segmentation, 273, 534\\ninstance-based learning, 21, 26\\ninter-op thread pool, 754\\nintercept term constant, 132\\ninterleaving lines from multiple files, 446-447\\ninterpretable ML, 199\\ninvariance, max pooling layer, 492\\ninverse_transform(), 78, 81, 249, 294\\nIPCA (incremental PCA), 250-251\\niris dataset, 167\\nirreducible error, 155\\nIS (importance sampling), 714\\nisolation forest, 293\\nIsomap, 256\\nisotropic noise, 674\\nIterativeImputer, 69\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.500,85.517,335.595,142.250 'J\\nJacobians, 386\\njoblib library, 97-98\\nJSON Lines, 739, 741\\nJupyter, 46\\n'>\n",
            "<LTTextBoxHorizontal(12) 384.453,40.500,402.516,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(13) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(14) 420.984,40.500,432.000,49.500 '819\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,507.917,243.262,607.850 'K\\nk-fold cross-validation, 90, 107, 108\\nk-means algorithm, 82, 263-273\\naccelerated k-means, 268\\ncentroid initialization methods, 267-268\\nfinding optimal number of clusters, 269-271\\nlimitations of, 272\\nmini-batch k-means, 268\\nworkings of, 265-266\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.001,453.917,195.454,506.117 'k-means++, 267\\nk-nearest neighbors regression, 26\\nKaiming initialization, 360\\nKalman Filters, 530\\nKeras API, xvi, 405\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.241,410.717,240.724,452.117 '(see also artificial neural networks)\\nand accessing TensorFlow API directly, 475\\nactivation function support, 362, 363, 367\\nconvolutional layer implementation,\\n'>\n",
            "<LTTextBoxHorizontal(3) 96.481,399.917,125.605,408.917 '487-490\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.241,173.117,219.196,398.117 'custom functions in, 435\\ngradient clipping, 373\\nimage preprocessing layers, 474\\nimplementing MLPs with, 317-344\\ninitialization handling, 360\\ninitializers, 322\\nlayer preprocessing, 459-474\\nlearning rate scheduling, 390-392\\nloading a dataset, 318\\nPG algorithm, 694-698\\npool layer implementation, 493-495\\npretrained CNN models, 516-518\\nResNet-34 CNN with, 515\\nsaving models, 724, 742\\nstacked encoder implementation, 641\\ntf.data API dataset, 452-453\\ntf.keras library, 318\\ntf.keras.activations.get(), 420\\ntf.keras.activations.relu(), 320\\ntf.keras.applications module, 516\\ntf.keras.applications.xception.prepro‐\\n'>\n",
            "<LTTextBoxHorizontal(5) 96.481,162.317,157.906,171.317 'cess_input(), 519\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.241,129.917,233.353,160.517 'tf.keras.backend module, 408\\ntf.keras.callbacks.EarlyStopping, 339\\ntf.keras.callbacks.LearningRateScheduler,\\n'>\n",
            "<LTTextBoxHorizontal(7) 96.481,119.117,109.441,128.117 '390\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.241,75.917,229.483,117.317 'tf.keras.callbacks.ModelCheckpoint, 338\\ntf.keras.callbacks.TensorBoard, 341, 592\\ntf.keras.datasets.imdb.load_data(), 587\\ntf.keras.initializers.VarianceScaling, 360\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.001,40.500,84.439,49.500 '820 \\n'>\n",
            "<LTTextBoxHorizontal(10) 92.017,40.500,95.329,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(11) 102.907,40.500,119.548,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.741,553.617,420.925,605.817 'tf.keras.layers.ActivityRegularization, 652\\ntf.keras.layers.AdditiveAttention, 608\\ntf.keras.layers.Attention, 608\\ntf.keras.layers.AvgPool2D, 495\\ntf.keras.layers.BatchNormalization, 369,\\n'>\n",
            "<LTTextBoxHorizontal(13) 283.981,542.817,330.160,551.817 '370-372, 566\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.741,445.617,431.779,541.017 'tf.keras.layers.Bidirectional, 602\\ntf.keras.layers.CategoryEncoding, 463\\ntf.keras.layers.CenterCrop, 474\\ntf.keras.layers.Concatenate, 331, 332, 502\\ntf.keras.layers.Conv1D, 533, 592\\ntf.keras.layers.Conv2D, 487\\ntf.keras.layers.Conv2DTranspose, 532\\ntf.keras.layers.Conv3D, 533\\ntf.keras.layers.Dense, 331, 332, 420-421, 471,\\n'>\n",
            "<LTTextBoxHorizontal(15) 283.981,434.817,313.996,443.817 '646, 657\\n'>\n",
            "<LTTextBoxHorizontal(16) 271.741,402.417,418.000,433.017 'tf.keras.layers.Discretization, 463\\ntf.keras.layers.Dropout, 396\\ntf.keras.layers.Embedding, 468, 581, 589,\\n'>\n",
            "<LTTextBoxHorizontal(17) 283.981,391.617,296.941,400.617 '613\\n'>\n",
            "<LTTextBoxHorizontal(18) 271.741,251.217,413.626,389.817 'tf.keras.layers.GlobalAvgPool2D, 495\\ntf.keras.layers.GRU, 572\\ntf.keras.layers.GRUCell, 572\\ntf.keras.layers.Hashing, 466\\ntf.keras.layers.Input, 331\\ntf.keras.layers.Lambda, 419, 582\\ntf.keras.layers.LayerNormalization, 566\\ntf.keras.layers.LeakyReLU, 362\\ntf.keras.layers.LSTM, 568\\ntf.keras.layers.Masking, 591\\ntf.keras.layers.MaxPool2D, 493\\ntf.keras.layers.MultiHeadAttention, 617\\ntf.keras.layers.Normalization, 329, 331,\\n'>\n",
            "<LTTextBoxHorizontal(19) 283.981,240.417,313.105,249.417 '460-462\\n'>\n",
            "<LTTextBoxHorizontal(20) 271.741,164.817,422.014,238.617 'tf.keras.layers.PReLU, 362\\ntf.keras.layers.Rescaling, 474\\ntf.keras.layers.Resizing, 474, 517\\ntf.keras.layers.RNN, 568\\ntf.keras.layers.SeparableConv2D, 510\\ntf.keras.layers.StringLookup, 465\\ntf.keras.layers.TextVectorization, 471-473,\\n'>\n",
            "<LTTextBoxHorizontal(21) 283.981,154.017,364.270,163.017 '579, 582, 588, 589-599\\n'>\n",
            "<LTTextBoxHorizontal(22) 271.741,121.617,406.606,152.217 'tf.keras.layers.TimeDistributed, 564\\ntf.keras.losses.Huber, 412\\ntf.keras.losses.kullback_leibler_diver‐\\n'>\n",
            "<LTTextBoxHorizontal(23) 283.981,110.817,327.856,119.817 'gence(), 653\\n'>\n",
            "<LTTextBoxHorizontal(24) 271.741,89.217,423.265,109.017 'tf.keras.losses.Loss, 413\\ntf.keras.losses.sparse_categorical_crossen‐\\n'>\n",
            "<LTTextBoxHorizontal(25) 283.981,78.417,395.005,87.417 'tropy(), 323, 497, 582, 596, 632\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,542.817,242.244,605.817 'tf.keras.metrics.MeanIoU, 522\\ntf.keras.metrics.Metric, 418\\ntf.keras.metrics.Precision, 417\\ntf.keras.Model, 331\\ntf.keras.models.clone_model(), 337, 375\\ntf.keras.models.load_model(), 338, 413-415,\\n'>\n",
            "<LTTextBoxHorizontal(1) 259.502,542.817,421.052,605.817 'large margin classification, 175\\nLasso, 161\\nlasso regression, 158-161\\nlatent diffusion models, 680\\nlatent loss, 656\\nlatent representation of inputs, 627, 635, 637,\\n'>\n",
            "<LTTextBoxHorizontal(2) 96.480,532.017,126.495,541.017 '423, 766\\n'>\n",
            "<LTTextBoxHorizontal(3) 271.742,532.017,284.702,541.017 '667\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.240,499.617,235.368,530.217 'tf.keras.optimizers.Adam, 329, 386\\ntf.keras.optimizers.Adamax, 386\\ntf.keras.optimizers.experimental.AdamW,\\n'>\n",
            "<LTTextBoxHorizontal(5) 96.480,488.817,109.440,497.817 '386\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.240,445.617,231.543,487.017 'tf.keras.optimizers.Nadam, 386\\ntf.keras.optimizers.schedules, 391\\ntf.keras.optimizers.SGD, 380\\ntf.keras.preprocessing.image.ImageData‐\\n'>\n",
            "<LTTextBoxHorizontal(7) 96.480,434.817,150.336,443.817 'Generator, 519\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.240,326.817,231.318,433.017 'tf.keras.regularizers.l1_l2(), 393\\ntf.keras.Sequential, 319, 329, 497\\ntf.keras.utils.get_file(), 579\\ntf.keras.utils.set_random_seed(), 319\\ntf.keras.utils.timeseries_data‐\\nset_from_array(), 552, 555\\ntf.keras.utils.to_categorical(), 324\\ntime series forecasting for RNN, 556-558\\ntransfer learning with, 375-377\\nusing TF functions (or not), 809\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.000,197.217,226.476,325.017 'Keras session, 322\\nKeras Tuner, 775\\nkernel trick, 180-184, 190-193\\nkernelized SVMs, 190-193\\nkernels (convolution kernels), 484, 496\\nkernels (runtimes), 48\\nKL (Kullback-Leibler) divergence, 172, 653\\nKLDivergenceRegularizer, 653\\nKMeans, 81\\nKNeighborsClassifier, 125, 128\\nKNNImputer, 69\\nKullback-Leibler (KL) divergence, 172, 652\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,150.317,168.140,185.450 'L\\nlabel propagation, 277-278\\nlabels, 43\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.242,107.117,173.414,148.517 'in clustering, 263\\nimage classification, 521\\nsupervised learning, 11\\nunlabeled data issue, 645\\n'>\n",
            "<LTTextBoxHorizontal(12) 72.002,85.517,149.348,105.317 'landmarks, 181\\nlanguage models, 578\\n'>\n",
            "<LTTextBoxHorizontal(13) 84.242,74.717,220.070,83.717 '(see also natural language processing)\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.502,456.417,416.525,530.217 'latent space, 656\\nlaw of large numbers, 213\\nlayer normalization, 439, 537, 566\\nLDA (linear discriminant analysis), 257\\nleaf node, decision tree, 197, 199, 202\\nleakyReLU, 361-363\\nlearning curves, overfit or underfit analysis,\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.742,445.617,300.866,454.617 '152-155\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.502,316.017,403.106,443.817 'learning rate, 20, 139, 143, 145, 351, 705\\nlearning rate schedules, 388-392\\nlearning schedules, 145\\nLeCun initialization, 360\\nLeNet-5, 481, 498\\nLevenshtein distance, 182\\nliblinear library, 183\\nlibsvm library, 183\\nlife satisfaction dataset, 22\\nlikelihood function, 290-291\\nlinear discriminant analysis (LDA), 257\\nlinear models\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.742,262.017,412.691,314.217 'forecasting time series, 555\\nlinear regression (see linear regression)\\nregularized, 156-163\\nSVM, 175-178\\ntraining and running example, 23\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.502,251.217,377.141,260.217 'linear regression, 24-26, 132-149\\n'>\n",
            "<LTTextBoxHorizontal(19) 259.502,78.417,421.232,249.417 'comparison of algorithms, 149\\ncomputational complexity, 138\\ngradient descent in, 138-149\\nlearning curves in, 151-155\\nNormal equation, 134-137\\nregularizing models (see regularization)\\nridge regression, 156-158, 161\\ntraining set evaluation, 88\\nusing stochastic gradient descent, 147\\nlinear SVM classification, 175-178, 186-193\\nlinear threshold units (LTUs), 304\\nlinearly separable, SVM classes, 175-176, 181\\nLinearRegression, 71, 79, 88, 137, 138, 150\\nLinearSVC, 177, 183, 188\\nLinearSVR, 184-186\\nLipschitz continuous, derivative as, 141\\n'>\n",
            "<LTTextBoxHorizontal(20) 384.456,40.500,402.519,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(21) 410.097,40.500,413.409,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(22) 420.987,40.500,432.003,49.500 '821\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,532.017,235.908,605.817 'LLE (locally linear embedding), 254-256\\nloading and preprocessing data, 441-477\\nimage preprocessing layers, 474\\nlayer preprocessing in Keras, 442, 459-474\\ntf.data API, 441-453\\nTFDS project, 475-477\\nTFRecord format, 441, 453-459\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,488.817,241.281,530.217 'local outlier factor (LOF), 293\\nlocal receptive field, 480\\nlocal response normalization (LRN), 501\\nlocal versus global minimum, gradient descent,\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,478.017,97.200,487.017 '140\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,370.017,217.053,476.217 'locality sensitive hashing (LSH), 254\\nlocalization, CNNs, 521-525\\nlocally linear embedding (LLE), 254-256\\nLOF (local outlier factor), 293\\nlog loss, 166\\nlog-odds function, 165\\nlog-transformer, 79\\nlogical GPU device, 750\\nlogistic function, 164\\nlogistic regression, 10, 78, 164-173\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.240,326.817,231.597,368.217 'decision boundaries illustration, 167-169\\nestimating probabilities, 164-165\\nsoftmax regression model, 170-173\\ntraining and cost function, 165-166\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,272.817,225.027,325.017 'LogisticRegression, 168, 172\\nlogit function, 165\\nlong sequences, training RNN on, 565-575\\nshort-term memory problem, 568-575\\nunstable gradients problem, 565-568\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,262.017,242.694,271.017 'long short-term memory (LSTM) cell, 568-571,\\n'>\n",
            "<LTTextBoxHorizontal(7) 84.240,251.217,131.310,260.217 '591, 599, 602\\n'>\n",
            "<LTTextBoxHorizontal(8) 72.000,240.417,121.662,249.417 'loss functions\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.240,197.217,207.846,238.617 'based on model internals, 424-426\\ncustom, 412\\nversus metrics, 416\\noutput, 334\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,164.817,242.694,195.417 'LRN (local response normalization), 501\\nLSH (locality sensitive hashing), 254\\nLSTM (long short-term memory) cell, 568-571,\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.240,154.017,131.310,163.017 '591, 599, 602\\n'>\n",
            "<LTTextBoxHorizontal(12) 72.000,132.417,194.013,152.217 'LTUs (linear threshold units), 304\\nLuong attention, 607\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.998,96.317,164.549,120.650 'M\\nmachine learning (ML), 4\\n'>\n",
            "<LTTextBoxHorizontal(14) 84.242,74.717,214.427,94.517 'application/technique examples, 8-9\\nchallenges of, 27-33\\n'>\n",
            "<LTTextBoxHorizontal(15) 72.002,40.500,84.441,49.500 '822 \\n'>\n",
            "<LTTextBoxHorizontal(16) 92.019,40.500,95.331,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(17) 102.909,40.500,119.550,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(18) 271.742,586.017,343.292,605.817 'notations, 44-45\\nproject checklist, 41\\n'>\n",
            "<LTTextBoxHorizontal(19) 283.982,575.217,419.756,584.217 '(see also end-to-end ML project exer‐\\n'>\n",
            "<LTTextBoxHorizontal(20) 296.222,564.417,312.683,573.417 'cise)\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.742,510.417,373.829,562.617 'reasons for using, 5-7\\nresources on, xxi-xxii\\nspam filter example, 3-7\\ntesting and validating, 34-37\\ntypes of systems, 9-26\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.502,424.017,406.220,508.617 'MAE (mean absolute error), 45\\nmajority-vote predictions, 209\\nmake_column_selector(), 86\\nmake_column_transformer(), 86\\nmake_pipeline(), 84, 154\\nManhattan norm, 45\\nmanifold hypothesis, 242\\nmanifold learning, dimension reduction,\\n'>\n",
            "<LTTextBoxHorizontal(23) 271.742,413.217,300.866,422.217 '241-242\\n'>\n",
            "<LTTextBoxHorizontal(24) 259.502,326.817,426.083,411.417 'MAP (maximum a-posteriori) estimation, 291\\nmAP (mean average precision), 528\\nMAPE (mean absolute percentage error), 546\\nmapping network, StyleGANs, 671\\nMapReduce, 43\\nmargin violations, 177, 187\\nMarkov chains, 699\\nMarkov decision processes (MDPs), 699-703,\\n'>\n",
            "<LTTextBoxHorizontal(25) 271.742,316.017,284.702,325.017 '706\\n'>\n",
            "<LTTextBoxHorizontal(26) 259.502,175.617,426.083,314.217 'mask R-CNN, 534\\nmask tensor, 590\\nmasked language model (MLM), 620\\nmasked multi-head attention layer, 612\\nmasking, 590-593, 617\\nMatching Engine service, Vertex AI, 732\\nMatplotlib, 46\\nmax pooling layer, 491, 497\\nmax-norm regularization, 399\\nmaximization step, Gaussian mixtures, 284\\nmaximum a-posteriori (MAP) estimation, 291\\nmaximum likelihood estimate (MLE), 291\\nMC (Monte Carlo) dropout regularization,\\n'>\n",
            "<LTTextBoxHorizontal(27) 271.742,164.817,300.866,173.817 '397-399\\n'>\n",
            "<LTTextBoxHorizontal(28) 259.502,143.217,422.294,163.017 'MCTS (Monte Carlo tree search), 716\\nMDPs (Markov decision processes), 699-703,\\n'>\n",
            "<LTTextBoxHorizontal(29) 271.742,132.417,284.702,141.417 '706\\n'>\n",
            "<LTTextBoxHorizontal(30) 259.502,78.417,422.915,130.617 'MDS (multidimensional scaling), 256\\nmean absolute error (MAE), 45\\nmean absolute percentage error (MAPE), 546\\nmean average precision (mAP), 528\\nmean squared error (MSE), 133, 656\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,542.817,231.723,605.817 'mean-shift, clustering algorithms, 282\\nmean_squared_error(), 88\\nmeasure of similarity, 21\\nmemory bandwidth, GPU card, 451\\nmemory cells (cells), RNNs, 540, 568-575\\nmemory requirements, convolutional layers,\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,532.017,97.200,541.017 '490\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,424.017,231.876,530.217 \"Mercer's theorem, 192\\nmeta learner, 232\\nmetagraphs, SavedModel, 724\\nmin-max scaling, 75\\nmini-batch discrimination, 664\\nmini-batch gradient descent, 148-149, 163\\nmini-batch k-means, 268\\nmini-batches, 19, 669\\nMinMaxScaler, 76\\nmirrored strategy, data parallelism, 759, 765,\\n\">\n",
            "<LTTextBoxHorizontal(3) 84.240,413.217,97.200,422.217 '768\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,283.617,229.266,411.417 'Mish activation function, 366\\nmixing regularization, StyleGAN, 673\\nML (see machine learning)\\nML Operations (MLOps), 100\\nMLE (maximum likelihood estimate), 291\\nMLM (masked language model), 620\\nMLPs (see multilayer perceptrons)\\nMNIST dataset, 103-105\\nmobile device, deploying model to, 741-744\\nmode collapse, 628, 664\\nmodel parallelism, 756-758\\nmodel parameters, 23\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,229.617,216.261,281.817 'early stopping regularization, 166\\nin gradient descent, 142, 144\\nlinear SVM classifier mechanics, 186\\nand variable updating, 410\\nweight matrix shape, 323\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,89.217,216.189,227.817 'model rot, 18\\nmodel selection, 23, 34-35\\nmodel server (see TensorFlow Serving)\\nmodel warmup, 730\\nmodel-based learning, 22-26\\nmodes, 77\\nmomentum optimization, 379\\nmomentum β, 380\\nMonte Carlo (MC) dropout, 397-399\\nMonte Carlo tree search (MCTS), 716\\nMSE (mean squared error), 133, 656\\nmulti-head attention layer, 611, 615-619\\nmulti-hot encoding, 464\\n'>\n",
            "<LTTextBoxHorizontal(7) 259.500,596.817,399.243,605.817 'multiclass (multinomial) classification,\\n'>\n",
            "<LTTextBoxHorizontal(8) 271.740,586.017,334.083,595.017 '119-121, 315-316\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.500,553.617,420.969,584.217 'multidimensional scaling (MDS), 256\\nmultilabel classifiers, 125-127\\nmultilayer perceptrons (MLPs), 299, 308-344\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.740,445.617,417.459,551.817 'and autoencoders, 638, 639\\nand backpropagation, 309-312\\ncallbacks, 338-340\\nclassification MLPs, 315-316\\ncomplex models, 329-335\\ndynamic models, 336-337\\nimage classifier, 318-328\\nregression MLPs, 313-314, 328\\nsaving and restoring a model, 337-338\\nvisualization with TensorBoard, 340-344\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.500,348.417,404.571,443.817 'multimodal distribution, 77\\nmultimodal transformers, 627\\nmultinomial logistic regression, 170-173\\nmultioutput classifiers, 127-128\\nmultiple regression, 43\\nmultiplicative attention, 607\\nmultitask classification, 333\\nmultivariate regression, 43\\nmultivariate time series, 545, 559-560\\n'>\n",
            "<LTTextBoxHorizontal(12) 259.494,269.117,426.893,336.650 'N\\nNadam, 386\\nNAG (Nesterov accelerated gradient), 381, 386\\nnaive forecasting, 545\\nNash equilibrium, 663\\nnatural language processing (NLP), 577-633\\n'>\n",
            "<LTTextBoxHorizontal(13) 271.742,247.517,425.174,267.317 'char-RNN model to generate text, 578-586\\nencoder–decoder network for machine\\n'>\n",
            "<LTTextBoxHorizontal(14) 283.982,236.717,356.189,245.717 'translation, 595-601\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.742,171.917,426.794,234.917 'machine learning examples, 8\\nsentiment analysis, 587-595\\ntext classification, 587-595\\ntext encoding, 471-474\\ntext summarization, 8\\ntransformer models (see transformer mod‐\\n'>\n",
            "<LTTextBoxHorizontal(16) 283.982,161.117,296.501,170.117 'els)\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.742,150.317,353.633,159.317 'word embeddings, 467\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.502,128.717,410.729,148.517 'natural language understanding (NLU), 8\\nNCCL (Nvidia collective communications\\n'>\n",
            "<LTTextBoxHorizontal(19) 271.742,117.917,316.193,126.917 'library), 766\\n'>\n",
            "<LTTextBoxHorizontal(20) 259.502,107.117,426.506,116.117 'NEAT (neuroevolution of augmenting topolo‐\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.742,96.317,305.663,105.317 'gies), 686\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.502,74.717,342.185,94.517 'negative class, 109, 164\\nnested dataset, 553\\n'>\n",
            "<LTTextBoxHorizontal(23) 384.455,40.500,402.518,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(24) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(25) 420.986,40.500,432.002,49.500 '823\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,553.617,239.391,605.817 'Nesterov accelerated gradient (NAG), 381, 386\\nNesterov momentum optimization, 381, 386\\nneural machine translation (NMT), 595-619\\nand attention mechanisms, 604-619\\nwith transformers, 620-624\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,532.017,242.262,551.817 'neural networks (see artificial neural networks)\\nneuroevolution of augmenting topologies\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,521.217,130.500,530.217 '(NEAT), 686\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,456.417,234.504,519.417 'next sentence prediction (NSP), 621\\nNLU (natural language understanding), 8\\nNMT (see neural machine translation)\\nNo Free Lunch theorem, 37\\nnon-max suppression, bounding boxes, 524\\nnonlinear dimensionality reduction (NLDR),\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.240,445.617,113.364,454.617 '254-256\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,348.417,229.608,443.817 'nonlinear SVM classifiers, 178-184\\nnonparametric models, 201\\nnonrepresentative training data, 28\\nnonresponse bias, 29\\nnormal distribution, 358, 360\\nNormal equation, 134-137\\nnormalization, 76, 367-372, 565\\nNormalization layer, 460-462\\nnormalized exponential (softmax function),\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.240,337.617,97.200,346.617 '170\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,218.817,220.896,335.817 'notations, 44-45, 134\\nnovelty detection, 13, 288\\nNP-Complete problem, CART as, 200\\nNSP (next sentence prediction), 621\\nnucleus sampling, 584\\nnull hypothesis, 202\\nnumber of inputs, 323, 359\\nnumber of neurons per hidden layer, 350\\nNumPy, 46\\nNumPy arrays, 71, 79, 407-411\\nNvidia collective communications library\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.240,208.017,131.022,217.017 '(NCCL), 766\\n'>\n",
            "<LTTextBoxHorizontal(9) 72.000,197.217,167.391,206.217 'Nvidia GPU card, 748-749\\n'>\n",
            "<LTTextBoxHorizontal(10) 71.995,117.917,242.674,185.450 'O\\nOAuth 2.0, 734\\nobject detection, CNNs, 523-530\\nobject tracking, CNNs, 530\\nobjectness score, 523\\nobservation space, reinforcement learning, 684,\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.238,107.117,97.198,116.117 '693\\n'>\n",
            "<LTTextBoxHorizontal(12) 71.998,74.717,211.075,105.317 'observations, 689-690\\nOCR (optical character recognition), 3\\nOEL (open-ended learning), 719\\n'>\n",
            "<LTTextBoxHorizontal(13) 71.998,40.500,84.436,49.500 '824 \\n'>\n",
            "<LTTextBoxHorizontal(14) 92.014,40.500,95.326,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(15) 102.904,40.500,119.545,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.498,272.817,418.213,605.817 'off-policy algorithm, 706\\noffline learning, 18\\non-policy algorithm, 706\\none-class SVM, 294\\none-hot encoding, 72-74, 464, 471\\none-versus-all (OvA) strategy, 119-121\\none-versus-one (OvO) strategy, 119-121\\none-versus-the-rest (OvR) strategy, 119-121\\n1cycle scheduling, 389, 392\\n1D convolutional layers, 533, 573\\nOneHotEncoder, 72-74, 77, 86\\nonline kernelized SVMs, 193\\nonline learning, 19-20\\nonline model, DQN, 713\\nOOB (out-of-bag) evaluation, 218-219\\nopen-ended learning (OEL), 719\\nOpenAI Gym, 687-691\\noperations (ops) and tensors, 404, 407-408\\noptical character recognition (OCR), 3\\noptimal state value, MDP, 701\\noptimizers, 379-387\\nAdaGrad, 382\\nAdam optimization, 384\\nAdaMax, 385\\nAdamW, 386\\nhyperparameters, 352\\nmomentum optimization, 379\\nNadam, 386\\nNesterov accelerated gradient, 381\\noutput layer, 600\\nRMSProp, 383\\n'>\n",
            "<LTTextBoxHorizontal(17) 259.498,121.617,420.211,271.017 'oracle, 346\\norder of integration (d) hyperparameter, 549\\nOrdinalEncoder, 71\\northogonal matrix, 600\\nout-of-bag (OOB) evaluation, 218-219\\nout-of-core learning, 19\\nout-of-sample error, 34\\noutlier detection (see anomaly detection)\\noutliers, 260\\noutput gate, LSTM, 569\\noutput layer, neural network, 600\\nOvA (one-versus-all) strategy, 119-121\\novercomplete autoencoder, 649\\noverfitting of data, 30-33, 55\\n'>\n",
            "<LTTextBoxHorizontal(18) 271.738,78.417,421.345,119.817 'avoiding through regularization, 392-400\\nand decision trees, 201, 205\\nand dropout regularization, 396\\ngamma (γ) hyperparameter to adjust, 182\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,553.617,231.795,605.817 'image classification, 324\\nlearning curves to assess, 152-155\\nnumber of neurons per hidden layer, 351\\npolynomial regression, 132\\nSVM model, 177\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,532.017,230.715,551.817 'OvO (one-versus-one) strategy, 119-121\\nOvR (one-versus-the-rest) strategy, 119-121\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.997,441.917,237.583,520.250 'P\\np-value, 202\\nPACF (partial autocorrelation function), 551\\npadding options, convolutional layer, 488-489\\nPaLM (Pathways language model), 623\\nPandas, 46, 64-66, 73, 78\\nparallelism, training models across devices,\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.241,355.517,232.876,440.117 '756-776\\ndata parallelism, 759-765\\ndistribution strategies API, 765\\nwith GPU, 753-756\\nhyperparameter tuning, 772-776\\nmodel parallelism, 756-758\\non TensorFlow cluster, 767-770\\nVertex AI for running large jobs, 770-772\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.001,150.317,232.183,353.717 \"parameter efficiency, 349\\nparameter matrix, 170\\nparameter servers, 760\\nparameter space, 142\\nparameter vector, 133, 138, 165, 170\\nparametric leaky ReLU (PReLU), 362\\nparametric models, 202\\npartial autocorrelation function (PACF), 551\\npartial derivative, 142\\npartial_fit(), 148\\nPathways language model (PaLM), 623\\nPCA (see principal component analysis)\\nPDF (probability density function), 260, 291\\nPearson's r, 63\\npenalties, reinforcement learning, 16\\nPER (prioritized experience replay), 714\\nPerceiver, 627\\npercentiles, 54\\nperceptrons, 299, 304-316\\n\">\n",
            "<LTTextBoxHorizontal(5) 84.241,139.517,201.574,148.517 '(see also multilayer perceptrons)\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.001,107.117,219.277,137.717 'performance measures, 107-119\\nconfusion matrix, 108-110\\ncross-validation to measure accuracy,\\n'>\n",
            "<LTTextBoxHorizontal(7) 96.481,96.317,125.605,105.317 '107-108\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.241,74.717,187.975,94.517 'precision and recall, 110-115\\nROC curve, 115-119\\n'>\n",
            "<LTTextBoxHorizontal(9) 271.741,596.817,327.649,605.817 'selecting, 43-45\\n'>\n",
            "<LTTextBoxHorizontal(10) 259.501,208.017,427.468,595.017 'performance scheduling, 389, 392\\npermutation(), 56\\nPG (policy gradients) algorithm, 687, 694-698\\npiecewise constant scheduling, 389, 391\\nPipeDream, 763\\nPipeline class, 83\\nPipeline constructor, 84-87\\npipelines, 42, 83-88, 154, 177\\npixelwise normalization layer, 670\\nplaceholders, function definitions, 803\\nPOET algorithm, 719\\npolicy gradients (PG) algorithm, 687, 694-698\\npolicy parameters, 686\\npolicy space, 686\\npolicy, reinforcement learning, 16, 685, 691\\npolynomial features, SVM classifiers, 180\\npolynomial kernel, 180, 190\\npolynomial regression, 132, 149-151\\npolynomial time, 200\\nPolynomialFeatures, 150, 179\\npooling kernel, 491\\npooling layers, 491-495\\npositional encodings, 612-614\\npositive class, 109, 164\\npost-training quantization, 743\\nposterior distribution, 654\\npower law distribution, 76\\npower scheduling, 389\\nPPO (proximal policy optimization), 718\\nprecision and recall, classifier metrics, 110-115\\nprecision/recall curve (PR), 113, 117\\nprecision/recall trade-off, 111-115\\npredict(), 71, 81, 84\\npredicted class, 109\\nprediction service, on Vertex AI, 732-741\\npredictions\\n'>\n",
            "<LTTextBoxHorizontal(11) 271.741,175.617,406.777,206.217 'backpropagation, 312\\nconfusion matrix, 108-110\\ncross-validation to measure accuracy,\\n'>\n",
            "<LTTextBoxHorizontal(12) 283.981,164.817,313.105,173.817 '107-108\\n'>\n",
            "<LTTextBoxHorizontal(13) 271.741,143.217,381.685,163.017 'decision trees, 197-200\\nwith linear SVM classifier, 186\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.501,132.417,309.064,141.417 'predictors, 11\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.741,121.617,372.955,130.617 '(see also ensemble learning)\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.501,78.417,393.124,119.817 'predict_log_proba(), 178\\npredict_proba(), 178\\nprefetching of data, 450-452\\nPReLU (parametric leaky ReLU), 362\\n'>\n",
            "<LTTextBoxHorizontal(17) 384.455,40.500,402.518,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(18) 410.096,40.500,413.408,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(19) 420.986,40.500,432.002,49.500 '825\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,586.017,230.292,605.817 'preprocessed attributes, 55\\npreprocessing data (see loading and prepro‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,575.217,130.410,584.217 'cessing data)\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,553.617,191.043,573.417 'preprocessing mismatch, 460\\npretraining and pretrained layers\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,478.017,242.127,551.817 'on auxiliary task, 378\\nCNNs, 516-521\\ngreedy layer-wise pretraining, 377, 647, 668\\nlanguage model components, 473\\nreusing embeddings, 593-595\\nreusing layers, 373-379\\nin unsupervised learning, 377, 593, 620, 644\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,337.617,236.799,476.217 'primal problem, 189\\nprincipal component (PC), 244-245\\nprincipal component analysis (PCA), 243-251\\nchoosing number of dimensions, 247-249\\nfor compression, 249-250\\nexplained variance ratio, 246\\nfinding principal components, 245\\nincremental PCA, 250-251\\npreserving variance, 243\\nprojecting down to d dimensions, 245\\nrandomized PCA, 250\\nfor scaling data in decision trees, 206\\nwith undercomplete linear autoencoder,\\n'>\n",
            "<LTTextBoxHorizontal(5) 96.480,326.817,125.604,335.817 '639-640\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.240,316.017,181.089,325.017 'using Scikit_Learn for, 246\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,143.217,238.140,314.217 'prior distribution, 654\\nprioritized experience replay (PER), 714\\nprobabilistic autoencoders, 654\\nprobabilities, estimating, 164-165, 199, 215\\nprobability density function (PDF), 260, 291\\nprobability versus likelihood, 290-291\\nprofiling the network, with TensorBoard, 340\\nprogressive web app (PWA), 745\\nprojection, dimensionality reduction, 239-240\\npropositional logic, 300\\nprotocol buffers (protobuf), 454-457, 459, 729\\nproximal policy optimization (PPO), 718\\npruning of decision tree nodes, 202\\npseudoinverse, 137\\nPWA (progressive web app), 745\\nPython API, 46\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.997,107.117,182.736,131.450 'Q\\nQ-learning algorithm, 704-716\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.240,74.717,229.104,105.317 'approximate Q-learning, 707\\nexploration policies, 706\\nimplementing deep Q-learning, 708-712\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.002,40.500,84.440,49.500 '826 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.018,40.500,95.330,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.908,40.500,119.549,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(13) 271.740,596.817,403.932,605.817 'variants in deep Q-learning, 713-716\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.500,499.617,419.277,595.017 'Q-value iteration algorithm, 702-703\\nQ-values, 702-703\\nquadratic equation, 150\\nquadratic programming (QP) problems, 188\\nquantization-aware training, 744\\nquartiles, 54\\nqueries per second (QPS), 721, 738\\nquestion-answering modules, 8\\nqueues, 411, 798\\n'>\n",
            "<LTTextBoxHorizontal(15) 259.500,431.117,377.248,487.850 'R\\nradial basis function (RBF), 77\\nragged dimensions, 411\\nRainbow agent, 716\\nrandom forests, 90, 211, 220-222\\n'>\n",
            "<LTTextBoxHorizontal(16) 271.741,387.917,408.802,429.317 'analysis of models and their errors, 95\\ndecision trees (see decision trees)\\nextra-trees, 221\\nfeature importance measurement, 221\\n'>\n",
            "<LTTextBoxHorizontal(17) 259.501,247.517,425.920,386.117 'random initialization, 138, 140\\nrandom patches, 219\\nrandom projection algorithm, 252-254\\nrandom subspaces, 219\\nRandomForestClassifier, 117-119\\nRandomForestRegressor, 90\\nrandomized leaky ReLU (RReLU), 362\\nrandomized PCA, 250\\nrandomized search, 93-94, 344-346\\nRBF (radial basis function), 77\\nrbf_kernel(), 77, 80\\nrecall metric, 110\\nreceiver operating characteristic (ROC) curve,\\n'>\n",
            "<LTTextBoxHorizontal(18) 271.741,236.717,300.865,245.717 '115-119\\n'>\n",
            "<LTTextBoxHorizontal(19) 259.501,225.917,350.428,234.917 'recognition network, 637\\n'>\n",
            "<LTTextBoxHorizontal(20) 271.741,215.117,355.387,224.117 '(see also autoencoders)\\n'>\n",
            "<LTTextBoxHorizontal(21) 259.501,139.517,417.298,213.317 'recommender system, 9\\nreconstruction error, 249, 646\\nreconstruction loss, 424, 638\\nrectified linear units (ReLU) (see ReLU)\\nrecurrent dropout, 537\\nrecurrent layer normalization, 537, 566\\nrecurrent neural networks (RNNs), 537-604\\n'>\n",
            "<LTTextBoxHorizontal(22) 271.741,74.717,428.899,137.717 'bidirectional, 601\\ndeep RNN, 557\\nforecasting time series (see time series data)\\ngradient clipping, 372\\nhandling long sequences, 565-575\\ninput and output sequences, 541-542\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,521.217,222.543,605.817 'memory cells, 540, 568-575\\nNLP (see natural language processing)\\nand Perceiver, 627\\nsplitting across devices, 758\\nstateful, 578, 584-586\\nstateless, 578, 586\\ntraining, 542\\nand vision transformers, 624\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,467.217,203.589,519.417 'recurrent neurons, 538\\nreduce operation, 760\\nregion proposal network (RPN), 530\\nregression MLPs, 313-314\\nregression models\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,316.017,235.251,465.417 'and classification, 10, 127\\ndecision tree tasks, 204-205\\nforecasting example, 8\\nlasso regression, 158-161\\nlinear regression (see linear regression)\\nlogistic regression (see logistic regression)\\nmultiple regression, 43\\nmultivariate regression, 43\\npolynomial regression, 132, 149-151\\nregression MLPs, 328\\nridge regression, 156-158, 161\\nsoftmax regression, 170-173\\nSVM, 184-186\\nunivariate regression, 43\\n'>\n",
            "<LTTextBoxHorizontal(3) 72.000,121.617,202.752,314.217 'regression to the mean, 10\\nregularization, 32, 392-400\\ncustom regularizers, 415\\ndecision trees, 203\\ndropout, 394-397\\nearly stopping, 162-163, 229, 555\\nelastic net, 161\\nhyperparameters, 201-203\\nlasso regression, 158-161\\nlinear models, 156-163\\nmax-norm, 399\\nMC dropout, 397-399\\nridge, 157\\nshrinkage, 228\\nsubword, 588\\nTikhonov, 156-158\\nweight decay, 386\\nℓ₁ and ℓ₂ regularization, 393\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,100.017,219.978,119.817 'REINFORCE algorithms, 694\\nreinforcement learning (RL), 16, 683-719\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,78.417,213.912,98.217 'actions, 693-694\\ncredit assignment problem, 693-694\\n'>\n",
            "<LTTextBoxHorizontal(6) 271.740,510.417,431.634,605.817 'examples of, 9, 684\\nlearning in order to optimizing rewards, 684\\nMarkov decision processes, 699-703\\nneural network policies, 691\\nOpenAI Gym, 687-691\\nPG algorithms, 694-698\\npolicy search, 685\\nQ-learning, 704-716\\nTD learning, 703\\n'>\n",
            "<LTTextBoxHorizontal(7) 259.500,413.217,408.243,508.617 'ReLU (rectified linear units)\\nand backpropagation, 312\\nin CNN architectures, 495\\nas default for simple tasks, 366\\nleakyReLU, 361-363\\nand MLPS, 314\\nRNN unstable gradients problem, 565\\nRReLU, 362\\ntuning hyperparameters, 345\\n'>\n",
            "<LTTextBoxHorizontal(8) 259.500,380.817,356.259,411.417 'replay buffer, 709\\nrepresentation learning, 74\\n(see also autoencoders)\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.500,154.017,425.919,379.017 'residual block, 423-424\\nresidual errors, 226-230\\nresidual learning, 505\\nresidual network (ResNet), 505-509\\nresidual units, 506\\nResNet-152, 508\\nResNet-34, 508, 515\\nResNet-50, 517\\nResNeXt, 512\\nREST API, querying through, 727\\nreturn, in reinforcement learning, 696\\nreverse process, diffusion model, 675-677\\nreverse-mode autodiff, 310, 428\\nrewards, reinforcement learning, 16, 684\\nRidge, 157\\nridge regression, 156-158, 161\\nridge regularization, 157\\nRidgeCV, 158\\nRL (see reinforcement learning)\\nRMSProp, 383\\nROC (receiver operating characteristic) curve,\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.740,143.217,300.864,152.217 '115-119\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.500,132.417,430.869,141.417 'root mean square error (RMSE), 43-45, 88, 133,\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.740,121.617,284.700,130.617 '162\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.500,89.217,397.884,119.817 'root node, decision tree, 197, 198\\nRPN (region proposal network), 530\\nRReLU (randomized leaky ReLU), 362\\n'>\n",
            "<LTTextBoxHorizontal(14) 384.453,40.500,402.516,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(15) 410.094,40.500,413.406,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(16) 420.984,40.500,432.000,49.500 '827\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 71.996,486.317,241.898,607.850 'S\\nSAC (soft actor-critic), 717\\n“same” padding, computer vision, 488\\nSAMME, 225\\nsample inefficiency, 698\\nsampled softmax, 600\\nsampling bias, 29, 57\\nsampling noise, 29\\nSARIMA model, 550-551\\nSavedModel, 723-725\\nsaving, loading, and restoring models, 337-338,\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.236,475.517,113.360,484.517 '413-415\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,443.117,210.992,473.717 'scaled dot-product attention layer, 616\\nscatter matrix, 64-65\\nScikit-Learn, xvi\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.236,291.917,243.410,441.317 'bagging and pasting in, 217\\nCART algorithm, 199, 205\\ncross-validation, 89-91\\ndesign principles, 70-71\\nPCA implementation, 246\\nPipeline constructor, 84-87\\nsklearn.base.BaseEstimator, 80\\nsklearn.base.clone(), 163\\nsklearn.base.TransformerMixin, 80\\nsklearn.cluster.DBSCAN, 279\\nsklearn.cluster.KMeans, 81, 263\\nsklearn.cluster.MiniBatchKMeans, 268\\nsklearn.compose.ColumnTransformer, 85\\nsklearn.compose.TransformedTargetRegres‐\\n'>\n",
            "<LTTextBoxHorizontal(4) 96.476,281.117,120.443,290.117 'sor, 79\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.236,248.717,231.494,279.317 'sklearn.datasets.load_iris(), 167\\nsklearn.datasets.make_moons(), 179\\nsklearn.decomposition.IncrementalPCA,\\n'>\n",
            "<LTTextBoxHorizontal(6) 96.476,237.917,109.436,246.917 '251\\n'>\n",
            "<LTTextBoxHorizontal(7) 84.236,194.717,240.098,236.117 'sklearn.decomposition.PCA, 246\\nsklearn.ensemble.AdaBoostClassifier, 226\\nsklearn.ensemble.BaggingClassifier, 217\\nsklearn.ensemble.GradientBoostingRegres‐\\n'>\n",
            "<LTTextBoxHorizontal(8) 96.476,183.917,140.927,192.917 'sor, 227-230\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.236,173.117,231.359,182.117 'sklearn.ensemble.HistGradientBoosting‐\\n'>\n",
            "<LTTextBoxHorizontal(10) 96.476,162.317,147.002,171.317 'Classifier, 230\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.236,151.517,240.773,160.517 'sklearn.ensemble.HistGradientBoostingRe‐\\n'>\n",
            "<LTTextBoxHorizontal(12) 96.476,140.717,139.442,149.717 'gressor, 230\\n'>\n",
            "<LTTextBoxHorizontal(13) 84.236,129.917,237.596,138.917 'sklearn.ensemble.RandomForestClassifier,\\n'>\n",
            "<LTTextBoxHorizontal(14) 96.476,119.117,193.820,128.117 '117-119, 214, 220, 221, 248\\n'>\n",
            "<LTTextBoxHorizontal(15) 84.236,108.317,239.450,117.317 'sklearn.ensemble.RandomForestRegressor,\\n'>\n",
            "<LTTextBoxHorizontal(16) 96.476,97.517,122.171,106.517 '90, 220\\n'>\n",
            "<LTTextBoxHorizontal(17) 84.236,75.917,231.503,95.717 'sklearn.ensemble.StackingClassifier, 234\\nsklearn.ensemble.StackingRegressor, 234\\n'>\n",
            "<LTTextBoxHorizontal(18) 71.997,40.500,84.435,49.500 '828 \\n'>\n",
            "<LTTextBoxHorizontal(19) 92.013,40.500,95.325,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(20) 102.903,40.500,119.544,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.736,575.217,429.173,605.817 'sklearn.ensemble.VotingClassifier, 214\\nsklearn.externals.joblib, 97-98\\nsklearn.feature_selection.SelectFromModel,\\n'>\n",
            "<LTTextBoxHorizontal(22) 283.976,564.417,292.616,573.417 '95\\n'>\n",
            "<LTTextBoxHorizontal(23) 271.736,499.617,427.364,562.617 'sklearn.impute.IterativeImputer, 69\\nsklearn.impute.KNNImputer, 69\\nsklearn.impute.SimpleImputer, 68\\nsklearn.linear_model.ElasticNet, 162\\nsklearn.linear_model.Lasso, 161\\nsklearn.linear_model.LinearRegression, 25,\\n'>\n",
            "<LTTextBoxHorizontal(24) 283.976,488.817,356.516,497.817 '71, 79, 137, 138, 150\\n'>\n",
            "<LTTextBoxHorizontal(25) 271.736,478.017,419.660,487.017 'sklearn.linear_model.LogisticRegression,\\n'>\n",
            "<LTTextBoxHorizontal(26) 283.976,467.217,331.046,476.217 '168, 172, 275\\n'>\n",
            "<LTTextBoxHorizontal(27) 271.736,402.417,421.712,465.417 'sklearn.linear_model.Perceptron, 307\\nsklearn.linear_model.Ridge, 157\\nsklearn.linear_model.RidgeCV, 158\\nsklearn.linear_model.SGDClassifier, 106,\\n111, 112, 117-119, 121, 183, 188, 307\\nsklearn.linear_model.SGDRegressor, 147,\\n'>\n",
            "<LTTextBoxHorizontal(28) 283.976,391.617,296.936,400.617 '163\\n'>\n",
            "<LTTextBoxHorizontal(29) 271.736,380.817,428.012,389.817 'sklearn.manifold.LocallyLinearEmbedding,\\n'>\n",
            "<LTTextBoxHorizontal(30) 283.976,370.017,296.936,379.017 '254\\n'>\n",
            "<LTTextBoxHorizontal(31) 271.736,359.217,420.578,368.217 'sklearn.metrics.ConfusionMatrixDisplay,\\n'>\n",
            "<LTTextBoxHorizontal(32) 283.976,348.417,296.936,357.417 '122\\n'>\n",
            "<LTTextBoxHorizontal(33) 271.736,337.617,418.445,346.617 'sklearn.metrics.confusion_matrix(), 109,\\n'>\n",
            "<LTTextBoxHorizontal(34) 283.976,326.817,296.936,335.817 '122\\n'>\n",
            "<LTTextBoxHorizontal(35) 271.736,294.417,423.738,325.017 'sklearn.metrics.f1_score(), 111, 126\\nsklearn.metrics.mean_squared_error(), 88\\nsklearn.metrics.precision_recall_curve(),\\n'>\n",
            "<LTTextBoxHorizontal(36) 283.977,283.617,313.992,292.617 '113, 117\\n'>\n",
            "<LTTextBoxHorizontal(37) 271.737,218.817,426.573,281.817 'sklearn.metrics.precision_score(), 110\\nsklearn.metrics.recall_score(), 110\\nsklearn.metrics.roc_auc_score(), 116\\nsklearn.metrics.roc_curve(), 115\\nsklearn.metrics.silhouette_score(), 271\\nsklearn.mixture.BayesianGaussianMixture,\\n'>\n",
            "<LTTextBoxHorizontal(38) 283.977,208.017,296.937,217.017 '292\\n'>\n",
            "<LTTextBoxHorizontal(39) 271.737,186.417,413.766,206.217 'sklearn.mixture.GaussianMixture, 283\\nsklearn.model_selection.cross_val_pre‐\\n'>\n",
            "<LTTextBoxHorizontal(40) 283.977,175.617,389.196,184.617 'dict(), 108, 113, 117, 122, 232\\n'>\n",
            "<LTTextBoxHorizontal(41) 271.737,164.817,425.826,173.817 'sklearn.model_selection.cross_val_score(),\\n'>\n",
            "<LTTextBoxHorizontal(42) 283.977,154.017,309.672,163.017 '90, 107\\n'>\n",
            "<LTTextBoxHorizontal(43) 271.737,143.217,416.241,152.217 'sklearn.model_selection.GridSearchCV,\\n'>\n",
            "<LTTextBoxHorizontal(44) 283.977,132.417,304.461,141.417 '91-93\\n'>\n",
            "<LTTextBoxHorizontal(45) 271.737,121.617,423.765,130.617 'sklearn.model_selection.learning_curve(),\\n'>\n",
            "<LTTextBoxHorizontal(46) 283.977,110.817,296.937,119.817 '152\\n'>\n",
            "<LTTextBoxHorizontal(47) 271.737,100.017,410.256,109.017 'sklearn.model_selection.Randomized‐\\n'>\n",
            "<LTTextBoxHorizontal(48) 283.977,89.217,337.338,98.217 'SearchCV, 248\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,596.817,230.706,605.817 'sklearn.model_selection.StratifiedKFold,\\n'>\n",
            "<LTTextBoxHorizontal(1) 96.480,586.017,109.440,595.017 '108\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,575.217,238.968,584.217 'sklearn.model_selection.StratifiedShuffleS‐\\n'>\n",
            "<LTTextBoxHorizontal(3) 96.480,564.417,121.365,573.417 'plit, 59\\n'>\n",
            "<LTTextBoxHorizontal(4) 84.240,553.617,236.169,562.617 'sklearn.model_selection.train_test_split(),\\n'>\n",
            "<LTTextBoxHorizontal(5) 96.480,542.817,130.590,551.817 '57, 59, 89\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.240,499.617,241.578,541.017 'sklearn.multiclass.OneVsOneClassifier, 120\\nsklearn.multiclass.OneVsRestClassifier, 120\\nsklearn.multioutput.ChainClassifier, 126\\nsklearn.neighbors.KNeighborsClassifier,\\n'>\n",
            "<LTTextBoxHorizontal(7) 96.480,488.817,143.550,497.817 '125, 128, 280\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.240,424.017,244.323,487.017 'sklearn.neighbors.KNeighborsRegressor, 26\\nsklearn.neural_network.MLPClassifier, 316\\nsklearn.neural_network.MLPRegressor, 313\\nsklearn.pipeline.make_pipeline(), 154\\nsklearn.pipeline.Pipeline, 83\\nsklearn.preprocessing.FunctionTransformer,\\n'>\n",
            "<LTTextBoxHorizontal(9) 96.480,413.217,105.120,422.217 '79\\n'>\n",
            "<LTTextBoxHorizontal(10) 84.240,391.617,230.355,411.417 'sklearn.preprocessing.MinMaxScaler, 76\\nsklearn.preprocessing.OneHotEncoder,\\n'>\n",
            "<LTTextBoxHorizontal(11) 96.480,380.817,142.434,389.817 '72-74, 77, 86\\n'>\n",
            "<LTTextBoxHorizontal(12) 84.240,370.017,238.248,379.017 'sklearn.preprocessing.OrdinalEncoder, 71,\\n'>\n",
            "<LTTextBoxHorizontal(13) 96.480,359.217,109.440,368.217 '231\\n'>\n",
            "<LTTextBoxHorizontal(14) 84.240,348.417,238.275,357.417 'sklearn.preprocessing.PolynomialFeatures,\\n'>\n",
            "<LTTextBoxHorizontal(15) 96.480,337.617,126.495,346.617 '150, 179\\n'>\n",
            "<LTTextBoxHorizontal(16) 84.240,326.817,233.667,335.817 'sklearn.preprocessing.StandardScaler, 76,\\n'>\n",
            "<LTTextBoxHorizontal(17) 96.480,316.017,143.550,325.017 '141, 156, 179\\n'>\n",
            "<LTTextBoxHorizontal(18) 84.240,305.217,235.818,314.217 'sklearn.random_projection.GaussianRan‐\\n'>\n",
            "<LTTextBoxHorizontal(19) 259.500,467.217,430.968,605.817 'score(), 71\\nsearch engines, clustering for, 262\\nsearch space, 93\\nseasonality, time series modeling, 545\\nsecond moment (variance of gradient), 384\\nsecond-order partial derivatives (Hessians), 386\\nsegment embedding, 621\\nSelectFromModel, 95\\nself-attention layers, 612, 618, 627\\nself-distillation, 627\\nself-normalization, 364, 397, 401\\nself-supervised learning, 15-16\\nSELU (scaled ELU) activation function, 364,\\n'>\n",
            "<LTTextBoxHorizontal(20) 271.740,456.417,284.700,465.417 '397\\n'>\n",
            "<LTTextBoxHorizontal(21) 259.500,305.217,426.261,454.617 'semantic interpolation, 659\\nsemantic segmentation, 8, 273, 531-535\\nsemi-supervised learning, 14, 262, 275-278\\nSENet, 510-512\\nsensitivity (recall), ROC curve, 115\\nsensitivity metric, 110\\nsensors, 684\\nsentence encoder, 474, 594\\nSentencePiece project, 588\\nsentiment analysis, 587-595\\nsentiment neuron, 586\\nseparable convolution layer, 509\\nsequence length, 573, 612\\nsequence-to-sequence (seq2seq) network, 541,\\n'>\n",
            "<LTTextBoxHorizontal(22) 96.480,294.417,167.850,303.417 'domProjection, 253\\n'>\n",
            "<LTTextBoxHorizontal(23) 271.740,294.417,300.864,303.417 '562-564\\n'>\n",
            "<LTTextBoxHorizontal(24) 84.240,283.617,243.045,292.617 'sklearn.random_projection.SparseRandom‐\\n'>\n",
            "<LTTextBoxHorizontal(25) 96.480,272.817,151.137,281.817 'Projection, 253\\n'>\n",
            "<LTTextBoxHorizontal(26) 84.240,262.017,240.363,271.017 'sklearn.semi_supervised.LabelPropagation,\\n'>\n",
            "<LTTextBoxHorizontal(27) 96.480,251.217,109.440,260.217 '278\\n'>\n",
            "<LTTextBoxHorizontal(28) 84.240,240.417,232.434,249.417 'sklearn.semi_supervised.LabelSpreading,\\n'>\n",
            "<LTTextBoxHorizontal(29) 96.480,229.617,109.440,238.617 '278\\n'>\n",
            "<LTTextBoxHorizontal(30) 84.240,218.817,242.964,227.817 'sklearn.semi_supervised.SelfTrainingClassi‐\\n'>\n",
            "<LTTextBoxHorizontal(31) 96.480,208.017,125.775,217.017 'fier, 278\\n'>\n",
            "<LTTextBoxHorizontal(32) 84.240,164.817,227.745,206.217 'sklearn.svm.LinearSVC, 177, 183, 188\\nsklearn.svm.SVC, 120, 180, 183, 188\\nsklearn.svm.SVR, 185\\nsklearn.tree.DecisionTreeClassifier, 195,\\n'>\n",
            "<LTTextBoxHorizontal(33) 96.480,154.017,160.605,163.017 '201, 202, 207, 220\\n'>\n",
            "<LTTextBoxHorizontal(34) 84.240,143.217,242.334,152.217 'sklearn.tree.DecisionTreeRegressor, 89, 195,\\n'>\n",
            "<LTTextBoxHorizontal(35) 96.480,132.417,126.495,141.417 '204, 226\\n'>\n",
            "<LTTextBoxHorizontal(36) 84.240,78.417,217.179,130.617 'sklearn.tree.export_graphviz(), 196\\nsklearn.tree.ExtraTreesClassifier, 221\\nsklearn.utils.estimator_checks, 82\\nsklearn.utils.validation module, 80\\nSVM classification classes, 183\\n'>\n",
            "<LTTextBoxHorizontal(37) 259.500,197.217,430.995,292.617 'sequence-to-vector network, 541\\nSequenceExample protobuf, 459\\nsequential API, image classifier with, 318-328\\nservice account, GCP, 735\\nservice worker, 745\\nsets, 797\\nset_params(), 80\\nSGD (see stochastic gradient descent)\\nSGDClassifier, 106, 111, 112, 117-119, 121, 183,\\n'>\n",
            "<LTTextBoxHorizontal(38) 271.740,186.417,301.755,195.417 '188, 188\\n'>\n",
            "<LTTextBoxHorizontal(39) 259.500,78.417,427.234,184.617 'SGDRegressor, 147, 163\\nsharpening, NLP transformers, 628\\nshrinkage, 228\\nshuffle_and_split_data(), 57, 57\\nshuffling data, 146, 445-446\\nSiamese neural network, 756\\nsigmoid activation function, 164, 312, 358, 651\\nsignals, 301\\nsilhouette coefficient, 270\\nsilhouette diagram, 271\\n'>\n",
            "<LTTextBoxHorizontal(40) 384.454,40.500,402.517,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(41) 410.095,40.500,413.407,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(42) 420.985,40.500,432.001,49.500 '829\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,521.217,243.567,605.817 'SiLU activation function, 366\\nsimilarity features, SVM, 181\\nSimpleImputer, 68\\nsimulated annealing, 145\\nsimulated environments, 687-691\\nsingle program, multiple data (SPMD), 759-765\\nsingle-shot learning, 534\\nsingular value decomposition (SVD), 137, 245,\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,510.417,97.200,519.417 '250\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,445.617,224.280,508.617 'skewed datasets, 108\\nskewed left/right, 55\\nskip connections, 364, 505\\nslack variable, 188\\nsmoothing terms, 368, 382\\nSMOTE (synthetic minority oversampling\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,434.817,140.130,443.817 'technique), 500\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,78.417,243.567,433.017 'soft actor-critic (SAC), 717\\nsoft clustering, 264\\nsoft margin classification, 177-178, 188\\nsoft voting, 215\\nsoftmax activation function, 170, 315, 320, 600\\nsoftmax regression, 170-173\\nsoftplus activation function, 314\\nspam filters, 3-7, 9-10\\nsparse autoencoders, 651-654\\nsparse features, SVC class, 183\\nsparse matrix, 73, 76, 86\\nsparse models, 159, 387\\nsparse tensors, 795\\nsparsity loss, 652\\nspecificity, ROC curve, 115\\nspectral clustering, 283\\nspeech recognition, 6, 8\\nsplit node, decision tree, 197\\nSPMD (single program, multiple data), 759-765\\nsquared hinge loss, 188\\nStable Diffusion, 680\\nstacked autoencoders, 640-648\\nstacking (stacked generalization), 232-235\\nstale gradients, 761\\nstandard correlation coefficient, 63\\nstandard deviation, 53\\nstandardization, 76\\nStandardScaler, 76, 141, 156, 179\\nstate-action values (Q-Values), 702-703\\nstateful metric, 417\\nstateful RNN, 578, 584-586\\nstateless RNN, 578, 584, 586\\nstationary time series, 548\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,40.500,84.438,49.500 '830 \\n'>\n",
            "<LTTextBoxHorizontal(6) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(7) 102.906,40.500,119.547,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(8) 259.500,532.017,431.913,605.817 'statistical mode, 216\\nstatistical significance, 202\\nstatsmodels library, 550\\nstemming, 130\\nstep functions, TLU, 304\\nstochastic gradient boosting, 230\\nstochastic gradient descent (SGD), 145-148, 183\\n'>\n",
            "<LTTextBoxHorizontal(9) 271.740,467.217,413.940,530.217 'early stopping, 163\\nimage classification, 324\\nand perceptron learning algorithm, 307\\nridge regularization, 158\\nand TD learning, 704\\ntraining binary classifier, 106\\n'>\n",
            "<LTTextBoxHorizontal(10) 259.500,251.217,412.050,465.417 'stratified sampling, 58-60, 108\\nstrat_train_set, 68\\nstreaming metric, 417\\nstrides, 482, 531, 573\\nstring kernels, 182\\nstring tensors, 411\\nStringLookup layer, 465\\nstrings, 793-794\\nstrong learners, 213\\nstyle mixing, StyleGAN, 673\\nstyle transfer, GANs, 671\\nStyleGANs, 671-673\\nsubclassing API, 336-337\\nsubgradient vector, 161\\nsubsampling, pooling layer, 491\\nsubword regularization, 588\\nsuper-convergence, 390\\nsuper-resolution, 534\\nsupervised learning, 10\\nsupport vector machines (SVMs), 175-193\\n'>\n",
            "<LTTextBoxHorizontal(11) 271.740,154.017,382.656,249.417 'decision function, 186-189\\ndual problem, 189-193\\nkernelized SVMs, 190-193\\nlinear classification, 175-178\\nmechanics of, 186-193\\nin multiclass classification, 120\\nnonlinear classifiers, 178\\none-class SVM, 294\\nSVM regression, 184-186\\n'>\n",
            "<LTTextBoxHorizontal(12) 259.500,121.617,427.287,152.217 'support vectors, 176\\nSVC class, 180, 183\\nSVD (singular value decomposition), 137, 245,\\n'>\n",
            "<LTTextBoxHorizontal(13) 271.740,110.817,284.700,119.817 '250\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.500,78.417,391.827,109.017 'SVMs (see support vector machines)\\nSVR class, 185\\nSwish activation function, 366\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,564.417,242.568,605.817 'Swiss roll dataset, 241-242\\nsymbolic differentiation, 788\\nsymbolic tensors, 436, 802\\nsynchronous updates, with centralized parame‐\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,553.617,114.507,562.617 'ters, 761\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,542.817,227.214,551.817 'synthetic minority oversampling technique\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,532.017,137.196,541.017 '(SMOTE), 500\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.997,495.917,243.609,520.250 'T\\nt-distributed stochastic neighbor embedding (t-\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.997,366.317,236.958,494.117 'SNE), 256, 643\\ntarget attributes, 55\\ntarget distribution, transforming, 78\\ntarget model, DQN, 713\\nTD error, 704\\nTD target, 704\\nTD-Gammon, 683\\nteacher forcing, 595\\ntemperature, Char-RNN model, 583\\ntemporal difference (TD) learning, 703, 715\\ntensor arrays, 796\\ntensor processing units (TPUs), 404, 731, 744,\\n'>\n",
            "<LTTextBoxHorizontal(6) 84.237,355.517,97.197,364.517 '770\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.997,333.917,210.723,353.717 'TensorBoard, 340-344, 406, 592, 770\\nTensorFlow, xvi, xvi, 403-477, 721-776\\n'>\n",
            "<LTTextBoxHorizontal(8) 84.237,290.717,227.571,332.117 '(see also Keras API)\\narchitecture, 405\\ncreating training function, 452\\ncustom models (see custom models and\\n'>\n",
            "<LTTextBoxHorizontal(9) 96.477,279.917,169.557,288.917 'training algorithms)\\n'>\n",
            "<LTTextBoxHorizontal(10) 84.237,269.117,214.278,278.117 'deploying model to a mobile device,\\n'>\n",
            "<LTTextBoxHorizontal(11) 96.477,258.317,125.601,267.317 '741-744\\n'>\n",
            "<LTTextBoxHorizontal(12) 84.237,85.517,243.744,256.517 'functions and graphs, 801-809\\nGPU management with, 749-752, 753-756\\ngraphs and functions, 404, 433-438, 801-809\\nhub.KerasLayer, 474\\nmath operations, 408\\nwith NumPy, 407-411\\noperations (ops) and tensors, 404, 407-409\\nparallelism to train models (see parallelism)\\nplatforms and APIs available, 405\\nserving a model (see TensorFlow Serving)\\nspecial data structures, 793-799\\ntf.add(), 408\\ntf.autograph.to_code(), 437\\ntf.cast(), 409\\ntf.config.set_soft_device_placement, 753\\ntf.config.threading.set_inter_op_parallel‐\\n'>\n",
            "<LTTextBoxHorizontal(13) 96.477,74.717,164.067,83.717 'ism_threads(), 755\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.737,596.817,420.570,605.817 'tf.config.threading.set_intra_op_parallel‐\\n'>\n",
            "<LTTextBoxHorizontal(15) 283.977,586.017,351.567,595.017 'ism_threads(), 755\\n'>\n",
            "<LTTextBoxHorizontal(16) 271.737,542.817,426.744,584.217 'tf.constant(), 407\\ntf.data API (see tf.data API)\\ntf.device(), 753\\ntf.distribute.experimental.CentralStorageS‐\\n'>\n",
            "<LTTextBoxHorizontal(17) 283.977,532.017,326.025,541.017 'trategy, 766\\n'>\n",
            "<LTTextBoxHorizontal(18) 271.737,499.617,429.030,530.217 'tf.distribute.experimental.TPUStrategy, 770\\ntf.distribute.MirroredStrategy, 766, 773\\ntf.distribute.MultiWorkerMirroredStrategy,\\n'>\n",
            "<LTTextBoxHorizontal(19) 283.977,488.817,296.937,497.817 '768\\n'>\n",
            "<LTTextBoxHorizontal(20) 271.737,262.017,431.604,487.017 'tf.float32, 409\\ntf.float32 data type, 418, 448\\ntf.function(), 433, 437\\ntf.int32 data type, 411\\ntf.io.decode_base64(), 741\\ntf.io.decode_csv(), 449\\ntf.io.decode_image(), 741\\ntf.io.decode_png(), 741\\ntf.io.decode_proto(), 456\\ntf.io.FixedLenFeature, 457\\ntf.io.parse_example(), 458\\ntf.io.parse_sequence_example(), 459\\ntf.io.parse_single_example(), 457\\ntf.io.parse_single_sequence_example(), 459\\ntf.io.parse_tensor(), 458\\ntf.io.serialize_tensor(), 458\\ntf.io.TFRecordOptions, 454\\ntf.io.TFRecordWriter, 453\\ntf.io.VarLenFeature, 457\\ntf.linalg.band_part(), 618\\ntf.lite.TFLiteConverter.from_keras_model(),\\n'>\n",
            "<LTTextBoxHorizontal(21) 283.977,251.217,296.937,260.217 '742\\n'>\n",
            "<LTTextBoxHorizontal(22) 271.737,78.417,423.198,249.417 'tf.make_tensor_proto(), 729\\ntf.matmul(), 408, 616\\ntf.nn.conv2d(), 487\\ntf.nn.embedding_lookup(), 475\\ntf.nn.local_response_normalization(), 501\\ntf.nn.moments(), 475\\ntf.nn.sampled_softmax_loss(), 600\\ntf.py_function(), 437, 724\\ntf.queue module, 411\\ntf.queue.FIFOQueue, 798\\ntf.RaggedTensor, 411\\ntf.random.categorical(), 583\\ntf.reduce_max(), 495\\ntf.reduce_mean(), 432\\ntf.reduce_sum(), 434\\ntf.saved_model_cli command, 724\\n'>\n",
            "<LTTextBoxHorizontal(23) 384.450,40.500,402.513,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(24) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(25) 420.981,40.500,431.997,49.500 '831\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,456.417,208.440,605.817 'tf.sets module, 411\\ntf.sort(), 437\\ntf.SparseTensor, 411\\ntf.stack(), 449, 585\\ntf.string data type, 411\\ntf.strings module, 411\\ntf.Tensor, 407, 410\\ntf.TensorArray, 411\\ntf.transpose(), 408\\ntf.Variable, 410\\ntf.Variable.assign(), 410\\ntype conversions, 409\\nvariables, 410\\nweb page, running a model in, 745\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,294.417,236.808,454.617 'TensorFlow cluster, 767-770\\nTensorFlow Datasets (TFDS) project, 475-477\\nTensorFlow Extended (TFX), 406\\nTensorFlow Hub, 406, 473, 594\\nTensorFlow Lite, 406\\nTensorFlow playground, 316\\nTensorFlow Serving (TF Serving), 722-741\\nbatch prediction jobs on Vertex AI, 739\\ncreating prediction service, 732-739\\ndeploying new model version, 730-731\\nDocker container, 726\\nexporting SavedModels, 723-725\\ngRPC API, querying through, 729\\ninstalling and starting up, 725-727\\nREST API, querying through, 727\\n'>\n",
            "<LTTextBoxHorizontal(2) 72.000,251.217,241.218,292.617 'TensorFlow Text, 473, 589\\nTensorFlow.js (TFJS) JavaScript library, 745\\ntensors, 407-409\\nterm-frequency x inverse-document-frequency\\n'>\n",
            "<LTTextBoxHorizontal(3) 84.240,240.417,135.306,249.417 '(TF-IDF), 472\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,197.217,236.286,238.617 'terminal state, Markov chain, 699\\ntest set, 34, 55-60\\ntext attributes, 71\\ntext processing (see natural language process‐\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,186.417,98.901,195.417 'ing)\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,164.817,204.921,184.617 'TF Serving (see TensorFlow Serving)\\ntf.data API, 442-453\\n'>\n",
            "<LTTextBoxHorizontal(7) 84.240,143.217,216.558,163.017 'chaining transformations, 443-445\\ninterleaving lines from multiple files,\\n'>\n",
            "<LTTextBoxHorizontal(8) 96.480,132.417,125.604,141.417 '446-447\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.240,78.417,212.283,130.617 'and Keras preprocessing layers, 462\\nprefetching, 450-452\\npreprocessing the data, 448-449\\nshuffling data, 445-446\\ntf.data.AUTOTUNE, 444\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.000,40.500,84.438,49.500 '832 \\n'>\n",
            "<LTTextBoxHorizontal(11) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(12) 102.906,40.500,119.547,49.500 'Index\\n'>\n",
            "<LTTextBoxHorizontal(13) 271.740,596.817,402.393,605.817 'tf.data.Dataset.from_tensor_slices(),\\n'>\n",
            "<LTTextBoxHorizontal(14) 283.980,586.017,313.104,595.017 '442-444\\n'>\n",
            "<LTTextBoxHorizontal(15) 271.740,564.417,410.826,584.217 'tf.data.TFRecordDataset, 454, 454, 457\\nusing dataset with Keras, 452-453\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.500,456.417,424.308,562.617 'TFDS (TensorFlow Datasets) project, 475-477\\nTFJS (TensorFlow.js) JavaScript library, 745\\nTFLite, 742-744\\nTFRecord format, 441, 453-459\\nTFX (TensorFlow Extended), 406\\ntheoretical information criterion, 289\\n3D convolutional layers, 533\\nthreshold logic units (TLUs), 304, 309\\nTikhonov regularization, 156-158, 161\\ntime series data, forecasting, 537, 543-564\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.740,391.617,420.267,454.617 'ARMA model family, 549-551\\ndata preparation for ML models, 552-555\\nwith deep RNN, 557\\nwith linear model, 555\\nmultivariate time series, 545, 559-560\\nwith sequence-to-sequence model, 541,\\n'>\n",
            "<LTTextBoxHorizontal(18) 283.980,380.817,313.104,389.817 '562-564\\n'>\n",
            "<LTTextBoxHorizontal(19) 271.740,359.217,391.305,379.017 'several time steps ahead, 560-562\\nwith simple RNN, 556-557\\n'>\n",
            "<LTTextBoxHorizontal(20) 259.500,294.417,424.461,357.417 'TLUs (threshold logic units), 304, 309\\nTNR (true negative rate), 115\\ntokenizers library, 589\\ntolerance (ε), 144, 183\\nTPR (true positive rate), 110, 115\\nTPUs (tensor processing units), 404, 731, 744,\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.740,283.617,284.700,292.617 '770\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.500,229.617,360.210,281.817 'train-dev set, 36\\ntraining, 24\\ntraining instance, 4\\ntraining loops, 430-433, 452\\ntraining models, 131-173\\n'>\n",
            "<LTTextBoxHorizontal(23) 271.740,175.617,402.366,227.817 'learning curves in, 151-155\\nlinear regression, 131, 132-149\\nlogistic regression, 164-173\\nperceptrons, 304-308\\npolynomial regression, 132, 149-151\\n'>\n",
            "<LTTextBoxHorizontal(24) 259.500,164.817,321.483,173.817 'training set, 4, 34\\n'>\n",
            "<LTTextBoxHorizontal(25) 271.740,78.417,391.575,163.017 'cost function of, 165-166\\ninsufficient quantities, 27\\nirrelevant features, 30\\nmin-max scaling, 75\\nnonrepresentative, 28\\noverfitting, 30-33\\npreparing for ML algorithms, 68\\ntraining and evaluating on, 88-89\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 84.240,575.217,162.018,605.817 'transforming data, 79\\nunderfitting, 33\\nvisualizing data, 60\\n'>\n",
            "<LTTextBoxHorizontal(1) 72.000,510.417,228.222,573.417 'training set expansion, 129, 500, 519\\ntraining/serving skew, 442\\ntrain_test_split(), 59, 89\\ntransfer learning, 16, 373, 375-377, 518-521\\ntransform(), 71, 73, 75, 80\\ntransformation of data\\n'>\n",
            "<LTTextBoxHorizontal(2) 84.240,467.217,239.292,508.617 'custom transformers, 79-83\\nestimator transformers, 70\\nand feature scaling, 75-79\\ntransformer models (see transformer mod‐\\n'>\n",
            "<LTTextBoxHorizontal(3) 96.480,456.417,108.999,465.417 'els)\\n'>\n",
            "<LTTextBoxHorizontal(4) 72.000,413.217,189.720,454.617 'transformation pipelines, 83-88\\nTransformedTargetRegressor, 79\\ntransformer, 609\\ntransformer models\\n'>\n",
            "<LTTextBoxHorizontal(5) 84.240,348.417,214.146,411.417 'attention mechanisms, 609-619, 625\\nBERT, 620\\nDistilBERT, 622, 631-633\\nHugging Face library, 629-633\\nPathways language model, 623\\nvision transformers, 624-629\\n'>\n",
            "<LTTextBoxHorizontal(6) 72.000,305.217,205.551,346.617 'TransformerMixin, 80\\ntransformers library, 629-632\\ntranslation, with RNNs, 578, 595-604\\n(see also transformer models)\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.000,186.417,235.008,303.417 'transpose operator, 44\\ntransposed convolutional layer, 532-534\\ntrue negative rate (TNR), 115\\ntrue negatives, confusion matrix, 109\\ntrue positive rate (TPR), 110, 115\\ntrue positives, confusion matrix, 109\\ntrust region policy optimization (TRPO), 718\\n2D convolutional layers, 487\\ntying weights, 645\\ntype I errors, confusion matrix, 109\\ntype II errors, confusion matrix, 109\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.996,74.717,218.165,174.650 'U\\nuncertainty sampling, 278\\nundercomplete, autoencoder as, 638-640\\nunderfitting of data, 33, 89, 151-155, 182\\nunivariate regression, 43\\nunivariate time series, 545\\nUniversal Sentence Encoder, 594-595\\nunreasonable effectiveness of data, 27\\nunrolling the network through time, 538\\n'>\n",
            "<LTTextBoxHorizontal(9) 259.496,596.817,392.048,605.817 'unstable gradients problem, 358, 565\\n'>\n",
            "<LTTextBoxHorizontal(10) 271.736,586.017,431.234,595.017 '(see also vanishing and exploding gradients)\\n'>\n",
            "<LTTextBoxHorizontal(11) 259.496,575.217,397.583,584.217 'unsupervised learning, 11-14, 259-294\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.736,499.617,429.074,573.417 'anomaly detection, 13\\nassociation rule learning, 14\\nautoencoders (see autoencoders)\\nclustering (see clustering algorithms)\\ndensity estimation, 260\\ndiffusion models, 673-681\\ndimensionality reduction (see dimensional‐\\n'>\n",
            "<LTTextBoxHorizontal(13) 283.976,488.817,333.593,497.817 'ity reduction)\\n'>\n",
            "<LTTextBoxHorizontal(14) 271.736,402.417,429.164,487.017 'GANs (see generative adversarial networks)\\nGMM, 283-294\\nk-means (see k-means algorithm)\\nnovelty detection, 13\\npretraining, 377, 593, 620, 644\\nstacked autoencoders, 644\\ntransformer models, 621\\nvisualization algorithms, 12-13\\n'>\n",
            "<LTTextBoxHorizontal(15) 259.496,380.817,338.444,400.617 'upsampling layer, 532\\nutility function, 24\\n'>\n",
            "<LTTextBoxHorizontal(16) 259.496,301.517,416.825,369.050 'V\\nVAEs (variational autoencoders), 654-658\\n“valid” padding, computer vision, 488\\nvalidation set, 35, 325-327\\nvalue_counts(), 53\\nvanishing and exploding gradients, 358-373\\n'>\n",
            "<LTTextBoxHorizontal(17) 271.736,247.517,427.661,299.717 'activation function improvements, 361-366\\nbatch normalization, 367-372\\nGlorot and He initialization, 359-361\\ngradient clipping, 372\\nunstable gradients problem, 565-568\\n'>\n",
            "<LTTextBoxHorizontal(18) 259.496,236.717,291.293,245.717 'variables\\n'>\n",
            "<LTTextBoxHorizontal(19) 271.736,193.517,395.135,234.917 'handling in TF functions, 807-808\\npersistence of, 419\\nplacing on GPUs, 752\\nin TensorFlow, 410\\n'>\n",
            "<LTTextBoxHorizontal(20) 259.496,182.717,289.871,191.717 'variance\\n'>\n",
            "<LTTextBoxHorizontal(21) 271.736,139.517,406.763,180.917 'bias/variance trade-off, 155\\nexplained, 246-249\\nhigh variance with decision trees, 207\\npreserving, 243\\n'>\n",
            "<LTTextBoxHorizontal(22) 259.497,74.717,410.184,137.717 'variational autoencoders (VAEs), 654-658\\nvector-to-sequence network, 541\\nvectors, norms for measuring distance, 45\\nVertex AI, 98, 732-739, 770-772\\nVGGNet, 505\\nvirtual GPU device, 750\\n'>\n",
            "<LTTextBoxHorizontal(23) 384.450,40.500,402.513,49.500 'Index \\n'>\n",
            "<LTTextBoxHorizontal(24) 410.091,40.500,413.403,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(25) 420.981,40.500,431.997,49.500 '833\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,575.217,201.672,605.817 'vision transformers (ViTs), 624-629\\nvisual cortex architecture, 480\\nvisualization of data, 9, 12-13, 60-67\\n'>\n",
            "<LTTextBoxHorizontal(1) 84.240,510.417,209.592,573.417 'decision trees, 195-196\\ndimensionality reduction, 238, 247\\nend-to-end exercise, 60-67\\nMLPs with TensorBoard, 340-344\\nstacked autoencoders, 642-643\\nt-SNE, 256\\n'>\n",
            "<LTTextBoxHorizontal(2) 271.744,586.017,401.407,605.817 'in prioritized experience replay, 715\\nsaving instead of whole model, 338\\n'>\n",
            "<LTTextBoxHorizontal(3) 259.504,532.017,398.779,584.217 'white box models, 199\\nWide & Deep neural network, 329-333\\nwindow length, 581\\nwisdom of the crowd, 211\\nword embeddings, 467\\n'>\n",
            "<LTTextBoxHorizontal(4) 271.744,510.417,402.226,530.217 'neural machine translation, 595-604\\nsentiment analysis, 587-595\\n'>\n",
            "<LTTextBoxHorizontal(5) 72.000,488.817,200.403,508.617 'ViTs (vision transformers), 624-629\\nvoting classifiers, 212-215\\n'>\n",
            "<LTTextBoxHorizontal(6) 259.504,488.817,336.247,508.617 'worker task type, 766\\nworkers, 760\\n'>\n",
            "<LTTextBoxHorizontal(7) 72.003,441.917,236.191,477.050 'W\\nwall time, 370\\nwarmup phase, asynchronous model updates,\\n'>\n",
            "<LTTextBoxHorizontal(8) 259.496,441.917,403.462,477.050 'X\\nXavier initialization, 359\\nXception (Extreme Inception), 509-510,\\n'>\n",
            "<LTTextBoxHorizontal(9) 84.244,431.117,97.204,440.117 '762\\n'>\n",
            "<LTTextBoxHorizontal(10) 72.004,355.517,196.204,429.317 'WaveNet, 538, 574-575\\nweak learners, 213\\nweb page, running a model in, 744\\nweight decay, 386\\nweight stashing, 764\\nweight-tying, 645\\nweights\\n'>\n",
            "<LTTextBoxHorizontal(11) 84.244,312.317,178.852,353.717 'boosting, 223-225\\nconvolutional layers, 490\\nfreezing reused layers, 374\\nof hidden layers, 319\\n'>\n",
            "<LTTextBoxHorizontal(12) 271.738,431.117,300.862,440.117 '518-521\\n'>\n",
            "<LTTextBoxHorizontal(13) 259.498,409.517,392.644,429.317 'XLA (accelerated linear algebra), 434\\nXOR (exclusive or) problem, 307\\n'>\n",
            "<LTTextBoxHorizontal(14) 259.497,373.417,400.510,397.750 'Y\\nYou Only Look Once (YOLO), 527-530\\n'>\n",
            "<LTTextBoxHorizontal(15) 259.500,315.717,382.503,361.650 'Z\\nzero padding, 482, 488\\nzero-shot learning (ZSL), 622, 628\\nZFNet, 501\\n'>\n",
            "<LTTextBoxHorizontal(16) 72.000,40.500,84.438,49.500 '834 \\n'>\n",
            "<LTTextBoxHorizontal(17) 92.016,40.500,95.328,49.500 '| \\n'>\n",
            "<LTTextBoxHorizontal(18) 102.906,40.500,119.547,49.500 'Index\\n'>\n",
            "<LTLine 72.000,54.125,432.000,54.125>\n",
            "<LTTextBoxHorizontal(0) 72.000,582.144,161.869,597.894 'About the Author\\n'>\n",
            "<LTTextBoxHorizontal(1) 71.997,511.424,432.004,572.324 'Aurélien Géron is a machine learning consultant and lecturer. A former Googler, he\\nled  YouTube’s  video  classification  team  from  2013  to  2016.  He’s  been  a  founder  of\\nand CTO at a few different companies: Wifirst, a leading wireless ISP in France; Poly‐\\nconseil, a consulting firm focused on telecoms, media, and strategy; and Kiwisoft, a\\nconsulting firm focused on machine learning and data privacy.\\n'>\n",
            "<LTTextBoxHorizontal(2) 71.996,455.024,432.003,503.324 'Before  all  that  Aurélien  worked  as  an  engineer  in  a  variety  of  domains:  finance\\n(JP  Morgan  and  Société  Générale),  defense  (Canada’s  DOD),  and  healthcare  (blood\\ntransfusion).  He  also  published  a  few  technical  books  (on  C++,  WiFi,  and  internet\\narchitectures) and lectured about computer science at a French engineering school.\\n'>\n",
            "<LTTextBoxHorizontal(3) 71.997,411.224,432.000,446.924 'A  few  fun  facts:  he  taught  his  three  children  to  count  in  binary  with  their  fingers\\n(up  to  1,023),  he  studied  microbiology  and  evolutionary  genetics  before  going  into\\nsoftware engineering, and his parachute didn’t open on the second jump.\\n'>\n",
            "<LTTextBoxHorizontal(4) 71.994,385.731,121.103,401.481 'Colophon\\n'>\n",
            "<LTTextBoxHorizontal(5) 71.996,277.211,432.005,375.911 'The  animal  on  the  cover  of  Hands-On  Machine  Learning  with  Scikit-Learn,  Keras,\\nand TensorFlow is the fire salamander (Salamandra salamandra), an amphibian found\\nacross most of Europe. Its black, glossy skin features large yellow spots on the head\\nand  back,  signaling  the  presence  of  alkaloid  toxins.  This  is  a  possible  source  of  this\\namphibian’s  common  name:  contact  with  these  toxins  (which  they  can  also  spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\n'>\n",
            "<LTTextBoxHorizontal(6) 71.997,208.211,432.005,269.111 'Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost  of  their  lives  on  land,  they  give  birth  to  their  young  in  water.  They  subsist\\nmostly on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up\\nto a foot in length, and in captivity may live as long as 50 years.\\n'>\n",
            "<LTTextBoxHorizontal(7) 71.996,151.811,432.004,200.111 'The fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat they face is the susceptibility\\nof  their  moisture-permeable  skin  to  pollutants  and  microbes.  Since  2014,  they  have\\nbecome extinct in parts of the Netherlands and Belgium due to an introduced fungus.\\n'>\n",
            "<LTTextBoxHorizontal(8) 71.998,82.811,432.005,143.711 'Many  of  the  animals  on  O’Reilly  covers  are  endangered;  all  of  them  are  important\\nto the world. The cover illustration is by Karen Montgomery, based on an engraving\\nfrom  Wood’s  Illustrated  Natural  History.  The  cover  fonts  are  URW  Typewriter  and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\\n'>\n",
            "<LTLine 72.000,579.413,432.000,579.413>\n",
            "<LTLine 72.000,383.000,432.000,383.000>\n",
            "<LTTextBoxHorizontal(0) 91.868,342.375,340.390,394.846 'Learn from experts.  \\nBecome one yourself.\\n'>\n",
            "<LTTextBoxHorizontal(1) 92.466,278.695,277.618,329.038 'Books | Live online courses   \\nInstant Answers | Virtual events \\nVideos | Interactive learning\\n'>\n",
            "<LTTextBoxHorizontal(2) 92.466,211.967,295.535,228.799 'Get started at oreilly.com. \\n'>\n",
            "<LTTextBoxHorizontal(3) 470.234,160.332,473.877,165.053 '5\\n7\\n1\\n'>\n",
            "<LTTextBoxHorizontal(4) 470.234,158.580,473.877,159.501 '|\\n'>\n",
            "<LTTextBoxHorizontal(5) 470.234,157.064,473.877,157.749 '.\\n'>\n",
            "<LTTextBoxHorizontal(6) 470.234,153.447,473.877,156.955 'c\\nn\\n'>\n",
            "<LTTextBoxHorizontal(7) 470.234,152.598,473.877,153.374 'I\\n'>\n",
            "<LTTextBoxHorizontal(8) 470.234,151.082,473.877,151.767 ',\\n'>\n",
            "<LTTextBoxHorizontal(9) 470.234,148.649,473.877,149.315 'i\\n'>\n",
            "<LTTextBoxHorizontal(10) 470.234,139.479,473.877,151.009 'a\\nd\\ne\\nM\\ny\\n'>\n",
            "<LTTextBoxHorizontal(11) 470.234,137.261,473.877,139.407 'l\\nl\\ni\\n'>\n",
            "<LTTextBoxHorizontal(12) 470.234,130.281,473.877,137.188 'e\\nR\\nO\\n'>\n",
            "<LTTextBoxHorizontal(13) 470.234,132.722,473.877,133.326 '’\\n'>\n",
            "<LTTextBoxHorizontal(14) 470.234,94.463,473.877,129.450 'f\\no\\nk\\nr\\na\\nm\\ne\\nd\\na\\nr\\nt\\nd\\ne\\nr\\ne\\nt\\ns\\ng\\ne\\nr\\n'>\n",
            "<LTTextBoxHorizontal(15) 470.234,99.290,473.877,99.956 'i\\n'>\n",
            "<LTTextBoxHorizontal(16) 470.234,92.011,473.877,93.632 'a\\n'>\n",
            "<LTTextBoxHorizontal(17) 470.234,89.756,473.877,91.180 's\\n'>\n",
            "<LTTextBoxHorizontal(18) 470.234,89.017,473.877,89.683 'i\\n'>\n",
            "<LTTextBoxHorizontal(19) 470.234,86.528,473.877,88.186 'y\\n'>\n",
            "<LTTextBoxHorizontal(20) 470.234,84.310,473.877,86.456 'l\\nl\\ni\\n'>\n",
            "<LTTextBoxHorizontal(21) 470.234,77.330,473.877,84.237 'e\\nR\\nO\\n'>\n",
            "<LTTextBoxHorizontal(22) 470.234,79.771,473.877,80.375 '’\\n'>\n",
            "<LTTextBoxHorizontal(23) 470.234,75.814,473.877,76.499 '.\\n'>\n",
            "<LTTextBoxHorizontal(24) 470.234,72.197,473.877,75.705 'c\\nn\\n'>\n",
            "<LTTextBoxHorizontal(25) 470.234,71.348,473.877,72.124 'I\\n'>\n",
            "<LTTextBoxHorizontal(26) 470.234,69.833,473.877,70.518 ',\\n'>\n",
            "<LTTextBoxHorizontal(27) 470.234,67.399,473.877,68.066 'i\\n'>\n",
            "<LTTextBoxHorizontal(28) 470.234,58.230,473.877,69.760 'a\\nd\\ne\\nM\\ny\\n'>\n",
            "<LTTextBoxHorizontal(29) 470.234,56.011,473.877,58.157 'l\\nl\\ni\\n'>\n",
            "<LTTextBoxHorizontal(30) 470.234,51.473,473.877,52.077 '’\\n'>\n",
            "<LTTextBoxHorizontal(31) 470.234,37.578,473.877,55.938 'e\\nR\\nO\\n2\\n2\\n0\\n2\\n©\\n'>\n",
            "<LTRect 0.000,0.000,504.000,661.500>\n",
            "<LTRect 36.000,36.985,468.000,624.468>\n",
            "<LTFigure(Im0) -160.462,-107.995,664.483,769.307 matrix=[-249.83,-659.41,-575.12,217.89, (664.48,551.41)]>\n",
            "<LTFigure(Fm0) -218.514,633.430,295.806,664.153 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
            "<LTCurve 130.087,427.370,141.316,443.523>\n",
            "<LTCurve 149.649,427.370,160.107,443.522>\n",
            "<LTCurve 162.301,427.370,172.760,443.522>\n",
            "<LTRect 144.142,427.370,146.833,443.522>\n",
            "<LTCurve 94.798,429.651,106.239,441.092>\n",
            "<LTCurve 92.106,426.959,108.932,443.785>\n",
            "<LTCurve 117.252,436.793,124.694,440.831>\n",
            "<LTCurve 114.560,427.370,128.203,443.523>\n",
            "<LTCurve 108.689,439.820,112.652,443.784>\n",
            "<LTCurve 169.931,427.370,184.761,443.522>\n",
            "<LTCurve 186.273,440.217,189.619,443.564>\n",
            "<LTCurve 185.954,439.898,189.939,443.882>\n",
            "<LTCurve 187.464,442.075,188.502,442.582>\n",
            "<LTCurve 187.127,440.891,188.840,442.920>\n",
            "<LTTextLineHorizontal 470.234,48.274,473.877,48.959 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,59.960,473.877,60.645 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,70.591,473.877,71.275 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,76.572,473.877,77.257 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,88.259,473.877,88.944 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,91.253,473.877,91.938 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,93.705,473.877,94.390 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,109.497,473.877,110.182 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,125.698,473.877,126.383 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,129.523,473.877,130.208 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,141.210,473.877,141.895 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,151.840,473.877,152.525 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,157.822,473.877,158.507 ' \\n'>\n",
            "<LTTextLineHorizontal 470.234,159.574,473.877,160.259 ' \\n'>\n"
          ]
        }
      ],
      "source": [
        "for page_layout in  extract_pages(\"sample.pdf\"):\n",
        "  for element in page_layout:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gftCQd9XkhDE",
        "outputId": "4b2729c0-b179-4713-f714-4e09b22ce856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T\n",
            "\n",
            "E\n",
            "\n",
            "h\n",
            "\n",
            "d\n",
            "\n",
            "ir\n",
            "\n",
            "d\n",
            "\n",
            "iti\n",
            "\n",
            "o\n",
            "\n",
            "n\n",
            "\n",
            "Hands-On  \n",
            "Machine Learning  \n",
            "with Scikit-Learn,  \n",
            "Keras & TensorFlow\n",
            "\n",
            "Concepts, Tools, and Techniques  \n",
            "to Build Intelligent Systems\n",
            "\n",
            "TM\n",
            "\n",
            "Aurélien Géron\n",
            "\n",
            " \n",
            "\fHands-On Machine Learning  \n",
            "with Scikit-Learn, Keras, and TensorFlow\n",
            "\n",
            "Through a recent series of breakthroughs, deep learning has \n",
            "boosted the entire field of machine learning. Now, even program-\n",
            "mers who know close to nothing about this technology can use \n",
            "simple, efficient tools to implement programs capable of learning \n",
            "from data. This bestselling book uses concrete examples, minimal \n",
            "theory, and production-ready Python frameworks (Scikit-Learn, \n",
            "Keras, and TensorFlow) to help you gain an intuitive understanding \n",
            "of the concepts and tools for building intelligent systems.\n",
            "\n",
            "With this updated third edition, author Aurélien Géron explores \n",
            "a range of techniques, starting with simple linear regression and \n",
            "progressing to deep neural networks. Numerous code examples \n",
            "and exercises throughout the book help you apply what you’ve \n",
            "learned. Programming experience is all you need to get started. \n",
            "\n",
            "•  Use Scikit-Learn to track an example ML project end to end\n",
            "\n",
            "•  Explore several models, including support vector machines, \n",
            "\n",
            "decision trees, random forests, and ensemble methods\n",
            "\n",
            "•  Exploit unsupervised learning techniques such as \n",
            "\n",
            "dimensionality reduction, clustering, and anomaly detection\n",
            "\n",
            "•  Dive into neural net architectures, including convolutional  \n",
            "\n",
            "nets, recurrent nets, generative adversarial networks, \n",
            "autoencoders, diffusion models, and transformers\n",
            "\n",
            "•  Use TensorFlow and Keras to build and train neural nets for \n",
            "computer vision, natural language processing, generative \n",
            "models, and deep reinforcement learning\n",
            "\n",
            "Aurélien Géron is a machine learning consultant. A former Googler, \n",
            "he led YouTube’s video classification team from 2013 to 2016. He was \n",
            "also a founder and CTO of Wifirst from 2002 to 2012, a leading wireless \n",
            "ISP in France, and a founder and CTO of Polyconseil in 2001, a telecom \n",
            "consulting firm.\n",
            "\n",
            "“An exceptional \n",
            "resource to study \n",
            "machine learning. You \n",
            "will find clear-minded, \n",
            "intuitive explanations, \n",
            "and a wealth of \n",
            "practical tips.” \n",
            "\n",
            "—François Chollet\n",
            "Author of Keras, author of  \n",
            "Deep Learning with Python\n",
            "\n",
            "“This book is a great \n",
            "introduction to the \n",
            "theory and practice \n",
            "of solving problems \n",
            "with neural networks; \n",
            "I recommend it to \n",
            "anyone interested \n",
            "in learning about \n",
            "practical ML.” \n",
            "\n",
            "—Pete Warden\n",
            "Mobile Lead for TensorFlow\n",
            "\n",
            "DATA SCIENCE / MACHINE LEARNING\n",
            "\n",
            " CAN $99.99\n",
            "US $79.99 \n",
            "ISBN: 978-1-098-12597-4\n",
            "\n",
            "Twitter: @oreillymedia\n",
            "linkedin.com/company/oreilly-media\n",
            "youtube.com/oreillymedia \n",
            "\n",
            "\fTHIRD EDITION\n",
            "\n",
            "Hands-On Machine Learning\n",
            "with Scikit-Learn, Keras,\n",
            "and TensorFlow\n",
            "Concepts, Tools, and Techniques to\n",
            "Build Intelligent Systems\n",
            "\n",
            "Aurélien Géron\n",
            "\n",
            "Beijing\n",
            "Beijing\n",
            "\n",
            "Boston\n",
            "Boston\n",
            "\n",
            "Farnham Sebastopol\n",
            "Farnham Sebastopol\n",
            "\n",
            "Tokyo\n",
            "Tokyo\n",
            "\n",
            "\fHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
            "by Aurélien Géron\n",
            "\n",
            "Copyright © 2023 Aurélien Géron. All rights reserved.\n",
            "\n",
            "Printed in the United States of America.\n",
            "\n",
            "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
            "\n",
            "O’Reilly  books  may  be  purchased  for  educational,  business,  or  sales  promotional  use.  Online  editions\n",
            "are also available for most titles (https://oreilly.com). For more information, contact our corporate/institu‐\n",
            "tional sales department: 800-998-9938 or corporate@oreilly.com.\n",
            "\n",
            "Proofreader: Rachel Head\n",
            "Indexer: Potomac Indexing, LLC\n",
            "Interior Designer: David Futato\n",
            "Cover Designer: Karen Montgomery\n",
            "Illustrator: Kate Dullea\n",
            "\n",
            "Acquisitions Editor: Nicole Butterfield\n",
            "Development Editors: Nicole Taché and\n",
            "Michele Cronin\n",
            "Production Editor: Beth Kelly\n",
            "Copyeditor: Kim Cofer\n",
            "\n",
            "March 2017:\n",
            "September 2019:\n",
            "October 2022:\n",
            "\n",
            " First Edition\n",
            " Second Edition\n",
            " Third Edition\n",
            "\n",
            "Revision History for the Third Edition\n",
            "2022-10-03:  First Release\n",
            "\n",
            "See https://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.\n",
            "\n",
            "The  O’Reilly  logo  is  a  registered  trademark  of  O’Reilly  Media,  Inc.  Hands-On  Machine  Learning  with\n",
            "Scikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly\n",
            "Media, Inc.\n",
            "\n",
            "The  views  expressed  in  this  work  are  those  of  the  author,  and  do  not  represent  the  publisher’s  views.\n",
            "While  the  publisher  and  the  author  have  used  good  faith  efforts  to  ensure  that  the  information  and\n",
            "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
            "for  errors  or  omissions,  including  without  limitation  responsibility  for  damages  resulting  from  the  use\n",
            "of  or  reliance  on  this  work.  Use  of  the  information  and  instructions  contained  in  this  work  is  at  your\n",
            "own  risk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open\n",
            "source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
            "thereof complies with such licenses and/or rights.\n",
            "\n",
            "978-1-098-12597-4\n",
            "\n",
            "[LSI]\n",
            "\n",
            "\fTable of Contents\n",
            "\n",
            "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\n",
            "\n",
            "Part I. \n",
            "\n",
            "The Fundamentals of Machine Learning\n",
            "\n",
            "1. The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3\n",
            "What Is Machine Learning?                                                                                            4\n",
            "Why Use Machine Learning?                                                                                          5\n",
            "Examples of Applications                                                                                                8\n",
            "Types of Machine Learning Systems                                                                              9\n",
            "Training Supervision                                                                                                  10\n",
            "Batch Versus Online Learning                                                                                  17\n",
            "Instance-Based Versus Model-Based Learning                                                      21\n",
            "Main Challenges of Machine Learning                                                                       27\n",
            "Insufficient Quantity of Training Data                                                                    27\n",
            "Nonrepresentative Training Data                                                                             28\n",
            "Poor-Quality Data                                                                                                       30\n",
            "Irrelevant Features                                                                                                      30\n",
            "Overfitting the Training Data                                                                                   30\n",
            "Underfitting the Training Data                                                                                 33\n",
            "Stepping Back                                                                                                              33\n",
            "Testing and Validating                                                                                                   34\n",
            "Hyperparameter Tuning and Model Selection                                                       34\n",
            "Data Mismatch                                                                                                            35\n",
            "Exercises                                                                                                                          37\n",
            "\n",
            "iii\n",
            "\n",
            "\f2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   39\n",
            "Working with Real Data                                                                                                39\n",
            "Look at the Big Picture                                                                                                  41\n",
            "Frame the Problem                                                                                                     41\n",
            "Select a Performance Measure                                                                                  43\n",
            "Check the Assumptions                                                                                             46\n",
            "Get the Data                                                                                                                    46\n",
            "Running the Code Examples Using Google Colab                                                46\n",
            "Saving Your Code Changes and Your Data                                                             48\n",
            "The Power and Danger of Interactivity                                                                   49\n",
            "Book Code Versus Notebook Code                                                                          50\n",
            "Download the Data                                                                                                     50\n",
            "Take a Quick Look at the Data Structure                                                                51\n",
            "Create a Test Set                                                                                                          55\n",
            "Explore and Visualize the Data to Gain Insights                                                       60\n",
            "Visualizing Geographical Data                                                                                 61\n",
            "Look for Correlations                                                                                                 63\n",
            "Experiment with Attribute Combinations                                                              66\n",
            "Prepare the Data for Machine Learning Algorithms                                                67\n",
            "Clean the Data                                                                                                             68\n",
            "Handling Text and Categorical Attributes                                                              71\n",
            "Feature Scaling and Transformation                                                                        75\n",
            "Custom Transformers                                                                                                79\n",
            "Transformation Pipelines                                                                                          83\n",
            "Select and Train a Model                                                                                               88\n",
            "Train and Evaluate on the Training Set                                                                   88\n",
            "Better Evaluation Using Cross-Validation                                                              89\n",
            "Fine-Tune Your Model                                                                                                   91\n",
            "Grid Search                                                                                                                  91\n",
            "Randomized Search                                                                                                    93\n",
            "Ensemble Methods                                                                                                     95\n",
            "Analyzing the Best Models and Their Errors                                                         95\n",
            "Evaluate Your System on the Test Set                                                                       96\n",
            "Launch, Monitor, and Maintain Your System                                                            97\n",
            "Try It Out!                                                                                                                      100\n",
            "Exercises                                                                                                                        101\n",
            "\n",
            "3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   103\n",
            "MNIST                                                                                                                           103\n",
            "Training a Binary Classifier                                                                                        106\n",
            "Performance Measures                                                                                                107\n",
            "\n",
            "iv \n",
            "\n",
            "| \n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\fMeasuring Accuracy Using Cross-Validation                                                       107\n",
            "Confusion Matrices                                                                                                  108\n",
            "Precision and Recall                                                                                                 110\n",
            "The Precision/Recall Trade-off                                                                               111\n",
            "The ROC Curve                                                                                                        115\n",
            "Multiclass Classification                                                                                              119\n",
            "Error Analysis                                                                                                               122\n",
            "Multilabel Classification                                                                                              125\n",
            "Multioutput Classification                                                                                          127\n",
            "Exercises                                                                                                                        129\n",
            "\n",
            "4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\n",
            "Linear Regression                                                                                                         132\n",
            "The Normal Equation                                                                                              134\n",
            "Computational Complexity                                                                                     137\n",
            "Gradient Descent                                                                                                          138\n",
            "Batch Gradient Descent                                                                                           142\n",
            "Stochastic Gradient Descent                                                                                   145\n",
            "Mini-Batch Gradient Descent                                                                                 148\n",
            "Polynomial Regression                                                                                                149\n",
            "Learning Curves                                                                                                           151\n",
            "Regularized Linear Models                                                                                         155\n",
            "Ridge Regression                                                                                                       156\n",
            "Lasso Regression                                                                                                       158\n",
            "Elastic Net Regression                                                                                              161\n",
            "Early Stopping                                                                                                           162\n",
            "Logistic Regression                                                                                                      164\n",
            "Estimating Probabilities                                                                                           164\n",
            "Training and Cost Function                                                                                    165\n",
            "Decision Boundaries                                                                                                167\n",
            "Softmax Regression                                                                                                  170\n",
            "Exercises                                                                                                                        173\n",
            "\n",
            "5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   175\n",
            "Linear SVM Classification                                                                                          175\n",
            "Soft Margin Classification                                                                                       176\n",
            "Nonlinear SVM Classification                                                                                    178\n",
            "Polynomial Kernel                                                                                                    180\n",
            "Similarity Features                                                                                                    181\n",
            "Gaussian RBF Kernel                                                                                               181\n",
            "SVM Classes and Computational Complexity                                                     183\n",
            "\n",
            "Table of Contents \n",
            "\n",
            "| \n",
            "\n",
            "v\n",
            "\n",
            "\fSVM Regression                                                                                                           184\n",
            "Under the Hood of Linear SVM Classifiers                                                              186\n",
            "The Dual Problem                                                                                                        189\n",
            "Kernelized SVMs                                                                                                      190\n",
            "Exercises                                                                                                                        193\n",
            "\n",
            "6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   195\n",
            "Training and Visualizing a Decision Tree                                                                 195\n",
            "Making Predictions                                                                                                      197\n",
            "Estimating Class Probabilities                                                                                    199\n",
            "The CART Training Algorithm                                                                                  199\n",
            "Computational Complexity                                                                                        200\n",
            "Gini Impurity or Entropy?                                                                                          201\n",
            "Regularization Hyperparameters                                                                               201\n",
            "Regression                                                                                                                     204\n",
            "Sensitivity to Axis Orientation                                                                                   206\n",
            "Decision Trees Have a High Variance                                                                       207\n",
            "Exercises                                                                                                                        208\n",
            "\n",
            "7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\n",
            "Voting Classifiers                                                                                                          212\n",
            "Bagging and Pasting                                                                                                     215\n",
            "Bagging and Pasting in Scikit-Learn                                                                      217\n",
            "Out-of-Bag Evaluation                                                                                             218\n",
            "Random Patches and Random Subspaces                                                            219\n",
            "Random Forests                                                                                                            220\n",
            "Extra-Trees                                                                                                                 220\n",
            "Feature Importance                                                                                                  221\n",
            "Boosting                                                                                                                         222\n",
            "AdaBoost                                                                                                                    222\n",
            "Gradient Boosting                                                                                                    226\n",
            "Histogram-Based Gradient Boosting                                                                     230\n",
            "Stacking                                                                                                                          232\n",
            "Exercises                                                                                                                        235\n",
            "\n",
            "8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   237\n",
            "The Curse of Dimensionality                                                                                     238\n",
            "Main Approaches for Dimensionality Reduction                                                   239\n",
            "Projection                                                                                                                   239\n",
            "Manifold Learning                                                                                                    241\n",
            "PCA                                                                                                                                243\n",
            "\n",
            "vi \n",
            "\n",
            "| \n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\fPreserving the Variance                                                                                           243\n",
            "Principal Components                                                                                             244\n",
            "Projecting Down to d Dimensions                                                                         245\n",
            "Using Scikit-Learn                                                                                                    246\n",
            "Explained Variance Ratio                                                                                        246\n",
            "Choosing the Right Number of Dimensions                                                        247\n",
            "PCA for Compression                                                                                              249\n",
            "Randomized PCA                                                                                                     250\n",
            "Incremental PCA                                                                                                      250\n",
            "Random Projection                                                                                                      252\n",
            "LLE                                                                                                                                 254\n",
            "Other Dimensionality Reduction Techniques                                                         256\n",
            "Exercises                                                                                                                        257\n",
            "\n",
            "9. Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   259\n",
            "Clustering Algorithms: k-means and DBSCAN                                                      260\n",
            "k-means                                                                                                                      263\n",
            "Limits of k-means                                                                                                     272\n",
            "Using Clustering for Image Segmentation                                                            273\n",
            "Using Clustering for Semi-Supervised Learning                                                 275\n",
            "DBSCAN                                                                                                                    279\n",
            "Other Clustering Algorithms                                                                                  282\n",
            "Gaussian Mixtures                                                                                                        283\n",
            "Using Gaussian Mixtures for Anomaly Detection                                               288\n",
            "Selecting the Number of Clusters                                                                           289\n",
            "Bayesian Gaussian Mixture Models                                                                       292\n",
            "Other Algorithms for Anomaly and Novelty Detection                                     293\n",
            "Exercises                                                                                                                        294\n",
            "\n",
            "Part II.  Neural Networks and Deep Learning\n",
            "\n",
            "10.\n",
            "\n",
            "Introduction to Artificial Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .  299\n",
            "From Biological to Artificial Neurons                                                                       300\n",
            "Biological Neurons                                                                                                   301\n",
            "Logical Computations with Neurons                                                                     303\n",
            "The Perceptron                                                                                                          304\n",
            "The Multilayer Perceptron and Backpropagation                                               309\n",
            "Regression MLPs                                                                                                      313\n",
            "Classification MLPs                                                                                                  315\n",
            "Implementing MLPs with Keras                                                                                317\n",
            "\n",
            "Table of Contents \n",
            "\n",
            "| \n",
            "\n",
            "vii\n",
            "\n",
            "\fBuilding an Image Classifier Using the Sequential API                                      318\n",
            "Building a Regression MLP Using the Sequential API                                        328\n",
            "Building Complex Models Using the Functional API                                         329\n",
            "Using the Subclassing API to Build Dynamic Models                                        336\n",
            "Saving and Restoring a Model                                                                                337\n",
            "Using Callbacks                                                                                                         338\n",
            "Using TensorBoard for Visualization                                                                    340\n",
            "Fine-Tuning Neural Network Hyperparameters                                                     344\n",
            "Number of Hidden Layers                                                                                       349\n",
            "Number of Neurons per Hidden Layer                                                                 350\n",
            "Learning Rate, Batch Size, and Other Hyperparameters                                    351\n",
            "Exercises                                                                                                                        353\n",
            "\n",
            "11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   357\n",
            "The Vanishing/Exploding Gradients Problems                                                       358\n",
            "Glorot and He Initialization                                                                                    359\n",
            "Better Activation Functions                                                                                    361\n",
            "Batch Normalization                                                                                                367\n",
            "Gradient Clipping                                                                                                     372\n",
            "Reusing Pretrained Layers                                                                                          373\n",
            "Transfer Learning with Keras                                                                                  375\n",
            "Unsupervised Pretraining                                                                                       377\n",
            "Pretraining on an Auxiliary Task                                                                           378\n",
            "Faster Optimizers                                                                                                         379\n",
            "Momentum                                                                                                                379\n",
            "Nesterov Accelerated Gradient                                                                               381\n",
            "AdaGrad                                                                                                                     382\n",
            "RMSProp                                                                                                                    383\n",
            "Adam                                                                                                                          384\n",
            "AdaMax                                                                                                                      385\n",
            "Nadam                                                                                                                        386\n",
            "AdamW                                                                                                                      386\n",
            "Learning Rate Scheduling                                                                                           388\n",
            "Avoiding Overfitting Through Regularization                                                        392\n",
            "ℓ1 and ℓ2 Regularization                                                                                           393\n",
            "Dropout                                                                                                                      394\n",
            "Monte Carlo (MC) Dropout                                                                                   397\n",
            "Max-Norm Regularization                                                                                      399\n",
            "Summary and Practical Guidelines                                                                           400\n",
            "Exercises                                                                                                                        402\n",
            "\n",
            "viii \n",
            "\n",
            "| \n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\f12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   403\n",
            "A Quick Tour of TensorFlow                                                                                      403\n",
            "Using TensorFlow like NumPy                                                                                   407\n",
            "Tensors and Operations                                                                                           407\n",
            "Tensors and NumPy                                                                                                 409\n",
            "Type Conversions                                                                                                     409\n",
            "Variables                                                                                                                     410\n",
            "Other Data Structures                                                                                              410\n",
            "Customizing Models and Training Algorithms                                                       412\n",
            "Custom Loss Functions                                                                                           412\n",
            "Saving and Loading Models That Contain Custom Components                     413\n",
            "Custom Activation Functions, Initializers, Regularizers, and Constraints      415\n",
            "Custom Metrics                                                                                                         416\n",
            "Custom Layers                                                                                                           419\n",
            "Custom Models                                                                                                         422\n",
            "Losses and Metrics Based on Model Internals                                                      424\n",
            "Computing Gradients Using Autodiff                                                                   426\n",
            "Custom Training Loops                                                                                           430\n",
            "TensorFlow Functions and Graphs                                                                            433\n",
            "AutoGraph and Tracing                                                                                           435\n",
            "TF Function Rules                                                                                                    437\n",
            "Exercises                                                                                                                        438\n",
            "\n",
            "13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  441\n",
            "The tf.data API                                                                                                              442\n",
            "Chaining Transformations                                                                                      443\n",
            "Shuffling the Data                                                                                                     445\n",
            "Interleaving Lines from Multiple Files                                                                  446\n",
            "Preprocessing the Data                                                                                            448\n",
            "Putting Everything Together                                                                                   449\n",
            "Prefetching                                                                                                                 450\n",
            "Using the Dataset with Keras                                                                                  452\n",
            "The TFRecord Format                                                                                                 453\n",
            "Compressed TFRecord Files                                                                                   454\n",
            "A Brief Introduction to Protocol Buffers                                                              454\n",
            "TensorFlow Protobufs                                                                                              456\n",
            "Loading and Parsing Examples                                                                               457\n",
            "Handling Lists of Lists Using the SequenceExample Protobuf                          459\n",
            "Keras Preprocessing Layers                                                                                         459\n",
            "The Normalization Layer                                                                                         460\n",
            "The Discretization Layer                                                                                         463\n",
            "\n",
            "Table of Contents \n",
            "\n",
            "| \n",
            "\n",
            "ix\n",
            "\n",
            "\fThe CategoryEncoding Layer                                                                                 463\n",
            "The StringLookup Layer                                                                                          465\n",
            "The Hashing Layer                                                                                                   466\n",
            "Encoding Categorical Features Using Embeddings                                             466\n",
            "Text Preprocessing                                                                                                    471\n",
            "Using Pretrained Language Model Components                                                 473\n",
            "Image Preprocessing Layers                                                                                    474\n",
            "The TensorFlow Datasets Project                                                                              475\n",
            "Exercises                                                                                                                        477\n",
            "\n",
            "14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .   479\n",
            "The Architecture of the Visual Cortex                                                                      480\n",
            "Convolutional Layers                                                                                                   481\n",
            "Filters                                                                                                                          484\n",
            "Stacking Multiple Feature Maps                                                                             485\n",
            "Implementing Convolutional Layers with Keras                                                 487\n",
            "Memory Requirements                                                                                            490\n",
            "Pooling Layers                                                                                                               491\n",
            "Implementing Pooling Layers with Keras                                                                 493\n",
            "CNN Architectures                                                                                                      495\n",
            "LeNet-5                                                                                                                       498\n",
            "AlexNet                                                                                                                      499\n",
            "GoogLeNet                                                                                                                 502\n",
            "VGGNet                                                                                                                     505\n",
            "ResNet                                                                                                                        505\n",
            "Xception                                                                                                                     509\n",
            "SENet                                                                                                                          510\n",
            "Other Noteworthy Architectures                                                                           512\n",
            "Choosing the Right CNN Architecture                                                                 514\n",
            "Implementing a ResNet-34 CNN Using Keras                                                        515\n",
            "Using Pretrained Models from Keras                                                                        516\n",
            "Pretrained Models for Transfer Learning                                                                 518\n",
            "Classification and Localization                                                                                   521\n",
            "Object Detection                                                                                                          523\n",
            "Fully Convolutional Networks                                                                                525\n",
            "You Only Look Once                                                                                                527\n",
            "Object Tracking                                                                                                            530\n",
            "Semantic Segmentation                                                                                               531\n",
            "Exercises                                                                                                                        535\n",
            "\n",
            "x \n",
            "\n",
            "| \n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\f15. Processing Sequences Using RNNs and CNNs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            " 537\n",
            "Recurrent Neurons and Layers                                                                                   538\n",
            "Memory Cells                                                                                                            540\n",
            "Input and Output Sequences                                                                                  541\n",
            "Training RNNs                                                                                                              542\n",
            "Forecasting a Time Series                                                                                            543\n",
            "The ARMA Model Family                                                                                       549\n",
            "Preparing the Data for Machine Learning Models                                              552\n",
            "Forecasting Using a Linear Model                                                                          555\n",
            "Forecasting Using a Simple RNN                                                                           556\n",
            "Forecasting Using a Deep RNN                                                                              557\n",
            "Forecasting Multivariate Time Series                                                                    559\n",
            "Forecasting Several Time Steps Ahead                                                                  560\n",
            "Forecasting Using a Sequence-to-Sequence Model                                             562\n",
            "Handling Long Sequences                                                                                           565\n",
            "Fighting the Unstable Gradients Problem                                                            565\n",
            "Tackling the Short-Term Memory Problem                                                         568\n",
            "Exercises                                                                                                                        576\n",
            "\n",
            "16. Natural Language Processing with RNNs and Attention. . . . . . . . . . . . . . . . . . . . . . . .   577\n",
            "Generating Shakespearean Text Using a Character RNN                                      578\n",
            "Creating the Training Dataset                                                                                 579\n",
            "Building and Training the Char-RNN Model                                                      581\n",
            "Generating Fake Shakespearean Text                                                                    582\n",
            "Stateful RNN                                                                                                             584\n",
            "Sentiment Analysis                                                                                                       587\n",
            "Masking                                                                                                                      590\n",
            "Reusing Pretrained Embeddings and Language Models                                    593\n",
            "An Encoder–Decoder Network for Neural Machine Translation                         595\n",
            "Bidirectional RNNs                                                                                                  601\n",
            "Beam Search                                                                                                              603\n",
            "Attention Mechanisms                                                                                                604\n",
            "Attention Is All You Need: The Original Transformer Architecture                609\n",
            "An Avalanche of Transformer Models                                                                      620\n",
            "Vision Transformers                                                                                                    624\n",
            "Hugging Face’s Transformers Library                                                                       629\n",
            "Exercises                                                                                                                        633\n",
            "\n",
            "17. Autoencoders, GANs, and Diffusion Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  635\n",
            "Efficient Data Representations                                                                                   637\n",
            "Performing PCA with an Undercomplete Linear Autoencoder                            639\n",
            "\n",
            "Table of Contents \n",
            "\n",
            "| \n",
            "\n",
            "xi\n",
            "\n",
            "\fStacked Autoencoders                                                                                                 640\n",
            "Implementing a Stacked Autoencoder Using Keras                                            641\n",
            "Visualizing the Reconstructions                                                                             642\n",
            "Visualizing the Fashion MNIST Dataset                                                               643\n",
            "Unsupervised Pretraining Using Stacked Autoencoders                                    644\n",
            "Tying Weights                                                                                                            645\n",
            "Training One Autoencoder at a Time                                                                    646\n",
            "Convolutional Autoencoders                                                                                      648\n",
            "Denoising Autoencoders                                                                                             649\n",
            "Sparse Autoencoders                                                                                                    651\n",
            "Variational Autoencoders                                                                                           654\n",
            "Generating Fashion MNIST Images                                                                          658\n",
            "Generative Adversarial Networks                                                                              659\n",
            "The Difficulties of Training GANs                                                                         663\n",
            "Deep Convolutional GANs                                                                                     665\n",
            "Progressive Growing of GANs                                                                                668\n",
            "StyleGANs                                                                                                                  671\n",
            "Diffusion Models                                                                                                          673\n",
            "Exercises                                                                                                                        681\n",
            "\n",
            "18. Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            " 683\n",
            "Learning to Optimize Rewards                                                                                  684\n",
            "Policy Search                                                                                                                 685\n",
            "Introduction to OpenAI Gym                                                                                    687\n",
            "Neural Network Policies                                                                                              691\n",
            "Evaluating Actions: The Credit Assignment Problem                                            693\n",
            "Policy Gradients                                                                                                           694\n",
            "Markov Decision Processes                                                                                        699\n",
            "Temporal Difference Learning                                                                                   703\n",
            "Q-Learning                                                                                                                    704\n",
            "Exploration Policies                                                                                                  706\n",
            "Approximate Q-Learning and Deep Q-Learning                                                707\n",
            "Implementing Deep Q-Learning                                                                               708\n",
            "Deep Q-Learning Variants                                                                                          713\n",
            "Fixed Q-value Targets                                                                                              713\n",
            "Double DQN                                                                                                             714\n",
            "Prioritized Experience Replay                                                                                 714\n",
            "Dueling DQN                                                                                                            715\n",
            "Overview of Some Popular RL Algorithms                                                              716\n",
            "Exercises                                                                                                                        720\n",
            "\n",
            "xii \n",
            "\n",
            "| \n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\f19. Training and Deploying TensorFlow Models at Scale. . . . . . . . . . . . . . . . . . . . . . . . . . .  721\n",
            "Serving a TensorFlow Model                                                                                      722\n",
            "Using TensorFlow Serving                                                                                       722\n",
            "Creating a Prediction Service on Vertex AI                                                          732\n",
            "Running Batch Prediction Jobs on Vertex AI                                                       739\n",
            "Deploying a Model to a Mobile or Embedded Device                                            741\n",
            "Running a Model in a Web Page                                                                                744\n",
            "Using GPUs to Speed Up Computations                                                                  746\n",
            "Getting Your Own GPU                                                                                          747\n",
            "Managing the GPU RAM                                                                                        749\n",
            "Placing Operations and Variables on Devices                                                      752\n",
            "Parallel Execution Across Multiple Devices                                                         753\n",
            "Training Models Across Multiple Devices                                                                756\n",
            "Model Parallelism                                                                                                     756\n",
            "Data Parallelism                                                                                                        759\n",
            "Training at Scale Using the Distribution Strategies API                                     765\n",
            "Training a Model on a TensorFlow Cluster                                                          766\n",
            "Running Large Training Jobs on Vertex AI                                                          770\n",
            "Hyperparameter Tuning on Vertex AI                                                                  772\n",
            "Exercises                                                                                                                        776\n",
            "Thank You!                                                                                                                    777\n",
            "\n",
            "A. Machine Learning Project Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  779\n",
            "\n",
            "B. Autodiff. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   785\n",
            "\n",
            "C. Special Data Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   793\n",
            "\n",
            "D. TensorFlow Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            " 801\n",
            "\n",
            "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   811\n",
            "\n",
            "Table of Contents \n",
            "\n",
            "| \n",
            "\n",
            "xiii\n",
            "\n",
            "\f\fPreface\n",
            "\n",
            "The Machine Learning Tsunami\n",
            "In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\n",
            "network  capable  of  recognizing  handwritten  digits  with  state-of-the-art  precision\n",
            "(>98%).  They  branded  this  technique  “deep  learning”.  A  deep  neural  network  is  a\n",
            "(very)  simplified  model  of  our  cerebral  cortex,  composed  of  a  stack  of  layers  of\n",
            "artificial neurons. Training a deep neural net was widely considered impossible at the\n",
            "time,2  and  most  researchers  had  abandoned  the  idea  in  the  late  1990s.  This  paper\n",
            "revived  the  interest  of  the  scientific  community,  and  before  long  many  new  papers\n",
            "demonstrated that deep learning was not only possible, but capable of mind-blowing\n",
            "achievements  that  no  other  machine  learning  (ML)  technique  could  hope  to  match\n",
            "(with  the  help  of  tremendous  computing  power  and  great  amounts  of  data).  This\n",
            "enthusiasm soon extended to many other areas of machine learning.\n",
            "\n",
            "A  decade  later,  machine  learning  had  conquered  the  industry,  and  today  it  is  at  the\n",
            "heart of much of the magic in high-tech products, ranking your web search results,\n",
            "powering your smartphone’s speech recognition, recommending videos, and perhaps\n",
            "even driving your car.\n",
            "\n",
            "Machine Learning in Your Projects\n",
            "So, naturally you are excited about machine learning and would love to join the party!\n",
            "\n",
            "Perhaps  you  would  like  to  give  your  homemade  robot  a  brain  of  its  own?  Make  it\n",
            "recognize faces? Or learn to walk around?\n",
            "\n",
            "1 Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets”, Neural Computation 18 (2006):\n",
            "\n",
            "1527–1554.\n",
            "\n",
            "2 Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for image recognition\n",
            "\n",
            "since the 1990s, although they were not as general-purpose.\n",
            "\n",
            "xv\n",
            "\n",
            "\fOr maybe your company has tons of data (user logs, financial data, production data,\n",
            "machine sensor data, hotline stats, HR reports, etc.), and more than likely you could\n",
            "unearth some hidden gems if you just knew where to look. With machine learning,\n",
            "you could accomplish the following and much more:\n",
            "\n",
            "•\n",
            "• Segment customers and find the best marketing strategy for each group.\n",
            "\n",
            "• Recommend products for each client based on what similar clients bought.\n",
            "•\n",
            "\n",
            "•\n",
            "• Detect which transactions are likely to be fraudulent.\n",
            "\n",
            "•\n",
            "• Forecast next year’s revenue.\n",
            "\n",
            "Whatever the reason, you have decided to learn machine learning and implement it\n",
            "in your projects. Great idea!\n",
            "\n",
            "Objective and Approach\n",
            "This book assumes that you know close to nothing about machine learning. Its goal\n",
            "is  to  give  you  the  concepts,  tools,  and  intuition  you  need  to  implement  programs\n",
            "capable of learning from data.\n",
            "\n",
            "We will cover a large number of techniques, from the simplest and most commonly\n",
            "used (such as linear regression) to some of the deep learning techniques that regularly\n",
            "win competitions. For this, we will be using production-ready Python frameworks:\n",
            "\n",
            "•\n",
            "• Scikit-Learn  is  very  easy  to  use,  yet  it  implements  many  machine  learning\n",
            "algorithms  efficiently,  so  it  makes  for  a  great  entry  point  to  learning  machine\n",
            "learning. It was created by David Cournapeau in 2007, and is now led by a team\n",
            "of  researchers  at  the  French  Institute  for  Research  in  Computer  Science  and\n",
            "Automation (Inria).\n",
            "\n",
            "• TensorFlow  is  a  more  complex  library  for  distributed  numerical  computation.\n",
            "•\n",
            "It makes it possible to train and run very large neural networks efficiently by dis‐\n",
            "tributing the computations across potentially hundreds of multi-GPU (graphics\n",
            "processing  unit)  servers.  TensorFlow  (TF)  was  created  at  Google  and  supports\n",
            "many  of  its  large-scale  machine  learning  applications.  It  was  open  sourced  in\n",
            "November 2015, and version 2.0 was released in September 2019.\n",
            "\n",
            "•\n",
            "• Keras  is  a  high-level  deep  learning  API  that  makes  it  very  simple  to  train  and\n",
            "run  neural  networks.  Keras  comes  bundled  with  TensorFlow,  and  it  relies  on\n",
            "TensorFlow for all the intensive computations.\n",
            "\n",
            "The  book  favors  a  hands-on  approach,  growing  an  intuitive  understanding  of\n",
            "machine learning through concrete working examples and just a little bit of theory.\n",
            "\n",
            "xvi \n",
            "\n",
            "|  Preface\n",
            "\n",
            "\fWhile  you  can  read  this  book  without  picking  up  your  laptop,  I\n",
            "highly recommend you experiment with the code examples.\n",
            "\n",
            "Code Examples\n",
            "All the code examples in this book are open source and available online at https://git\n",
            "hub.com/ageron/handson-ml3, as Jupyter notebooks. These are interactive documents\n",
            "containing  text,  images,  and  executable  code  snippets  (Python  in  our  case).  The\n",
            "easiest and quickest way to get started is to run these notebooks using Google Colab:\n",
            "this  is  a  free  service  that  allows  you  to  run  any  Jupyter  notebook  directly  online,\n",
            "without having to install anything on your machine. All you need is a web browser\n",
            "and a Google account.\n",
            "\n",
            "In this book, I will assume that you are using Google Colab, but I\n",
            "have  also  tested  the  notebooks  on  other  online  platforms  such  as\n",
            "Kaggle and Binder, so you can use those if you prefer. Alternatively,\n",
            "you  can  install  the  required  libraries  and  tools  (or  the  Docker\n",
            "image for this book) and run the notebooks directly on your own\n",
            "machine. See the instructions at https://homl.info/install.\n",
            "\n",
            "This book is here to help you get your job done. If you wish to use additional content\n",
            "beyond the code examples, and that use falls outside the scope of fair use guidelines,\n",
            "(such  as  selling  or  distributing  content  from  O’Reilly  books,  or  incorporating  a\n",
            "significant  amount  of  material  from  this  book  into  your  product’s  documentation),\n",
            "please reach out to us for permission, at permissions@oreilly.com.\n",
            "\n",
            "We  appreciate,  but  do  not  require,  attribution.  An  attribution  usually  includes  the\n",
            "title,  author,  publisher,  and  ISBN.  For  example:  “Hands-On  Machine  Learning  with\n",
            "Scikit-Learn,  Keras,  and  TensorFlow  by  Aurélien  Géron.  Copyright  2023  Aurélien\n",
            "Géron, 978-1-098-12597-4.”\n",
            "\n",
            "Prerequisites\n",
            "This book assumes that you have some Python programming experience. If you don’t\n",
            "know Python yet, https://learnpython.org is a great place to start. The official tutorial\n",
            "on Python.org is also quite good.\n",
            "\n",
            "This book also assumes that you are familiar with Python’s main scientific libraries—\n",
            "in particular, NumPy, Pandas, and Matplotlib. If you have never used these libraries,\n",
            "don’t worry; they’re easy to learn, and I’ve created a tutorial for each of them. You can\n",
            "access them online at https://homl.info/tutorials.\n",
            "\n",
            "Preface \n",
            "\n",
            "| \n",
            "\n",
            "xvii\n",
            "\n",
            "\fMoreover, if you want to fully understand how the machine learning algorithms work\n",
            "(not  just  how  to  use  them),  then  you  should  have  at  least  a  basic  understanding  of\n",
            "a  few  math  concepts,  especially  linear  algebra.  Specifically,  you  should  know  what\n",
            "vectors  and  matrices  are,  and  how  to  perform  some  simple  operations  like  adding\n",
            "vectors, or transposing and multiplying matrices. If you need a quick introduction to\n",
            "linear algebra (it’s really not rocket science!), I provide a tutorial at https://homl.info/\n",
            "tutorials. You will also find a tutorial on differential calculus, which may be helpful to\n",
            "understand how neural networks are trained, but it’s not entirely essential to grasp the\n",
            "important  concepts.  This  book  also  uses  other  mathematical  concepts  occasionally,\n",
            "such  as  exponentials  and  logarithms,  a  bit  of  probability  theory,  and  some  basic\n",
            "statistics  concepts,  but  nothing  too  advanced.  If  you  need  help  on  any  of  these,\n",
            "please check out https://khanacademy.org, which offers many excellent and free math\n",
            "courses online.\n",
            "\n",
            "Roadmap\n",
            "This  book \n",
            "Machine Learning”, covers the following topics:\n",
            "\n",
            "is  organized \n",
            "\n",
            "in \n",
            "\n",
            "two  parts.  Part \n",
            "\n",
            "I,  “The  Fundamentals  of\n",
            "\n",
            "•\n",
            "• What  machine  learning  is,  what  problems  it  tries  to  solve,  and  the  main  cate‐\n",
            "\n",
            "gories and fundamental concepts of its systems\n",
            "\n",
            "•\n",
            "• The steps in a typical machine learning project\n",
            "\n",
            "•\n",
            "• Learning by fitting a model to data\n",
            "\n",
            "•\n",
            "• Optimizing a cost function\n",
            "\n",
            "•\n",
            "• Handling, cleaning, and preparing data\n",
            "\n",
            "•\n",
            "• Selecting and engineering features\n",
            "\n",
            "•\n",
            "• Selecting a model and tuning hyperparameters using cross-validation\n",
            "\n",
            "• The  challenges  of  machine  learning,  in  particular  underfitting  and  overfitting\n",
            "•\n",
            "\n",
            "(the bias/variance trade-off)\n",
            "\n",
            "• The most common learning algorithms: linear and polynomial regression, logis‐\n",
            "•\n",
            "tic regression, k-nearest neighbors, support vector machines, decision trees, ran‐\n",
            "dom forests, and ensemble methods\n",
            "\n",
            "•\n",
            "• Reducing  the  dimensionality  of  the  training  data  to  fight  the  “curse  of\n",
            "\n",
            "dimensionality”\n",
            "\n",
            "• Other  unsupervised  learning  techniques,  including  clustering,  density  estima‐\n",
            "•\n",
            "\n",
            "tion, and anomaly detection\n",
            "\n",
            "Part II, “Neural Networks and Deep Learning”, covers the following topics:\n",
            "\n",
            "• What neural nets are and what they’re good for\n",
            "•\n",
            "\n",
            "xviii \n",
            "\n",
            "|  Preface\n",
            "\n",
            "\f•\n",
            "• Building and training neural nets using TensorFlow and Keras\n",
            "\n",
            "• The most important neural net architectures: feedforward neural nets for tabular\n",
            "•\n",
            "data, convolutional nets for computer vision, recurrent nets and long short-term\n",
            "memory  (LSTM)  nets  for  sequence  processing,  encoder–decoders  and  trans‐\n",
            "formers  for  natural  language  processing  (and  more!),  autoencoders,  generative\n",
            "adversarial networks (GANs), and diffusion models for generative learning\n",
            "\n",
            "•\n",
            "• Techniques for training deep neural nets\n",
            "\n",
            "•\n",
            "• How  to  build  an  agent  (e.g.,  a  bot  in  a  game)  that  can  learn  good  strategies\n",
            "\n",
            "through trial and error, using reinforcement learning\n",
            "\n",
            "•\n",
            "• Loading and preprocessing large amounts of data efficiently\n",
            "\n",
            "•\n",
            "• Training and deploying TensorFlow models at scale\n",
            "\n",
            "The first part is based mostly on Scikit-Learn, while the second part uses TensorFlow\n",
            "and Keras.\n",
            "\n",
            "Don’t  jump  into  deep  waters  too  hastily:  while  deep  learning  is\n",
            "no doubt one of the most exciting areas in machine learning, you\n",
            "should  master  the  fundamentals  first.  Moreover,  most  problems\n",
            "can be solved quite well using simpler techniques such as random\n",
            "forests and ensemble methods (discussed in Part I). deep learning\n",
            "is  best  suited  for  complex  problems  such  as  image  recognition,\n",
            "speech recognition, or natural language processing, and it requires\n",
            "a lot of data, computing power, and patience (unless you can lever‐\n",
            "age a pretrained neural network, as you will see).\n",
            "\n",
            "Changes Between the First and the Second Edition\n",
            "If you have already read the first edition, here are the main changes between the first\n",
            "and the second edition:\n",
            "\n",
            "• All the code was migrated from TensorFlow 1.x to TensorFlow 2.x, and I replaced\n",
            "•\n",
            "most  of  the  low-level  TensorFlow  code  (graphs,  sessions,  feature  columns,  esti‐\n",
            "mators, and so on) with much simpler Keras code.\n",
            "\n",
            "• The second edition introduced the Data API for loading and preprocessing large\n",
            "•\n",
            "datasets,  the  distribution  strategies  API  to  train  and  deploy  TF  models  at  scale,\n",
            "TF Serving and Google Cloud AI Platform to productionize models, and (briefly)\n",
            "TF Transform, TFLite, TF Addons/Seq2Seq, TensorFlow.js, and TF Agents.\n",
            "\n",
            "•\n",
            "• It also introduced many additional ML topics, including a new chapter on unsu‐\n",
            "pervised learning, computer vision techniques for object detection and semantic\n",
            "segmentation, handling sequences using convolutional neural networks (CNNs),\n",
            "\n",
            "Preface \n",
            "\n",
            "| \n",
            "\n",
            "xix\n",
            "\n",
            "\fnatural  language  processing  (NLP)  using  recurrent  neural  networks  (RNNs),\n",
            "CNNs and transformers, GANs, and more.\n",
            "\n",
            "See https://homl.info/changes2 for more details.\n",
            "\n",
            "Changes Between the Second and the Third Edition\n",
            "If you read the second edition, here are the main changes between the second and the\n",
            "third edition:\n",
            "\n",
            "• All  the  code  was  updated  to  the  latest  library  versions.  In  particular,  this  third\n",
            "•\n",
            "edition introduces many new additions to Scikit-Learn (e.g., feature name track‐\n",
            "ing,  histogram-based  gradient  boosting,  label  propagation,  and  more).  It  also\n",
            "introduces  the  Keras  Tuner  library  for  hyperparameter  tuning,  Hugging  Face’s\n",
            "Transformers  library  for  natural  language  processing,  and  Keras’s  new  prepro‐\n",
            "cessing and data augmentation layers.\n",
            "\n",
            "•\n",
            "• Several vision models were added (ResNeXt, DenseNet, MobileNet, CSPNet, and\n",
            "\n",
            "EfficientNet), as well as guidelines for choosing the right one.\n",
            "\n",
            "•\n",
            "• Chapter  15  now  analyzes  the  Chicago  bus  and  rail  ridership  data  instead  of\n",
            "\n",
            "generated time series, and it introduces the ARMA model and its variants.\n",
            "\n",
            "• Chapter  16  on  natural  language  processing  now  builds  an  English-to-Spanish\n",
            "•\n",
            "translation  model,  first  using  an  encoder–decoder  RNN,  then  using  a  trans‐\n",
            "former  model.  The  chapter  also  covers  language  models  such  as  Switch  Trans‐\n",
            "formers,  DistilBERT,  T5,  and  PaLM  (with  chain-of-thought  prompting).  In\n",
            "addition,  it  introduces  vision  transformers  (ViTs)  and  gives  an  overview  of  a\n",
            "few  transformer-based  visual  models,  such  as  data-efficient  image  transformers\n",
            "(DeiTs), Perceiver, and DINO, as well as a brief overview of some large multimo‐\n",
            "dal models, including CLIP, DALL·E, Flamingo, and GATO.\n",
            "\n",
            "• Chapter 17 on generative learning now introduces diffusion models, and shows\n",
            "•\n",
            "how  to  implement  a  denoising  diffusion  probabilistic  model  (DDPM)  from\n",
            "scratch.\n",
            "\n",
            "• Chapter 19 migrated from Google Cloud AI Platform to Google Vertex AI, and\n",
            "•\n",
            "uses distributed Keras Tuner for large-scale hyperparameter search. The chapter\n",
            "now  includes  TensorFlow.js  code  that  you  can  experiment  with  online.  It  also\n",
            "introduces additional distributed training techniques, including PipeDream and\n",
            "Pathways.\n",
            "\n",
            "• In  order  to  allow  for  all  the  new  content,  some  sections  were  moved  online,\n",
            "•\n",
            "including  installation  instructions,  kernel  principal  component  analysis  (PCA),\n",
            "mathematical  details  of  Bayesian  Gaussian  mixtures,  TF  Agents,  and  former\n",
            "\n",
            "xx \n",
            "\n",
            "|  Preface\n",
            "\n",
            "\fappendices  A  (exercise  solutions),  C  (support  vector  machine  math),  and  E\n",
            "(extra neural net architectures).\n",
            "\n",
            "See https://homl.info/changes3 for more details.\n",
            "\n",
            "Other Resources\n",
            "Many excellent resources are available to learn about machine learning. For example,\n",
            "Andrew  Ng’s  ML  course  on  Coursera  is  amazing,  although  it  requires  a  significant\n",
            "time investment.\n",
            "\n",
            "There  are  also  many  interesting  websites  about  machine  learning,  including  Scikit-\n",
            "Learn’s exceptional User Guide. You may also enjoy Dataquest, which provides very\n",
            "nice interactive tutorials, and ML blogs such as those listed on Quora.\n",
            "\n",
            "There are many other introductory books about machine learning. In particular:\n",
            "\n",
            "• Joel Grus’s Data Science from Scratch, 2nd edition (O’Reilly), presents the funda‐\n",
            "•\n",
            "mentals  of  machine  learning  and  implements  some  of  the  main  algorithms  in\n",
            "pure Python (from scratch, as the name suggests).\n",
            "\n",
            "•\n",
            "• Stephen  Marsland’s  Machine  Learning:  An  Algorithmic  Perspective,  2nd  edition\n",
            "(Chapman & Hall), is a great introduction to machine learning, covering a wide\n",
            "range  of  topics  in  depth  with  code  examples  in  Python  (also  from  scratch,  but\n",
            "using NumPy).\n",
            "\n",
            "•\n",
            "• Sebastian Raschka’s Python Machine Learning, 3rd edition (Packt Publishing), is\n",
            "also a great introduction to machine learning and leverages Python open source\n",
            "libraries (Pylearn 2 and Theano).\n",
            "\n",
            "•\n",
            "• François Chollet’s Deep Learning with Python, 2nd edition (Manning), is a very\n",
            "practical  book  that  covers  a  large  range  of  topics  in  a  clear  and  concise  way,  as\n",
            "you  might  expect  from  the  author  of  the  excellent  Keras  library.  It  favors  code\n",
            "examples over mathematical theory.\n",
            "\n",
            "•\n",
            "• Andriy  Burkov’s  The  Hundred-Page  Machine  Learning  Book  (self-published)\n",
            "is  very  short  but  covers  an  impressive  range  of  topics,  introducing  them  in\n",
            "approachable terms without shying away from the math equations.\n",
            "\n",
            "• Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s Learning from\n",
            "•\n",
            "Data  (AMLBook)  is  a  rather  theoretical  approach  to  ML  that  provides  deep\n",
            "insights, in particular on the bias/variance trade-off (see Chapter 4).\n",
            "\n",
            "• Stuart Russell and Peter Norvig’s Artificial Intelligence: A Modern Approach, 4th\n",
            "•\n",
            "edition  (Pearson),  is  a  great  (and  huge)  book  covering  an  incredible  amount  of\n",
            "topics, including machine learning. It helps put ML into perspective.\n",
            "\n",
            "Preface \n",
            "\n",
            "| \n",
            "\n",
            "xxi\n",
            "\n",
            "\f• Jeremy  Howard  and  Sylvain  Gugger’s  Deep  Learning  for  Coders  with  fastai  and\n",
            "•\n",
            "PyTorch  (O’Reilly)  provides  a  wonderfully  clear  and  practical  introduction  to\n",
            "deep learning using the fastai and PyTorch libraries.\n",
            "\n",
            "Finally, joining ML competition websites such as Kaggle.com will allow you to prac‐\n",
            "tice your skills on real-world problems, with help and insights from some of the best\n",
            "ML professionals out there.\n",
            "\n",
            "Conventions Used in This Book\n",
            "The following typographical conventions are used in this book:\n",
            "\n",
            "Italic\n",
            "\n",
            "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
            "\n",
            "Constant width\n",
            "\n",
            "Used  for  program  listings,  as  well  as  within  paragraphs  to  refer  to  program\n",
            "elements such as variable or function names, databases, data types, environment\n",
            "variables, statements, and keywords.\n",
            "\n",
            "Constant width bold\n",
            "\n",
            "Shows commands or other text that should be typed literally by the user.\n",
            "\n",
            "Constant width italic\n",
            "\n",
            "Shows text that should be replaced with user-supplied values or by values deter‐\n",
            "mined by context.\n",
            "\n",
            "Punctuation\n",
            "\n",
            "To avoid any confusion, punctutation appears outside of quotes throughout the\n",
            "book. My apologies to the purists.\n",
            "\n",
            "This element signifies a tip or suggestion.\n",
            "\n",
            "This element signifies a general note.\n",
            "\n",
            "xxii \n",
            "\n",
            "|  Preface\n",
            "\n",
            "\fThis element indicates a warning or caution.\n",
            "\n",
            "O’Reilly Online Learning\n",
            "\n",
            "For more than 40 years, O’Reilly Media has provided technol‐\n",
            "ogy  and  business  training,  knowledge,  and  insight  to  help\n",
            "companies succeed.\n",
            "\n",
            "Our unique network of experts and innovators share their knowledge and expertise\n",
            "through books, articles, and our online learning platform. O’Reilly’s online learning\n",
            "platform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning\n",
            "paths, interactive coding environments, and a vast collection of text and video from\n",
            "O’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.\n",
            "\n",
            "How to Contact Us\n",
            "Please address comments and questions concerning this book to the publisher:\n",
            "\n",
            "O’Reilly Media, Inc.\n",
            "1005 Gravenstein Highway North\n",
            "Sebastopol, CA 95472\n",
            "800-998-9938 (in the United States or Canada)\n",
            "707-829-0515 (international or local)\n",
            "707-829-0104 (fax)\n",
            "\n",
            "We have a web page for this book, where we list errata, examples, and any additional\n",
            "information. You can access this page at https://homl.info/oreilly3.\n",
            "\n",
            "Email  bookquestions@oreilly.com  to  comment  or  ask  technical  questions  about  this\n",
            "book.\n",
            "\n",
            "For news and information about our books and courses, visit https://oreilly.com.\n",
            "\n",
            "Find us on LinkedIn: https://linkedin.com/company/oreilly-media\n",
            "\n",
            "Follow us on Twitter: https://twitter.com/oreillymedia\n",
            "\n",
            "Watch us on YouTube: https://youtube.com/oreillymedia\n",
            "\n",
            "Preface \n",
            "\n",
            "| \n",
            "\n",
            "xxiii\n",
            "\n",
            "\fAcknowledgments\n",
            "Never in my wildest dreams did I imagine that the first and second editions of this\n",
            "book  would  get  such  a  large  audience.  I  received  so  many  messages  from  readers,\n",
            "many  asking  questions,  some  kindly  pointing  out  errata,  and  most  sending  me\n",
            "encouraging words. I cannot express how grateful I am to all these readers for their\n",
            "tremendous support. Thank you all so very much! Please do not hesitate to file issues\n",
            "on  GitHub  if  you  find  errors  in  the  code  examples  (or  just  to  ask  questions),  or  to\n",
            "submit errata if you find errors in the text. Some readers also shared how this book\n",
            "helped them get their first job, or how it helped them solve a concrete problem they\n",
            "were  working  on.  I  find  such  feedback  incredibly  motivating.  If  you  find  this  book\n",
            "helpful, I would love it if you could share your story with me, either privately (e.g., via\n",
            "LinkedIn) or publicly (e.g., tweet me at @aureliengeron or write an Amazon review).\n",
            "\n",
            "Huge thanks as well to all the wonderful people who offered their time and expertise\n",
            "to  review  this  third  edition,  correcting  errors  and  making  countless  suggestions.\n",
            "This edition is so much better thanks to them: Olzhas Akpambetov, George Bonner,\n",
            "François Chollet, Siddha Gangju, Sam Goodman, Matt Harrison, Sasha Sobran, Lewis\n",
            "Tunstall, Leandro von Werra, and my dear brother Sylvain. You are all amazing!\n",
            "\n",
            "I  am  also  very  grateful  to  the  many  people  who  supported  me  along  the  way,  by\n",
            "answering my questions, suggesting improvements, and contributing to the code on\n",
            "GitHub:  in  particular,  Yannick  Assogba,  Ian  Beauregard,  Ulf  Bissbort,  Rick  Chao,\n",
            "Peretz  Cohen,  Kyle  Gallatin,  Hannes  Hapke,  Victor  Khaustov,  Soonson  Kwon,  Eric\n",
            "Lebigot,  Jason  Mayes,  Laurence  Moroney,  Sara  Robinson,  Joaquín  Ruales,  and  Yue‐\n",
            "feng Zhou.\n",
            "\n",
            "This book wouldn’t exist without O’Reilly’s fantastic staff, in particular Nicole Taché,\n",
            "who gave me insightful feedback and was always cheerful, encouraging, and helpful:\n",
            "I  could  not  dream  of  a  better  editor.  Big  thanks  to  Michele  Cronin  as  well,  who\n",
            "cheered  me  on  through  the  final  chapters  and  managed  to  get  me  past  the  finish\n",
            "line. Thanks to the whole production team, in particular Elizabeth Kelly and Kristen\n",
            "Brown.  Thanks  as  well  to  Kim  Cofer  for  the  thorough  copyediting,  and  to  Johnny\n",
            "O’Toole,  who  managed  the  relationship  with  Amazon  and  answered  many  of  my\n",
            "questions.  Thanks  to  Kate  Dullea  for  greatly  improving  my  illustrations.  Thanks  to\n",
            "Marie  Beaugureau,  Ben  Lorica,  Mike  Loukides,  and  Laurel  Ruma  for  believing  in\n",
            "this  project  and  helping  me  define  its  scope.  Thanks  to  Matt  Hacker  and  all  of  the\n",
            "Atlas team for answering all my technical questions regarding formatting, AsciiDoc,\n",
            "MathML,  and  LaTeX,  and  thanks  to  Nick  Adams,  Rebecca  Demarest,  Rachel  Head,\n",
            "Judith  McConville,  Helen  Monroe,  Karen  Montgomery,  Rachel  Roumeliotis,  and\n",
            "everyone else at O’Reilly who contributed to this book.\n",
            "\n",
            "I’ll  never  forget  all  the  wonderful  people  who  helped  me  with  the  first  and  second\n",
            "editions  of  this  book:  friends,  colleagues,  experts,  including  many  members  of  the\n",
            "\n",
            "xxiv \n",
            "\n",
            "|  Preface\n",
            "\n",
            "\fTensorFlow  team.  The  list  is  long:  Olzhas  Akpambetov,  Karmel  Allison,  Martin\n",
            "Andrews, David Andrzejewski, Paige Bailey, Lukas Biewald, Eugene Brevdo, William\n",
            "Chargin,  François  Chollet,  Clément  Courbet,  Robert  Crowe,  Mark  Daoust,  Daniel\n",
            "“Wolff ”  Dobson,  Julien  Dubois,  Mathias  Kende,  Daniel  Kitachewsky,  Nick  Felt,\n",
            "Bruce Fontaine, Justin Francis, Goldie Gadde, Irene Giannoumis, Ingrid von Glehn,\n",
            "Vincent  Guilbeau,  Sandeep  Gupta,  Priya  Gupta,  Kevin  Haas,  Eddy  Hung,  Konstan‐\n",
            "tinos  Katsiapis,  Viacheslav  Kovalevskyi,  Jon  Krohn,  Allen  Lavoie,  Karim  Matrah,\n",
            "Grégoire  Mesnil,  Clemens  Mewald,  Dan  Moldovan,  Dominic  Monn,  Sean  Morgan,\n",
            "Tom  O’Malley,  James  Pack,  Alexander  Pak,  Haesun  Park,  Alexandre  Passos,  Ankur\n",
            "Patel,  Josh  Patterson,  André  Susano  Pinto,  Anthony  Platanios,  Anosh  Raj,  Oscar\n",
            "Ramirez,  Anna  Revinskaya,  Saurabh  Saxena,  Salim  Sémaoune,  Ryan  Sepassi,  Vitor\n",
            "Sessak,  Jiri  Simsa,  Iain  Smears,  Xiaodan  Song,  Christina  Sorokin,  Michel  Tessier,\n",
            "Wiktor Tomczak, Dustin Tran, Todd Wang, Pete Warden, Rich Washington, Martin\n",
            "Wicke,  Edd  Wilder-James,  Sam  Witteveen,  Jason  Zaman,  Yuefeng  Zhou,  and  my\n",
            "brother Sylvain.\n",
            "\n",
            "Last  but  not  least,  I  am  infinitely  grateful  to  my  beloved  wife,  Emmanuelle,  and  to\n",
            "our three wonderful children, Alexandre, Rémi, and Gabrielle, for encouraging me to\n",
            "work hard on this book. Their insatiable curiosity was priceless: explaining some of\n",
            "the most difficult concepts in this book to my wife and children helped me clarify my\n",
            "thoughts and directly improved many parts of it. Plus, they keep bringing me cookies\n",
            "and coffee, who could ask for more?\n",
            "\n",
            "Preface \n",
            "\n",
            "| \n",
            "\n",
            "xxv\n",
            "\n",
            "\f\fPART I\n",
            "The Fundamentals of\n",
            "Machine Learning\n",
            "\n",
            "\f\fCHAPTER 1\n",
            "The Machine Learning Landscape\n",
            "\n",
            "Not  so  long  ago,  if  you  had  picked  up  your  phone  and  asked  it  the  way  home,\n",
            "it  would  have  ignored  you—and  people  would  have  questioned  your  sanity.  But\n",
            "machine learning is no longer science fiction: billions of people use it every day. And\n",
            "the truth is it has actually been around for decades in some specialized applications,\n",
            "such  as  optical  character  recognition  (OCR).  The  first  ML  application  that  really\n",
            "became mainstream, improving the lives of hundreds of millions of people, took over\n",
            "the world back in the 1990s: the spam filter. It’s not exactly a self-aware robot, but it\n",
            "does technically qualify as machine learning: it has actually learned so well that you\n",
            "seldom need to flag an email as spam anymore. It was followed by hundreds of ML\n",
            "applications that now quietly power hundreds of products and features that you use\n",
            "regularly: voice prompts, automatic translation, image search, product recommenda‐\n",
            "tions, and many more.\n",
            "\n",
            "Where does machine learning start and where does it end? What exactly does it mean\n",
            "for a machine to learn something? If I download a copy of all Wikipedia articles, has\n",
            "my computer really learned something? Is it suddenly smarter? In this chapter I will\n",
            "start by clarifying what machine learning is and why you may want to use it.\n",
            "\n",
            "Then,  before  we  set  out  to  explore  the  machine  learning  continent,  we  will  take  a\n",
            "look at the map and learn about the main regions and the most notable landmarks:\n",
            "supervised versus unsupervised learning and their variants, online versus batch learn‐\n",
            "ing, instance-based versus model-based learning. Then we will look at the workflow\n",
            "of a typical ML project, discuss the main challenges you may face, and cover how to\n",
            "evaluate and fine-tune a machine learning system.\n",
            "\n",
            "This  chapter  introduces  a  lot  of  fundamental  concepts  (and  jargon)  that  every  data\n",
            "scientist should know by heart. It will be a high-level overview (it’s the only chapter\n",
            "without much code), all rather simple, but my goal is to ensure everything is crystal\n",
            "\n",
            "3\n",
            "\n",
            "\fclear to you before we continue on to the rest of the book. So grab a coffee and let’s\n",
            "get started!\n",
            "\n",
            "If you are already familiar with machine learning basics, you may\n",
            "want to skip directly to Chapter 2. If you are not sure, try to answer\n",
            "all the questions listed at the end of the chapter before moving on.\n",
            "\n",
            "What Is Machine Learning?\n",
            "Machine  learning  is  the  science  (and  art)  of  programming  computers  so  they  can\n",
            "learn from data.\n",
            "\n",
            "Here is a slightly more general definition:\n",
            "\n",
            "[Machine  learning  is  the]  field  of  study  that  gives  computers  the  ability  to  learn\n",
            "without being explicitly programmed.\n",
            "\n",
            "—Arthur Samuel, 1959\n",
            "\n",
            "And a more engineering-oriented one:\n",
            "\n",
            "A  computer  program  is  said  to  learn  from  experience  E  with  respect  to  some  task\n",
            "T  and  some  performance  measure  P,  if  its  performance  on  T,  as  measured  by  P,\n",
            "improves with experience E.\n",
            "\n",
            "—Tom Mitchell, 1997\n",
            "\n",
            "Your spam filter is a machine learning program that, given examples of spam emails\n",
            "(flagged by users) and examples of regular emails (nonspam, also called “ham”), can\n",
            "learn to flag spam. The examples that the system uses to learn are called the training\n",
            "set.  Each  training  example  is  called  a  training  instance  (or  sample).  The  part  of  a\n",
            "machine learning system that learns and makes predictions is called a model. Neural\n",
            "networks and random forests are examples of models.\n",
            "\n",
            "In this case, the task T is to flag spam for new emails, the experience E is the training\n",
            "data, and the performance measure P needs to be defined; for example, you can use\n",
            "the ratio of correctly classified emails. This particular performance measure is called\n",
            "accuracy, and it is often used in classification tasks.\n",
            "\n",
            "If you just download a copy of all Wikipedia articles, your computer has a lot more\n",
            "data, but it is not suddenly better at any task. This is not machine learning.\n",
            "\n",
            "4 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fWhy Use Machine Learning?\n",
            "Consider how you would write a spam filter using traditional programming techni‐\n",
            "ques (Figure 1-1):\n",
            "\n",
            "1. First  you  would  examine  what  spam  typically  looks  like.  You  might  notice  that\n",
            "1.\n",
            "some  words  or  phrases  (such  as  “4U”,  “credit  card”,  “free”,  and  “amazing”)  tend\n",
            "to come up a lot in the subject line. Perhaps you would also notice a few other\n",
            "patterns in the sender’s name, the email’s body, and other parts of the email.\n",
            "\n",
            "2. You would write a detection algorithm for each of the patterns that you noticed,\n",
            "2.\n",
            "and your program would flag emails as spam if a number of these patterns were\n",
            "detected.\n",
            "\n",
            "3.\n",
            "3. You would test your program and repeat steps 1 and 2 until it was good enough\n",
            "\n",
            "to launch.\n",
            "\n",
            "Figure 1-1. The traditional approach\n",
            "\n",
            "Since the problem is difficult, your program will likely become a long list of complex\n",
            "rules—pretty hard to maintain.\n",
            "\n",
            "In contrast, a spam filter based on machine learning techniques automatically learns\n",
            "which  words  and  phrases  are  good  predictors  of  spam  by  detecting  unusually  fre‐\n",
            "quent  patterns  of  words  in  the  spam  examples  compared  to  the  ham  examples\n",
            "(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\n",
            "accurate.\n",
            "\n",
            "Why Use Machine Learning? \n",
            "\n",
            "| \n",
            "\n",
            "5\n",
            "\n",
            "\fFigure 1-2. The machine learning approach\n",
            "\n",
            "What  if  spammers  notice  that  all  their  emails  containing  “4U”  are  blocked?  They\n",
            "might  start  writing  “For  U”  instead.  A  spam  filter  using  traditional  programming\n",
            "techniques  would  need  to  be  updated  to  flag  “For  U”  emails.  If  spammers  keep\n",
            "working around your spam filter, you will need to keep writing new rules forever.\n",
            "\n",
            "In contrast, a spam filter based on machine learning techniques automatically notices\n",
            "that  “For  U”  has  become  unusually  frequent  in  spam  flagged  by  users,  and  it  starts\n",
            "flagging them without your intervention (Figure 1-3).\n",
            "\n",
            "Figure 1-3. Automatically adapting to change\n",
            "\n",
            "Another area where machine learning shines is for problems that either are too com‐\n",
            "plex for traditional approaches or have no known algorithm. For example, consider\n",
            "speech  recognition.  Say  you  want  to  start  simple  and  write  a  program  capable  of\n",
            "distinguishing the words “one” and “two”. You might notice that the word “two” starts\n",
            "with  a  high-pitch  sound  (“T”),  so  you  could  hardcode  an  algorithm  that  measures\n",
            "high-pitch sound intensity and use that to distinguish ones and twos—but obviously\n",
            "\n",
            "6 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fthis  technique  will  not  scale  to  thousands  of  words  spoken  by  millions  of  very\n",
            "different people in noisy environments and in dozens of languages. The best solution\n",
            "(at  least  today)  is  to  write  an  algorithm  that  learns  by  itself,  given  many  example\n",
            "recordings for each word.\n",
            "\n",
            "Finally,  machine  learning  can  help  humans  learn  (Figure  1-4).  ML  models  can  be\n",
            "inspected  to  see  what  they  have  learned  (although  for  some  models  this  can  be\n",
            "tricky).  For  instance,  once  a  spam  filter  has  been  trained  on  enough  spam,  it  can\n",
            "easily  be  inspected  to  reveal  the  list  of  words  and  combinations  of  words  that  it\n",
            "believes  are  the  best  predictors  of  spam.  Sometimes  this  will  reveal  unsuspected\n",
            "correlations or new trends, and thereby lead to a better understanding of the prob‐\n",
            "lem.  Digging  into  large  amounts  of  data  to  discover  hidden  patterns  is  called  data\n",
            "mining, and machine learning excels at it.\n",
            "\n",
            "Figure 1-4. Machine learning can help humans learn\n",
            "\n",
            "To summarize, machine learning is great for:\n",
            "\n",
            "• Problems for which existing solutions require a lot of fine-tuning or long lists of\n",
            "•\n",
            "rules (a machine learning model can often simplify code and perform better than\n",
            "the traditional approach)\n",
            "\n",
            "• Complex problems for which using a traditional approach yields no good solu‐\n",
            "•\n",
            "\n",
            "tion (the best machine learning techniques can perhaps find a solution)\n",
            "\n",
            "•\n",
            "• Fluctuating environments (a machine learning system can easily be retrained on\n",
            "\n",
            "new data, always keeping it up to date)\n",
            "\n",
            "•\n",
            "• Getting insights about complex problems and large amounts of data\n",
            "\n",
            "Why Use Machine Learning? \n",
            "\n",
            "| \n",
            "\n",
            "7\n",
            "\n",
            "\fExamples of Applications\n",
            "Let’s  look  at  some  concrete  examples  of  machine  learning  tasks,  along  with  the\n",
            "techniques that can tackle them:\n",
            "\n",
            "Analyzing images of products on a production line to automatically classify them\n",
            "\n",
            "This is image classification, typically performed using convolutional neural net‐\n",
            "works (CNNs; see Chapter 14) or sometimes transformers (see Chapter 16).\n",
            "\n",
            "Detecting tumors in brain scans\n",
            "\n",
            "This is semantic image segmentation, where each pixel in the image is classified\n",
            "(as we want to determine the exact location and shape of tumors), typically using\n",
            "CNNs or transformers.\n",
            "\n",
            "Automatically classifying news articles\n",
            "\n",
            "This is natural language processing (NLP), and more specifically text classifica‐\n",
            "tion, which can be tackled using recurrent neural networks (RNNs) and CNNs,\n",
            "but transformers work even better (see Chapter 16).\n",
            "\n",
            "Automatically flagging offensive comments on discussion forums\n",
            "This is also text classification, using the same NLP tools.\n",
            "\n",
            "Summarizing long documents automatically\n",
            "\n",
            "This is a branch of NLP called text summarization, again using the same tools.\n",
            "\n",
            "Creating a chatbot or a personal assistant\n",
            "\n",
            "This involves many NLP components, including natural language understanding\n",
            "(NLU) and question-answering modules.\n",
            "\n",
            "Forecasting your company’s revenue next year, based on many performance metrics\n",
            "\n",
            "This  is  a  regression  task  (i.e.,  predicting  values)  that  may  be  tackled  using  any\n",
            "regression  model,  such  as  a  linear  regression  or  polynomial  regression  model\n",
            "(see  Chapter  4),  a  regression  support  vector  machine  (see  Chapter  5),  a  regres‐\n",
            "sion  random  forest  (see  Chapter  7),  or  an  artificial  neural  network  (see  Chap‐\n",
            "ter 10). If you want to take into account sequences of past performance metrics,\n",
            "you may want to use RNNs, CNNs, or transformers (see Chapters 15 and 16).\n",
            "\n",
            "Making your app react to voice commands\n",
            "\n",
            "This is speech recognition, which requires processing audio samples: since they\n",
            "are long and complex sequences, they are typically processed using RNNs, CNNs,\n",
            "or transformers (see Chapters 15 and 16).\n",
            "\n",
            "Detecting credit card fraud\n",
            "\n",
            "This is anomaly detection, which can be tackled using isolation forests, Gaussian\n",
            "mixture models (see Chapter 9), or autoencoders (see Chapter 17).\n",
            "\n",
            "8 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fSegmenting clients based on their purchases so that you can design a different marketing\n",
            "strategy for each segment\n",
            "\n",
            "This  is  clustering,  which  can  be  achieved  using  k-means,  DBSCAN,  and  more\n",
            "(see Chapter 9).\n",
            "\n",
            "Representing a complex, high-dimensional dataset in a clear and insightful diagram\n",
            "\n",
            "This  is  data  visualization,  often  involving  dimensionality  reduction  techniques\n",
            "(see Chapter 8).\n",
            "\n",
            "Recommending a product that a client may be interested in, based on past purchases\n",
            "\n",
            "This  is  a  recommender  system.  One  approach  is  to  feed  past  purchases  (and\n",
            "other  information  about  the  client)  to  an  artificial  neural  network  (see  Chap‐\n",
            "ter 10), and get it to output the most likely next purchase. This neural net would\n",
            "typically be trained on past sequences of purchases across all clients.\n",
            "\n",
            "Building an intelligent bot for a game\n",
            "\n",
            "This is often tackled using reinforcement learning (RL; see Chapter 18), which is\n",
            "a branch of machine learning that trains agents (such as bots) to pick the actions\n",
            "that  will  maximize  their  rewards  over  time  (e.g.,  a  bot  may  get  a  reward  every\n",
            "time the player loses some life points), within a given environment (such as the\n",
            "game). The famous AlphaGo program that beat the world champion at the game\n",
            "of Go was built using RL.\n",
            "\n",
            "This  list  could  go  on  and  on,  but  hopefully  it  gives  you  a  sense  of  the  incredible\n",
            "breadth and complexity of the tasks that machine learning can tackle, and the types of\n",
            "techniques that you would use for each task.\n",
            "\n",
            "Types of Machine Learning Systems\n",
            "There  are  so  many  different  types  of  machine  learning  systems  that  it  is  useful  to\n",
            "classify them in broad categories, based on the following criteria:\n",
            "\n",
            "• How  they  are  supervised  during  training  (supervised,  unsupervised,  semi-\n",
            "•\n",
            "\n",
            "supervised, self-supervised, and others)\n",
            "\n",
            "• Whether  or  not  they  can  learn  incrementally  on  the  fly  (online  versus  batch\n",
            "•\n",
            "\n",
            "learning)\n",
            "\n",
            "•\n",
            "• Whether they work by simply comparing new data points to known data points,\n",
            "or  instead  by  detecting  patterns  in  the  training  data  and  building  a  predictive\n",
            "model, much like scientists do (instance-based versus model-based learning)\n",
            "\n",
            "These  criteria  are  not  exclusive;  you  can  combine  them  in  any  way  you  like.  For\n",
            "example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\n",
            "work model trained using human-provided examples of spam and ham; this makes it\n",
            "an online, model-based, supervised learning system.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "9\n",
            "\n",
            "\fLet’s look at each of these criteria a bit more closely.\n",
            "\n",
            "Training Supervision\n",
            "ML systems can be classified according to the amount and type of supervision they\n",
            "get  during  training.  There  are  many  categories,  but  we’ll  discuss  the  main  ones:\n",
            "supervised learning, unsupervised learning, self-supervised learning, semi-supervised\n",
            "learning, and reinforcement learning.\n",
            "\n",
            "Supervised learning\n",
            "\n",
            "In supervised learning, the training set you feed to the algorithm includes the desired\n",
            "solutions, called labels (Figure 1-5).\n",
            "\n",
            "Figure 1-5. A labeled training set for spam classification (an example of supervised\n",
            "learning)\n",
            "\n",
            "A typical supervised learning task is classification. The spam filter is a good example\n",
            "of this: it is trained with many example emails along with their class (spam or ham),\n",
            "and it must learn how to classify new emails.\n",
            "\n",
            "Another  typical  task  is  to  predict  a  target  numeric  value,  such  as  the  price  of  a\n",
            "car,  given  a  set  of  features  (mileage,  age,  brand,  etc.).  This  sort  of  task  is  called\n",
            "regression  (Figure  1-6).1  To  train  the  system,  you  need  to  give  it  many  examples  of\n",
            "cars, including both their features and their targets (i.e., their prices).\n",
            "\n",
            "Note  that  some  regression  models  can  be  used  for  classification  as  well,  and  vice\n",
            "versa.  For  example,  logistic  regression  is  commonly  used  for  classification,  as  it  can\n",
            "\n",
            "1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\n",
            "fact that the children of tall people tend to be shorter than their parents. Since the children were shorter, he\n",
            "called this regression to the mean. This name was then applied to the methods he used to analyze correlations\n",
            "between variables.\n",
            "\n",
            "10 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\foutput a value that corresponds to the probability of belonging to a given class (e.g.,\n",
            "20% chance of being spam).\n",
            "\n",
            "Figure 1-6. A regression problem: predict a value, given an input feature (there are\n",
            "usually multiple input features, and sometimes multiple output values)\n",
            "\n",
            "The  words  target  and  label  are  generally  treated  as  synonyms  in\n",
            "supervised learning, but target is more common in regression tasks\n",
            "and  label  is  more  common  in  classification  tasks.  Moreover,  fea‐\n",
            "tures are sometimes called predictors or attributes. These terms may\n",
            "refer to individual samples (e.g., “this car’s mileage feature is equal\n",
            "to  15,000”)  or  to  all  samples  (e.g.,  “the  mileage  feature  is  strongly\n",
            "correlated with price”).\n",
            "\n",
            "Unsupervised learning\n",
            "\n",
            "In  unsupervised  learning,  as  you  might  guess,  the  training  data  is  unlabeled  (Fig‐\n",
            "ure 1-7). The system tries to learn without a teacher.\n",
            "\n",
            "For  example,  say  you  have  a  lot  of  data  about  your  blog’s  visitors.  You  may  want\n",
            "to run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8).\n",
            "At  no  point  do  you  tell  the  algorithm  which  group  a  visitor  belongs  to:  it  finds\n",
            "those connections without your help. For example, it might notice that 40% of your\n",
            "visitors are teenagers who love comic books and generally read your blog after school,\n",
            "while  20%  are  adults  who  enjoy  sci-fi  and  who  visit  during  the  weekends.  If  you\n",
            "use a hierarchical clustering algorithm, it may also subdivide each group into smaller\n",
            "groups. This may help you target your posts for each group.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "11\n",
            "\n",
            "\fFigure 1-7. An unlabeled training set for unsupervised learning\n",
            "\n",
            "Figure 1-8. Clustering\n",
            "\n",
            "Visualization  algorithms  are  also  good  examples  of  unsupervised  learning:  you  feed\n",
            "them a lot of complex and unlabeled data, and they output a 2D or 3D representation\n",
            "of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve\n",
            "as much structure as they can (e.g., trying to keep separate clusters in the input space\n",
            "from  overlapping  in  the  visualization)  so  that  you  can  understand  how  the  data  is\n",
            "organized and perhaps identify unsuspected patterns.\n",
            "\n",
            "A  related  task  is  dimensionality  reduction,  in  which  the  goal  is  to  simplify  the  data\n",
            "without losing too much information. One way to do this is to merge several correla‐\n",
            "ted features into one. For example, a car’s mileage may be strongly correlated with its\n",
            "age, so the dimensionality reduction algorithm will merge them into one feature that\n",
            "represents the car’s wear and tear. This is called feature extraction.\n",
            "\n",
            "12 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters2\n",
            "\n",
            "It is often a good idea to try to reduce the number of dimensions\n",
            "in  your  training  data  using  a  dimensionality  reduction  algorithm\n",
            "before you feed it to another machine learning algorithm (such as\n",
            "a  supervised  learning  algorithm).  It  will  run  much  faster,  the  data\n",
            "will take up less disk and memory space, and in some cases it may\n",
            "also perform better.\n",
            "\n",
            "Yet another important unsupervised task is anomaly detection—for example, detect‐\n",
            "ing unusual credit card transactions to prevent fraud, catching manufacturing defects,\n",
            "or automatically removing outliers from a dataset before feeding it to another learn‐\n",
            "ing  algorithm.  The  system  is  shown  mostly  normal  instances  during  training,  so  it\n",
            "learns to recognize them; then, when it sees a new instance, it can tell whether it looks\n",
            "like a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\n",
            "task  is  novelty  detection:  it  aims  to  detect  new  instances  that  look  different  from  all\n",
            "instances in the training set. This requires having a very “clean” training set, devoid\n",
            "of any instance that you would like the algorithm to detect. For example, if you have\n",
            "thousands of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a\n",
            "\n",
            "2 Notice how animals are rather well separated from vehicles and how horses are close to deer but far\n",
            "\n",
            "from birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through\n",
            "Cross-Modal Transfer”, Proceedings of the 26th International Conference on Neural Information Processing\n",
            "Systems 1 (2013): 935–943.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "13\n",
            "\n",
            "\fnovelty detection algorithm should not treat new pictures of Chihuahuas as novelties.\n",
            "On the other hand, anomaly detection algorithms may consider these dogs as so rare\n",
            "and so different from other dogs that they would likely classify them as anomalies (no\n",
            "offense to Chihuahuas).\n",
            "\n",
            "Figure 1-10. Anomaly detection\n",
            "\n",
            "Finally, another common unsupervised task is association rule learning, in which the\n",
            "goal  is  to  dig  into  large  amounts  of  data  and  discover  interesting  relations  between\n",
            "attributes. For example, suppose you own a supermarket. Running an association rule\n",
            "on  your  sales  logs  may  reveal  that  people  who  purchase  barbecue  sauce  and  potato\n",
            "chips  also  tend  to  buy  steak.  Thus,  you  may  want  to  place  these  items  close  to  one\n",
            "another.\n",
            "\n",
            "Semi-supervised learning\n",
            "\n",
            "Since labeling data is usually time-consuming and costly, you will often have plenty\n",
            "of unlabeled instances, and few labeled instances. Some algorithms can deal with data\n",
            "that’s partially labeled. This is called semi-supervised learning (Figure 1-11).\n",
            "\n",
            "Figure 1-11. Semi-supervised learning with two classes (triangles and squares): the\n",
            "unlabeled examples (circles) help classify a new instance (the cross) into the triangle class\n",
            "rather than the square class, even though it is closer to the labeled squares\n",
            "\n",
            "14 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fSome photo-hosting services, such as Google Photos, are good examples of this. Once\n",
            "you upload all your family photos to the service, it automatically recognizes that the\n",
            "same person A shows up in photos 1, 5, and 11, while another person B shows up in\n",
            "photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now\n",
            "all the system needs is for you to tell it who these people are. Just add one label per\n",
            "person3 and it is able to name everyone in every photo, which is useful for searching\n",
            "photos.\n",
            "\n",
            "Most  semi-supervised  learning  algorithms  are  combinations  of  unsupervised  and\n",
            "supervised  algorithms.  For  example,  a  clustering  algorithm  may  be  used  to  group\n",
            "similar instances together, and then every unlabeled instance can be labeled with the\n",
            "most common label in its cluster. Once the whole dataset is labeled, it is possible to\n",
            "use any supervised learning algorithm.\n",
            "\n",
            "Self-supervised learning\n",
            "\n",
            "Another  approach  to  machine  learning  involves  actually  generating  a  fully  labeled\n",
            "dataset  from  a  fully  unlabeled  one.  Again,  once  the  whole  dataset  is  labeled,  any\n",
            "supervised  learning  algorithm  can  be  used.  This  approach  is  called  self-supervised\n",
            "learning.\n",
            "\n",
            "For example, if you have a large dataset of unlabeled images, you can randomly mask\n",
            "a  small  part  of  each  image  and  then  train  a  model  to  recover  the  original  image\n",
            "(Figure  1-12).  During  training,  the  masked  images  are  used  as  the  inputs  to  the\n",
            "model, and the original images are used as the labels.\n",
            "\n",
            "Figure 1-12. Self-supervised learning example: input (left) and target (right)\n",
            "\n",
            "3 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\n",
            "mixes up two people who look alike, so you may need to provide a few labels per person and manually clean\n",
            "up some clusters.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "15\n",
            "\n",
            "\fThe  resulting  model  may  be  quite  useful  in  itself—for  example,  to  repair  damaged\n",
            "images or to erase unwanted objects from pictures. But more often than not, a model\n",
            "trained  using  self-supervised  learning  is  not  the  final  goal.  You’ll  usually  want  to\n",
            "tweak and fine-tune the model for a slightly different task—one that you actually care\n",
            "about.\n",
            "\n",
            "For example, suppose that what you really want is to have a pet classification model:\n",
            "given  a  picture  of  any  pet,  it  will  tell  you  what  species  it  belongs  to.  If  you  have  a\n",
            "large dataset of unlabeled photos of pets, you can start by training an image-repairing\n",
            "model  using  self-supervised  learning.  Once  it’s  performing  well,  it  should  be  able\n",
            "to  distinguish  different  pet  species:  when  it  repairs  an  image  of  a  cat  whose  face  is\n",
            "masked,  it  must  know  not  to  add  a  dog’s  face.  Assuming  your  model’s  architecture\n",
            "allows  it  (and  most  neural  network  architectures  do),  it  is  then  possible  to  tweak\n",
            "the  model  so  that  it  predicts  pet  species  instead  of  repairing  images.  The  final  step\n",
            "consists of fine-tuning the model on a labeled dataset: the model already knows what\n",
            "cats,  dogs,  and  other  pet  species  look  like,  so  this  step  is  only  needed  so  the  model\n",
            "can learn the mapping between the species it already knows and the labels we expect\n",
            "from it.\n",
            "\n",
            "Transferring knowledge from one task to another is called transfer\n",
            "learning, and it’s one of the most important techniques in machine\n",
            "learning  today,  especially  when  using  deep  neural  networks  (i.e.,\n",
            "neural  networks  composed  of  many  layers  of  neurons).  We  will\n",
            "discuss this in detail in Part II.\n",
            "\n",
            "Some people consider self-supervised learning to be a part of unsupervised learning,\n",
            "since it deals with fully unlabeled datasets. But self-supervised learning uses (gener‐\n",
            "ated) labels during training, so in that regard it’s closer to supervised learning. And\n",
            "the  term  “unsupervised  learning”  is  generally  used  when  dealing  with  tasks  like\n",
            "clustering,  dimensionality  reduction,  or  anomaly  detection,  whereas  self-supervised\n",
            "learning  focuses  on  the  same  tasks  as  supervised  learning:  mainly  classification  and\n",
            "regression. In short, it’s best to treat self-supervised learning as its own category.\n",
            "\n",
            "Reinforcement learning\n",
            "\n",
            "Reinforcement learning is a very different beast. The learning system, called an agent\n",
            "in  this  context,  can  observe  the  environment,  select  and  perform  actions,  and  get\n",
            "rewards  in  return  (or  penalties  in  the  form  of  negative  rewards,  as  shown  in  Fig‐\n",
            "ure 1-13). It must then learn by itself what is the best strategy, called a policy, to get\n",
            "the  most  reward  over  time.  A  policy  defines  what  action  the  agent  should  choose\n",
            "when it is in a given situation.\n",
            "\n",
            "16 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fFigure 1-13. Reinforcement learning\n",
            "\n",
            "For  example,  many  robots  implement  reinforcement  learning  algorithms  to  learn\n",
            "how to walk. DeepMind’s AlphaGo program is also a good example of reinforcement\n",
            "learning:  it  made  the  headlines  in  May  2017  when  it  beat  Ke  Jie,  the  number  one\n",
            "ranked player in the world at the time, at the game of Go. It learned its winning policy\n",
            "by analyzing millions of games, and then playing many games against itself. Note that\n",
            "learning  was  turned  off  during  the  games  against  the  champion;  AlphaGo  was  just\n",
            "applying  the  policy  it  had  learned.  As  you  will  see  in  the  next  section,  this  is  called\n",
            "offline learning.\n",
            "\n",
            "Batch Versus Online Learning\n",
            "Another  criterion  used  to  classify  machine  learning  systems  is  whether  or  not  the\n",
            "system can learn incrementally from a stream of incoming data.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "17\n",
            "\n",
            "\fBatch learning\n",
            "\n",
            "In batch learning, the system is incapable of learning incrementally: it must be trained\n",
            "using  all  the  available  data.  This  will  generally  take  a  lot  of  time  and  computing\n",
            "resources,  so  it  is  typically  done  offline.  First  the  system  is  trained,  and  then  it  is\n",
            "launched into production and runs without learning anymore; it just applies what it\n",
            "has learned. This is called offline learning.\n",
            "\n",
            "Unfortunately, a model’s performance tends to decay slowly over time, simply because\n",
            "the  world  continues  to  evolve  while  the  model  remains  unchanged.  This  phenom‐\n",
            "enon  is  often  called  model  rot  or  data  drift.  The  solution  is  to  regularly  retrain  the\n",
            "model on up-to-date data. How often you need to do that depends on the use case: if\n",
            "the model classifies pictures of cats and dogs, its performance will decay very slowly,\n",
            "but if the model deals with fast-evolving systems, for example making predictions on\n",
            "the financial market, then it is likely to decay quite fast.\n",
            "\n",
            "Even a model trained to classify pictures of cats and dogs may need\n",
            "to  be  retrained  regularly,  not  because  cats  and  dogs  will  mutate\n",
            "overnight,  but  because  cameras  keep  changing,  along  with  image\n",
            "formats,  sharpness,  brightness,  and  size  ratios.  Moreover,  people\n",
            "may  love  different  breeds  next  year,  or  they  may  decide  to  dress\n",
            "their pets with tiny hats—who knows?\n",
            "\n",
            "If you want a batch learning system to know about new data (such as a new type of\n",
            "spam), you need to train a new version of the system from scratch on the full dataset\n",
            "(not just the new data, but also the old data), then replace the old model with the new\n",
            "one. Fortunately, the whole process of training, evaluating, and launching a machine\n",
            "learning  system  can  be  automated  fairly  easily  (as  we  saw  in  Figure  1-3),  so  even  a\n",
            "batch learning system can adapt to change. Simply update the data and train a new\n",
            "version of the system from scratch as often as needed.\n",
            "\n",
            "This  solution  is  simple  and  often  works  fine,  but  training  using  the  full  set  of  data\n",
            "can take many hours, so you would typically train a new system only every 24 hours\n",
            "or even just weekly. If your system needs to adapt to rapidly changing data (e.g., to\n",
            "predict stock prices), then you need a more reactive solution.\n",
            "\n",
            "Also,  training  on  the  full  set  of  data  requires  a  lot  of  computing  resources  (CPU,\n",
            "memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\n",
            "you automate your system to train from scratch every day, it will end up costing you\n",
            "a lot of money. If the amount of data is huge, it may even be impossible to use a batch\n",
            "learning algorithm.\n",
            "\n",
            "18 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fFinally,  if  your  system  needs  to  be  able  to  learn  autonomously  and  it  has  limited\n",
            "resources (e.g., a smartphone application or a rover on Mars), then carrying around\n",
            "large  amounts  of  training  data  and  taking  up  a  lot  of  resources  to  train  for  hours\n",
            "every day is a showstopper.\n",
            "\n",
            "A  better  option  in  all  these  cases  is  to  use  algorithms  that  are  capable  of  learning\n",
            "incrementally.\n",
            "\n",
            "Online learning\n",
            "\n",
            "In  online  learning,  you  train  the  system  incrementally  by  feeding  it  data  instances\n",
            "sequentially, either individually or in small groups called mini-batches. Each learning\n",
            "step is fast and cheap, so the system can learn about new data on the fly, as it arrives\n",
            "(see Figure 1-14).\n",
            "\n",
            "Figure 1-14. In online learning, a model is trained and launched into production, and\n",
            "then it keeps learning as new data comes in\n",
            "\n",
            "Online learning is useful for systems that need to adapt to change extremely rapidly\n",
            "(e.g., to detect new patterns in the stock market). It is also a good option if you have\n",
            "limited computing resources; for example, if the model is trained on a mobile device.\n",
            "\n",
            "Additionally, online learning algorithms can be used to train models on huge datasets\n",
            "that  cannot  fit  in  one  machine’s  main  memory  (this  is  called  out-of-core  learning).\n",
            "The algorithm loads part of the data, runs a training step on that data, and repeats the\n",
            "process until it has run on all of the data (see Figure 1-15).\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "19\n",
            "\n",
            "\fFigure 1-15. Using online learning to handle huge datasets\n",
            "\n",
            "One  important  parameter  of  online  learning  systems  is  how  fast  they  should  adapt\n",
            "to changing data: this is called the learning rate. If you set a high learning rate, then\n",
            "your system will rapidly adapt to new data, but it will also tend to quickly forget the\n",
            "old data (and you don’t want a spam filter to flag only the latest kinds of spam it was\n",
            "shown). Conversely, if you set a low learning rate, the system will have more inertia;\n",
            "that is, it will learn more slowly, but it will also be less sensitive to noise in the new\n",
            "data or to sequences of nonrepresentative data points (outliers).\n",
            "\n",
            "Out-of-core  learning  is  usually  done  offline  (i.e.,  not  on  the  live\n",
            "system), so online learning can be a confusing name. Think of it as\n",
            "incremental learning.\n",
            "\n",
            "A  big  challenge  with  online  learning  is  that  if  bad  data  is  fed  to  the  system,  the\n",
            "system’s  performance  will  decline,  possibly  quickly  (depending  on  the  data  quality\n",
            "and learning rate). If it’s a live system, your clients will notice. For example, bad data\n",
            "could come from a bug (e.g., a malfunctioning sensor on a robot), or it could come\n",
            "from  someone  trying  to  game  the  system  (e.g.,  spamming  a  search  engine  to  try  to\n",
            "rank  high  in  search  results).  To  reduce  this  risk,  you  need  to  monitor  your  system\n",
            "closely and promptly switch learning off (and possibly revert to a previously working\n",
            "state) if you detect a drop in performance. You may also want to monitor the input\n",
            "data and react to abnormal data; for example, using an anomaly detection algorithm\n",
            "(see Chapter 9).\n",
            "\n",
            "20 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fInstance-Based Versus Model-Based Learning\n",
            "One  more  way  to  categorize  machine  learning  systems  is  by  how  they  generalize.\n",
            "Most machine learning tasks are about making predictions. This means that given a\n",
            "number of training examples, the system needs to be able to make good predictions\n",
            "for  (generalize  to)  examples  it  has  never  seen  before.  Having  a  good  performance\n",
            "measure on the training data is good, but insufficient; the true goal is to perform well\n",
            "on new instances.\n",
            "\n",
            "There  are  two  main  approaches  to  generalization:  instance-based  learning  and\n",
            "model-based learning.\n",
            "\n",
            "Instance-based learning\n",
            "\n",
            "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
            "create a spam filter this way, it would just flag all emails that are identical to emails\n",
            "that have already been flagged by users—not the worst solution, but certainly not the\n",
            "best.\n",
            "\n",
            "Instead  of  just  flagging  emails  that  are  identical  to  known  spam  emails,  your  spam\n",
            "filter could be programmed to also flag emails that are very similar to known spam\n",
            "emails.  This  requires  a  measure  of  similarity  between  two  emails.  A  (very  basic)\n",
            "similarity measure between two emails could be to count the number of words they\n",
            "have  in  common.  The  system  would  flag  an  email  as  spam  if  it  has  many  words  in\n",
            "common with a known spam email.\n",
            "\n",
            "This is called instance-based learning: the system learns the examples by heart, then\n",
            "generalizes  to  new  cases  by  using  a  similarity  measure  to  compare  them  to  the\n",
            "learned examples (or a subset of them). For example, in Figure 1-16 the new instance\n",
            "would  be  classified  as  a  triangle  because  the  majority  of  the  most  similar  instances\n",
            "belong to that class.\n",
            "\n",
            "Figure 1-16. Instance-based learning\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "21\n",
            "\n",
            "\fModel-based learning and a typical machine learning workflow\n",
            "\n",
            "Another  way  to  generalize  from  a  set  of  examples  is  to  build  a  model  of  these\n",
            "examples  and  then  use  that  model  to  make  predictions.  This  is  called  model-based\n",
            "learning (Figure 1-17).\n",
            "\n",
            "Figure 1-17. Model-based learning\n",
            "\n",
            "For  example,  suppose  you  want  to  know  if  money  makes  people  happy,  so  you\n",
            "download the Better Life Index data from the OECD’s website and World Bank stats\n",
            "about gross domestic product (GDP) per capita. Then you join the tables and sort by\n",
            "GDP per capita. Table 1-1 shows an excerpt of what you get.\n",
            "\n",
            "Table 1-1. Does money make people happier?\n",
            "\n",
            "GDP per capita (USD)\n",
            "28,384\n",
            "\n",
            "Life satisfaction\n",
            "5.5\n",
            "\n",
            "Country\n",
            "Turkey\n",
            "\n",
            "Hungary\n",
            "\n",
            "France\n",
            "\n",
            "31,008\n",
            "\n",
            "42,026\n",
            "\n",
            "United States\n",
            "\n",
            "60,236\n",
            "\n",
            "New Zealand\n",
            "\n",
            "42,404\n",
            "\n",
            "Australia\n",
            "\n",
            "Denmark\n",
            "\n",
            "48,698\n",
            "\n",
            "55,938\n",
            "\n",
            "5.6\n",
            "\n",
            "6.5\n",
            "\n",
            "6.9\n",
            "\n",
            "7.3\n",
            "\n",
            "7.3\n",
            "\n",
            "7.6\n",
            "\n",
            "Let’s plot the data for these countries (Figure 1-18).\n",
            "\n",
            "22 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fFigure 1-18. Do you see a trend here?\n",
            "\n",
            "There does seem to be a trend here! Although the data is noisy (i.e., partly random),\n",
            "it  looks  like  life  satisfaction  goes  up  more  or  less  linearly  as  the  country’s  GDP  per\n",
            "capita increases. So you decide to model life satisfaction as a linear function of GDP\n",
            "per  capita.  This  step  is  called  model  selection:  you  selected  a  linear  model  of  life\n",
            "satisfaction with just one attribute, GDP per capita (Equation 1-1).\n",
            "\n",
            "Equation 1-1. A simple linear model\n",
            "\n",
            "life_satisfaction = θ0 + θ1 × GDP_per_capita\n",
            "\n",
            "This model has two model parameters, θ0 and θ1.4 By tweaking these parameters, you\n",
            "can make your model represent any linear function, as shown in Figure 1-19.\n",
            "\n",
            "Figure 1-19. A few possible linear models\n",
            "\n",
            "4 By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "23\n",
            "\n",
            "\fBefore  you  can  use  your  model,  you  need  to  define  the  parameter  values  θ0  and  θ1.\n",
            "How can you know which values will make your model perform best? To answer this\n",
            "question, you need to specify a performance measure. You can either define a utility\n",
            "function (or fitness function) that measures how good your model is, or you can define\n",
            "a  cost  function  that  measures  how  bad  it  is.  For  linear  regression  problems,  people\n",
            "typically  use  a  cost  function  that  measures  the  distance  between  the  linear  model’s\n",
            "predictions and the training examples; the objective is to minimize this distance.\n",
            "\n",
            "This  is  where  the  linear  regression  algorithm  comes  in:  you  feed  it  your  training\n",
            "examples, and it finds the parameters that make the linear model fit best to your data.\n",
            "This  is  called  training  the  model.  In  our  case,  the  algorithm  finds  that  the  optimal\n",
            "parameter values are θ0 = 3.75 and θ1 = 6.78 × 10–5.\n",
            "\n",
            "Confusingly,  the  word  “model”  can  refer  to  a  type  of  model  (e.g.,\n",
            "linear regression), to a fully specified model architecture (e.g., linear\n",
            "regression  with  one  input  and  one  output),  or  to  the  final  trained\n",
            "model ready to be used for predictions (e.g., linear regression with\n",
            "one  input  and  one  output,  using  θ0  =  3.75  and  θ1  =  6.78  ×  10–5).\n",
            "Model  selection  consists  in  choosing  the  type  of  model  and  fully\n",
            "specifying  its  architecture.  Training  a  model  means  running  an\n",
            "algorithm to find the model parameters that will make it best fit the\n",
            "training data, and hopefully make good predictions on new data.\n",
            "\n",
            "Now the model fits the training data as closely as possible (for a linear model), as you\n",
            "can see in Figure 1-20.\n",
            "\n",
            "Figure 1-20. The linear model that fits the training data best\n",
            "\n",
            "24 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fYou  are  finally  ready  to  run  the  model  to  make  predictions.  For  example,  say  you\n",
            "want  to  know  how  happy  Cypriots  are,  and  the  OECD  data  does  not  have  the\n",
            "answer. Fortunately, you can use your model to make a good prediction: you look up\n",
            "Cyprus’s GDP per capita, find $37,655, and then apply your model and find that life\n",
            "satisfaction is likely to be somewhere around 3.75 + 37,655 × 6.78 × 10–5 = 6.30.\n",
            "\n",
            "To  whet  your  appetite,  Example  1-1  shows  the  Python  code  that  loads  the  data,\n",
            "separates  the  inputs  X  from  the  labels  y,  creates  a  scatterplot  for  visualization,  and\n",
            "then trains a linear model and makes a prediction.5\n",
            "\n",
            "Example 1-1. Training and running a linear model using Scikit-Learn\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from sklearn.linear_model import LinearRegression\n",
            "\n",
            "# Download and prepare the data\n",
            "data_root = \"https://github.com/ageron/data/raw/main/\"\n",
            "lifesat = pd.read_csv(data_root + \"lifesat/lifesat.csv\")\n",
            "X = lifesat[[\"GDP per capita (USD)\"]].values\n",
            "y = lifesat[[\"Life satisfaction\"]].values\n",
            "\n",
            "# Visualize the data\n",
            "lifesat.plot(kind='scatter', grid=True,\n",
            "             x=\"GDP per capita (USD)\", y=\"Life satisfaction\")\n",
            "plt.axis([23_500, 62_500, 4, 9])\n",
            "plt.show()\n",
            "\n",
            "# Select a linear model\n",
            "model = LinearRegression()\n",
            "\n",
            "# Train the model\n",
            "model.fit(X, y)\n",
            "\n",
            "# Make a prediction for Cyprus\n",
            "X_new = [[37_655.2]]  # Cyprus' GDP per capita in 2020\n",
            "print(model.predict(X_new)) # output: [[6.30165767]]\n",
            "\n",
            "5 It’s OK if you don’t understand all the code yet; I will present Scikit-Learn in the following chapters.\n",
            "\n",
            "Types of Machine Learning Systems \n",
            "\n",
            "| \n",
            "\n",
            "25\n",
            "\n",
            "\fIf you had used an instance-based learning algorithm instead, you\n",
            "would have found that Israel has the closest GDP per capita to that\n",
            "of Cyprus ($38,341), and since the OECD data tells us that Israelis’\n",
            "life  satisfaction  is  7.2,  you  would  have  predicted  a  life  satisfaction\n",
            "of  7.2  for  Cyprus.  If  you  zoom  out  a  bit  and  look  at  the  two\n",
            "next-closest  countries,  you  will  find  Lithuania  and  Slovenia,  both\n",
            "with a life satisfaction of 5.9. Averaging these three values, you get\n",
            "6.33,  which  is  pretty  close  to  your  model-based  prediction.  This\n",
            "simple  algorithm  is  called  k-nearest  neighbors  regression  (in  this\n",
            "example, k = 3).\n",
            "\n",
            "Replacing  the  linear  regression  model  with  k-nearest  neighbors\n",
            "regression in the previous code is as easy as replacing these lines:\n",
            "\n",
            "from sklearn.linear_model import LinearRegression\n",
            "model = LinearRegression()\n",
            "\n",
            "with these two:\n",
            "\n",
            "from sklearn.neighbors import KNeighborsRegressor\n",
            "model = KNeighborsRegressor(n_neighbors=3)\n",
            "\n",
            "If  all  went  well,  your  model  will  make  good  predictions.  If  not,  you  may  need  to\n",
            "use more attributes (employment rate, health, air pollution, etc.), get more or better-\n",
            "quality  training  data,  or  perhaps  select  a  more  powerful  model  (e.g.,  a  polynomial\n",
            "regression model).\n",
            "\n",
            "In summary:\n",
            "\n",
            "•\n",
            "• You studied the data.\n",
            "\n",
            "•\n",
            "• You selected a model.\n",
            "\n",
            "•\n",
            "• You trained it on the training data (i.e., the learning algorithm searched for the\n",
            "\n",
            "model parameter values that minimize a cost function).\n",
            "\n",
            "• Finally,  you  applied  the  model  to  make  predictions  on  new  cases  (this  is  called\n",
            "•\n",
            "\n",
            "inference), hoping that this model will generalize well.\n",
            "\n",
            "This  is  what  a  typical  machine  learning  project  looks  like.  In  Chapter  2  you  will\n",
            "experience this firsthand by going through a project end to end.\n",
            "\n",
            "We  have  covered  a  lot  of  ground  so  far:  you  now  know  what  machine  learning  is\n",
            "really  about,  why  it  is  useful,  what  some  of  the  most  common  categories  of  ML\n",
            "systems are, and what a typical project workflow looks like. Now let’s look at what can\n",
            "go wrong in learning and prevent you from making accurate predictions.\n",
            "\n",
            "26 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fMain Challenges of Machine Learning\n",
            "In short, since your main task is to select a model and train it on some data, the two\n",
            "things that can go wrong are “bad model” and “bad data”. Let’s start with examples of\n",
            "bad data.\n",
            "\n",
            "Insufficient Quantity of Training Data\n",
            "For a toddler to learn what an apple is, all it takes is for you to point to an apple and\n",
            "say “apple” (possibly repeating this procedure a few times). Now the child is able to\n",
            "recognize apples in all sorts of colors and shapes. Genius.\n",
            "\n",
            "Machine  learning  is  not  quite  there  yet;  it  takes  a  lot  of  data  for  most  machine\n",
            "learning  algorithms  to  work  properly.  Even  for  very  simple  problems  you  typically\n",
            "need  thousands  of  examples,  and  for  complex  problems  such  as  image  or  speech\n",
            "recognition  you  may  need  millions  of  examples  (unless  you  can  reuse  parts  of  an\n",
            "existing model).\n",
            "\n",
            "The Unreasonable Effectiveness of Data\n",
            "In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\n",
            "Brill showed that very different machine learning algorithms, including fairly simple\n",
            "ones,  performed  almost  identically  well  on  a  complex  problem  of  natural  language\n",
            "disambiguation6 once they were given enough data (as you can see in Figure 1-21).\n",
            "\n",
            "As the authors put it, “these results suggest that we may want to reconsider the trade-\n",
            "off between spending time and money on algorithm development versus spending it\n",
            "on corpus development”.\n",
            "\n",
            "The idea that data matters more than algorithms for complex problems was further\n",
            "popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness of\n",
            "Data”, published in 2009.7 It should be noted, however, that small and medium-sized\n",
            "datasets are still very common, and it is not always easy or cheap to get extra training\n",
            "data—so don’t abandon algorithms just yet.\n",
            "\n",
            "6 For example, knowing whether to write “to”, “two”, or “too”, depending on the context.\n",
            "\n",
            "7 Peter Norvig et al., “The Unreasonable Effectiveness of Data”, IEEE Intelligent Systems 24, no. 2 (2009): 8–12.\n",
            "\n",
            "Main Challenges of Machine Learning \n",
            "\n",
            "| \n",
            "\n",
            "27\n",
            "\n",
            "\fFigure 1-21. The importance of data versus algorithms8\n",
            "\n",
            "Nonrepresentative Training Data\n",
            "In  order  to  generalize  well,  it  is  crucial  that  your  training  data  be  representative  of\n",
            "the new cases you want to generalize to. This is true whether you use instance-based\n",
            "learning or model-based learning.\n",
            "\n",
            "For example, the set of countries you used earlier for training the linear model was\n",
            "not  perfectly  representative;  it  did  not  contain  any  country  with  a  GDP  per  capita\n",
            "lower  than  $23,500  or  higher  than  $62,500.  Figure  1-22  shows  what  the  data  looks\n",
            "like when you add such countries.\n",
            "\n",
            "If you train a linear model on this data, you get the solid line, while the old model is\n",
            "represented  by  the  dotted  line.  As  you  can  see,  not  only  does  adding  a  few  missing\n",
            "countries significantly alter the model, but it makes it clear that such a simple linear\n",
            "model is probably never going to work well. It seems that very rich countries are not\n",
            "happier than moderately rich countries (in fact, they seem slightly unhappier!), and\n",
            "conversely some poor countries seem happier than many rich countries.\n",
            "\n",
            "By  using  a  nonrepresentative  training  set,  you  trained  a  model  that  is  unlikely  to\n",
            "make accurate predictions, especially for very poor and very rich countries.\n",
            "\n",
            "8 Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very Large Corpora\n",
            "\n",
            "for Natural Language Disambiguation”, Proceedings of the 39th Annual Meeting of the Association for Computa‐\n",
            "tional Linguistics (2001): 26–33.\n",
            "\n",
            "28 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fFigure 1-22. A more representative training sample\n",
            "\n",
            "It  is  crucial  to  use  a  training  set  that  is  representative  of  the  cases  you  want  to\n",
            "generalize  to.  This  is  often  harder  than  it  sounds:  if  the  sample  is  too  small,  you\n",
            "will have sampling noise (i.e., nonrepresentative data as a result of chance), but even\n",
            "very large samples can be nonrepresentative if the sampling method is flawed. This is\n",
            "called sampling bias.\n",
            "\n",
            "Examples of Sampling Bias\n",
            "Perhaps  the  most  famous  example  of  sampling  bias  happened  during  the  US  presi‐\n",
            "dential  election  in  1936,  which  pitted  Landon  against  Roosevelt:  the  Literary  Digest\n",
            "conducted a very large poll, sending mail to about 10 million people. It got 2.4 million\n",
            "answers, and predicted with high confidence that Landon would get 57% of the votes.\n",
            "Instead,  Roosevelt  won  with  62%  of  the  votes.  The  flaw  was  in  the  Literary  Digest’s\n",
            "sampling method:\n",
            "\n",
            "•\n",
            "• First,  to  obtain  the  addresses  to  send  the  polls  to,  the  Literary  Digest  used  tele‐\n",
            "phone directories, lists of magazine subscribers, club membership lists, and the\n",
            "like. All of these lists tended to favor wealthier people, who were more likely to\n",
            "vote Republican (hence Landon).\n",
            "\n",
            "•\n",
            "• Second,  less  than  25%  of  the  people  who  were  polled  answered.  Again  this\n",
            "introduced  a  sampling  bias,  by  potentially  ruling  out  people  who  didn’t  care\n",
            "much  about  politics,  people  who  didn’t  like  the  Literary  Digest,  and  other  key\n",
            "groups. This is a special type of sampling bias called nonresponse bias.\n",
            "\n",
            "Here  is  another  example:  say  you  want  to  build  a  system  to  recognize  funk  music\n",
            "videos. One way to build your training set is to search for “funk music” on YouTube\n",
            "and  use  the  resulting  videos.  But  this  assumes  that  YouTube’s  search  engine  returns\n",
            "a  set  of  videos  that  are  representative  of  all  the  funk  music  videos  on  YouTube.  In\n",
            "reality, the search results are likely to be biased toward popular artists (and if you live\n",
            "\n",
            "Main Challenges of Machine Learning \n",
            "\n",
            "| \n",
            "\n",
            "29\n",
            "\n",
            "\fin Brazil you will get a lot of “funk carioca” videos, which sound nothing like James\n",
            "Brown). On the other hand, how else can you get a large training set?\n",
            "\n",
            "Poor-Quality Data\n",
            "Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\n",
            "quality measurements), it will make it harder for the system to detect the underlying\n",
            "patterns, so your system is less likely to perform well. It is often well worth the effort\n",
            "to spend time cleaning up your training data. The truth is, most data scientists spend\n",
            "a significant part of their time doing just that. The following are a couple examples of\n",
            "when you’d want to clean up training data:\n",
            "\n",
            "•\n",
            "• If some instances are clearly outliers, it may help to simply discard them or try to\n",
            "\n",
            "fix the errors manually.\n",
            "\n",
            "•\n",
            "• If some instances are missing a few features (e.g., 5% of your customers did not\n",
            "specify  their  age),  you  must  decide  whether  you  want  to  ignore  this  attribute\n",
            "altogether, ignore these instances, fill in the missing values (e.g., with the median\n",
            "age), or train one model with the feature and one model without it.\n",
            "\n",
            "Irrelevant Features\n",
            "As  the  saying  goes:  garbage  in,  garbage  out.  Your  system  will  only  be  capable  of\n",
            "learning  if  the  training  data  contains  enough  relevant  features  and  not  too  many\n",
            "irrelevant ones. A critical part of the success of a machine learning project is coming\n",
            "up  with  a  good  set  of  features  to  train  on.  This  process,  called  feature  engineering,\n",
            "involves the following steps:\n",
            "\n",
            "•\n",
            "• Feature  selection  (selecting  the  most  useful  features  to  train  on  among  existing\n",
            "\n",
            "features)\n",
            "\n",
            "• Feature extraction (combining existing features to produce a more useful one—as\n",
            "•\n",
            "\n",
            "we saw earlier, dimensionality reduction algorithms can help)\n",
            "\n",
            "• Creating new features by gathering new data\n",
            "•\n",
            "\n",
            "Now  that  we  have  looked  at  many  examples  of  bad  data,  let’s  look  at  a  couple\n",
            "examples of bad algorithms.\n",
            "\n",
            "Overfitting the Training Data\n",
            "Say you are visiting a foreign country and the taxi driver rips you off. You might be\n",
            "tempted  to  say  that  all  taxi  drivers  in  that  country  are  thieves.  Overgeneralizing  is\n",
            "something that we humans do all too often, and unfortunately machines can fall into\n",
            "the  same  trap  if  we  are  not  careful.  In  machine  learning  this  is  called  overfitting:  it\n",
            "\n",
            "30 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fmeans that the model performs well on the training data, but it does not generalize\n",
            "well.\n",
            "\n",
            "Figure  1-23  shows  an  example  of  a  high-degree  polynomial  life  satisfaction  model\n",
            "that strongly overfits the training data. Even though it performs much better on the\n",
            "training data than the simple linear model, would you really trust its predictions?\n",
            "\n",
            "Figure 1-23. Overfitting the training data\n",
            "\n",
            "Complex  models  such  as  deep  neural  networks  can  detect  subtle  patterns  in  the\n",
            "data,  but  if  the  training  set  is  noisy,  or  if  it  is  too  small,  which  introduces  sampling\n",
            "noise,  then  the  model  is  likely  to  detect  patterns  in  the  noise  itself  (as  in  the  taxi\n",
            "driver  example).  Obviously  these  patterns  will  not  generalize  to  new  instances.  For\n",
            "example,  say  you  feed  your  life  satisfaction  model  many  more  attributes,  including\n",
            "uninformative ones such as the country’s name. In that case, a complex model may\n",
            "detect  patterns  like  the  fact  that  all  countries  in  the  training  data  with  a  w  in  their\n",
            "name have a life satisfaction greater than 7: New Zealand (7.3), Norway (7.6), Sweden\n",
            "(7.3),  and  Switzerland  (7.5).  How  confident  are  you  that  the  w-satisfaction  rule\n",
            "generalizes to Rwanda or Zimbabwe? Obviously this pattern occurred in the training\n",
            "data  by  pure  chance,  but  the  model  has  no  way  to  tell  whether  a  pattern  is  real  or\n",
            "simply the result of noise in the data.\n",
            "\n",
            "Overfitting  happens  when  the  model  is  too  complex  relative  to\n",
            "the  amount  and  noisiness  of  the  training  data.  Here  are  possible\n",
            "solutions:\n",
            "\n",
            "• Simplify  the  model  by  selecting  one  with  fewer  parameters\n",
            "•\n",
            "(e.g.,  a  linear  model  rather  than  a  high-degree  polynomial\n",
            "model),  by  reducing  the  number  of  attributes  in  the  training\n",
            "data, or by constraining the model.\n",
            "\n",
            "• Gather more training data.\n",
            "•\n",
            "\n",
            "• Reduce the noise in the training data (e.g., fix data errors and\n",
            "•\n",
            "\n",
            "remove outliers).\n",
            "\n",
            "Main Challenges of Machine Learning \n",
            "\n",
            "| \n",
            "\n",
            "31\n",
            "\n",
            "\fConstraining a model to make it simpler and reduce the risk of overfitting is called\n",
            "regularization. For example, the linear model we defined earlier has two parameters,\n",
            "θ0 and θ1. This gives the learning algorithm two degrees of freedom to adapt the model\n",
            "to the training data: it can tweak both the height (θ0) and the slope (θ1) of the line. If\n",
            "we forced θ1 = 0, the algorithm would have only one degree of freedom and would\n",
            "have  a  much  harder  time  fitting  the  data  properly:  all  it  could  do  is  move  the  line\n",
            "up or down to get as close as possible to the training instances, so it would end up\n",
            "around the mean. A very simple model indeed! If we allow the algorithm to modify\n",
            "θ1  but  we  force  it  to  keep  it  small,  then  the  learning  algorithm  will  effectively  have\n",
            "somewhere in between one and two degrees of freedom. It will produce a model that’s\n",
            "simpler than one with two degrees of freedom, but more complex than one with just\n",
            "one. You want to find the right balance between fitting the training data perfectly and\n",
            "keeping the model simple enough to ensure that it will generalize well.\n",
            "\n",
            "Figure  1-24  shows  three  models.  The  dotted  line  represents  the  original  model  that\n",
            "was trained on the countries represented as circles (without the countries represented\n",
            "as squares), the solid line is our second model trained with all countries (circles and\n",
            "squares), and the dashed line is a model trained with the same data as the first model\n",
            "but with a regularization constraint. You can see that regularization forced the model\n",
            "to  have  a  smaller  slope:  this  model  does  not  fit  the  training  data  (circles)  as  well  as\n",
            "the first model, but it actually generalizes better to new examples that it did not see\n",
            "during training (squares).\n",
            "\n",
            "Figure 1-24. Regularization reduces the risk of overfitting\n",
            "\n",
            "The amount of regularization to apply during learning can be controlled by a hyper‐\n",
            "parameter.  A  hyperparameter  is  a  parameter  of  a  learning  algorithm  (not  of  the\n",
            "model).  As  such,  it  is  not  affected  by  the  learning  algorithm  itself;  it  must  be  set\n",
            "prior to training and remains constant during training. If you set the regularization\n",
            "hyperparameter to a very large value, you will get an almost flat model (a slope close\n",
            "to zero); the learning algorithm will almost certainly not overfit the training data, but\n",
            "it will be less likely to find a good solution. Tuning hyperparameters is an important\n",
            "\n",
            "32 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fpart  of  building  a  machine  learning  system  (you  will  see  a  detailed  example  in  the\n",
            "next chapter).\n",
            "\n",
            "Underfitting the Training Data\n",
            "As  you  might  guess,  underfitting  is  the  opposite  of  overfitting:  it  occurs  when  your\n",
            "model  is  too  simple  to  learn  the  underlying  structure  of  the  data.  For  example,  a\n",
            "linear  model  of  life  satisfaction  is  prone  to  underfit;  reality  is  just  more  complex\n",
            "than  the  model,  so  its  predictions  are  bound  to  be  inaccurate,  even  on  the  training\n",
            "examples.\n",
            "\n",
            "Here are the main options for fixing this problem:\n",
            "\n",
            "•\n",
            "• Select a more powerful model, with more parameters.\n",
            "\n",
            "•\n",
            "• Feed better features to the learning algorithm (feature engineering).\n",
            "\n",
            "•\n",
            "• Reduce the constraints on the model (for example by reducing the regularization\n",
            "\n",
            "hyperparameter).\n",
            "\n",
            "Stepping Back\n",
            "By now you know a lot about machine learning. However, we went through so many\n",
            "concepts  that  you  may  be  feeling  a  little  lost,  so  let’s  step  back  and  look  at  the  big\n",
            "picture:\n",
            "\n",
            "•\n",
            "• Machine learning is about making machines get better at some task by learning\n",
            "\n",
            "from data, instead of having to explicitly code rules.\n",
            "\n",
            "•\n",
            "• There are many different types of ML systems: supervised or not, batch or online,\n",
            "\n",
            "instance-based or model-based.\n",
            "\n",
            "• In an ML project you gather data in a training set, and you feed the training set to\n",
            "•\n",
            "a learning algorithm. If the algorithm is model-based, it tunes some parameters\n",
            "to fit the model to the training set (i.e., to make good predictions on the training\n",
            "set  itself),  and  then  hopefully  it  will  be  able  to  make  good  predictions  on  new\n",
            "cases  as  well.  If  the  algorithm  is  instance-based,  it  just  learns  the  examples  by\n",
            "heart and generalizes to new instances by using a similarity measure to compare\n",
            "them to the learned instances.\n",
            "\n",
            "• The system will not perform well if your training set is too small, or if the data\n",
            "•\n",
            "is not representative, is noisy, or is polluted with irrelevant features (garbage in,\n",
            "garbage out). Lastly, your model needs to be neither too simple (in which case it\n",
            "will underfit) nor too complex (in which case it will overfit).\n",
            "\n",
            "There’s  just  one  last  important  topic  to  cover:  once  you  have  trained  a  model,  you\n",
            "don’t  want  to  just  “hope”  it  generalizes  to  new  cases.  You  want  to  evaluate  it  and\n",
            "fine-tune it if necessary. Let’s see how to do that.\n",
            "\n",
            "Main Challenges of Machine Learning \n",
            "\n",
            "| \n",
            "\n",
            "33\n",
            "\n",
            "\fTesting and Validating\n",
            "The  only  way  to  know  how  well  a  model  will  generalize  to  new  cases  is  to  actually\n",
            "try it out on new cases. One way to do that is to put your model in production and\n",
            "monitor  how  well  it  performs.  This  works  well,  but  if  your  model  is  horribly  bad,\n",
            "your users will complain—not the best idea.\n",
            "\n",
            "A  better  option  is  to  split  your  data  into  two  sets:  the  training  set  and  the  test  set.\n",
            "As  these  names  imply,  you  train  your  model  using  the  training  set,  and  you  test  it\n",
            "using  the  test  set.  The  error  rate  on  new  cases  is  called  the  generalization  error  (or\n",
            "out-of-sample error), and by evaluating your model on the test set, you get an estimate\n",
            "of  this  error.  This  value  tells  you  how  well  your  model  will  perform  on  instances  it\n",
            "has never seen before.\n",
            "\n",
            "If  the  training  error  is  low  (i.e.,  your  model  makes  few  mistakes  on  the  training\n",
            "set)  but  the  generalization  error  is  high,  it  means  that  your  model  is  overfitting  the\n",
            "training data.\n",
            "\n",
            "It is common to use 80% of the data for training and hold out 20%\n",
            "for  testing.  However,  this  depends  on  the  size  of  the  dataset:  if  it\n",
            "contains 10 million instances, then holding out 1% means your test\n",
            "set  will  contain  100,000  instances,  probably  more  than  enough  to\n",
            "get a good estimate of the generalization error.\n",
            "\n",
            "Hyperparameter Tuning and Model Selection\n",
            "Evaluating a model is simple enough: just use a test set. But suppose you are hesitat‐\n",
            "ing between two types of models (say, a linear model and a polynomial model): how\n",
            "can  you  decide  between  them?  One  option  is  to  train  both  and  compare  how  well\n",
            "they generalize using the test set.\n",
            "\n",
            "Now  suppose  that  the  linear  model  generalizes  better,  but  you  want  to  apply  some\n",
            "regularization to avoid overfitting. The question is, how do you choose the value of\n",
            "the regularization hyperparameter? One option is to train 100 different models using\n",
            "100 different values for this hyperparameter. Suppose you find the best hyperparame‐\n",
            "ter  value  that  produces  a  model  with  the  lowest  generalization  error—say,  just  5%\n",
            "error. You launch this model into production, but unfortunately it does not perform\n",
            "as well as expected and produces 15% errors. What just happened?\n",
            "\n",
            "The problem is that you measured the generalization error multiple times on the test\n",
            "set, and you adapted the model and hyperparameters to produce the best model for\n",
            "that particular set. This means the model is unlikely to perform as well on new data.\n",
            "\n",
            "A  common  solution  to  this  problem  is  called  holdout  validation  (Figure  1-25):  you\n",
            "simply  hold  out  part  of  the  training  set  to  evaluate  several  candidate  models  and\n",
            "\n",
            "34 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fselect  the  best  one.  The  new  held-out  set  is  called  the  validation  set  (or  the  devel‐\n",
            "opment  set,  or  dev  set).  More  specifically,  you  train  multiple  models  with  various\n",
            "hyperparameters  on  the  reduced  training  set  (i.e.,  the  full  training  set  minus  the\n",
            "validation  set),  and  you  select  the  model  that  performs  best  on  the  validation  set.\n",
            "After this holdout validation process, you train the best model on the full training set\n",
            "(including the validation set), and this gives you the final model. Lastly, you evaluate\n",
            "this final model on the test set to get an estimate of the generalization error.\n",
            "\n",
            "Figure 1-25. Model selection using holdout validation\n",
            "\n",
            "This  solution  usually  works  quite  well.  However,  if  the  validation  set  is  too  small,\n",
            "then the model evaluations will be imprecise: you may end up selecting a suboptimal\n",
            "model  by  mistake.  Conversely,  if  the  validation  set  is  too  large,  then  the  remaining\n",
            "training  set  will  be  much  smaller  than  the  full  training  set.  Why  is  this  bad?  Well,\n",
            "since the final model will be trained on the full training set, it is not ideal to compare\n",
            "candidate  models  trained  on  a  much  smaller  training  set.  It  would  be  like  selecting\n",
            "the  fastest  sprinter  to  participate  in  a  marathon.  One  way  to  solve  this  problem  is\n",
            "to  perform  repeated  cross-validation,  using  many  small  validation  sets.  Each  model\n",
            "is  evaluated  once  per  validation  set  after  it  is  trained  on  the  rest  of  the  data.  By\n",
            "averaging out all the evaluations of a model, you get a much more accurate measure\n",
            "of its performance. There is a drawback, however: the training time is multiplied by\n",
            "the number of validation sets.\n",
            "\n",
            "Data Mismatch\n",
            "In  some  cases,  it’s  easy  to  get  a  large  amount  of  data  for  training,  but  this  data\n",
            "probably won’t be perfectly representative of the data that will be used in production.\n",
            "For example, suppose you want to create a mobile app to take pictures of flowers and\n",
            "automatically  determine  their  species.  You  can  easily  download  millions  of  pictures\n",
            "\n",
            "Testing and Validating \n",
            "\n",
            "| \n",
            "\n",
            "35\n",
            "\n",
            "\fof flowers on the web, but they won’t be perfectly representative of the pictures that\n",
            "will actually be taken using the app on a mobile device. Perhaps you only have 1,000\n",
            "representative pictures (i.e., actually taken with the app).\n",
            "\n",
            "In this case, the most important rule to remember is that both the validation set and\n",
            "the  test  set  must  be  as  representative  as  possible  of  the  data  you  expect  to  use  in\n",
            "production,  so  they  should  be  composed  exclusively  of  representative  pictures:  you\n",
            "can  shuffle  them  and  put  half  in  the  validation  set  and  half  in  the  test  set  (making\n",
            "sure  that  no  duplicates  or  near-duplicates  end  up  in  both  sets).  After  training  your\n",
            "model on the web pictures, if you observe that the performance of the model on the\n",
            "validation set is disappointing, you will not know whether this is because your model\n",
            "has overfit the training set, or whether this is just due to the mismatch between the\n",
            "web pictures and the mobile app pictures.\n",
            "\n",
            "One  solution  is  to  hold  out  some  of  the  training  pictures  (from  the  web)  in  yet\n",
            "another set that Andrew Ng dubbed the train-dev set (Figure 1-26). After the model\n",
            "is  trained  (on  the  training  set,  not  on  the  train-dev  set),  you  can  evaluate  it  on  the\n",
            "train-dev set. If the model performs poorly, then it must have overfit the training set,\n",
            "so  you  should  try  to  simplify  or  regularize  the  model,  get  more  training  data,  and\n",
            "clean up the training data. But if it performs well on the train-dev set, then you can\n",
            "evaluate  the  model  on  the  dev  set.  If  it  performs  poorly,  then  the  problem  must  be\n",
            "coming from the data mismatch. You can try to tackle this problem by preprocessing\n",
            "the  web  images  to  make  them  look  more  like  the  pictures  that  will  be  taken  by  the\n",
            "mobile  app,  and  then  retraining  the  model.  Once  you  have  a  model  that  performs\n",
            "well on both the train-dev set and the dev set, you can evaluate it one last time on the\n",
            "test set to know how well it is likely to perform in production.\n",
            "\n",
            "Figure 1-26. When real data is scarce (right), you may use similar abundant data (left)\n",
            "for training and hold out some of it in a train-dev set to evaluate overfitting; the real\n",
            "data is then used to evaluate data mismatch (dev set) and to evaluate the final model’s\n",
            "performance (test set)\n",
            "\n",
            "36 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fNo Free Lunch Theorem\n",
            "A model is a simplified representation of the data. The simplifications are meant to\n",
            "discard the superfluous details that are unlikely to generalize to new instances. When\n",
            "you  select  a  particular  type  of  model,  you  are  implicitly  making  assumptions  about\n",
            "the data. For example, if you choose a linear model, you are implicitly assuming that\n",
            "the data is fundamentally linear and that the distance between the instances and the\n",
            "straight line is just noise, which can safely be ignored.\n",
            "\n",
            "In  a  famous  1996  paper,9  David  Wolpert  demonstrated  that  if  you  make  absolutely\n",
            "no assumption about the data, then there is no reason to prefer one model over any\n",
            "other.  This  is  called  the  No  Free  Lunch  (NFL)  theorem.  For  some  datasets  the  best\n",
            "model is a linear model, while for other datasets it is a neural network. There is no\n",
            "model  that  is  a  priori  guaranteed  to  work  better  (hence  the  name  of  the  theorem).\n",
            "The  only  way  to  know  for  sure  which  model  is  best  is  to  evaluate  them  all.  Since\n",
            "this  is  not  possible,  in  practice  you  make  some  reasonable  assumptions  about  the\n",
            "data  and  evaluate  only  a  few  reasonable  models.  For  example,  for  simple  tasks  you\n",
            "may  evaluate  linear  models  with  various  levels  of  regularization,  and  for  a  complex\n",
            "problem you may evaluate various neural networks.\n",
            "\n",
            "Exercises\n",
            "In  this  chapter  we  have  covered  some  of  the  most  important  concepts  in  machine\n",
            "learning. In the next chapters we will dive deeper and write more code, but before we\n",
            "do, make sure you can answer the following questions:\n",
            "\n",
            "1.\n",
            "1. How would you define machine learning?\n",
            "\n",
            "2.\n",
            "2. Can you name four types of applications where it shines?\n",
            "\n",
            "3.\n",
            "3. What is a labeled training set?\n",
            "\n",
            "4.\n",
            "4. What are the two most common supervised tasks?\n",
            "\n",
            "5.\n",
            "5. Can you name four common unsupervised tasks?\n",
            "\n",
            "6. What  type  of  algorithm  would  you  use  to  allow  a  robot  to  walk  in  various\n",
            "6.\n",
            "\n",
            "unknown terrains?\n",
            "\n",
            "7. What type of algorithm would you use to segment your customers into multiple\n",
            "7.\n",
            "\n",
            "groups?\n",
            "\n",
            "8.\n",
            "8. Would you frame the problem of spam detection as a supervised learning prob‐\n",
            "\n",
            "lem or an unsupervised learning problem?\n",
            "\n",
            "9 David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms”, Neural Computation 8, no.\n",
            "\n",
            "7 (1996): 1341–1390.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "37\n",
            "\n",
            "\f9.\n",
            "9. What is an online learning system?\n",
            "\n",
            "10.\n",
            "10. What is out-of-core learning?\n",
            "\n",
            "11.\n",
            "11. What type of algorithm relies on a similarity measure to make predictions?\n",
            "\n",
            "12.\n",
            "12. What is the difference between a model parameter and a model hyperparameter?\n",
            "\n",
            "13.\n",
            "13. What do model-based algorithms search for? What is the most common strategy\n",
            "\n",
            "they use to succeed? How do they make predictions?\n",
            "\n",
            "14.\n",
            "14. Can you name four of the main challenges in machine learning?\n",
            "\n",
            "15.\n",
            "15. If your model performs great on the training data but generalizes poorly to new\n",
            "\n",
            "instances, what is happening? Can you name three possible solutions?\n",
            "\n",
            "16.\n",
            "16. What is a test set, and why would you want to use it?\n",
            "\n",
            "17.\n",
            "17. What is the purpose of a validation set?\n",
            "\n",
            "18.\n",
            "18. What is the train-dev set, when do you need it, and how do you use it?\n",
            "\n",
            "19.\n",
            "19. What can go wrong if you tune hyperparameters using the test set?\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "38 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 1: The Machine Learning Landscape\n",
            "\n",
            "\fCHAPTER 2\n",
            "End-to-End Machine Learning Project\n",
            "\n",
            "In this chapter you will work through an example project end to end, pretending to\n",
            "be a recently hired data scientist at a real estate company. This example is fictitious;\n",
            "the  goal  is  to  illustrate  the  main  steps  of  a  machine  learning  project,  not  to  learn\n",
            "anything about the real estate business. Here are the main steps we will walk through:\n",
            "\n",
            "1.\n",
            "1. Look at the big picture.\n",
            "\n",
            "2.\n",
            "2. Get the data.\n",
            "\n",
            "3.\n",
            "3. Explore and visualize the data to gain insights.\n",
            "\n",
            "4.\n",
            "4. Prepare the data for machine learning algorithms.\n",
            "\n",
            "5.\n",
            "5. Select a model and train it.\n",
            "\n",
            "6.\n",
            "6. Fine-tune your model.\n",
            "\n",
            "7.\n",
            "7. Present your solution.\n",
            "\n",
            "8. Launch, monitor, and maintain your system.\n",
            "8.\n",
            "\n",
            "Working with Real Data\n",
            "When  you  are  learning  about  machine  learning,  it  is  best  to  experiment  with  real-\n",
            "world data, not artificial datasets. Fortunately, there are thousands of open datasets to\n",
            "choose from, ranging across all sorts of domains. Here are a few places you can look\n",
            "to get data:\n",
            "\n",
            "• Popular open data repositories:\n",
            "•\n",
            "\n",
            "— OpenML.org\n",
            "—\n",
            "\n",
            "—\n",
            "— Kaggle.com\n",
            "\n",
            "39\n",
            "\n",
            "\f—\n",
            "— PapersWithCode.com\n",
            "\n",
            "—\n",
            "— UC Irvine Machine Learning Repository\n",
            "\n",
            "—\n",
            "— Amazon’s AWS datasets\n",
            "\n",
            "—\n",
            "— TensorFlow datasets\n",
            "\n",
            "•\n",
            "• Meta portals (they list open data repositories):\n",
            "\n",
            "— DataPortals.org\n",
            "—\n",
            "\n",
            "—\n",
            "— OpenDataMonitor.eu\n",
            "\n",
            "•\n",
            "• Other pages listing many popular open data repositories:\n",
            "\n",
            "—\n",
            "— Wikipedia’s list of machine learning datasets\n",
            "\n",
            "—\n",
            "— Quora.com\n",
            "\n",
            "—\n",
            "— The datasets subreddit\n",
            "\n",
            "In  this  chapter  we’ll  use  the  California  Housing  Prices  dataset  from  the  StatLib\n",
            "repository1  (see  Figure  2-1).  This  dataset  is  based  on  data  from  the  1990  California\n",
            "census. It is not exactly recent (a nice house in the Bay Area was still affordable at the\n",
            "time), but it has many qualities for learning, so we will pretend it is recent data. For\n",
            "teaching purposes I’ve added a categorical attribute and removed a few features.\n",
            "\n",
            "Figure 2-1. California housing prices\n",
            "\n",
            "1 The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions”, Statistics\n",
            "\n",
            "& Probability Letters 33, no. 3 (1997): 291–297.\n",
            "\n",
            "40 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fLook at the Big Picture\n",
            "Welcome  to  the  Machine  Learning  Housing  Corporation!  Your  first  task  is  to  use\n",
            "California  census  data  to  build  a  model  of  housing  prices  in  the  state.  This  data\n",
            "includes metrics such as the population, median income, and median housing price\n",
            "for  each  block  group  in  California.  Block  groups  are  the  smallest  geographical  unit\n",
            "for which the US Census Bureau publishes sample data (a block group typically has a\n",
            "population of 600 to 3,000 people). I will call them “districts” for short.\n",
            "\n",
            "Your  model  should  learn  from  this  data  and  be  able  to  predict  the  median  housing\n",
            "price in any district, given all the other metrics.\n",
            "\n",
            "Since  you  are  a  well-organized  data  scientist,  the  first  thing  you\n",
            "should do is pull out your machine learning project checklist. You\n",
            "can  start  with  the  one  in  Appendix  A;  it  should  work  reasonably\n",
            "well  for  most  machine  learning  projects,  but  make  sure  to  adapt\n",
            "it  to  your  needs.  In  this  chapter  we  will  go  through  many  check‐\n",
            "list  items,  but  we  will  also  skip  a  few,  either  because  they  are\n",
            "self-explanatory or because they will be discussed in later chapters.\n",
            "\n",
            "Frame the Problem\n",
            "The first question to ask your boss is what exactly the business objective is. Building a\n",
            "model is probably not the end goal. How does the company expect to use and benefit\n",
            "from  this  model?  Knowing  the  objective  is  important  because  it  will  determine\n",
            "how  you  frame  the  problem,  which  algorithms  you  will  select,  which  performance\n",
            "measure  you  will  use  to  evaluate  your  model,  and  how  much  effort  you  will  spend\n",
            "tweaking it.\n",
            "\n",
            "Your  boss  answers  that  your  model’s  output  (a  prediction  of  a  district’s  median\n",
            "housing price) will be fed to another machine learning system (see Figure 2-2), along\n",
            "with many other signals.2 This downstream system will determine whether it is worth\n",
            "investing in a given area. Getting this right is critical, as it directly affects revenue.\n",
            "\n",
            "The  next  question  to  ask  your  boss  is  what  the  current  solution  looks  like  (if\n",
            "any).  The  current  situation  will  often  give  you  a  reference  for  performance,  as\n",
            "well  as  insights  on  how  to  solve  the  problem.  Your  boss  answers  that  the  district\n",
            "housing prices are currently estimated manually by experts: a team gathers up-to-date\n",
            "information  about  a  district,  and  when  they  cannot  get  the  median  housing  price,\n",
            "they estimate it using complex rules.\n",
            "\n",
            "2 A piece of information fed to a machine learning system is often called a signal, in reference to Claude\n",
            "\n",
            "Shannon’s information theory, which he developed at Bell Labs to improve telecommunications. His theory:\n",
            "you want a high signal-to-noise ratio.\n",
            "\n",
            "Look at the Big Picture \n",
            "\n",
            "| \n",
            "\n",
            "41\n",
            "\n",
            "\fFigure 2-2. A machine learning pipeline for real estate investments\n",
            "\n",
            "This is costly and time-consuming, and their estimates are not great; in cases where\n",
            "they manage to find out the actual median housing price, they often realize that their\n",
            "estimates were off by more than 30%. This is why the company thinks that it would\n",
            "be  useful  to  train  a  model  to  predict  a  district’s  median  housing  price,  given  other\n",
            "data about that district. The census data looks like a great dataset to exploit for this\n",
            "purpose, since it includes the median housing prices of thousands of districts, as well\n",
            "as other data.\n",
            "\n",
            "Pipelines\n",
            "A sequence of data processing components is called a data pipeline. Pipelines are very\n",
            "common in machine learning systems, since there is a lot of data to manipulate and\n",
            "many data transformations to apply.\n",
            "\n",
            "Components typically run asynchronously. Each component pulls in a large amount\n",
            "of data, processes it, and spits out the result in another data store. Then, some time\n",
            "later,  the  next  component  in  the  pipeline  pulls  in  this  data  and  spits  out  its  own\n",
            "output.  Each  component  is  fairly  self-contained:  the  interface  between  components\n",
            "is  simply  the  data  store.  This  makes  the  system  simple  to  grasp  (with  the  help  of  a\n",
            "data flow graph), and different teams can focus on different components. Moreover,\n",
            "if  a  component  breaks  down,  the  downstream  components  can  often  continue  to\n",
            "run  normally  (at  least  for  a  while)  by  just  using  the  last  output  from  the  broken\n",
            "component. This makes the architecture quite robust.\n",
            "\n",
            "On  the  other  hand,  a  broken  component  can  go  unnoticed  for  some  time  if  proper\n",
            "monitoring  is  not  implemented.  The  data  gets  stale  and  the  overall  system’s  perfor‐\n",
            "mance drops.\n",
            "\n",
            "42 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fWith  all  this  information,  you  are  now  ready  to  start  designing  your  system.  First,\n",
            "determine what kind of training supervision the model will need: is it a supervised,\n",
            "unsupervised, semi-supervised, self-supervised, or reinforcement learning task? And\n",
            "is it a classification task, a regression task, or something else? Should you use batch\n",
            "learning or online learning techniques? Before you read on, pause and try to answer\n",
            "these questions for yourself.\n",
            "\n",
            "Have  you  found  the  answers?  Let’s  see.  This  is  clearly  a  typical  supervised  learning\n",
            "task,  since  the  model  can  be  trained  with  labeled  examples  (each  instance  comes\n",
            "with  the  expected  output,  i.e.,  the  district’s  median  housing  price).  It  is  a  typical\n",
            "regression  task,  since  the  model  will  be  asked  to  predict  a  value.  More  specifically,\n",
            "this  is  a  multiple  regression  problem,  since  the  system  will  use  multiple  features  to\n",
            "make  a  prediction  (the  district’s  population,  the  median  income,  etc.).  It  is  also\n",
            "a univariate regression problem, since we are only trying to predict a single value for\n",
            "each  district.  If  we  were  trying  to  predict  multiple  values  per  district,  it  would  be  a\n",
            "multivariate regression problem. Finally, there is no continuous flow of data coming\n",
            "into the system, there is no particular need to adjust to changing data rapidly, and the\n",
            "data is small enough to fit in memory, so plain batch learning should do just fine.\n",
            "\n",
            "If  the  data  were  huge,  you  could  either  split  your  batch  learning\n",
            "work across multiple servers (using the MapReduce technique) or\n",
            "use an online learning technique.\n",
            "\n",
            "Select a Performance Measure\n",
            "Your  next  step  is  to  select  a  performance  measure.  A  typical  performance  measure\n",
            "for regression problems is the root mean square error (RMSE). It gives an idea of how\n",
            "much error the system typically makes in its predictions, with a higher weight given\n",
            "to large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.\n",
            "\n",
            "Equation 2-1. Root mean square error (RMSE)\n",
            "\n",
            "RMSE X, ℎ =\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "ℎ x i − y i 2\n",
            "\n",
            "Look at the Big Picture \n",
            "\n",
            "| \n",
            "\n",
            "43\n",
            "\n",
            "\fNotations\n",
            "This equation introduces several very common machine learning notations that I will\n",
            "use throughout this book:\n",
            "\n",
            "•\n",
            "• m is the number of instances in the dataset you are measuring the RMSE on.\n",
            "\n",
            "—\n",
            "— For  example,  if  you  are  evaluating  the  RMSE  on  a  validation  set  of  2,000\n",
            "\n",
            "districts, then m = 2,000.\n",
            "\n",
            "• x(i) is a vector of all the feature values (excluding the label) of the ith instance in\n",
            "•\n",
            "\n",
            "the dataset, and y(i) is its label (the desired output value for that instance).\n",
            "\n",
            "—\n",
            "— For example, if the first district in the dataset is located at longitude –118.29°,\n",
            "latitude 33.91°, and it has 1,416 inhabitants with a median income of $38,372,\n",
            "and  the  median  house  value  is  $156,400  (ignoring  other  features  for  now),\n",
            "then:\n",
            "\n",
            "x 1 =\n",
            "\n",
            "−118.29\n",
            "33.91\n",
            "1,416\n",
            "38,372\n",
            "\n",
            "and:\n",
            "\n",
            "y 1 = 156,400\n",
            "\n",
            "• X is a matrix containing all the feature values (excluding labels) of all instances\n",
            "•\n",
            "in  the  dataset.  There  is  one  row  per  instance,  and  the  ith  row  is  equal  to  the\n",
            "transpose of x(i), noted (x(i))⊺.3\n",
            "\n",
            "—\n",
            "— For example, if the first district is as just described, then the matrix X looks\n",
            "\n",
            "like this:\n",
            "\n",
            "X =\n",
            "\n",
            "x 1 ⊺\n",
            "x 2 ⊺\n",
            "⋮\n",
            "x 1999 ⊺\n",
            "x 2000 ⊺\n",
            "\n",
            "=\n",
            "\n",
            "−118.29 33.91 1,416 38,372\n",
            "\n",
            "⋮\n",
            "\n",
            "⋮\n",
            "\n",
            "⋮\n",
            "\n",
            "⋮\n",
            "\n",
            "3 Recall that the transpose operator flips a column vector into a row vector (and vice versa).\n",
            "\n",
            "44 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\f• h is your system’s prediction function, also called a hypothesis. When your system\n",
            "•\n",
            "is given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = h(x(i))\n",
            "for that instance (ŷ is pronounced “y-hat”).\n",
            "\n",
            "—\n",
            "— For example, if your system predicts that the median housing price in the first\n",
            "district is $158,400, then ŷ(1) = h(x(1)) = 158,400. The prediction error for this\n",
            "district is ŷ(1) – y(1) = 2,000.\n",
            "\n",
            "• RMSE(X,h)  is  the  cost  function  measured  on  the  set  of  examples  using  your\n",
            "•\n",
            "\n",
            "hypothesis h.\n",
            "\n",
            "We use lowercase italic font for scalar values (such as m or y(i)) and function names\n",
            "(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\n",
            "matrices (such as X).\n",
            "\n",
            "Although  the  RMSE  is  generally  the  preferred  performance  measure  for  regression\n",
            "tasks, in some contexts you may prefer to use another function. For example, if there\n",
            "are  many  outlier  districts.  In  that  case,  you  may  consider  using  the  mean  absolute\n",
            "error (MAE, also called the average absolute deviation), shown in Equation 2-2:\n",
            "\n",
            "Equation 2-2. Mean absolute error (MAE)\n",
            "\n",
            "MAE X, ℎ =\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "ℎ x i − y i\n",
            "\n",
            "Both the RMSE and the MAE are ways to measure the distance between two vectors:\n",
            "the vector of predictions and the vector of target values. Various distance measures,\n",
            "or norms, are possible:\n",
            "\n",
            "•\n",
            "• Computing  the  root  of  a  sum  of  squares  (RMSE)  corresponds  to  the  Euclidean\n",
            "norm: this is the notion of distance we are all familiar with. It is also called the ℓ2\n",
            "norm, noted ∥ · ∥2 (or just ∥ · ∥).\n",
            "\n",
            "• Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1.\n",
            "•\n",
            "This  is  sometimes  called  the  Manhattan  norm  because  it  measures  the  distance\n",
            "between two points in a city if you can only travel along orthogonal city blocks.\n",
            "\n",
            "• More  generally,  the  ℓk  norm  of  a  vector  v  containing  n  elements  is  defined  as\n",
            "•\n",
            "∥v∥k = (|v1|k + |v2|k + ... + |vn|k)1/k. ℓ0 gives the number of nonzero elements in the\n",
            "vector, and ℓ∞ gives the maximum absolute value in the vector.\n",
            "\n",
            "The  higher  the  norm  index,  the  more  it  focuses  on  large  values  and  neglects  small\n",
            "ones.  This  is  why  the  RMSE  is  more  sensitive  to  outliers  than  the  MAE.  But  when\n",
            "outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very\n",
            "well and is generally preferred.\n",
            "\n",
            "Look at the Big Picture \n",
            "\n",
            "| \n",
            "\n",
            "45\n",
            "\n",
            "\fCheck the Assumptions\n",
            "Lastly, it is good practice to list and verify the assumptions that have been made so\n",
            "far (by you or others); this can help you catch serious issues early on. For example,\n",
            "the  district  prices  that  your  system  outputs  are  going  to  be  fed  into  a  downstream\n",
            "machine  learning  system,  and  you  assume  that  these  prices  are  going  to  be  used  as\n",
            "such.  But  what  if  the  downstream  system  converts  the  prices  into  categories  (e.g.,\n",
            "“cheap”, “medium”, or “expensive”) and then uses those categories instead of the prices\n",
            "themselves? In this case, getting the price perfectly right is not important at all; your\n",
            "system just needs to get the category right. If that’s so, then the problem should have\n",
            "been framed as a classification task, not a regression task. You don’t want to find this\n",
            "out after working on a regression system for months.\n",
            "\n",
            "Fortunately, after talking with the team in charge of the downstream system, you are\n",
            "confident that they do indeed need the actual prices, not just categories. Great! You’re\n",
            "all set, the lights are green, and you can start coding now!\n",
            "\n",
            "Get the Data\n",
            "It’s  time  to  get  your  hands  dirty.  Don’t  hesitate  to  pick  up  your  laptop  and  walk\n",
            "through  the  code  examples.  As  I  mentioned  in  the  preface,  all  the  code  examples\n",
            "in  this  book  are  open  source  and  available  online  as  Jupyter  notebooks,  which  are\n",
            "interactive documents containing text, images, and executable code snippets (Python\n",
            "in our case). In this book I will assume you are running these notebooks on Google\n",
            "Colab, a free service that lets you run any Jupyter notebook directly online, without\n",
            "having to install anything on your machine. If you want to use another online plat‐\n",
            "form (e.g., Kaggle) or if you want to install everything locally on your own machine,\n",
            "please see the instructions on the book’s GitHub page.\n",
            "\n",
            "Running the Code Examples Using Google Colab\n",
            "First,  open  a  web  browser  and  visit  https://homl.info/colab3:  this  will  lead  you  to\n",
            "Google  Colab,  and  it  will  display  the  list  of  Jupyter  notebooks  for  this  book  (see\n",
            "Figure 2-3). You will find one notebook per chapter, plus a few extra notebooks and\n",
            "tutorials for NumPy, Matplotlib, Pandas, linear algebra, and differential calculus. For\n",
            "example,  if  you  click  02_end_to_end_machine_learning_project.ipynb,  the  notebook\n",
            "from Chapter 2 will open up in Google Colab (see Figure 2-4).\n",
            "\n",
            "A Jupyter notebook is composed of a list of cells. Each cell contains either executable\n",
            "code  or  text.  Try  double-clicking  the  first  text  cell  (which  contains  the  sentence\n",
            "“Welcome to Machine Learning Housing Corp.!”). This will open the cell for editing.\n",
            "Notice that Jupyter notebooks use Markdown syntax for formatting (e.g., **bold**,\n",
            "*italics*,  #  Title,  [url](link  text),  and  so  on).  Try  modifying  this  text,  then\n",
            "press Shift-Enter to see the result.\n",
            "\n",
            "46 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fFigure 2-3. List of notebooks in Google Colab\n",
            "\n",
            "Figure 2-4. Your notebook in Google Colab\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "47\n",
            "\n",
            "\fNext,  create  a  new  code  cell  by  selecting  Insert  →  “Code  cell”  from  the  menu.\n",
            "Alternatively,  you  can  click  the  +  Code  button  in  the  toolbar,  or  hover  your  mouse\n",
            "over the bottom of a cell until you see + Code and + Text appear, then click + Code.\n",
            "In the new code cell, type some Python code, such as print(\"Hello World\"), then\n",
            "press Shift-Enter to run this code (or click the ▷ button on the left side of the cell).\n",
            "\n",
            "If you’re not logged in to your Google account, you’ll be asked to log in now (if you\n",
            "don’t already have a Google account, you’ll need to create one). Once you are logged\n",
            "in,  when  you  try  to  run  the  code  you’ll  see  a  security  warning  telling  you  that  this\n",
            "notebook was not authored by Google. A malicious person could create a notebook\n",
            "that tries to trick you into entering your Google credentials so they can access your\n",
            "personal  data,  so  before  you  run  a  notebook,  always  make  sure  you  trust  its  author\n",
            "(or double-check what each code cell will do before running it). Assuming you trust\n",
            "me (or you plan to check every code cell), you can now click “Run anyway”.\n",
            "\n",
            "Colab will then allocate a new runtime for you: this is a free virtual machine located\n",
            "on  Google’s  servers  that  contains  a  bunch  of  tools  and  Python  libraries,  including\n",
            "everything  you’ll  need  for  most  chapters  (in  some  chapters,  you’ll  need  to  run  a\n",
            "command  to  install  additional  libraries).  This  will  take  a  few  seconds.  Next,  Colab\n",
            "will automatically connect to this runtime and use it to execute your new code cell.\n",
            "Importantly, the code runs on the runtime, not on your machine. The code’s output\n",
            "will be displayed under the cell. Congrats, you’ve run some Python code on Colab!\n",
            "\n",
            "To  insert  a  new  code  cell,  you  can  also  type  Ctrl-M  (or  Cmd-M\n",
            "on macOS) followed by A (to insert above the active cell) or B (to\n",
            "insert  below).  There  are  many  other  keyboard  shortcuts  available:\n",
            "you  can  view  and  edit  them  by  typing  Ctrl-M  (or  Cmd-M)  then\n",
            "H. If you choose to run the notebooks on Kaggle or on your own\n",
            "machine  using  JupyterLab  or  an  IDE  such  as  Visual  Studio  Code\n",
            "with the Jupyter extension, you will see some minor differences—\n",
            "runtimes are called kernels, the user interface and keyboard short‐\n",
            "cuts  are  slightly  different,  etc.—but  switching  from  one  Jupyter\n",
            "environment to another is not too hard.\n",
            "\n",
            "Saving Your Code Changes and Your Data\n",
            "You  can  make  changes  to  a  Colab  notebook,  and  they  will  persist  for  as  long  as\n",
            "you  keep  your  browser  tab  open.  But  once  you  close  it,  the  changes  will  be  lost.\n",
            "To  avoid  this,  make  sure  you  save  a  copy  of  the  notebook  to  your  Google  Drive  by\n",
            "selecting File → “Save a copy in Drive”. Alternatively, you can download the notebook\n",
            "to your computer by selecting File → Download → “Download .ipynb”. Then you can\n",
            "later visit https://colab.research.google.com and open the notebook again (either from\n",
            "Google Drive or by uploading it from your computer).\n",
            "\n",
            "48 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fGoogle  Colab  is  meant  only  for  interactive  use:  you  can  play\n",
            "around  in  the  notebooks  and  tweak  the  code  as  you  like,  but  you\n",
            "cannot let the notebooks run unattended for a long period of time,\n",
            "or else the runtime will be shut down and all of its data will be lost.\n",
            "\n",
            "If  the  notebook  generates  data  that  you  care  about,  make  sure  you  download  this\n",
            "data  before  the  runtime  shuts  down.  To  do  this,  click  the  Files  icon  (see  step  1  in\n",
            "Figure 2-5), find the file you want to download, click the vertical dots next to it (step\n",
            "2), and click Download (step 3). Alternatively, you can mount your Google Drive on\n",
            "the runtime, allowing the notebook to read and write files directly to Google Drive as\n",
            "if it were a local directory. For this, click the Files icon (step 1), then click the Google\n",
            "Drive icon (circled in Figure 2-5) and follow the on-screen instructions.\n",
            "\n",
            "Figure 2-5. Downloading a file from a Google Colab runtime (steps 1 to 3), or mounting\n",
            "your Google Drive (circled icon)\n",
            "\n",
            "By  default,  your  Google  Drive  will  be  mounted  at  /content/drive/MyDrive.\n",
            "If  you  want  to  back  up  a  data  file,  simply  copy  it  to  this  directory  by\n",
            "running  !cp  /content/my_great_model  /content/drive/MyDrive.  Any  command\n",
            "starting  with  a  bang  (!)  is  treated  as  a  shell  command,  not  as  Python  code:  cp  is\n",
            "the  Linux  shell  command  to  copy  a  file  from  one  path  to  another.  Note  that  Colab\n",
            "runtimes run on Linux (specifically, Ubuntu).\n",
            "\n",
            "The Power and Danger of Interactivity\n",
            "Jupyter notebooks are interactive, and that’s a great thing: you can run each cell one\n",
            "by one, stop at any point, insert a cell, play with the code, go back and run the same\n",
            "cell again, etc., and I highly encourage you to do so. If you just run the cells one by\n",
            "one  without  ever  playing  around  with  them,  you  won’t  learn  as  fast.  However,  this\n",
            "flexibility comes at a price: it’s very easy to run cells in the wrong order, or to forget to\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "49\n",
            "\n",
            "\frun a cell. If this happens, the subsequent code cells are likely to fail. For example, the\n",
            "very first code cell in each notebook contains setup code (such as imports), so make\n",
            "sure you run it first, or else nothing will work.\n",
            "\n",
            "If  you  ever  run  into  a  weird  error,  try  restarting  the  runtime  (by\n",
            "selecting  Runtime  →  “Restart  runtime”  from  the  menu)  and  then\n",
            "run  all  the  cells  again  from  the  beginning  of  the  notebook.  This\n",
            "often solves the problem. If not, it’s likely that one of the changes\n",
            "you made broke the notebook: just revert to the original notebook\n",
            "and try again. If it still fails, please file an issue on GitHub.\n",
            "\n",
            "Book Code Versus Notebook Code\n",
            "You may sometimes notice some little differences between the code in this book and\n",
            "the code in the notebooks. This may happen for several reasons:\n",
            "\n",
            "•\n",
            "• A library may have changed slightly by the time you read these lines, or perhaps\n",
            "despite my best efforts I made an error in the book. Sadly, I cannot magically fix\n",
            "the code in your copy of this book (unless you are reading an electronic copy and\n",
            "you can download the latest version), but I can fix the notebooks. So, if you run\n",
            "into an error after copying code from this book, please look for the fixed code in\n",
            "the notebooks: I will strive to keep them error-free and up-to-date with the latest\n",
            "library versions.\n",
            "\n",
            "•\n",
            "• The  notebooks  contain  some  extra  code  to  beautify  the  figures  (adding  labels,\n",
            "setting font sizes, etc.) and to save them in high resolution for this book. You can\n",
            "safely ignore this extra code if you want.\n",
            "\n",
            "I  optimized  the  code  for  readability  and  simplicity:  I  made  it  as  linear  and  flat  as\n",
            "possible,  defining  very  few  functions  or  classes.  The  goal  is  to  ensure  that  the  code\n",
            "you are running is generally right in front of you, and not nested within several layers\n",
            "of abstractions that you have to search through. This also makes it easier for you to\n",
            "play with the code. For simplicity, there’s limited error handling, and I placed some\n",
            "of the least common imports right where they are needed (instead of placing them at\n",
            "the  top  of  the  file,  as  is  recommended  by  the  PEP  8  Python  style  guide).  That  said,\n",
            "your  production  code  will  not  be  very  different:  just  a  bit  more  modular,  and  with\n",
            "additional tests and error handling.\n",
            "\n",
            "OK! Once you’re comfortable with Colab, you’re ready to download the data.\n",
            "\n",
            "Download the Data\n",
            "In  typical  environments  your  data  would  be  available  in  a  relational  database  or\n",
            "some  other  common  data  store,  and  spread  across  multiple  tables/documents/files.\n",
            "\n",
            "50 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fTo access it, you would first need to get your credentials and access authorizations4\n",
            "and  familiarize  yourself  with  the  data  schema.  In  this  project,  however,  things  are\n",
            "much  simpler:  you  will  just  download  a  single  compressed  file,  housing.tgz,  which\n",
            "contains a comma-separated values (CSV) file called housing.csv with all the data.\n",
            "\n",
            "Rather than manually downloading and decompressing the data, it’s usually prefera‐\n",
            "ble  to  write  a  function  that  does  it  for  you.  This  is  useful  in  particular  if  the  data\n",
            "changes  regularly:  you  can  write  a  small  script  that  uses  the  function  to  fetch  the\n",
            "latest  data  (or  you  can  set  up  a  scheduled  job  to  do  that  automatically  at  regular\n",
            "intervals).  Automating  the  process  of  fetching  the  data  is  also  useful  if  you  need  to\n",
            "install the dataset on multiple machines.\n",
            "\n",
            "Here is the function to fetch and load the data:\n",
            "\n",
            "from pathlib import Path\n",
            "import pandas as pd\n",
            "import tarfile\n",
            "import urllib.request\n",
            "\n",
            "def load_housing_data():\n",
            "    tarball_path = Path(\"datasets/housing.tgz\")\n",
            "    if not tarball_path.is_file():\n",
            "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
            "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
            "        urllib.request.urlretrieve(url, tarball_path)\n",
            "        with tarfile.open(tarball_path) as housing_tarball:\n",
            "            housing_tarball.extractall(path=\"datasets\")\n",
            "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
            "\n",
            "housing = load_housing_data()\n",
            "\n",
            "When  load_housing_data()  is  called,  it  looks  for  the  datasets/housing.tgz  file.  If  it\n",
            "does not find it, it creates the datasets directory inside the current directory (which\n",
            "is /content by default, in Colab), downloads the housing.tgz file from the ageron/data\n",
            "GitHub repository, and extracts its content into the datasets directory; this creates the\n",
            "datasets/housing directory with the housing.csv file inside it. Lastly, the function loads\n",
            "this CSV file into a Pandas DataFrame object containing all the data, and returns it.\n",
            "\n",
            "Take a Quick Look at the Data Structure\n",
            "You  start  by  looking  at  the  top  five  rows  of  data  using  the  DataFrame’s  head()\n",
            "method (see Figure 2-6).\n",
            "\n",
            "4 You might also need to check legal constraints, such as private fields that should never be copied to unsafe\n",
            "\n",
            "data stores.\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "51\n",
            "\n",
            "\fFigure 2-6. Top five rows in the dataset\n",
            "\n",
            "Each  row  represents  one  district.  There  are  10  attributes  (they  are  not  all\n",
            "shown in the screenshot): longitude, latitude, housing_median_age, total_rooms,\n",
            "total_bedrooms,  population,  households,  median_income,  median_house_value,\n",
            "and ocean_proximity.\n",
            "\n",
            "The info() method is useful to get a quick description of the data, in particular the\n",
            "total number of rows, each attribute’s type, and the number of non-null values:\n",
            "\n",
            ">>> housing.info()\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype\n",
            "---  ------              --------------  -----\n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object\n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n",
            "\n",
            "In  this  book,  when  a  code  example  contains  a  mix  of  code  and\n",
            "outputs,  as  is  the  case  here,  it  is  formatted  like  in  the  Python\n",
            "interpreter,  for  better  readability:  the  code  lines  are  prefixed  with\n",
            ">>> (or ... for indented blocks), and the outputs have no prefix.\n",
            "\n",
            "There  are  20,640  instances  in  the  dataset,  which  means  that  it  is  fairly  small  by\n",
            "machine  learning  standards,  but  it’s  perfect  to  get  started.  You  notice  that  the\n",
            "\n",
            "52 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\ftotal_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts\n",
            "are missing this feature. You will need to take care of this later.\n",
            "\n",
            "All  attributes  are  numerical,  except  for  ocean_proximity.  Its  type  is  object,  so  it\n",
            "could  hold  any  kind  of  Python  object.  But  since  you  loaded  this  data  from  a  CSV\n",
            "file,  you  know  that  it  must  be  a  text  attribute.  When  you  looked  at  the  top  five\n",
            "rows,  you  probably  noticed  that  the  values  in  the  ocean_proximity  column  were\n",
            "repetitive,  which  means  that  it  is  probably  a  categorical  attribute.  You  can  find  out\n",
            "what  categories  exist  and  how  many  districts  belong  to  each  category  by  using  the\n",
            "value_counts() method:\n",
            "\n",
            ">>> housing[\"ocean_proximity\"].value_counts()\n",
            "<1H OCEAN     9136\n",
            "INLAND        6551\n",
            "NEAR OCEAN    2658\n",
            "NEAR BAY      2290\n",
            "ISLAND           5\n",
            "Name: ocean_proximity, dtype: int64\n",
            "\n",
            "Let’s  look  at  the  other  fields.  The  describe()  method  shows  a  summary  of  the\n",
            "numerical attributes (Figure 2-7).\n",
            "\n",
            "Figure 2-7. Summary of each numerical attribute\n",
            "\n",
            "The  count,  mean,  min,  and  max  rows  are  self-explanatory.  Note  that  the  null  values\n",
            "are  ignored  (so,  for  example,  the  count  of  total_bedrooms  is  20,433,  not  20,640).\n",
            "The std row shows the standard deviation, which measures how dispersed the values\n",
            "are.5  The  25%,  50%,  and  75%  rows  show  the  corresponding  percentiles:  a  percentile\n",
            "\n",
            "5 The standard deviation is generally denoted σ (the Greek letter sigma), and it is the square root of the\n",
            "\n",
            "variance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped\n",
            "normal distribution (also called a Gaussian distribution), which is very common, the “68-95-99.7” rule applies:\n",
            "about 68% of the values fall within 1σ of the mean, 95% within 2σ, and 99.7% within 3σ.\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "53\n",
            "\n",
            "\findicates  the  value  below  which  a  given  percentage  of  observations  in  a  group  of\n",
            "observations fall. For example, 25% of the districts have a housing_median_age lower\n",
            "than  18,  while  50%  are  lower  than  29  and  75%  are  lower  than  37.  These  are  often\n",
            "called the 25th percentile (or first quartile), the median, and the 75th percentile (or\n",
            "third quartile).\n",
            "\n",
            "Another quick way to get a feel of the type of data you are dealing with is to plot a\n",
            "histogram for each numerical attribute. A histogram shows the number of instances\n",
            "(on the vertical axis) that have a given value range (on the horizontal axis). You can\n",
            "either  plot  this  one  attribute  at  a  time,  or  you  can  call  the  hist()  method  on  the\n",
            "whole dataset (as shown in the following code example), and it will plot a histogram\n",
            "for each numerical attribute (see Figure 2-8):\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "housing.hist(bins=50, figsize=(12, 8))\n",
            "plt.show()\n",
            "\n",
            "Figure 2-8. A histogram for each numerical attribute\n",
            "\n",
            "54 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fLooking at these histograms, you notice a few things:\n",
            "\n",
            "• First, the median income attribute does not look like it is expressed in US dollars\n",
            "•\n",
            "(USD).  After  checking  with  the  team  that  collected  the  data,  you  are  told  that\n",
            "the data has been scaled and capped at 15 (actually, 15.0001) for higher median\n",
            "incomes,  and  at  0.5  (actually,  0.4999)  for  lower  median  incomes.  The  numbers\n",
            "represent  roughly  tens  of  thousands  of  dollars  (e.g.,  3  actually  means  about\n",
            "$30,000). Working with preprocessed attributes is common in machine learning,\n",
            "and  it  is  not  necessarily  a  problem,  but  you  should  try  to  understand  how  the\n",
            "data was computed.\n",
            "\n",
            "• The  housing  median  age  and  the  median  house  value  were  also  capped.  The\n",
            "•\n",
            "latter may be a serious problem since it is your target attribute (your labels). Your\n",
            "machine  learning  algorithms  may  learn  that  prices  never  go  beyond  that  limit.\n",
            "You  need  to  check  with  your  client  team  (the  team  that  will  use  your  system’s\n",
            "output) to see if this is a problem or not. If they tell you that they need precise\n",
            "predictions even beyond $500,000, then you have two options:\n",
            "\n",
            "—\n",
            "— Collect proper labels for the districts whose labels were capped.\n",
            "\n",
            "— Remove  those  districts  from  the  training  set  (and  also  from  the  test  set,\n",
            "—\n",
            "since your system should not be evaluated poorly if it predicts values beyond\n",
            "$500,000).\n",
            "\n",
            "•\n",
            "• These  attributes  have  very  different  scales.  We  will  discuss  this  later  in  this\n",
            "\n",
            "chapter, when we explore feature scaling.\n",
            "\n",
            "•\n",
            "• Finally,  many  histograms  are  skewed  right:  they  extend  much  farther  to  the\n",
            "right  of  the  median  than  to  the  left.  This  may  make  it  a  bit  harder  for  some\n",
            "machine  learning  algorithms  to  detect  patterns.  Later,  you’ll  try  transforming\n",
            "these attributes to have more symmetrical and bell-shaped distributions.\n",
            "\n",
            "You should now have a better understanding of the kind of data you’re dealing with.\n",
            "\n",
            "Wait! Before you look at the data any further, you need to create a\n",
            "test set, put it aside, and never look at it.\n",
            "\n",
            "Create a Test Set\n",
            "It  may  seem  strange  to  voluntarily  set  aside  part  of  the  data  at  this  stage.  After  all,\n",
            "you have only taken a quick glance at the data, and surely you should learn a whole\n",
            "lot  more  about  it  before  you  decide  what  algorithms  to  use,  right?  This  is  true,\n",
            "but  your  brain  is  an  amazing  pattern  detection  system,  which  also  means  that  it  is\n",
            "highly  prone  to  overfitting:  if  you  look  at  the  test  set,  you  may  stumble  upon  some\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "55\n",
            "\n",
            "\fseemingly interesting pattern in the test data that leads you to select a particular kind\n",
            "of machine learning model. When you estimate the generalization error using the test\n",
            "set,  your  estimate  will  be  too  optimistic,  and  you  will  launch  a  system  that  will  not\n",
            "perform as well as expected. This is called data snooping bias.\n",
            "\n",
            "Creating  a  test  set  is  theoretically  simple;  pick  some  instances  randomly,  typically\n",
            "20% of the dataset (or less if your dataset is very large), and set them aside:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "def shuffle_and_split_data(data, test_ratio):\n",
            "    shuffled_indices = np.random.permutation(len(data))\n",
            "    test_set_size = int(len(data) * test_ratio)\n",
            "    test_indices = shuffled_indices[:test_set_size]\n",
            "    train_indices = shuffled_indices[test_set_size:]\n",
            "    return data.iloc[train_indices], data.iloc[test_indices]\n",
            "\n",
            "You can then use this function like this:\n",
            "\n",
            ">>> train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
            ">>> len(train_set)\n",
            "16512\n",
            ">>> len(test_set)\n",
            "4128\n",
            "\n",
            "Well, this works, but it is not perfect: if you run the program again, it will generate a\n",
            "different test set! Over time, you (or your machine learning algorithms) will get to see\n",
            "the whole dataset, which is what you want to avoid.\n",
            "\n",
            "One  solution  is  to  save  the  test  set  on  the  first  run  and  then  load  it  in  subsequent\n",
            "runs.  Another  option  is  to  set  the  random  number  generator’s  seed  (e.g.,  with\n",
            "np.random.seed(42))6  before  calling  np.random.permutation()  so  that  it  always\n",
            "generates the same shuffled indices.\n",
            "\n",
            "However, both these solutions will break the next time you fetch an updated dataset.\n",
            "To have a stable train/test split even after updating the dataset, a common solution is\n",
            "to use each instance’s identifier to decide whether or not it should go in the test set\n",
            "(assuming instances have unique and immutable identifiers). For example, you could\n",
            "compute a hash of each instance’s identifier and put that instance in the test set if the\n",
            "hash is lower than or equal to 20% of the maximum hash value. This ensures that the\n",
            "test  set  will  remain  consistent  across  multiple  runs,  even  if  you  refresh  the  dataset.\n",
            "The  new  test  set  will  contain  20%  of  the  new  instances,  but  it  will  not  contain  any\n",
            "instance that was previously in the training set.\n",
            "\n",
            "6 You will often see people set the random seed to 42. This number has no special property, other than being the\n",
            "\n",
            "Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
            "\n",
            "56 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fHere is a possible implementation:\n",
            "\n",
            "from zlib import crc32\n",
            "\n",
            "def is_id_in_test_set(identifier, test_ratio):\n",
            "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
            "\n",
            "def split_data_with_id_hash(data, test_ratio, id_column):\n",
            "    ids = data[id_column]\n",
            "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
            "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
            "\n",
            "Unfortunately, the housing dataset does not have an identifier column. The simplest\n",
            "solution is to use the row index as the ID:\n",
            "\n",
            "housing_with_id = housing.reset_index()  # adds an `index` column\n",
            "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")\n",
            "\n",
            "If you use the row index as a unique identifier, you need to make sure that new data\n",
            "gets appended to the end of the dataset and that no row ever gets deleted. If this is not\n",
            "possible, then you can try to use the most stable features to build a unique identifier.\n",
            "For example, a district’s latitude and longitude are guaranteed to be stable for a few\n",
            "million years, so you could combine them into an ID like so:7\n",
            "\n",
            "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
            "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")\n",
            "\n",
            "Scikit-Learn provides a few functions to split datasets into multiple subsets in various\n",
            "ways.  The  simplest  function  is  train_test_split(),  which  does  pretty  much  the\n",
            "same  thing  as  the  shuffle_and_split_data()  function  we  defined  earlier,  with  a\n",
            "couple  of  additional  features.  First,  there  is  a  random_state  parameter  that  allows\n",
            "you to set the random generator seed. Second, you can pass it multiple datasets with\n",
            "an identical number of rows, and it will split them on the same indices (this is very\n",
            "useful, for example, if you have a separate DataFrame for labels):\n",
            "\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
            "\n",
            "So far we have considered purely random sampling methods. This is generally fine if\n",
            "your dataset is large enough (especially relative to the number of attributes), but if it\n",
            "is not, you run the risk of introducing a significant sampling bias. When employees\n",
            "at  a  survey  company  decides  to  call  1,000  people  to  ask  them  a  few  questions,  they\n",
            "don’t  just  pick  1,000  people  randomly  in  a  phone  book.  They  try  to  ensure  that\n",
            "these  1,000  people  are  representative  of  the  whole  population,  with  regard  to  the\n",
            "questions  they  want  to  ask.  For  example,  the  US  population  is  51.1%  females  and\n",
            "\n",
            "7 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\n",
            "\n",
            "they will end up in the same set (test or train). This introduces some unfortunate sampling bias.\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "57\n",
            "\n",
            "\f48.9% males, so a well-conducted survey in the US would try to maintain this ratio in\n",
            "the sample: 511 females and 489 males (at least if it seems possible that the answers\n",
            "may vary across genders). This is called stratified sampling: the population is divided\n",
            "into  homogeneous  subgroups  called  strata,  and  the  right  number  of  instances  are\n",
            "sampled  from  each  stratum  to  guarantee  that  the  test  set  is  representative  of  the\n",
            "overall  population.  If  the  people  running  the  survey  used  purely  random  sampling,\n",
            "there  would  be  about  a  10.7%  chance  of  sampling  a  skewed  test  set  with  less  than\n",
            "48.5% female or more than 53.5% female participants. Either way, the survey results\n",
            "would likely be quite biased.\n",
            "\n",
            "Suppose you’ve chatted with some experts who told you that the median income is a\n",
            "very important attribute to predict median housing prices. You may want to ensure\n",
            "that  the  test  set  is  representative  of  the  various  categories  of  incomes  in  the  whole\n",
            "dataset. Since the median income is a continuous numerical attribute, you first need\n",
            "to  create  an  income  category  attribute.  Let’s  look  at  the  median  income  histogram\n",
            "more closely (back in Figure 2-8): most median income values are clustered around\n",
            "1.5  to  6  (i.e.,  $15,000–$60,000),  but  some  median  incomes  go  far  beyond  6.  It  is\n",
            "important to have a sufficient number of instances in your dataset for each stratum,\n",
            "or  else  the  estimate  of  a  stratum’s  importance  may  be  biased.  This  means  that  you\n",
            "should  not  have  too  many  strata,  and  each  stratum  should  be  large  enough.  The\n",
            "following  code  uses  the  pd.cut()  function  to  create  an  income  category  attribute\n",
            "with  five  categories  (labeled  from  1  to  5);  category  1  ranges  from  0  to  1.5  (i.e.,  less\n",
            "than $15,000), category 2 from 1.5 to 3, and so on:\n",
            "\n",
            "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
            "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
            "                               labels=[1, 2, 3, 4, 5])\n",
            "\n",
            "These income categories are represented in Figure 2-9:\n",
            "\n",
            "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
            "plt.xlabel(\"Income category\")\n",
            "plt.ylabel(\"Number of districts\")\n",
            "plt.show()\n",
            "\n",
            "Now  you  are  ready  to  do  stratified  sampling  based  on  the  income  category.  Scikit-\n",
            "Learn provides a number of splitter classes in the sklearn.model_selection package\n",
            "that implement various strategies to split your dataset into a training set and a test set.\n",
            "Each  splitter  has  a  split()  method  that  returns  an  iterator  over  different  training/\n",
            "test splits of the same data.\n",
            "\n",
            "58 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fFigure 2-9. Histogram of income categories\n",
            "\n",
            "To  be  precise,  the  split()  method  yields  the  training  and  test  indices,  not  the\n",
            "data  itself.  Having  multiple  splits  can  be  useful  if  you  want  to  better  estimate  the\n",
            "performance of your model, as you will see when we discuss cross-validation later in\n",
            "this chapter. For example, the following code generates 10 different stratified splits of\n",
            "the same dataset:\n",
            "\n",
            "from sklearn.model_selection import StratifiedShuffleSplit\n",
            "\n",
            "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
            "strat_splits = []\n",
            "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
            "    strat_train_set_n = housing.iloc[train_index]\n",
            "    strat_test_set_n = housing.iloc[test_index]\n",
            "    strat_splits.append([strat_train_set_n, strat_test_set_n])\n",
            "\n",
            "For now, you can just use the first split:\n",
            "\n",
            "strat_train_set, strat_test_set = strat_splits[0]\n",
            "\n",
            "Or,  since  stratified  sampling  is  fairly  common,  there’s  a  shorter  way  to  get  a  single\n",
            "split using the train_test_split() function with the stratify argument:\n",
            "\n",
            "strat_train_set, strat_test_set = train_test_split(\n",
            "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
            "\n",
            "Get the Data \n",
            "\n",
            "| \n",
            "\n",
            "59\n",
            "\n",
            "\fLet’s see if this worked as expected. You can start by looking at the income category\n",
            "proportions in the test set:\n",
            "\n",
            ">>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
            "3    0.350533\n",
            "2    0.318798\n",
            "4    0.176357\n",
            "5    0.114341\n",
            "1    0.039971\n",
            "Name: income_cat, dtype: float64\n",
            "\n",
            "With similar code you can measure the income category proportions in the full data‐\n",
            "set. Figure 2-10 compares the income category proportions in the overall dataset, in\n",
            "the test set generated with stratified sampling, and in a test set generated using purely\n",
            "random sampling. As you can see, the test set generated using stratified sampling has\n",
            "income category proportions almost identical to those in the full dataset, whereas the\n",
            "test set generated using purely random sampling is skewed.\n",
            "\n",
            "Figure 2-10. Sampling bias comparison of stratified versus purely random sampling\n",
            "\n",
            "You won’t use the income_cat column again, so you might as well drop it, reverting\n",
            "the data back to its original state:\n",
            "\n",
            "for set_ in (strat_train_set, strat_test_set):\n",
            "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
            "\n",
            "We spent quite a bit of time on test set generation for a good reason: this is an often\n",
            "neglected  but  critical  part  of  a  machine  learning  project.  Moreover,  many  of  these\n",
            "ideas will be useful later when we discuss cross-validation. Now it’s time to move on\n",
            "to the next stage: exploring the data.\n",
            "\n",
            "Explore and Visualize the Data to Gain Insights\n",
            "So far you have only taken a quick glance at the data to get a general understanding of\n",
            "the kind of data you are manipulating. Now the goal is to go into a little more depth.\n",
            "\n",
            "First, make sure you have put the test set aside and you are only exploring the train‐\n",
            "ing set. Also, if the training set is very large, you may want to sample an exploration\n",
            "\n",
            "60 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fset, to make manipulations easy and fast during the exploration phase. In this case,\n",
            "the  training  set  is  quite  small,  so  you  can  just  work  directly  on  the  full  set.  Since\n",
            "you’re going to experiment with various transformations of the full training set, you\n",
            "should make a copy of the original so you can revert to it afterwards:\n",
            "\n",
            "housing = strat_train_set.copy()\n",
            "\n",
            "Visualizing Geographical Data\n",
            "Because the dataset includes geographical information (latitude and longitude), it is a\n",
            "good idea to create a scatterplot of all the districts to visualize the data (Figure 2-11):\n",
            "\n",
            "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
            "plt.show()\n",
            "\n",
            "Figure 2-11. A geographical scatterplot of the data\n",
            "\n",
            "This looks like California all right, but other than that it is hard to see any particular\n",
            "pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places\n",
            "where there is a high density of data points (Figure 2-12):\n",
            "\n",
            "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
            "plt.show()\n",
            "\n",
            "Now  that’s  much  better:  you  can  clearly  see  the  high-density  areas,  namely  the  Bay\n",
            "Area and around Los Angeles and San Diego, plus a long line of fairly high-density\n",
            "areas in the Central Valley (in particular, around Sacramento and Fresno).\n",
            "\n",
            "Our brains are very good at spotting patterns in pictures, but you may need to play\n",
            "around with visualization parameters to make the patterns stand out.\n",
            "\n",
            "Explore and Visualize the Data to Gain Insights \n",
            "\n",
            "| \n",
            "\n",
            "61\n",
            "\n",
            "\fFigure 2-12. A better visualization that highlights high-density areas\n",
            "\n",
            "Next,  you  look  at  the  housing  prices  (Figure  2-13).  The  radius  of  each  circle  repre‐\n",
            "sents  the  district’s  population  (option  s),  and  the  color  represents  the  price  (option\n",
            "c). Here you use a predefined color map (option cmap) called jet, which ranges from\n",
            "blue (low values) to red (high prices):8\n",
            "\n",
            "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
            "             s=housing[\"population\"] / 100, label=\"population\",\n",
            "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
            "             legend=True, sharex=False, figsize=(10, 7))\n",
            "plt.show()\n",
            "\n",
            "This  image  tells  you  that  the  housing  prices  are  very  much  related  to  the  location\n",
            "(e.g., close to the ocean) and to the population density, as you probably knew already.\n",
            "A clustering algorithm should be useful for detecting the main cluster and for adding\n",
            "new features that measure the proximity to the cluster centers. The ocean proximity\n",
            "attribute may be useful as well, although in Northern California the housing prices in\n",
            "coastal districts are not too high, so it is not a simple rule.\n",
            "\n",
            "8 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\n",
            "\n",
            "down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.\n",
            "\n",
            "62 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fFigure 2-13. California housing prices: red is expensive, blue is cheap, larger circles\n",
            "indicate areas with a larger population\n",
            "\n",
            "Look for Correlations\n",
            "Since  the  dataset  is  not  too  large,  you  can  easily  compute  the  standard  correlation\n",
            "coefficient  (also  called  Pearson’s  r)  between  every  pair  of  attributes  using  the  corr()\n",
            "method:\n",
            "\n",
            "corr_matrix = housing.corr()\n",
            "\n",
            "Now  you  can  look  at  how  much  each  attribute  correlates  with  the  median  house\n",
            "value:\n",
            "\n",
            ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
            "median_house_value    1.000000\n",
            "median_income         0.688380\n",
            "total_rooms           0.137455\n",
            "housing_median_age    0.102175\n",
            "households            0.071426\n",
            "total_bedrooms        0.054635\n",
            "population           -0.020153\n",
            "longitude            -0.050859\n",
            "latitude             -0.139584\n",
            "Name: median_house_value, dtype: float64\n",
            "\n",
            "Explore and Visualize the Data to Gain Insights \n",
            "\n",
            "| \n",
            "\n",
            "63\n",
            "\n",
            "\fThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
            "there  is  a  strong  positive  correlation;  for  example,  the  median  house  value  tends  to\n",
            "go up when the median income goes up. When the coefficient is close to –1, it means\n",
            "that  there  is  a  strong  negative  correlation;  you  can  see  a  small  negative  correlation\n",
            "between the latitude and the median house value (i.e., prices have a slight tendency\n",
            "to go down when you go north). Finally, coefficients close to 0 mean that there is no\n",
            "linear correlation.\n",
            "\n",
            "Another  way  to  check  for  correlation  between  attributes  is  to  use  the  Pandas\n",
            "scatter_matrix()  function,  which  plots  every  numerical  attribute  against  every\n",
            "other  numerical  attribute.  Since  there  are  now  11  numerical  attributes,  you  would\n",
            "get  112  =  121  plots,  which  would  not  fit  on  a  page—so  you  decide  to  focus  on  a\n",
            "few  promising  attributes  that  seem  most  correlated  with  the  median  housing  value\n",
            "(Figure 2-14):\n",
            "\n",
            "from pandas.plotting import scatter_matrix\n",
            "\n",
            "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
            "              \"housing_median_age\"]\n",
            "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
            "plt.show()\n",
            "\n",
            "Figure 2-14. This scatter matrix plots every numerical attribute against every other\n",
            "numerical attribute, plus a histogram of each numerical attribute’s values on the main\n",
            "diagonal (top left to bottom right)\n",
            "\n",
            "64 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fThe  main  diagonal  would  be  full  of  straight  lines  if  Pandas  plotted  each  variable\n",
            "against itself, which would not be very useful. So instead, the Pandas displays a histo‐\n",
            "gram of each attribute (other options are available; see the Pandas documentation for\n",
            "more details).\n",
            "\n",
            "Looking  at  the  correlation  scatterplots,  it  seems  like  the  most  promising  attribute\n",
            "to  predict  the  median  house  value  is  the  median  income,  so  you  zoom  in  on  their\n",
            "scatterplot (Figure 2-15):\n",
            "\n",
            "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
            "             alpha=0.1, grid=True)\n",
            "plt.show()\n",
            "\n",
            "Figure 2-15. Median income versus median house value\n",
            "\n",
            "This  plot  reveals  a  few  things.  First,  the  correlation  is  indeed  quite  strong;  you  can\n",
            "clearly see the upward trend, and the points are not too dispersed. Second, the price\n",
            "cap  you  noticed  earlier  is  clearly  visible  as  a  horizontal  line  at  $500,000.  But  the\n",
            "plot also reveals other less obvious straight lines: a horizontal line around $450,000,\n",
            "another around $350,000, perhaps one around $280,000, and a few more below that.\n",
            "You may want to try removing the corresponding districts to prevent your algorithms\n",
            "from learning to reproduce these data quirks.\n",
            "\n",
            "Explore and Visualize the Data to Gain Insights \n",
            "\n",
            "| \n",
            "\n",
            "65\n",
            "\n",
            "\fThe correlation coefficient only measures linear correlations (“as x\n",
            "goes up, y generally goes up/down”). It may completely miss out on\n",
            "nonlinear  relationships  (e.g.,  “as  x  approaches  0,  y  generally  goes\n",
            "up”). Figure 2-16 shows a variety of datasets along with their corre‐\n",
            "lation coefficient. Note how all the plots of the bottom row have a\n",
            "correlation coefficient equal to 0, despite the fact that their axes are\n",
            "clearly  not  independent:  these  are  examples  of  nonlinear  relation‐\n",
            "ships. Also, the second row shows examples where the correlation\n",
            "coefficient  is  equal  to  1  or  –1;  notice  that  this  has  nothing  to  do\n",
            "with the slope. For example, your height in inches has a correlation\n",
            "coefficient of 1 with your height in feet or in nanometers.\n",
            "\n",
            "Figure 2-16. Standard correlation coefficient of various datasets (source: Wikipedia;\n",
            "public domain image)\n",
            "\n",
            "Experiment with Attribute Combinations\n",
            "Hopefully  the  previous  sections  gave  you  an  idea  of  a  few  ways  you  can  explore\n",
            "the  data  and  gain  insights.  You  identified  a  few  data  quirks  that  you  may  want  to\n",
            "clean  up  before  feeding  the  data  to  a  machine  learning  algorithm,  and  you  found\n",
            "interesting correlations between attributes, in particular with the target attribute. You\n",
            "also noticed that some attributes have a skewed-right distribution, so you may want\n",
            "to  transform  them  (e.g.,  by  computing  their  logarithm  or  square  root).  Of  course,\n",
            "your  mileage  will  vary  considerably  with  each  project,  but  the  general  ideas  are\n",
            "similar.\n",
            "\n",
            "One  last  thing  you  may  want  to  do  before  preparing  the  data  for  machine  learning\n",
            "algorithms is to try out various attribute combinations. For example, the total num‐\n",
            "ber of rooms in a district is not very useful if you don’t know how many households\n",
            "there  are.  What  you  really  want  is  the  number  of  rooms  per  household.  Similarly,\n",
            "the  total  number  of  bedrooms  by  itself  is  not  very  useful:  you  probably  want  to\n",
            "\n",
            "66 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fcompare  it  to  the  number  of  rooms.  And  the  population  per  household  also  seems\n",
            "like an interesting attribute combination to look at. You create these new attributes as\n",
            "follows:\n",
            "\n",
            "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
            "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
            "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n",
            "\n",
            "And then you look at the correlation matrix again:\n",
            "\n",
            ">>> corr_matrix = housing.corr()\n",
            ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
            "median_house_value    1.000000\n",
            "median_income         0.688380\n",
            "rooms_per_house       0.143663\n",
            "total_rooms           0.137455\n",
            "housing_median_age    0.102175\n",
            "households            0.071426\n",
            "total_bedrooms        0.054635\n",
            "population           -0.020153\n",
            "people_per_house     -0.038224\n",
            "longitude            -0.050859\n",
            "latitude             -0.139584\n",
            "bedrooms_ratio       -0.256397\n",
            "Name: median_house_value, dtype: float64\n",
            "\n",
            "Hey,  not  bad!  The  new  bedrooms_ratio  attribute  is  much  more  correlated  with  the\n",
            "median house value than the total number of rooms or bedrooms. Apparently houses\n",
            "with a lower bedroom/room ratio tend to be more expensive. The number of rooms\n",
            "per household is also more informative than the total number of rooms in a district—\n",
            "obviously the larger the houses, the more expensive they are.\n",
            "\n",
            "This  round  of  exploration  does  not  have  to  be  absolutely  thorough;  the  point  is  to\n",
            "start  off  on  the  right  foot  and  quickly  gain  insights  that  will  help  you  get  a  first\n",
            "reasonably good prototype. But this is an iterative process: once you get a prototype\n",
            "up and running, you can analyze its output to gain more insights and come back to\n",
            "this exploration step.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms\n",
            "It’s time to prepare the data for your machine learning algorithms. Instead of doing\n",
            "this manually, you should write functions for this purpose, for several good reasons:\n",
            "\n",
            "•\n",
            "• This will allow you to reproduce these transformations easily on any dataset (e.g.,\n",
            "\n",
            "the next time you get a fresh dataset).\n",
            "\n",
            "• You will gradually build a library of transformation functions that you can reuse\n",
            "•\n",
            "\n",
            "in future projects.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "67\n",
            "\n",
            "\f•\n",
            "• You can use these functions in your live system to transform the new data before\n",
            "\n",
            "feeding it to your algorithms.\n",
            "\n",
            "•\n",
            "• This  will  make  it  possible  for  you  to  easily  try  various  transformations  and  see\n",
            "\n",
            "which combination of transformations works best.\n",
            "\n",
            "But first, revert to a clean training set (by copying strat_train_set once again). You\n",
            "should  also  separate  the  predictors  and  the  labels,  since  you  don’t  necessarily  want\n",
            "to apply the same transformations to the predictors and the target values (note that\n",
            "drop() creates a copy of the data and does not affect strat_train_set):\n",
            "\n",
            "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
            "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
            "\n",
            "Clean the Data\n",
            "Most machine learning algorithms cannot work with missing features, so you’ll need\n",
            "to  take  care  of  these.  For  example,  you  noticed  earlier  that  the  total_bedrooms\n",
            "attribute has some missing values. You have three options to fix this:\n",
            "\n",
            "1.\n",
            "1. Get rid of the corresponding districts.\n",
            "\n",
            "2.\n",
            "2. Get rid of the whole attribute.\n",
            "\n",
            "3.\n",
            "3. Set  the  missing  values  to  some  value  (zero,  the  mean,  the  median,  etc.).  This  is\n",
            "\n",
            "called imputation.\n",
            "\n",
            "You can accomplish these easily using the Pandas DataFrame’s dropna(), drop(), and\n",
            "fillna() methods:\n",
            "\n",
            "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n",
            "\n",
            "housing.drop(\"total_bedrooms\", axis=1)  # option 2\n",
            "\n",
            "median = housing[\"total_bedrooms\"].median()  # option 3\n",
            "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
            "\n",
            "You  decide  to  go  for  option  3  since  it  is  the  least  destructive,  but  instead  of  the\n",
            "preceding code, you will use a handy Scikit-Learn class: SimpleImputer. The benefit\n",
            "is  that  it  will  store  the  median  value  of  each  feature:  this  will  make  it  possible  to\n",
            "impute  missing  values  not  only  on  the  training  set,  but  also  on  the  validation  set,\n",
            "the test set, and any new data fed to the model. To use it, first you need to create a\n",
            "SimpleImputer instance, specifying that you want to replace each attribute’s missing\n",
            "values with the median of that attribute:\n",
            "\n",
            "from sklearn.impute import SimpleImputer\n",
            "\n",
            "imputer = SimpleImputer(strategy=\"median\")\n",
            "\n",
            "68 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fSince  the  median  can  only  be  computed  on  numerical  attributes,  you  then  need  to\n",
            "create a copy of the data with only the numerical attributes (this will exclude the text\n",
            "attribute ocean_proximity):\n",
            "\n",
            "housing_num = housing.select_dtypes(include=[np.number])\n",
            "\n",
            "Now you can fit the imputer instance to the training data using the fit() method:\n",
            "\n",
            "imputer.fit(housing_num)\n",
            "\n",
            "The imputer has simply computed the median of each attribute and stored the result\n",
            "in its statistics_ instance variable. Only the total_bedrooms attribute had missing\n",
            "values,  but  you  cannot  be  sure  that  there  won’t  be  any  missing  values  in  new  data\n",
            "after  the  system  goes  live,  so  it  is  safer  to  apply  the  imputer  to  all  the  numerical\n",
            "attributes:\n",
            "\n",
            ">>> imputer.statistics_\n",
            "array([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\n",
            ">>> housing_num.median().values\n",
            "array([-118.51 , 34.26 , 29. , 2125. , 434. , 1167. , 408. , 3.5385])\n",
            "\n",
            "Now  you  can  use  this  “trained”  imputer  to  transform  the  training  set  by  replacing\n",
            "missing values with the learned medians:\n",
            "\n",
            "X = imputer.transform(housing_num)\n",
            "\n",
            "Missing  values  can  also  be  replaced  with  the  mean  value  (strategy=\"mean\"),  or\n",
            "with  the  most  frequent  value  (strategy=\"most_frequent\"),  or  with  a  constant\n",
            "value (strategy=\"constant\", fill_value=…). The last two strategies support non-\n",
            "numerical data.\n",
            "\n",
            "There  are  also  more  powerful \n",
            "sklearn.impute package (both for numerical features only):\n",
            "\n",
            "imputers  available \n",
            "\n",
            "in \n",
            "\n",
            "the\n",
            "\n",
            "• KNNImputer replaces each missing value with the mean of the\n",
            "•\n",
            "k-nearest  neighbors’  values  for  that  feature.  The  distance  is\n",
            "based on all the available features.\n",
            "\n",
            "• IterativeImputer  trains  a  regression  model  per  feature  to\n",
            "•\n",
            "predict  the  missing  values  based  on  all  the  other  available\n",
            "features.  It  then  trains  the  model  again  on  the  updated  data,\n",
            "and  repeats  the  process  several  times,  improving  the  models\n",
            "and the replacement values at each iteration.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "69\n",
            "\n",
            "\fScikit-Learn Design\n",
            "Scikit-Learn’s API is remarkably well designed. These are the main design principles:9\n",
            "\n",
            "Consistency\n",
            "\n",
            "All objects share a consistent and simple interface:\n",
            "\n",
            "Estimators\n",
            "\n",
            "Any  object  that  can  estimate  some  parameters  based  on  a  dataset  is  called\n",
            "an estimator (e.g., a SimpleImputer is an estimator). The estimation itself is\n",
            "performed by the fit() method, and it takes a dataset as a parameter, or two\n",
            "for  supervised  learning  algorithms—the  second  dataset  contains  the  labels.\n",
            "Any other parameter needed to guide the estimation process is considered a\n",
            "hyperparameter (such as a SimpleImputer’s strategy), and it must be set as\n",
            "an instance variable (generally via a constructor parameter).\n",
            "\n",
            "Transformers\n",
            "\n",
            "Some  estimators  (such  as  a  SimpleImputer)  can  also  transform  a  dataset;\n",
            "these are called transformers. Once again, the API is simple: the transforma‐\n",
            "tion is performed by the transform() method with the dataset to transform\n",
            "as a parameter. It returns the transformed dataset. This transformation gen‐\n",
            "erally  relies  on  the  learned  parameters,  as  is  the  case  for  a  SimpleImputer.\n",
            "All  transformers  also  have  a  convenience  method  called  fit_transform(),\n",
            "which is equivalent to calling fit() and then transform() (but sometimes\n",
            "fit_transform() is optimized and runs much faster).\n",
            "\n",
            "Predictors\n",
            "\n",
            "Finally, some estimators, given a dataset, are capable of making predictions;\n",
            "they  are  called  predictors.  For  example,  the  LinearRegression  model  in\n",
            "the  previous  chapter  was  a  predictor:  given  a  country’s  GDP  per  capita,  it\n",
            "predicted life satisfaction. A predictor has a predict() method that takes a\n",
            "dataset of new instances and returns a dataset of corresponding predictions.\n",
            "It  also  has  a  score()  method  that  measures  the  quality  of  the  predictions,\n",
            "given  a  test  set  (and  the  corresponding  labels,  in  the  case  of  supervised\n",
            "learning algorithms).10\n",
            "\n",
            "Inspection\n",
            "\n",
            "All  the  estimator’s  hyperparameters  are  accessible  directly  via  public  instance\n",
            "variables  (e.g.,  imputer.strategy),  and  all  the  estimator’s  learned  parameters\n",
            "are  accessible  via  public  instance  variables  with  an  underscore  suffix  (e.g.,\n",
            "imputer.statistics_).\n",
            "\n",
            "9 For more details on the design principles, see Lars Buitinck et al., “API Design for Machine Learning Software:\n",
            "\n",
            "Experiences from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238 (2013).\n",
            "\n",
            "10 Some predictors also provide methods to measure the confidence of their predictions.\n",
            "\n",
            "70 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fNonproliferation of classes\n",
            "\n",
            "Datasets  are  represented  as  NumPy  arrays  or  SciPy  sparse  matrices,  instead  of\n",
            "homemade classes. Hyperparameters are just regular Python strings or numbers.\n",
            "\n",
            "Composition\n",
            "\n",
            "Existing building blocks are reused as much as possible. For example, it is easy to\n",
            "create a Pipeline estimator from an arbitrary sequence of transformers followed\n",
            "by a final estimator, as you will see.\n",
            "\n",
            "Sensible defaults\n",
            "\n",
            "Scikit-Learn  provides  reasonable  default  values  for  most  parameters,  making  it\n",
            "easy to quickly create a baseline working system.\n",
            "\n",
            "Scikit-Learn  transformers  output  NumPy  arrays  (or  sometimes  SciPy  sparse  matri‐\n",
            "ces)  even  when  they  are  fed  Pandas  DataFrames  as  input.11  So,  the  output  of\n",
            "imputer.transform(housing_num)  is  a  NumPy  array:  X  has  neither  column  names\n",
            "nor index. Luckily, it’s not too hard to wrap X in a DataFrame and recover the column\n",
            "names and index from housing_num:\n",
            "\n",
            "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
            "                          index=housing_num.index)\n",
            "\n",
            "Handling Text and Categorical Attributes\n",
            "So far we have only dealt with numerical attributes, but your data may also contain\n",
            "text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s\n",
            "look at its value for the first few instances:\n",
            "\n",
            ">>> housing_cat = housing[[\"ocean_proximity\"]]\n",
            ">>> housing_cat.head(8)\n",
            "      ocean_proximity\n",
            "13096        NEAR BAY\n",
            "14973       <1H OCEAN\n",
            "3785           INLAND\n",
            "14689          INLAND\n",
            "20507      NEAR OCEAN\n",
            "1286           INLAND\n",
            "18078       <1H OCEAN\n",
            "4396         NEAR BAY\n",
            "\n",
            "It’s  not  arbitrary  text:  there  are  a  limited  number  of  possible  values,  each  of  which\n",
            "represents  a  category.  So  this  attribute  is  a  categorical  attribute.  Most  machine\n",
            "\n",
            "11 By the time you read these lines, it may be possible to make all transformers output Pandas DataFrames when\n",
            "they receive a DataFrame as input: Pandas in, Pandas out. There will likely be a global configuration option\n",
            "for this: sklearn.set_config(pandas_in_out=True).\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "71\n",
            "\n",
            "\flearning  algorithms  prefer  to  work  with  numbers,  so  let’s  convert  these  categories\n",
            "from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class:\n",
            "\n",
            "from sklearn.preprocessing import OrdinalEncoder\n",
            "\n",
            "ordinal_encoder = OrdinalEncoder()\n",
            "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
            "\n",
            "Here’s what the first few encoded values in housing_cat_encoded look like:\n",
            "\n",
            ">>> housing_cat_encoded[:8]\n",
            "array([[3.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [4.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [3.]])\n",
            "\n",
            "You can get the list of categories using the categories_ instance variable. It is a list\n",
            "containing  a  1D  array  of  categories  for  each  categorical  attribute  (in  this  case,  a  list\n",
            "containing a single array since there is just one categorical attribute):\n",
            "\n",
            ">>> ordinal_encoder.categories_\n",
            "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
            "       dtype=object)]\n",
            "\n",
            "One issue with this representation is that ML algorithms will assume that two nearby\n",
            "values  are  more  similar  than  two  distant  values.  This  may  be  fine  in  some  cases\n",
            "(e.g., for ordered categories such as “bad”, “average”, “good”, and “excellent”), but it is\n",
            "obviously  not  the  case  for  the  ocean_proximity  column  (for  example,  categories  0\n",
            "and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common\n",
            "solution is to create one binary attribute per category: one attribute equal to 1 when\n",
            "the category is \"<1H OCEAN\" (and 0 otherwise), another attribute equal to 1 when the\n",
            "category  is  \"INLAND\"  (and  0  otherwise),  and  so  on.  This  is  called  one-hot  encoding,\n",
            "because only one attribute will be equal to 1 (hot), while the others will be 0 (cold).\n",
            "The  new  attributes  are  sometimes  called  dummy  attributes.  Scikit-Learn  provides  a\n",
            "OneHotEncoder class to convert categorical values into one-hot vectors:\n",
            "\n",
            "from sklearn.preprocessing import OneHotEncoder\n",
            "\n",
            "cat_encoder = OneHotEncoder()\n",
            "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
            "\n",
            "72 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fBy  default,  the  output  of  a  OneHotEncoder  is  a  SciPy  sparse  matrix,  instead  of  a\n",
            "NumPy array:\n",
            "\n",
            ">>> housing_cat_1hot\n",
            "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
            " with 16512 stored elements in Compressed Sparse Row format>\n",
            "\n",
            "A  sparse  matrix  is  a  very  efficient  representation  for  matrices  that  contain  mostly\n",
            "zeros. Indeed, internally it only stores the nonzero values and their positions. When\n",
            "a categorical attribute has hundreds or thousands of categories, one-hot encoding it\n",
            "results  in  a  very  large  matrix  full  of  0s  except  for  a  single  1  per  row.  In  this  case,  a\n",
            "sparse matrix is exactly what you need: it will save plenty of memory and speed up\n",
            "computations. You can use a sparse matrix mostly like a normal 2D array,12 but if you\n",
            "want to convert it to a (dense) NumPy array, just call the toarray() method:\n",
            "\n",
            ">>> housing_cat_1hot.toarray()\n",
            "array([[0., 0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.]])\n",
            "\n",
            "Alternatively, you can set sparse=False when creating the OneHotEncoder, in which\n",
            "case the transform() method will return a regular (dense) NumPy array directly.\n",
            "\n",
            "As  with  the  OrdinalEncoder,  you  can  get  the  list  of  categories  using  the  encoder’s\n",
            "categories_ instance variable:\n",
            "\n",
            ">>> cat_encoder.categories_\n",
            "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
            "       dtype=object)]\n",
            "\n",
            "Pandas  has  a  function  called  get_dummies(),  which  also  converts  each  categorical\n",
            "feature into a one-hot representation, with one binary feature per category:\n",
            "\n",
            ">>> df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
            ">>> pd.get_dummies(df_test)\n",
            "   ocean_proximity_INLAND  ocean_proximity_NEAR BAY\n",
            "0                       1                         0\n",
            "1                       0                         1\n",
            "\n",
            "It  looks  nice  and  simple,  so  why  not  use  it  instead  of  OneHotEncoder?  Well,  the\n",
            "advantage  of  OneHotEncoder  is  that  it  remembers  which  categories  it  was  trained\n",
            "on.  This  is  very  important  because  once  your  model  is  in  production,  it  should  be\n",
            "fed  exactly  the  same  features  as  during  training:  no  more,  no  less.  Look  what  our\n",
            "\n",
            "12 See SciPy’s documentation for more details.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "73\n",
            "\n",
            "\ftrained  cat_encoder  outputs  when  we  make  it  transform  the  same  df_test  (using\n",
            "transform(), not fit_transform()):\n",
            "\n",
            ">>> cat_encoder.transform(df_test)\n",
            "array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0.]])\n",
            "\n",
            "See the difference? get_dummies() saw only two categories, so it output two columns,\n",
            "whereas OneHotEncoder output one column per learned category, in the right order.\n",
            "Moreover, if you feed get_dummies() a DataFrame containing an unknown category\n",
            "(e.g., \"<2H OCEAN\"), it will happily generate a column for it:\n",
            "\n",
            ">>> df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
            ">>> pd.get_dummies(df_test_unknown)\n",
            "   ocean_proximity_<2H OCEAN  ocean_proximity_ISLAND\n",
            "0                          1                       0\n",
            "1                          0                       1\n",
            "\n",
            "But  OneHotEncoder  is  smarter:  it  will  detect  the  unknown  category  and  raise\n",
            "an  exception.  If  you  prefer,  you  can  set  the  handle_unknown  hyperparameter  to\n",
            "\"ignore\", in which case it will just represent the unknown category with zeros:\n",
            "\n",
            ">>> cat_encoder.handle_unknown = \"ignore\"\n",
            ">>> cat_encoder.transform(df_test_unknown)\n",
            "array([[0., 0., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0.]])\n",
            "\n",
            "If a categorical attribute has a large number of possible categories\n",
            "(e.g.,  country  code,  profession,  species),  then  one-hot  encoding\n",
            "will  result  in  a  large  number  of  input  features.  This  may  slow\n",
            "down training and degrade performance. If this happens, you may\n",
            "want  to  replace  the  categorical  input  with  useful  numerical  fea‐\n",
            "tures  related  to  the  categories:  for  example,  you  could  replace  the\n",
            "ocean_proximity feature with the distance to the ocean (similarly,\n",
            "a country code could be replaced with the country’s population and\n",
            "GDP  per  capita).  Alternatively,  you  can  use  one  of  the  encoders\n",
            "provided by the category_encoders package on GitHub. Or, when\n",
            "dealing  with  neural  networks,  you  can  replace  each  category  with\n",
            "a  learnable,  low-dimensional  vector  called  an  embedding.  This  is\n",
            "an  example  of  representation  learning  (see  Chapters  13  and  17  for\n",
            "more details).\n",
            "\n",
            "74 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the\n",
            "column  names  in  the  feature_names_in_  attribute.  Scikit-Learn  then  ensures  that\n",
            "any  DataFrame  fed  to  this  estimator  after  that  (e.g.,  to  transform()  or  predict())\n",
            "has the same column names. Transformers also provide a get_feature_names_out()\n",
            "method that you can use to build a DataFrame around the transformer’s output:\n",
            "\n",
            ">>> cat_encoder.feature_names_in_\n",
            "array(['ocean_proximity'], dtype=object)\n",
            ">>> cat_encoder.get_feature_names_out()\n",
            "array(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\n",
            "       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n",
            "       'ocean_proximity_NEAR OCEAN'], dtype=object)\n",
            ">>> df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n",
            "...                          columns=cat_encoder.get_feature_names_out(),\n",
            "...                          index=df_test_unknown.index)\n",
            "...\n",
            "\n",
            "Feature Scaling and Transformation\n",
            "One of the most important transformations you need to apply to your data is feature\n",
            "scaling. With few exceptions, machine learning algorithms don’t perform well when\n",
            "the  input  numerical  attributes  have  very  different  scales.  This  is  the  case  for  the\n",
            "housing  data:  the  total  number  of  rooms  ranges  from  about  6  to  39,320,  while  the\n",
            "median incomes only range from 0 to 15. Without any scaling, most models will be\n",
            "biased  toward  ignoring  the  median  income  and  focusing  more  on  the  number  of\n",
            "rooms.\n",
            "\n",
            "There  are  two  common  ways  to  get  all  attributes  to  have  the  same  scale:  min-max\n",
            "scaling and standardization.\n",
            "\n",
            "As with all estimators, it is important to fit the scalers to the train‐\n",
            "ing  data  only:  never  use  fit()  or  fit_transform()  for  anything\n",
            "else than the training set. Once you have a trained scaler, you can\n",
            "then use it to  transform() any other set, including the validation\n",
            "set,  the  test  set,  and  new  data.  Note  that  while  the  training  set\n",
            "values  will  always  be  scaled  to  the  specified  range,  if  new  data\n",
            "contains outliers, these may end up scaled outside the range. If you\n",
            "want to avoid this, just set the clip hyperparameter to True.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "75\n",
            "\n",
            "\fMin-max  scaling  (many  people  call  this  normalization)  is  the  simplest:  for  each\n",
            "attribute,  the  values  are  shifted  and  rescaled  so  that  they  end  up  ranging  from\n",
            "0  to  1.  This  is  performed  by  subtracting  the  min  value  and  dividing  by  the  dif‐\n",
            "ference  between  the  min  and  the  max.  Scikit-Learn  provides  a  transformer  called\n",
            "MinMaxScaler for this. It has a feature_range hyperparameter that lets you change\n",
            "the  range  if,  for  some  reason,  you  don’t  want  0–1  (e.g.,  neural  networks  work  best\n",
            "with zero-mean inputs, so a range of –1 to 1 is preferable). It’s quite easy to use:\n",
            "\n",
            "from sklearn.preprocessing import MinMaxScaler\n",
            "\n",
            "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
            "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n",
            "\n",
            "Standardization is different: first it subtracts the mean value (so standardized values\n",
            "have a zero mean), then it divides the result by the standard deviation (so standard‐\n",
            "ized values have a standard deviation equal to 1). Unlike min-max scaling, standardi‐\n",
            "zation does not restrict values to a specific range. However, standardization is much\n",
            "less affected by outliers. For example, suppose a district has a median income equal\n",
            "to  100  (by  mistake),  instead  of  the  usual  0–15.  Min-max  scaling  to  the  0–1  range\n",
            "would  map  this  outlier  down  to  1  and  it  would  crush  all  the  other  values  down  to\n",
            "0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a\n",
            "transformer called StandardScaler for standardization:\n",
            "\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "std_scaler = StandardScaler()\n",
            "housing_num_std_scaled = std_scaler.fit_transform(housing_num)\n",
            "\n",
            "If you want to scale a sparse matrix without converting it to a dense\n",
            "matrix  first,  you  can  use  a  StandardScaler  with  its  with_mean\n",
            "hyperparameter  set  to  False:  it  will  only  divide  the  data  by  the\n",
            "standard  deviation,  without  subtracting  the  mean  (as  this  would\n",
            "break sparsity).\n",
            "\n",
            "When  a  feature’s  distribution  has  a  heavy  tail  (i.e.,  when  values  far  from  the  mean\n",
            "are  not  exponentially  rare),  both  min-max  scaling  and  standardization  will  squash\n",
            "most  values  into  a  small  range.  Machine  learning  models  generally  don’t  like  this\n",
            "at  all,  as  you  will  see  in  Chapter  4.  So  before  you  scale  the  feature,  you  should  first\n",
            "transform it to shrink the heavy tail, and if possible to make the distribution roughly\n",
            "symmetrical. For example, a common way to do this for positive features with a heavy\n",
            "tail  to  the  right  is  to  replace  the  feature  with  its  square  root  (or  raise  the  feature  to\n",
            "a  power  between  0  and  1).  If  the  feature  has  a  really  long  and  heavy  tail,  such  as  a\n",
            "power  law  distribution,  then  replacing  the  feature  with  its  logarithm  may  help.  For\n",
            "example,  the  population  feature  roughly  follows  a  power  law:  districts  with  10,000\n",
            "inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not\n",
            "\n",
            "76 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fexponentially  less  frequent.  Figure  2-17  shows  how  much  better  this  feature  looks\n",
            "when you compute its log: it’s very close to a Gaussian distribution (i.e., bell-shaped).\n",
            "\n",
            "Figure 2-17. Transforming a feature to make it closer to a Gaussian distribution\n",
            "\n",
            "Another approach to handle heavy-tailed features consists in bucketizing the feature.\n",
            "This means chopping its distribution into roughly equal-sized buckets, and replacing\n",
            "each  feature  value  with  the  index  of  the  bucket  it  belongs  to,  much  like  we  did  to\n",
            "create  the  income_cat  feature  (although  we  only  used  it  for  stratified  sampling).\n",
            "For  example,  you  could  replace  each  value  with  its  percentile.  Bucketizing  with\n",
            "equal-sized buckets results in a feature with an almost uniform distribution, so there’s\n",
            "no need for further scaling, or you can just divide by the number of buckets to force\n",
            "the values to the 0–1 range.\n",
            "\n",
            "When  a  feature  has  a  multimodal  distribution  (i.e.,  with  two  or  more  clear  peaks,\n",
            "called  modes),  such  as  the  housing_median_age  feature,  it  can  also  be  helpful  to\n",
            "bucketize it, but this time treating the bucket IDs as categories, rather than as numeri‐\n",
            "cal values. This means that the bucket indices must be encoded, for example using a\n",
            "OneHotEncoder (so you usually don’t want to use too many buckets). This approach\n",
            "will allow the regression model to more easily learn different rules for different ranges\n",
            "of  this  feature  value.  For  example,  perhaps  houses  built  around  35  years  ago  have\n",
            "a  peculiar  style  that  fell  out  of  fashion,  and  therefore  they’re  cheaper  than  their  age\n",
            "alone would suggest.\n",
            "\n",
            "Another  approach  to  transforming  multimodal  distributions  is  to  add  a  feature  for\n",
            "each  of  the  modes  (at  least  the  main  ones),  representing  the  similarity  between  the\n",
            "housing  median  age  and  that  particular  mode.  The  similarity  measure  is  typically\n",
            "computed using a radial basis function (RBF)—any function that depends only on the\n",
            "distance between the input value and a fixed point. The most commonly used RBF is\n",
            "the Gaussian RBF, whose output value decays exponentially as the input value moves\n",
            "away  from  the  fixed  point.  For  example,  the  Gaussian  RBF  similarity  between  the\n",
            "housing age x and 35 is given by the equation exp(–γ(x – 35)²). The hyperparameter\n",
            "γ  (gamma)  determines  how  quickly  the  similarity  measure  decays  as  x  moves  away\n",
            "from 35. Using Scikit-Learn’s rbf_kernel() function, you can create a new Gaussian\n",
            "RBF feature measuring the similarity between the housing median age and 35:\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "77\n",
            "\n",
            "\ffrom sklearn.metrics.pairwise import rbf_kernel\n",
            "\n",
            "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\n",
            "\n",
            "Figure  2-18  shows  this  new  feature  as  a  function  of  the  housing  median  age  (solid\n",
            "line). It also shows what the feature would look like if you used a smaller gamma value.\n",
            "As the chart shows, the new age similarity feature peaks at 35, right around the spike\n",
            "in the housing median age distribution: if this particular age group is well correlated\n",
            "with lower prices, there’s a good chance that this new feature will help.\n",
            "\n",
            "Figure 2-18. Gaussian RBF feature measuring the similarity between the housing\n",
            "median age and 35\n",
            "\n",
            "So  far  we’ve  only  looked  at  the  input  features,  but  the  target  values  may  also  need\n",
            "to  be  transformed.  For  example,  if  the  target  distribution  has  a  heavy  tail,  you  may\n",
            "choose  to  replace  the  target  with  its  logarithm.  But  if  you  do,  the  regression  model\n",
            "will now predict the log of the median house value, not the median house value itself.\n",
            "You will need to compute the exponential of the model’s prediction if you want the\n",
            "predicted median house value.\n",
            "\n",
            "Luckily,  most  of  Scikit-Learn’s  transformers  have  an  inverse_transform()  method,\n",
            "making  it  easy  to  compute  the  inverse  of  their  transformations.  For  example,  the\n",
            "following code example shows how to scale the labels using a StandardScaler (just\n",
            "like we did for inputs), then train a simple linear regression model on the resulting\n",
            "scaled labels and use it to make predictions on some new data, which we transform\n",
            "back  to  the  original  scale  using  the  trained  scaler’s  inverse_transform()  method.\n",
            "Note  that  we  convert  the  labels  from  a  Pandas  Series  to  a  DataFrame,  since  the\n",
            "\n",
            "78 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fStandardScaler expects 2D inputs. Also, in this example we just train the model on a\n",
            "single raw input feature (median income), for simplicity:\n",
            "\n",
            "from sklearn.linear_model import LinearRegression\n",
            "\n",
            "target_scaler = StandardScaler()\n",
            "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
            "\n",
            "model = LinearRegression()\n",
            "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
            "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
            "\n",
            "scaled_predictions = model.predict(some_new_data)\n",
            "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
            "\n",
            "This works fine, but a simpler option is to use a TransformedTargetRegressor. We\n",
            "just  need  to  construct  it,  giving  it  the  regression  model  and  the  label  transformer,\n",
            "then fit it on the training set, using the original unscaled labels. It will automatically\n",
            "use the transformer to scale the labels and train the regression model on the resulting\n",
            "scaled labels, just like we did previously. Then, when we want to make a prediction, it\n",
            "will call the regression model’s predict() method and use the scaler’s inverse_trans\n",
            "form() method to produce the prediction:\n",
            "\n",
            "from sklearn.compose import TransformedTargetRegressor\n",
            "\n",
            "model = TransformedTargetRegressor(LinearRegression(),\n",
            "                                   transformer=StandardScaler())\n",
            "model.fit(housing[[\"median_income\"]], housing_labels)\n",
            "predictions = model.predict(some_new_data)\n",
            "\n",
            "Custom Transformers\n",
            "Although  Scikit-Learn  provides  many  useful  transformers,  you  will  need  to  write\n",
            "your own for tasks such as custom transformations, cleanup operations, or combin‐\n",
            "ing specific attributes.\n",
            "\n",
            "For  transformations  that  don’t  require  any  training,  you  can  just  write  a  function\n",
            "that takes a NumPy array as input and outputs the transformed array. For example,\n",
            "as  discussed  in  the  previous  section,  it’s  often  a  good  idea  to  transform  features\n",
            "with heavy-tailed distributions by replacing them with their logarithm (assuming the\n",
            "feature is positive and the tail is on the right). Let’s create a log-transformer and apply\n",
            "it to the population feature:\n",
            "\n",
            "from sklearn.preprocessing import FunctionTransformer\n",
            "\n",
            "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
            "log_pop = log_transformer.transform(housing[[\"population\"]])\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "79\n",
            "\n",
            "\fThe  inverse_func  argument  is  optional.  It  lets  you  specify  an  inverse  transform\n",
            "function, e.g., if you plan to use your transformer in a TransformedTargetRegressor.\n",
            "\n",
            "Your transformation function can take hyperparameters as additional arguments. For\n",
            "example,  here’s  how  to  create  a  transformer  that  computes  the  same  Gaussian  RBF\n",
            "similarity measure as earlier:\n",
            "\n",
            "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
            "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
            "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\n",
            "\n",
            "Note that there’s no inverse function for the RBF kernel, since there are always two\n",
            "values  at  a  given  distance  from  a  fixed  point  (except  at  distance  0).  Also  note  that\n",
            "rbf_kernel()  does  not  treat  the  features  separately.  If  you  pass  it  an  array  with\n",
            "two  features,  it  will  measure  the  2D  distance  (Euclidean)  to  measure  similarity.  For\n",
            "example,  here’s  how  to  add  a  feature  that  will  measure  the  geographic  similarity\n",
            "between each district and San Francisco:\n",
            "\n",
            "sf_coords = 37.7749, -122.41\n",
            "sf_transformer = FunctionTransformer(rbf_kernel,\n",
            "                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\n",
            "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\n",
            "\n",
            "Custom  transformers  are  also  useful  to  combine  features.  For  example,  here’s  a\n",
            "FunctionTransformer that computes the ratio between the input features 0 and 1:\n",
            "\n",
            ">>> ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
            ">>> ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n",
            "array([[0.5 ],\n",
            "       [0.75]])\n",
            "\n",
            "FunctionTransformer is very handy, but what if you would like your transformer to\n",
            "be  trainable,  learning  some  parameters  in  the  fit()  method  and  using  them  later\n",
            "in the transform() method? For this, you need to write a custom class. Scikit-Learn\n",
            "relies on duck typing, so this class does not have to inherit from any particular base\n",
            "class.  All  it  needs  is  three  methods:  fit()  (which  must  return  self),  transform(),\n",
            "and fit_transform().\n",
            "\n",
            "You can get fit_transform() for free by simply adding TransformerMixin as a base\n",
            "class:  the  default  implementation  will  just  call  fit()  and  then  transform().  If  you\n",
            "add  BaseEstimator  as  a  base  class  (and  avoid  using  *args  and  **kwargs  in  your\n",
            "constructor), you will also get two extra methods: get_params() and set_params().\n",
            "These will be useful for automatic hyperparameter tuning.\n",
            "\n",
            "For example, here’s a custom transformer that acts much like the StandardScaler:\n",
            "\n",
            "80 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\ffrom sklearn.base import BaseEstimator, TransformerMixin\n",
            "from sklearn.utils.validation import check_array, check_is_fitted\n",
            "\n",
            "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
            "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
            "        self.with_mean = with_mean\n",
            "\n",
            "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
            "        X = check_array(X)  # checks that X is an array with finite float values\n",
            "        self.mean_ = X.mean(axis=0)\n",
            "        self.scale_ = X.std(axis=0)\n",
            "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
            "        return self  # always return self!\n",
            "\n",
            "    def transform(self, X):\n",
            "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
            "        X = check_array(X)\n",
            "        assert self.n_features_in_ == X.shape[1]\n",
            "        if self.with_mean:\n",
            "            X = X - self.mean_\n",
            "        return X / self.scale_\n",
            "\n",
            "Here are a few things to note:\n",
            "\n",
            "• The  sklearn.utils.validation package contains several functions we can use\n",
            "•\n",
            "to  validate  the  inputs.  For  simplicity,  we  will  skip  such  tests  in  the  rest  of  this\n",
            "book, but production code should have them.\n",
            "\n",
            "• Scikit-Learn pipelines require the fit() method to have two arguments X and y,\n",
            "•\n",
            "\n",
            "which is why we need the y=None argument even though we don’t use y.\n",
            "\n",
            "• All  Scikit-Learn  estimators  set  n_features_in_  in  the  fit()  method,  and  they\n",
            "•\n",
            "ensure  that  the  data  passed  to  transform()  or  predict()  has  this  number  of\n",
            "features.\n",
            "\n",
            "• The fit() method must return self.\n",
            "•\n",
            "\n",
            "• This \n",
            "•\n",
            "\n",
            "implementation \n",
            "\n",
            "is  not  100%  complete:  all  estimators  should  set\n",
            "feature_names_in_  in  the  fit()  method  when  they  are  passed  a  DataFrame.\n",
            "Moreover, all transformers should provide a get_feature_names_out() method,\n",
            "as  well  as  an  inverse_transform()  method  when  their  transformation  can  be\n",
            "reversed. See the last exercise at the end of this chapter for more details.\n",
            "\n",
            "A custom transformer can (and often does) use other estimators in its implementa‐\n",
            "tion.  For  example,  the  following  code  demonstrates  custom  transformer  that  uses  a\n",
            "KMeans  clusterer  in  the  fit()  method  to  identify  the  main  clusters  in  the  training\n",
            "data,  and  then  uses  rbf_kernel()  in  the  transform()  method  to  measure  how\n",
            "similar each sample is to each cluster center:\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "81\n",
            "\n",
            "\ffrom sklearn.cluster import KMeans\n",
            "\n",
            "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
            "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
            "        self.n_clusters = n_clusters\n",
            "        self.gamma = gamma\n",
            "        self.random_state = random_state\n",
            "\n",
            "    def fit(self, X, y=None, sample_weight=None):\n",
            "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
            "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
            "        return self  # always return self!\n",
            "\n",
            "    def transform(self, X):\n",
            "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
            "\n",
            "    def get_feature_names_out(self, names=None):\n",
            "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
            "\n",
            "You  can  check  whether  your  custom  estimator  respects  Scikit-\n",
            "Learn’s  API  by  passing  an  instance  to  check_estimator()  from\n",
            "the  sklearn.utils.estimator_checks  package.  For  the  full  API,\n",
            "check out https://scikit-learn.org/stable/developers.\n",
            "\n",
            "As you will see in Chapter 9, k-means is a clustering algorithm that locates clusters in\n",
            "the data. How many it searches for is controlled by the n_clusters hyperparameter.\n",
            "After  training,  the  cluster  centers  are  available  via  the  cluster_centers_  attribute.\n",
            "The fit() method of KMeans supports an optional argument sample_weight, which\n",
            "lets  the  user  specify  the  relative  weights  of  the  samples.  k-means  is  a  stochastic\n",
            "algorithm,  meaning  that  it  relies  on  randomness  to  locate  the  clusters,  so  if  you\n",
            "want reproducible results, you must set the random_state parameter. As you can see,\n",
            "despite the complexity of the task, the code is fairly straightforward. Now let’s use this\n",
            "custom transformer:\n",
            "\n",
            "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
            "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
            "                                           sample_weight=housing_labels)\n",
            "\n",
            "This code creates a ClusterSimilarity transformer, setting the number of clusters to\n",
            "10. Then it calls fit_transform() with the latitude and longitude of every district in\n",
            "the training set, weighting each district by its median house value. The transformer\n",
            "uses  k-means  to  locate  the  clusters,  then  measures  the  Gaussian  RBF  similarity\n",
            "between each district and all 10 cluster centers. The result is a matrix with one row\n",
            "per district, and one column per cluster. Let’s look at the first three rows, rounding to\n",
            "two decimal places:\n",
            "\n",
            "82 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\f>>> similarities[:3].round(2)\n",
            "array([[0.  , 0.14, 0.  , 0.  , 0.  , 0.08, 0.  , 0.99, 0.  , 0.6 ],\n",
            "       [0.63, 0.  , 0.99, 0.  , 0.  , 0.  , 0.04, 0.  , 0.11, 0.  ],\n",
            "       [0.  , 0.29, 0.  , 0.  , 0.01, 0.44, 0.  , 0.7 , 0.  , 0.3 ]])\n",
            "\n",
            "Figure 2-19 shows the 10 cluster centers found by k-means. The districts are colored\n",
            "according to their geographic similarity to their closest cluster center. As you can see,\n",
            "most clusters are located in highly populated and expensive areas.\n",
            "\n",
            "Figure 2-19. Gaussian RBF similarity to the nearest cluster center\n",
            "\n",
            "Transformation Pipelines\n",
            "As  you  can  see,  there  are  many  data  transformation  steps  that  need  to  be  executed\n",
            "in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\n",
            "such sequences of transformations. Here is a small pipeline for numerical attributes,\n",
            "which will first impute then scale the input features:\n",
            "\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "num_pipeline = Pipeline([\n",
            "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
            "    (\"standardize\", StandardScaler()),\n",
            "])\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "83\n",
            "\n",
            "\fThe  Pipeline  constructor  takes  a  list  of  name/estimator  pairs  (2-tuples)  defining  a\n",
            "sequence  of  steps.  The  names  can  be  anything  you  like,  as  long  as  they  are  unique\n",
            "and don’t contain double underscores (__). They will be useful later, when we discuss\n",
            "hyperparameter  tuning.  The  estimators  must  all  be  transformers  (i.e.,  they  must\n",
            "have a fit_transform() method), except for the last one, which can be anything: a\n",
            "transformer, a predictor, or any other type of estimator.\n",
            "\n",
            "In  a  Jupyter  notebook,  if  you  import  sklearn  and  run  sklearn.\n",
            "set_config(display=\"diagram\"),  all  Scikit-Learn  estimators  will\n",
            "be rendered as interactive diagrams. This is particularly useful for\n",
            "visualizing  pipelines.  To  visualize  num_pipeline,  run  a  cell  with\n",
            "num_pipeline as the last line. Clicking an estimator will show more\n",
            "details.\n",
            "\n",
            "If you don’t want to name the transformers, you can use the make_pipeline() func‐\n",
            "tion  instead;  it  takes  transformers  as  positional  arguments  and  creates  a  Pipeline\n",
            "using  the  names  of  the  transformers’  classes,  in  lowercase  and  without  underscores\n",
            "(e.g., \"simpleimputer\"):\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
            "\n",
            "If  multiple  transformers  have  the  same  name,  an  index  is  appended  to  their  names\n",
            "(e.g., \"foo-1\", \"foo-2\", etc.).\n",
            "\n",
            "When you call the pipeline’s fit() method, it calls fit_transform() sequentially on\n",
            "all the transformers, passing the output of each call as the parameter to the next call\n",
            "until it reaches the final estimator, for which it just calls the fit() method.\n",
            "\n",
            "The  pipeline  exposes  the  same  methods  as  the  final  estimator.  In  this  example  the\n",
            "last estimator is a  StandardScaler, which is a transformer, so the pipeline also acts\n",
            "like a transformer. If you call the pipeline’s transform() method, it will sequentially\n",
            "apply all the transformations to the data. If the last estimator were a predictor instead\n",
            "of  a  transformer,  then  the  pipeline  would  have  a  predict()  method  rather  than  a\n",
            "transform()  method.  Calling  it  would  sequentially  apply  all  the  transformations  to\n",
            "the data and pass the result to the predictor’s predict() method.\n",
            "\n",
            "Let’s  call  the  pipeline’s  fit_transform()  method  and  look  at  the  output’s  first  two\n",
            "rows, rounded to two decimal places:\n",
            "\n",
            ">>> housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
            ">>> housing_num_prepared[:2].round(2)\n",
            "array([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n",
            "       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\n",
            "\n",
            "84 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fAs you saw earlier, if you want to recover a nice DataFrame, you can use the pipeline’s\n",
            "get_feature_names_out() method:\n",
            "\n",
            "df_housing_num_prepared = pd.DataFrame(\n",
            "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
            "    index=housing_num.index)\n",
            "\n",
            "Pipelines  support  indexing;  for  example,  pipeline[1]  returns  the  second  estimator\n",
            "in the pipeline, and pipeline[:-1] returns a Pipeline object containing all but the\n",
            "last estimator. You can also access the estimators via the  steps attribute, which is a\n",
            "list of name/estimator pairs, or via the named_steps dictionary attribute, which maps\n",
            "the names to the estimators. For example, num_pipeline[\"simpleimputer\"] returns\n",
            "the estimator named \"simpleimputer\".\n",
            "\n",
            "So  far,  we  have  handled  the  categorical  columns  and  the  numerical  columns  sepa‐\n",
            "rately. It would be more convenient to have a single transformer capable of handling\n",
            "all  columns,  applying  the  appropriate  transformations  to  each  column.  For  this,\n",
            "you  can  use  a  ColumnTransformer.  For  example,  the  following  ColumnTransformer\n",
            "will  apply  num_pipeline  (the  one  we  just  defined)  to  the  numerical  attributes  and\n",
            "cat_pipeline to the categorical attribute:\n",
            "\n",
            "from sklearn.compose import ColumnTransformer\n",
            "\n",
            "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
            "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
            "cat_attribs = [\"ocean_proximity\"]\n",
            "\n",
            "cat_pipeline = make_pipeline(\n",
            "    SimpleImputer(strategy=\"most_frequent\"),\n",
            "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
            "\n",
            "preprocessing = ColumnTransformer([\n",
            "    (\"num\", num_pipeline, num_attribs),\n",
            "    (\"cat\", cat_pipeline, cat_attribs),\n",
            "])\n",
            "\n",
            "First  we  import  the  ColumnTransformer  class,  then  we  define  the  list  of  numeri‐\n",
            "cal  and  categorical  column  names  and  construct  a  simple  pipeline  for  categorical\n",
            "attributes. Lastly, we construct a ColumnTransformer. Its constructor requires a list of\n",
            "triplets  (3-tuples),  each  containing  a  name  (which  must  be  unique  and  not  contain\n",
            "double underscores), a transformer, and a list of names (or indices) of columns that\n",
            "the transformer should be applied to.\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "85\n",
            "\n",
            "\fInstead  of  using  a  transformer,  you  can  specify  the  string  \"drop\"\n",
            "if  you  want  the  columns  to  be  dropped,  or  you  can  specify\n",
            "\"passthrough\"  if  you  want  the  columns  to  be  left  untouched.  By\n",
            "default, the remaining columns (i.e., the ones that were not listed)\n",
            "will be dropped, but you can set the remainder hyperparameter to\n",
            "any transformer (or to \"passthrough\") if you want these columns\n",
            "to be handled differently.\n",
            "\n",
            "Since  listing  all  the  column  names  is  not  very  convenient,  Scikit-Learn  provides\n",
            "a  make_column_selector()  function  that  returns  a  selector  function  you  can  use\n",
            "to  automatically  select  all  the  features  of  a  given  type,  such  as  numerical  or  cate‐\n",
            "gorical.  You  can  pass  this  selector  function  to  the  ColumnTransformer  instead  of\n",
            "column names or indices. Moreover, if you don’t care about naming the transform‐\n",
            "ers,  you  can  use  make_column_transformer(),  which  chooses  the  names  for  you,\n",
            "just  like  make_pipeline()  does.  For  example,  the  following  code  creates  the  same\n",
            "ColumnTransformer  as  earlier,  except  the  transformers  are  automatically  named\n",
            "\"pipeline-1\" and \"pipeline-2\" instead of \"num\" and \"cat\":\n",
            "\n",
            "from sklearn.compose import make_column_selector, make_column_transformer\n",
            "\n",
            "preprocessing = make_column_transformer(\n",
            "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
            "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
            ")\n",
            "\n",
            "Now we’re ready to apply this ColumnTransformer to the housing data:\n",
            "\n",
            "housing_prepared = preprocessing.fit_transform(housing)\n",
            "\n",
            "Great!  We  have  a  preprocessing  pipeline  that  takes  the  entire  training  dataset  and\n",
            "applies  each  transformer  to  the  appropriate  columns,  then  concatenates  the  trans‐\n",
            "formed columns horizontally (transformers must never change the number of rows).\n",
            "Once  again  this  returns  a  NumPy  array,  but  you  can  get  the  column  names  using\n",
            "preprocessing.get_feature_names_out()  and  wrap  the  data  in  a  nice  DataFrame\n",
            "as we did before.\n",
            "\n",
            "The OneHotEncoder returns a sparse matrix and the num_pipeline\n",
            "returns  a  dense  matrix.  When  there  is  such  a  mix  of  sparse  and\n",
            "dense matrices, the ColumnTransformer estimates the density of the\n",
            "final matrix (i.e., the ratio of nonzero cells), and it returns a sparse\n",
            "matrix  if  the  density  is  lower  than  a  given  threshold  (by  default,\n",
            "sparse_threshold=0.3). In this example, it returns a dense matrix.\n",
            "\n",
            "Your project is going really well and you’re almost ready to train some models! You\n",
            "now want to create a single pipeline that will perform all the transformations you’ve\n",
            "experimented with up to now. Let’s recap what the pipeline will do and why:\n",
            "\n",
            "86 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\f• Missing  values  in  numerical  features  will  be  imputed  by  replacing  them  with\n",
            "•\n",
            "the  median,  as  most  ML  algorithms  don’t  expect  missing  values.  In  categorical\n",
            "features, missing values will be replaced by the most frequent category.\n",
            "\n",
            "•\n",
            "• The  categorical  feature  will  be  one-hot  encoded,  as  most  ML  algorithms  only\n",
            "\n",
            "accept numerical inputs.\n",
            "\n",
            "•\n",
            "• A \n",
            "\n",
            "few  ratio \n",
            "\n",
            "features  will  be  computed  and  added:  bedrooms_ratio,\n",
            "rooms_per_house,  and  people_per_house.  Hopefully  these  will  better  correlate\n",
            "with the median house value, and thereby help the ML models.\n",
            "\n",
            "•\n",
            "• A  few  cluster  similarity  features  will  also  be  added.  These  will  likely  be  more\n",
            "\n",
            "useful to the model than latitude and longitude.\n",
            "\n",
            "•\n",
            "• Features  with  a  long  tail  will  be  replaced  by  their  logarithm,  as  most  models\n",
            "\n",
            "prefer features with roughly uniform or Gaussian distributions.\n",
            "\n",
            "•\n",
            "• All numerical features will be standardized, as most ML algorithms prefer when\n",
            "\n",
            "all features have roughly the same scale.\n",
            "\n",
            "The code that builds the pipeline to do all of this should look familiar to you by now:\n",
            "\n",
            "def column_ratio(X):\n",
            "    return X[:, [0]] / X[:, [1]]\n",
            "\n",
            "def ratio_name(function_transformer, feature_names_in):\n",
            "    return [\"ratio\"]  # feature names out\n",
            "\n",
            "def ratio_pipeline():\n",
            "    return make_pipeline(\n",
            "        SimpleImputer(strategy=\"median\"),\n",
            "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
            "        StandardScaler())\n",
            "\n",
            "log_pipeline = make_pipeline(\n",
            "    SimpleImputer(strategy=\"median\"),\n",
            "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
            "    StandardScaler())\n",
            "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
            "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
            "                                     StandardScaler())\n",
            "preprocessing = ColumnTransformer([\n",
            "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
            "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
            "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
            "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
            "                               \"households\", \"median_income\"]),\n",
            "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
            "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
            "    ],\n",
            "    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n",
            "\n",
            "Prepare the Data for Machine Learning Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "87\n",
            "\n",
            "\fIf you run this ColumnTransformer, it performs all the transformations and outputs a\n",
            "NumPy array with 24 features:\n",
            "\n",
            ">>> housing_prepared = preprocessing.fit_transform(housing)\n",
            ">>> housing_prepared.shape\n",
            "(16512, 24)\n",
            ">>> preprocessing.get_feature_names_out()\n",
            "array(['bedrooms__ratio', 'rooms_per_house__ratio',\n",
            "       'people_per_house__ratio', 'log__total_bedrooms',\n",
            "       'log__total_rooms', 'log__population', 'log__households',\n",
            "       'log__median_income', 'geo__Cluster 0 similarity', [...],\n",
            "       'geo__Cluster 9 similarity', 'cat__ocean_proximity_<1H OCEAN',\n",
            "       'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',\n",
            "       'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',\n",
            "       'remainder__housing_median_age'], dtype=object)\n",
            "\n",
            "Select and Train a Model\n",
            "At  last!  You  framed  the  problem,  you  got  the  data  and  explored  it,  you  sampled  a\n",
            "training  set  and  a  test  set,  and  you  wrote  a  preprocessing  pipeline  to  automatically\n",
            "clean up and prepare your data for machine learning algorithms. You are now ready\n",
            "to select and train a machine learning model.\n",
            "\n",
            "Train and Evaluate on the Training Set\n",
            "The good news is that thanks to all these previous steps, things are now going to be\n",
            "easy! You decide to train a very basic linear regression model to get started:\n",
            "\n",
            "from sklearn.linear_model import LinearRegression\n",
            "\n",
            "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
            "lin_reg.fit(housing, housing_labels)\n",
            "\n",
            "Done! You now have a working linear regression model. You try it out on the training\n",
            "set, looking at the first five predictions and comparing them to the labels:\n",
            "\n",
            ">>> housing_predictions = lin_reg.predict(housing)\n",
            ">>> housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\n",
            "array([243700., 372400., 128800.,  94400., 328300.])\n",
            ">>> housing_labels.iloc[:5].values\n",
            "array([458300., 483800., 101700.,  96100., 361800.])\n",
            "\n",
            "Well,  it  works,  but  not  always:  the  first  prediction  is  way  off  (by  over  $200,000!),\n",
            "while  the  other  predictions  are  better:  two  are  off  by  about  25%,  and  two  are  off\n",
            "by  less  than  10%.  Remember  that  you  chose  to  use  the  RMSE  as  your  performance\n",
            "measure, so you want to measure this regression model’s RMSE on the whole training\n",
            "set using Scikit-Learn’s mean_squared_error() function, with the squared argument\n",
            "set to False:\n",
            "\n",
            "88 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\f>>> from sklearn.metrics import mean_squared_error\n",
            ">>> lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
            "...                               squared=False)\n",
            "...\n",
            ">>> lin_rmse\n",
            "68687.89176589991\n",
            "\n",
            "This is better than nothing, but clearly not a great score: the median_housing_values\n",
            "of most districts range between $120,000 and $265,000, so a typical prediction error\n",
            "of $68,628 is really not very satisfying. This is an example of a model underfitting the\n",
            "training data. When this happens it can mean that the features do not provide enough\n",
            "information  to  make  good  predictions,  or  that  the  model  is  not  powerful  enough.\n",
            "As  we  saw  in  the  previous  chapter,  the  main  ways  to  fix  underfitting  are  to  select  a\n",
            "more powerful model, to feed the training algorithm with better features, or to reduce\n",
            "the constraints on the model. This model is not regularized, which rules out the last\n",
            "option. You could try to add more features, but first you want to try a more complex\n",
            "model to see how it does.\n",
            "\n",
            "You decide to try a DecisionTreeRegressor, as this is a fairly powerful model capable\n",
            "of finding complex nonlinear relationships in the data (decision trees are presented in\n",
            "more detail in Chapter 6):\n",
            "\n",
            "from sklearn.tree import DecisionTreeRegressor\n",
            "\n",
            "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
            "tree_reg.fit(housing, housing_labels)\n",
            "\n",
            "Now that the model is trained, you evaluate it on the training set:\n",
            "\n",
            ">>> housing_predictions = tree_reg.predict(housing)\n",
            ">>> tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
            "...                                squared=False)\n",
            "...\n",
            ">>> tree_rmse\n",
            "0.0\n",
            "\n",
            "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
            "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
            "As you saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
            "model you are confident about, so you need to use part of the training set for training\n",
            "and part of it for model validation.\n",
            "\n",
            "Better Evaluation Using Cross-Validation\n",
            "One  way  to  evaluate  the  decision  tree  model  would  be  to  use  the  train_\n",
            "test_split()  function  to  split  the  training  set  into  a  smaller  training  set  and  a\n",
            "validation  set,  then  train  your  models  against  the  smaller  training  set  and  evaluate\n",
            "them  against  the  validation  set.  It’s  a  bit  of  effort,  but  nothing  too  difficult,  and  it\n",
            "would work fairly well.\n",
            "\n",
            "Select and Train a Model \n",
            "\n",
            "| \n",
            "\n",
            "89\n",
            "\n",
            "\fA great alternative is to use Scikit-Learn’s k_-fold cross-validation feature. The follow‐\n",
            "ing code randomly splits the training set into 10 nonoverlapping subsets called folds,\n",
            "then it trains and evaluates the decision tree model 10 times, picking a different fold\n",
            "for  evaluation  every  time  and  using  the  other  9  folds  for  training.  The  result  is  an\n",
            "array containing the 10 evaluation scores:\n",
            "\n",
            "from sklearn.model_selection import cross_val_score\n",
            "\n",
            "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
            "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
            "\n",
            "Scikit-Learn’s  cross-validation  features  expect  a  utility  function\n",
            "(greater  is  better)  rather  than  a  cost  function  (lower  is  better),  so\n",
            "the  scoring  function  is  actually  the  opposite  of  the  RMSE.  It’s  a\n",
            "negative value, so you need to switch the sign of the output to get\n",
            "the RMSE scores.\n",
            "\n",
            "Let’s look at the results:\n",
            "\n",
            ">>> pd.Series(tree_rmses).describe()\n",
            "count       10.000000\n",
            "mean     66868.027288\n",
            "std       2060.966425\n",
            "min      63649.536493\n",
            "25%      65338.078316\n",
            "50%      66801.953094\n",
            "75%      68229.934454\n",
            "max      70094.778246\n",
            "dtype: float64\n",
            "\n",
            "Now  the  decision  tree  doesn’t  look  as  good  as  it  did  earlier.  In  fact,  it  seems  to\n",
            "perform almost as poorly as the linear regression model! Notice that cross-validation\n",
            "allows you to get not only an estimate of the performance of your model, but also a\n",
            "measure of how precise this estimate is (i.e., its standard deviation). The decision tree\n",
            "has an RMSE of about 66,868, with a standard deviation of about 2,061. You would\n",
            "not  have  this  information  if  you  just  used  one  validation  set.  But  cross-validation\n",
            "comes at the cost of training the model several times, so it is not always feasible.\n",
            "\n",
            "If  you  compute  the  same  metric  for  the  linear  regression  model,  you  will  find  that\n",
            "the  mean  RMSE  is  69,858  and  the  standard  deviation  is  4,182.  So  the  decision  tree\n",
            "model seems to perform very slightly better than the linear model, but the difference\n",
            "is minimal due to severe overfitting. We know there’s an overfitting problem because\n",
            "the training error is low (actually zero) while the validation error is high.\n",
            "\n",
            "Let’s try one last model now: the RandomForestRegressor. As you will see in Chap‐\n",
            "ter  7,  random  forests  work  by  training  many  decision  trees  on  random  subsets  of\n",
            "the  features,  then  averaging  out  their  predictions.  Such  models  composed  of  many\n",
            "other  models  are  called  ensembles:  they  are  capable  of  boosting  the  performance  of\n",
            "\n",
            "90 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fthe  underlying  model  (in  this  case,  decision  trees).  The  code  is  much  the  same  as\n",
            "earlier:\n",
            "\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "\n",
            "forest_reg = make_pipeline(preprocessing,\n",
            "                           RandomForestRegressor(random_state=42))\n",
            "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
            "                                scoring=\"neg_root_mean_squared_error\", cv=10)\n",
            "\n",
            "Let’s look at the scores:\n",
            "\n",
            ">>> pd.Series(forest_rmses).describe()\n",
            "count       10.000000\n",
            "mean     47019.561281\n",
            "std       1033.957120\n",
            "min      45458.112527\n",
            "25%      46464.031184\n",
            "50%      46967.596354\n",
            "75%      47325.694987\n",
            "max      49243.765795\n",
            "dtype: float64\n",
            "\n",
            "Wow,  this  is  much  better:  random  forests  really  look  very  promising  for  this  task!\n",
            "However,  if  you  train  a  RandomForest  and  measure  the  RMSE  on  the  training  set,\n",
            "you  will  find  roughly  17,474:  that’s  much  lower,  meaning  that  there’s  still  quite  a\n",
            "lot  of  overfitting  going  on.  Possible  solutions  are  to  simplify  the  model,  constrain\n",
            "it  (i.e.,  regularize  it),  or  get  a  lot  more  training  data.  Before  you  dive  much  deeper\n",
            "into  random  forests,  however,  you  should  try  out  many  other  models  from  various\n",
            "categories of machine learning algorithms (e.g., several support vector machines with\n",
            "different  kernels,  and  possibly  a  neural  network),  without  spending  too  much  time\n",
            "tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising\n",
            "models.\n",
            "\n",
            "Fine-Tune Your Model\n",
            "Let’s  assume  that  you  now  have  a  shortlist  of  promising  models.  You  now  need  to\n",
            "fine-tune them. Let’s look at a few ways you can do that.\n",
            "\n",
            "Grid Search\n",
            "One option would be to fiddle with the hyperparameters manually, until you find a\n",
            "great combination of hyperparameter values. This would be very tedious work, and\n",
            "you may not have time to explore many combinations.\n",
            "\n",
            "Instead, you can use Scikit-Learn’s GridSearchCV class to search for you. All you need\n",
            "to do is tell it which hyperparameters you want it to experiment with and what values\n",
            "to  try  out,  and  it  will  use  cross-validation  to  evaluate  all  the  possible  combinations\n",
            "\n",
            "Fine-Tune Your Model \n",
            "\n",
            "| \n",
            "\n",
            "91\n",
            "\n",
            "\fof  hyperparameter  values.  For  example,  the  following  code  searches  for  the  best\n",
            "combination of hyperparameter values for the RandomForestRegressor:\n",
            "\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "\n",
            "full_pipeline = Pipeline([\n",
            "    (\"preprocessing\", preprocessing),\n",
            "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
            "])\n",
            "param_grid = [\n",
            "    {'preprocessing__geo__n_clusters': [5, 8, 10],\n",
            "     'random_forest__max_features': [4, 6, 8]},\n",
            "    {'preprocessing__geo__n_clusters': [10, 15],\n",
            "     'random_forest__max_features': [6, 8, 10]},\n",
            "]\n",
            "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
            "                           scoring='neg_root_mean_squared_error')\n",
            "grid_search.fit(housing, housing_labels)\n",
            "\n",
            "Notice  that  you  can  refer  to  any  hyperparameter  of  any  estimator  in  a  pipeline,\n",
            "even if this estimator is nested deep inside several pipelines and column transform‐\n",
            "ers.  For  example,  when  Scikit-Learn  sees  \"preprocessing__geo__n_clusters\",  it\n",
            "splits  this  string  at  the  double  underscores,  then  it  looks  for  an  estimator  named\n",
            "\"preprocessing\"  in  the  pipeline  and  finds  the  preprocessing  ColumnTransformer.\n",
            "Next,  it  looks  for  a  transformer  named  \"geo\"  inside  this  ColumnTransformer  and\n",
            "finds  the  ClusterSimilarity  transformer  we  used  on  the  latitude  and  longitude\n",
            "attributes.  Then  it  finds  this  transformer’s  n_clusters  hyperparameter.  Similarly,\n",
            "random_forest__max_features  refers  to  the  max_features  hyperparameter  of  the\n",
            "estimator named \"random_forest\", which is of course the RandomForest model (the\n",
            "max_features hyperparameter will be explained in Chapter 7).\n",
            "\n",
            "Wrapping preprocessing steps in a Scikit-Learn pipeline allows you\n",
            "to  tune  the  preprocessing  hyperparameters  along  with  the  model\n",
            "hyperparameters.  This  is  a  good  thing  since  they  often  interact.\n",
            "For  example,  perhaps  increasing  n_clusters  requires  increasing\n",
            "max_features  as  well.  If  fitting  the  pipeline  transformers  is  com‐\n",
            "putationally expensive, you can set the pipeline’s memory hyperpara‐\n",
            "meter  to  the  path  of  a  caching  directory:  when  you  first  fit  the\n",
            "pipeline, Scikit-Learn will save the fitted transformers to this direc‐\n",
            "tory. If you then fit the pipeline again with the same hyperparame‐\n",
            "ters, Scikit-Learn will just load the cached transformers.\n",
            "\n",
            "There  are  two  dictionaries  in  this  param_grid,  so  GridSearchCV  will  first  evaluate\n",
            "all 3 × 3 = 9 combinations of n_clusters and max_features hyperparameter values\n",
            "specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparame‐\n",
            "ter  values  in  the  second  dict.  So  in  total  the  grid  search  will  explore  9  +  6  =  15\n",
            "\n",
            "92 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fcombinations  of  hyperparameter  values,  and  it  will  train  the  pipeline  3  times  per\n",
            "combination,  since  we  are  using  3-fold  cross  validation.  This  means  there  will  be  a\n",
            "grand total of 15 × 3 = 45 rounds of training! It may take a while, but when it is done\n",
            "you can get the best combination of parameters like this:\n",
            "\n",
            ">>> grid_search.best_params_\n",
            "{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}\n",
            "\n",
            "In this example, the best model is obtained by setting n_clusters to 15 and setting\n",
            "max_features to 8.\n",
            "\n",
            "Since 15 is the maximum value that was evaluated for n_clusters,\n",
            "you  should  probably  try  searching  again  with  higher  values;  the\n",
            "score may continue to improve.\n",
            "\n",
            "You  can  access  the  best  estimator  using  grid_search.best_estimator_.  If\n",
            "GridSearchCV is initialized with refit=True (which is the default), then once it finds\n",
            "the best estimator using cross-validation, it retrains it on the whole training set. This\n",
            "is usually a good idea, since feeding it more data will likely improve its performance.\n",
            "\n",
            "The evaluation scores are available using grid_search.cv_results_. This is a dictio‐\n",
            "nary,  but  if  you  wrap  it  in  a  DataFrame  you  get  a  nice  list  of  all  the  test  scores  for\n",
            "each  combination  of  hyperparameters  and  for  each  cross-validation  split,  as  well  as\n",
            "the mean test score across all splits:\n",
            "\n",
            ">>> cv_res = pd.DataFrame(grid_search.cv_results_)\n",
            ">>> cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
            ">>> [...]  # change column names to fit on this page, and show rmse = -score\n",
            ">>> cv_res.head()  # note: the 1st column is the row ID\n",
            "   n_clusters max_features  split0  split1  split2  mean_test_rmse\n",
            "12         15            6   43460   43919   44748           44042\n",
            "13         15            8   44132   44075   45010           44406\n",
            "14         15           10   44374   44286   45316           44659\n",
            "7          10            6   44683   44655   45657           44999\n",
            "9          10            6   44683   44655   45657           44999\n",
            "\n",
            "The  mean  test  RMSE  score  for  the  best  model  is  44,042,  which  is  better  than  the\n",
            "score  you  got  earlier  using  the  default  hyperparameter  values  (which  was  47,019).\n",
            "Congratulations, you have successfully fine-tuned your best model!\n",
            "\n",
            "Randomized Search\n",
            "The grid search approach is fine when you are exploring relatively few combinations,\n",
            "like in the previous example, but RandomizedSearchCV is often preferable, especially\n",
            "when  the  hyperparameter  search  space  is  large.  This  class  can  be  used  in  much\n",
            "the  same  way  as  the  GridSearchCV  class,  but  instead  of  trying  out  all  possible\n",
            "\n",
            "Fine-Tune Your Model \n",
            "\n",
            "| \n",
            "\n",
            "93\n",
            "\n",
            "\fcombinations it evaluates a fixed number of combinations, selecting a random value\n",
            "for  each  hyperparameter  at  every  iteration.  This  may  sound  surprising,  but  this\n",
            "approach has several benefits:\n",
            "\n",
            "• If  some  of  your  hyperparameters  are  continuous  (or  discrete  but  with  many\n",
            "•\n",
            "possible values), and you let randomized search run for, say, 1,000 iterations, then\n",
            "it will explore 1,000 different values for each of these hyperparameters, whereas\n",
            "grid search would only explore the few values you listed for each one.\n",
            "\n",
            "• Suppose a hyperparameter does not actually make much difference, but you don’t\n",
            "•\n",
            "know it yet. If it has 10 possible values and you add it to your grid search, then\n",
            "training will take 10 times longer. But if you add it to a random search, it will not\n",
            "make any difference.\n",
            "\n",
            "• If there are 6 hyperparameters to explore, each with 10 possible values, then grid\n",
            "•\n",
            "search  offers  no  other  choice  than  training  the  model  a  million  times,  whereas\n",
            "random search can always run for any number of iterations you choose.\n",
            "\n",
            "For  each  hyperparameter,  you  must  provide  either  a  list  of  possible  values,  or  a\n",
            "probability distribution:\n",
            "\n",
            "from sklearn.model_selection import RandomizedSearchCV\n",
            "from scipy.stats import randint\n",
            "\n",
            "param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
            "                  'random_forest__max_features': randint(low=2, high=20)}\n",
            "\n",
            "rnd_search = RandomizedSearchCV(\n",
            "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
            "    scoring='neg_root_mean_squared_error', random_state=42)\n",
            "\n",
            "rnd_search.fit(housing, housing_labels)\n",
            "\n",
            "Scikit-Learn also has HalvingRandomSearchCV and HalvingGridSearchCV hyperpara‐\n",
            "meter search classes. Their goal is to use the computational resources more efficiently,\n",
            "either  to  train  faster  or  to  explore  a  larger  hyperparameter  space.  Here’s  how  they\n",
            "work: in the first round, many hyperparameter combinations (called “candidates”) are\n",
            "generated using either the grid approach or the random approach. These candidates\n",
            "are  then  used  to  train  models  that  are  evaluated  using  cross-validation,  as  usual.\n",
            "However, training uses limited resources, which speeds up this first round consider‐\n",
            "ably.  By  default,  “limited  resources”  means  that  the  models  are  trained  on  a  small\n",
            "part  of  the  training  set.  However,  other  limitations  are  possible,  such  as  reducing\n",
            "the  number  of  training  iterations  if  the  model  has  a  hyperparameter  to  set  it.  Once\n",
            "every  candidate  has  been  evaluated,  only  the  best  ones  go  on  to  the  second  round,\n",
            "where  they  are  allowed  more  resources  to  compete.  After  several  rounds,  the  final\n",
            "candidates  are  evaluated  using  full  resources.  This  may  save  you  some  time  tuning\n",
            "hyperparameters.\n",
            "\n",
            "94 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fEnsemble Methods\n",
            "Another way to fine-tune your system is to try to combine the models that perform\n",
            "best.  The  group  (or  “ensemble”)  will  often  perform  better  than  the  best  individual\n",
            "model—just  like  random  forests  perform  better  than  the  individual  decision  trees\n",
            "they rely on—especially if the individual models make very different types of errors.\n",
            "For example, you could train and fine-tune a k-nearest neighbors model, then create\n",
            "an ensemble model that just predicts the mean of the random forest prediction and\n",
            "that model’s prediction. We will cover this topic in more detail in Chapter 7.\n",
            "\n",
            "Analyzing the Best Models and Their Errors\n",
            "You will often gain good insights on the problem by inspecting the best models. For\n",
            "example,  the  RandomForestRegressor  can  indicate  the  relative  importance  of  each\n",
            "attribute for making accurate predictions:\n",
            "\n",
            ">>> final_model = rnd_search.best_estimator_  # includes preprocessing\n",
            ">>> feature_importances = final_model[\"random_forest\"].feature_importances_\n",
            ">>> feature_importances.round(2)\n",
            "array([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, [...], 0.01])\n",
            "\n",
            "Let’s sort these importance scores in descending order and display them next to their\n",
            "corresponding attribute names:\n",
            "\n",
            ">>> sorted(zip(feature_importances,\n",
            "...            final_model[\"preprocessing\"].get_feature_names_out()),\n",
            "...            reverse=True)\n",
            "...\n",
            "[(0.18694559869103852, 'log__median_income'),\n",
            " (0.0748194905715524, 'cat__ocean_proximity_INLAND'),\n",
            " (0.06926417748515576, 'bedrooms__ratio'),\n",
            " (0.05446998753775219, 'rooms_per_house__ratio'),\n",
            " (0.05262301809680712, 'people_per_house__ratio'),\n",
            " (0.03819415873915732, 'geo__Cluster 0 similarity'),\n",
            " [...]\n",
            " (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),\n",
            " (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]\n",
            "\n",
            "With this information, you may want to try dropping some of the less useful features\n",
            "(e.g., apparently only one ocean_proximity category is really useful, so you could try\n",
            "dropping the others).\n",
            "\n",
            "The  sklearn.feature_selection.SelectFromModel  transformer\n",
            "can automatically drop the least useful features for you: when you\n",
            "fit it, it trains a model (typically a random forest), looks at its fea\n",
            "ture_importances_ attribute, and selects the most useful features.\n",
            "Then when you call transform(), it drops the other features.\n",
            "\n",
            "Fine-Tune Your Model \n",
            "\n",
            "| \n",
            "\n",
            "95\n",
            "\n",
            "\fYou  should  also  look  at  the  specific  errors  that  your  system  makes,  then  try  to\n",
            "understand why it makes them and what could fix the problem: adding extra features\n",
            "or getting rid of uninformative ones, cleaning up outliers, etc.\n",
            "\n",
            "Now is also a good time to ensure that your model not only works well on average,\n",
            "but  also  on  all  categories  of  districts,  whether  they’re  rural  or  urban,  rich  or  poor,\n",
            "northern or southern, minority or not, etc. Creating subsets of your validation set for\n",
            "each category takes a bit of work, but it’s important: if your model performs poorly on\n",
            "a whole category of districts, then it should probably not be deployed until the issue\n",
            "is solved, or at least it should not be used to make predictions for that category, as it\n",
            "may do more harm than good.\n",
            "\n",
            "Evaluate Your System on the Test Set\n",
            "After tweaking your models for a while, you eventually have a system that performs\n",
            "sufficiently  well.  You  are  ready  to  evaluate  the  final  model  on  the  test  set.  There  is\n",
            "nothing  special  about  this  process;  just  get  the  predictors  and  the  labels  from  your\n",
            "test set and run your final_model to transform the data and make predictions, then\n",
            "evaluate these predictions:\n",
            "\n",
            "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
            "y_test = strat_test_set[\"median_house_value\"].copy()\n",
            "\n",
            "final_predictions = final_model.predict(X_test)\n",
            "\n",
            "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
            "print(final_rmse)  # prints 41424.40026462184\n",
            "\n",
            "In  some  cases,  such  a  point  estimate  of  the  generalization  error  will  not  be  quite\n",
            "enough  to  convince  you  to  launch:  what  if  it  is  just  0.1%  better  than  the  model\n",
            "currently in production? You might want to have an idea of how precise this estimate\n",
            "is.  For  this,  you  can  compute  a  95%  confidence  interval  for  the  generalization  error\n",
            "using  scipy.stats.t.interval().  You  get  a  fairly  large  interval  from  39,275  to\n",
            "43,467, and your previous point estimate of 41,424 is roughly in the middle of it:\n",
            "\n",
            ">>> from scipy import stats\n",
            ">>> confidence = 0.95\n",
            ">>> squared_errors = (final_predictions - y_test) ** 2\n",
            ">>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
            "...                          loc=squared_errors.mean(),\n",
            "...                          scale=stats.sem(squared_errors)))\n",
            "...\n",
            "array([39275.40861216, 43467.27680583])\n",
            "\n",
            "96 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fIf  you  did  a  lot  of  hyperparameter  tuning,  the  performance  will  usually  be  slightly\n",
            "worse  than  what  you  measured  using  cross-validation.  That’s  because  your  system\n",
            "ends up fine-tuned to perform well on the validation data and will likely not perform\n",
            "as  well  on  unknown  datasets.  That’s  not  the  case  in  this  example  since  the  test\n",
            "RMSE  is  lower  than  the  validation  RMSE,  but  when  it  happens  you  must  resist  the\n",
            "temptation to tweak the hyperparameters to make the numbers look good on the test\n",
            "set; the improvements would be unlikely to generalize to new data.\n",
            "\n",
            "Now  comes  the  project  prelaunch  phase:  you  need  to  present  your  solution  (high‐\n",
            "lighting  what  you  have  learned,  what  worked  and  what  did  not,  what  assumptions\n",
            "were made, and what your system’s limitations are), document everything, and create\n",
            "nice  presentations  with  clear  visualizations  and  easy-to-remember  statements  (e.g.,\n",
            "“the median income is the number one predictor of housing prices”). In this Califor‐\n",
            "nia housing example, the final performance of the system is not much better than the\n",
            "experts’ price estimates, which were often off by 30%, but it may still be a good idea\n",
            "to launch it, especially if this frees up some time for the experts so they can work on\n",
            "more interesting and productive tasks.\n",
            "\n",
            "Launch, Monitor, and Maintain Your System\n",
            "Perfect,  you  got  approval  to  launch!  You  now  need  to  get  your  solution  ready  for\n",
            "production  (e.g.,  polish  the  code,  write  documentation  and  tests,  and  so  on).  Then\n",
            "you can deploy your model to your production environment. The most basic way to\n",
            "do this is just to save the best model you trained, transfer the file to your production\n",
            "environment, and load it. To save the model, you can use the joblib library like this:\n",
            "\n",
            "import joblib\n",
            "\n",
            "joblib.dump(final_model, \"my_california_housing_model.pkl\")\n",
            "\n",
            "It’s often a good idea to save every model you experiment with so\n",
            "that you can come back easily to any model you want. You may also\n",
            "save the cross-validation scores and perhaps the actual predictions\n",
            "on the validation set. This will allow you to easily compare scores\n",
            "across model types, and compare the types of errors they make.\n",
            "\n",
            "Once your model is transferred to production, you can load it and use it. For this you\n",
            "must first import any custom classes and functions the model relies on (which means\n",
            "transferring the code to production), then load the model using joblib and use it to\n",
            "make predictions:\n",
            "\n",
            "Launch, Monitor, and Maintain Your System \n",
            "\n",
            "| \n",
            "\n",
            "97\n",
            "\n",
            "\fimport joblib\n",
            "[...]  # import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.\n",
            "\n",
            "def column_ratio(X): [...]\n",
            "def ratio_name(function_transformer, feature_names_in): [...]\n",
            "class ClusterSimilarity(BaseEstimator, TransformerMixin): [...]\n",
            "\n",
            "final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n",
            "\n",
            "new_data = [...]  # some new districts to make predictions for\n",
            "predictions = final_model_reloaded.predict(new_data)\n",
            "\n",
            "For  example,  perhaps  the  model  will  be  used  within  a  website:  the  user  will  type  in\n",
            "some  data  about  a  new  district  and  click  the  Estimate  Price  button.  This  will  send\n",
            "a  query  containing  the  data  to  the  web  server,  which  will  forward  it  to  your  web\n",
            "application,  and  finally  your  code  will  simply  call  the  model’s  predict()  method\n",
            "(you want to load the model upon server startup, rather than every time the model\n",
            "is  used).  Alternatively,  you  can  wrap  the  model  within  a  dedicated  web  service  that\n",
            "your web application can query through a REST API13 (see Figure 2-20). This makes\n",
            "it easier to upgrade your model to new versions without interrupting the main appli‐\n",
            "cation. It also simplifies scaling, since you can start as many web services as needed\n",
            "and  load-balance  the  requests  coming  from  your  web  application  across  these  web\n",
            "services. Moreover, it allows your web application to use any programming language,\n",
            "not just Python.\n",
            "\n",
            "Figure 2-20. A model deployed as a web service and used by a web application\n",
            "\n",
            "Another  popular  strategy  is  to  deploy  your  model  to  the  cloud,  for  example  on\n",
            "Google’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud\n",
            "ML  Engine):  just  save  your  model  using  joblib  and  upload  it  to  Google  Cloud\n",
            "Storage (GCS), then head over to Vertex AI and create a new model version, pointing\n",
            "it to the GCS file. That’s it! This gives you a simple web service that takes care of load\n",
            "balancing and scaling for you. It takes JSON requests containing the input data (e.g.,\n",
            "of  a  district)  and  returns  JSON  responses  containing  the  predictions.  You  can  then\n",
            "use  this  web  service  in  your  website  (or  whatever  production  environment  you  are\n",
            "using). As you will see in Chapter 19, deploying TensorFlow models on Vertex AI is\n",
            "not much different from deploying Scikit-Learn models.\n",
            "\n",
            "13 In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions, such as using\n",
            "standard HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and using\n",
            "JSON for the inputs and outputs.\n",
            "\n",
            "98 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fBut  deployment  is  not  the  end  of  the  story.  You  also  need  to  write  monitoring\n",
            "code  to  check  your  system’s  live  performance  at  regular  intervals  and  trigger  alerts\n",
            "when it drops. It may drop very quickly, for example if a component breaks in your\n",
            "infrastructure, but be aware that it could also decay very slowly, which can easily go\n",
            "unnoticed for a long time. This is quite common because of model rot: if the model\n",
            "was trained with last year’s data, it may not be adapted to today’s data.\n",
            "\n",
            "So,  you  need  to  monitor  your  model’s  live  performance.  But  how  do  you  do  that?\n",
            "Well, it depends. In some cases, the model’s performance can be inferred from down‐\n",
            "stream metrics. For example, if your model is part of a recommender system and it\n",
            "suggests  products  that  the  users  may  be  interested  in,  then  it’s  easy  to  monitor  the\n",
            "number  of  recommended  products  sold  each  day.  If  this  number  drops  (compared\n",
            "to  non-recommended  products),  then  the  prime  suspect  is  the  model.  This  may  be\n",
            "because  the  data  pipeline  is  broken,  or  perhaps  the  model  needs  to  be  retrained  on\n",
            "fresh data (as we will discuss shortly).\n",
            "\n",
            "However, you may also need human analysis to assess the model’s performance. For\n",
            "example,  suppose  you  trained  an  image  classification  model  (we’ll  look  at  these  in\n",
            "Chapter 3) to detect various product defects on a production line. How can you get\n",
            "an  alert  if  the  model’s  performance  drops,  before  thousands  of  defective  products\n",
            "get  shipped  to  your  clients?  One  solution  is  to  send  to  human  raters  a  sample  of\n",
            "all  the  pictures  that  the  model  classified  (especially  pictures  that  the  model  wasn’t\n",
            "so  sure  about).  Depending  on  the  task,  the  raters  may  need  to  be  experts,  or  they\n",
            "could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon\n",
            "Mechanical  Turk).  In  some  applications  they  could  even  be  the  users  themselves,\n",
            "responding, for example, via surveys or repurposed captchas.14\n",
            "\n",
            "Either  way,  you  need  to  put  in  place  a  monitoring  system  (with  or  without  human\n",
            "raters to evaluate the live model), as well as all the relevant processes to define what to\n",
            "do in case of failures and how to prepare for them. Unfortunately, this can be a lot of\n",
            "work. In fact, it is often much more work than building and training a model.\n",
            "\n",
            "If  the  data  keeps  evolving,  you  will  need  to  update  your  datasets  and  retrain  your\n",
            "model regularly. You should probably automate the whole process as much as possi‐\n",
            "ble. Here are a few things you can automate:\n",
            "\n",
            "•\n",
            "• Collect fresh data regularly and label it (e.g., using human raters).\n",
            "\n",
            "•\n",
            "• Write  a  script  to  train  the  model  and  fine-tune  the  hyperparameters  automati‐\n",
            "cally. This script could run automatically, for example every day or every week,\n",
            "depending on your needs.\n",
            "\n",
            "14 A captcha is a test to ensure a user is not a robot. These tests have often been used as a cheap way to label\n",
            "\n",
            "training data.\n",
            "\n",
            "Launch, Monitor, and Maintain Your System \n",
            "\n",
            "| \n",
            "\n",
            "99\n",
            "\n",
            "\f• Write  another  script  that  will  evaluate  both  the  new  model  and  the  previous\n",
            "•\n",
            "model on the updated test set, and deploy the model to production if the perfor‐\n",
            "mance  has  not  decreased  (if  it  did,  make  sure  you  investigate  why).  The  script\n",
            "should  probably  test  the  performance  of  your  model  on  various  subsets  of  the\n",
            "test set, such as poor or rich districts, rural or urban districts, etc.\n",
            "\n",
            "You  should  also  make  sure  you  evaluate  the  model’s  input  data  quality.  Sometimes\n",
            "performance  will  degrade  slightly  because  of  a  poor-quality  signal  (e.g.,  a  malfunc‐\n",
            "tioning sensor sending random values, or another team’s output becoming stale), but\n",
            "it may take a while before your system’s performance degrades enough to trigger an\n",
            "alert. If you monitor your model’s inputs, you may catch this earlier. For example, you\n",
            "could trigger an alert if more and more inputs are missing a feature, or the mean or\n",
            "standard deviation drifts too far from the training set, or a categorical feature starts\n",
            "containing new categories.\n",
            "\n",
            "Finally, make sure you keep backups of every model you create and have the process\n",
            "and  tools  in  place  to  roll  back  to  a  previous  model  quickly,  in  case  the  new  model\n",
            "starts failing badly for some reason. Having backups also makes it possible to easily\n",
            "compare new models with previous ones. Similarly, you should keep backups of every\n",
            "version of your datasets so that you can roll back to a previous dataset if the new one\n",
            "ever  gets  corrupted  (e.g.,  if  the  fresh  data  that  gets  added  to  it  turns  out  to  be  full\n",
            "of outliers). Having backups of your datasets also allows you to evaluate any model\n",
            "against any previous dataset.\n",
            "\n",
            "As  you  can  see,  machine  learning  involves  quite  a  lot  of  infrastructure.  Chapter  19\n",
            "discusses  some  aspects  of  this,  but  it’s  a  very  broad  topic  called  ML  Operations\n",
            "(MLOps), which deserves its own book. So don’t be surprised if your first ML project\n",
            "takes a lot of effort and time to build and deploy to production. Fortunately, once all\n",
            "the infrastructure is in place, going from idea to production will be much faster.\n",
            "\n",
            "Try It Out!\n",
            "Hopefully this chapter gave you a good idea of what a machine learning project looks\n",
            "like as well as showing you some of the tools you can use to train a great system. As\n",
            "you can see, much of the work is in the data preparation step: building monitoring\n",
            "tools, setting up human evaluation pipelines, and automating regular model training.\n",
            "The machine learning algorithms are important, of course, but it is probably prefera‐\n",
            "ble to be comfortable with the overall process and know three or four algorithms well\n",
            "rather than to spend all your time exploring advanced algorithms.\n",
            "\n",
            "So, if you have not already done so, now is a good time to pick up a laptop, select a\n",
            "dataset that you are interested in, and try to go through the whole process from A to\n",
            "Z. A good place to start is on a competition website such as Kaggle: you will have a\n",
            "dataset to play with, a clear goal, and people to share the experience with. Have fun!\n",
            "\n",
            "100 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 2: End-to-End Machine Learning Project\n",
            "\n",
            "\fExercises\n",
            "The following exercises are based on this chapter’s housing dataset:\n",
            "\n",
            "1. Try  a  support  vector  machine  regressor  (sklearn.svm.SVR)  with  various\n",
            "1.\n",
            "hyperparameters,  such  as  kernel=\"linear\"  (with  various  values  for  the\n",
            "C  hyperparameter)  or  kernel=\"rbf\"  (with  various  values  for  the  C  and  gamma\n",
            "hyperparameters).  Note  that  support  vector  machines  don’t  scale  well  to  large\n",
            "datasets, so you should probably train your model on just the first 5,000 instances\n",
            "of the training set and use only 3-fold cross-validation, or else it will take hours.\n",
            "Don’t worry about what the hyperparameters mean for now; we’ll discuss them\n",
            "in Chapter 5. How does the best SVR predictor perform?\n",
            "2. Try replacing the GridSearchCV with a RandomizedSearchCV.\n",
            "2.\n",
            "3. Try adding a SelectFromModel transformer in the preparation pipeline to select\n",
            "3.\n",
            "\n",
            "only the most important attributes.\n",
            "\n",
            "4. Try  creating  a  custom  transformer  that  trains  a  k-nearest  neighbors  regressor\n",
            "4.\n",
            "(sklearn.neighbors.KNeighborsRegressor)  in  its  fit()  method,  and  outputs\n",
            "the  model’s  predictions  in  its  transform()  method.  Then  add  this  feature  to\n",
            "the  preprocessing  pipeline,  using  latitude  and  longitude  as  the  inputs  to  this\n",
            "transformer. This will add a feature in the model that corresponds to the housing\n",
            "median price of the nearest districts.\n",
            "\n",
            "5. Automatically explore some preparation options using GridSearchCV.\n",
            "5.\n",
            "\n",
            "6.\n",
            "6. Try  to \n",
            "\n",
            "implement  the  StandardScalerClone  class  again \n",
            "\n",
            "from  scratch,\n",
            "then  add  support  for  the  inverse_transform()  method:  executing  scaler.\n",
            "inverse_transform(scaler.fit_transform(X))  should  return  an  array  very\n",
            "close  to  X.  Then  add  support  for  feature  names:  set  feature_names_in_  in  the\n",
            "fit()  method  if  the  input  is  a  DataFrame.  This  attribute  should  be  a  NumPy\n",
            "array  of  column  names.  Lastly,  implement  the  get_feature_names_out()\n",
            "method: it should have one optional input_features=None argument. If passed,\n",
            "the method should check that its length matches n_features_in_, and it should\n",
            "match  feature_names_in_  if  it  is  defined;  then  input_features  should  be\n",
            "returned.  If  input_features  is  None,  then  the  method  should  either  return\n",
            "feature_names_in_  if  it  is  defined  or  np.array([\"x0\",  \"x1\",  ...])  with\n",
            "length n_features_in_ otherwise.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "101\n",
            "\n",
            "\f\fCHAPTER 3\n",
            "Classification\n",
            "\n",
            "In  Chapter  1  I  mentioned  that  the  most  common  supervised  learning  tasks  are\n",
            "regression  (predicting  values)  and  classification  (predicting  classes).  In  Chapter  2\n",
            "we  explored  a  regression  task,  predicting  housing  values,  using  various  algorithms\n",
            "such as linear regression, decision trees, and random forests (which will be explained\n",
            "in  further  detail  in  later  chapters).  Now  we  will  turn  our  attention  to  classification\n",
            "systems.\n",
            "\n",
            "MNIST\n",
            "In  this  chapter  we  will  be  using  the  MNIST  dataset,  which  is  a  set  of  70,000  small\n",
            "images  of  digits  handwritten  by  high  school  students  and  employees  of  the  US\n",
            "Census  Bureau.  Each  image  is  labeled  with  the  digit  it  represents.  This  set  has\n",
            "been  studied  so  much  that  it  is  often  called  the  “hello  world”  of  machine  learning:\n",
            "whenever people come up with a new classification algorithm they are curious to see\n",
            "how it will perform on MNIST, and anyone who learns machine learning tackles this\n",
            "dataset sooner or later.\n",
            "\n",
            "Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
            "one of them. The following code fetches the MNIST dataset from OpenML.org:1\n",
            "\n",
            "from sklearn.datasets import fetch_openml\n",
            "\n",
            "mnist = fetch_openml('mnist_784', as_frame=False)\n",
            "\n",
            "The  sklearn.datasets  package  contains  mostly  three  types  of  functions:  fetch_*\n",
            "functions  such  as  fetch_openml()  to  download  real-life  datasets,  load_*  functions\n",
            "\n",
            "1 By default Scikit-Learn caches downloaded datasets in a directory called scikit_learn_data in your home\n",
            "\n",
            "directory.\n",
            "\n",
            "103\n",
            "\n",
            "\fto load small toy datasets bundled with Scikit-Learn (so they don’t need to be down‐\n",
            "loaded  over  the  internet),  and  make_*  functions  to  generate  fake  datasets,  useful\n",
            "for  tests.  Generated  datasets  are  usually  returned  as  an  (X,  y)  tuple  containing\n",
            "the  input  data  and  the  targets,  both  as  NumPy  arrays.  Other  datasets  are  returned\n",
            "as  sklearn.utils.Bunch  objects,  which  are  dictionaries  whose  entries  can  also  be\n",
            "accessed as attributes. They generally contain the following entries:\n",
            "\n",
            "\"DESCR\"\n",
            "\n",
            "A description of the dataset\n",
            "\n",
            "\"data\"\n",
            "\n",
            "The input data, usually as a 2D NumPy array\n",
            "\n",
            "\"target\"\n",
            "\n",
            "The labels, usually as a 1D NumPy array\n",
            "\n",
            "The fetch_openml() function is a bit unusual since by default it returns the inputs as\n",
            "a Pandas DataFrame and the labels as a Pandas Series (unless the dataset is sparse).\n",
            "But the MNIST dataset contains images, and DataFrames aren’t ideal for that, so it’s\n",
            "preferable to set as_frame=False to get the data as NumPy arrays instead. Let’s look\n",
            "at these arrays:\n",
            "\n",
            ">>> X, y = mnist.data, mnist.target\n",
            ">>> X\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]])\n",
            ">>> X.shape\n",
            "(70000, 784)\n",
            ">>> y\n",
            "array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)\n",
            ">>> y.shape\n",
            "(70000,)\n",
            "\n",
            "There are 70,000 images, and each image has 784 features. This is because each image\n",
            "is  28  ×  28  pixels,  and  each  feature  simply  represents  one  pixel’s  intensity,  from  0\n",
            "(white)  to  255  (black).  Let’s  take  a  peek  at  one  digit  from  the  dataset  (Figure  3-1).\n",
            "All  we  need  to  do  is  grab  an  instance’s  feature  vector,  reshape  it  to  a  28  ×  28  array,\n",
            "and display it using Matplotlib’s imshow() function. We use cmap=\"binary\" to get a\n",
            "grayscale color map where 0 is white and 255 is black:\n",
            "\n",
            "104 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fimport matplotlib.pyplot as plt\n",
            "\n",
            "def plot_digit(image_data):\n",
            "    image = image_data.reshape(28, 28)\n",
            "    plt.imshow(image, cmap=\"binary\")\n",
            "    plt.axis(\"off\")\n",
            "\n",
            "some_digit = X[0]\n",
            "plot_digit(some_digit)\n",
            "plt.show()\n",
            "\n",
            "Figure 3-1. Example of an MNIST image\n",
            "\n",
            "This looks like a 5, and indeed that’s what the label tells us:\n",
            "\n",
            ">>> y[0]\n",
            "'5'\n",
            "\n",
            "To give you a feel for the complexity of the classification task, Figure 3-2 shows a few\n",
            "more images from the MNIST dataset.\n",
            "\n",
            "But wait! You should always create a test set and set it aside before inspecting the data\n",
            "closely. The MNIST dataset returned by fetch_openml() is actually already split into\n",
            "a training set (the first 60,000 images) and a test set (the last 10,000 images):2\n",
            "\n",
            "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
            "\n",
            "The  training  set  is  already  shuffled  for  us,  which  is  good  because  this  guarantees\n",
            "that  all  cross-validation  folds  will  be  similar  (we  don’t  want  one  fold  to  be  missing\n",
            "some  digits).  Moreover,  some  learning  algorithms  are  sensitive  to  the  order  of  the\n",
            "training  instances,  and  they  perform  poorly  if  they  get  many  similar  instances  in  a\n",
            "row. Shuffling the dataset ensures that this won’t happen.3\n",
            "\n",
            "2 Datasets returned by fetch_openml() are not always shuffled or split.\n",
            "\n",
            "3 Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\n",
            "\n",
            "stock market prices or weather conditions). We will explore this in Chapter 15.\n",
            "\n",
            "MNIST \n",
            "\n",
            "| \n",
            "\n",
            "105\n",
            "\n",
            "\fFigure 3-2. Digits from the MNIST dataset\n",
            "\n",
            "Training a Binary Classifier\n",
            "Let’s  simplify  the  problem  for  now  and  only  try  to  identify  one  digit—for  example,\n",
            "the  number  5.  This  “5-detector”  will  be  an  example  of  a  binary  classifier,  capable\n",
            "of  distinguishing  between  just  two  classes,  5  and  non-5.  First  we’ll  create  the  target\n",
            "vectors for this classification task:\n",
            "\n",
            "y_train_5 = (y_train == '5')  # True for all 5s, False for all other digits\n",
            "y_test_5 = (y_test == '5')\n",
            "\n",
            "Now let’s pick a classifier and train it. A good place to start is with a stochastic gra‐\n",
            "dient  descent  (SGD,  or  stochastic  GD)  classifier,  using  Scikit-Learn’s  SGDClassifier\n",
            "class.  This  classifier  is  capable  of  handling  very  large  datasets  efficiently.  This  is  in\n",
            "part because SGD deals with training instances independently, one at a time, which\n",
            "also  makes  SGD  well  suited  for  online  learning,  as  you  will  see  later.  Let’s  create  an\n",
            "SGDClassifier and train it on the whole training set:\n",
            "\n",
            "from sklearn.linear_model import SGDClassifier\n",
            "\n",
            "sgd_clf = SGDClassifier(random_state=42)\n",
            "sgd_clf.fit(X_train, y_train_5)\n",
            "\n",
            "106 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fNow we can use it to detect images of the number 5:\n",
            "\n",
            ">>> sgd_clf.predict([some_digit])\n",
            "array([ True])\n",
            "\n",
            "The classifier guesses that this image represents a 5 (True). Looks like it guessed right\n",
            "in this particular case! Now, let’s evaluate this model’s performance.\n",
            "\n",
            "Performance Measures\n",
            "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we\n",
            "will  spend  a  large  part  of  this  chapter  on  this  topic.  There  are  many  performance\n",
            "measures  available,  so  grab  another  coffee  and  get  ready  to  learn  a  bunch  of  new\n",
            "concepts and acronyms!\n",
            "\n",
            "Measuring Accuracy Using Cross-Validation\n",
            "A  good  way  to  evaluate  a  model  is  to  use  cross-validation,  just  as  you  did  in\n",
            "Chapter 2. Let’s use the cross_val_score() function to evaluate our SGDClassifier\n",
            "model,  using  k-fold  cross-validation  with  three  folds.  Remember  that  k-fold  cross-\n",
            "validation  means  splitting  the  training  set  into  k  folds  (in  this  case,  three),  then\n",
            "training the model k times, holding out a different fold each time for evaluation (see\n",
            "Chapter 2):\n",
            "\n",
            ">>> from sklearn.model_selection import cross_val_score\n",
            ">>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
            "array([0.95035, 0.96035, 0.9604 ])\n",
            "\n",
            "Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds?\n",
            "This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a dummy\n",
            "classifier  that  just  classifies  every  single  image  in  the  most  frequent  class,  which  in\n",
            "this case is the negative class (i.e., non 5):\n",
            "\n",
            "from sklearn.dummy import DummyClassifier\n",
            "\n",
            "dummy_clf = DummyClassifier()\n",
            "dummy_clf.fit(X_train, y_train_5)\n",
            "print(any(dummy_clf.predict(X_train)))  # prints False: no 5s detected\n",
            "\n",
            "Can you guess this model’s accuracy? Let’s find out:\n",
            "\n",
            ">>> cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
            "array([0.90965, 0.90965, 0.90965])\n",
            "\n",
            "That’s right, it has over 90% accuracy! This is simply because only about 10% of the\n",
            "images are 5s, so if you always guess that an image is not a 5, you will be right about\n",
            "90% of the time. Beats Nostradamus.\n",
            "\n",
            "This demonstrates why accuracy is generally not the preferred performance measure\n",
            "for classifiers, especially when you are dealing with skewed datasets (i.e., when some\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "107\n",
            "\n",
            "\fclasses  are  much  more  frequent  than  others).  A  much  better  way  to  evaluate  the\n",
            "performance of a classifier is to look at the confusion matrix (CM).\n",
            "\n",
            "Implementing Cross-Validation\n",
            "Occasionally  you  will  need  more  control  over  the  cross-validation  process  than\n",
            "what  Scikit-Learn  provides  off  the  shelf.  In  these  cases,  you  can  implement  cross-\n",
            "validation yourself. The following code does roughly the same thing as Scikit-Learn’s\n",
            "cross_val_score() function, and it prints the same result:\n",
            "\n",
            "from sklearn.model_selection import StratifiedKFold\n",
            "from sklearn.base import clone\n",
            "\n",
            "skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is\n",
            "                                       # not already shuffled\n",
            "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
            "    clone_clf = clone(sgd_clf)\n",
            "    X_train_folds = X_train[train_index]\n",
            "    y_train_folds = y_train_5[train_index]\n",
            "    X_test_fold = X_train[test_index]\n",
            "    y_test_fold = y_train_5[test_index]\n",
            "\n",
            "    clone_clf.fit(X_train_folds, y_train_folds)\n",
            "    y_pred = clone_clf.predict(X_test_fold)\n",
            "    n_correct = sum(y_pred == y_test_fold)\n",
            "    print(n_correct / len(y_pred))  # prints 0.95035, 0.96035, and 0.9604\n",
            "\n",
            "The StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\n",
            "to produce folds that contain a representative ratio of each class. At each iteration the\n",
            "code creates a clone of the classifier, trains that clone on the training folds, and makes\n",
            "predictions  on  the  test  fold.  Then  it  counts  the  number  of  correct  predictions  and\n",
            "outputs the ratio of correct predictions.\n",
            "\n",
            "Confusion Matrices\n",
            "The general idea of a confusion matrix is to count the number of times instances of\n",
            "class A are classified as class B, for all A/B pairs. For example, to know the number of\n",
            "times the classifier confused images of 8s with 0s, you would look at row #8, column\n",
            "#0 of the confusion matrix.\n",
            "\n",
            "To compute the confusion matrix, you first need to have a set of predictions so that\n",
            "they  can  be  compared  to  the  actual  targets.  You  could  make  predictions  on  the  test\n",
            "set, but it’s best to keep that untouched for now (remember that you want to use the\n",
            "test  set  only  at  the  very  end  of  your  project,  once  you  have  a  classifier  that  you  are\n",
            "ready to launch). Instead, you can use the cross_val_predict() function:\n",
            "\n",
            "108 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\ffrom sklearn.model_selection import cross_val_predict\n",
            "\n",
            "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
            "\n",
            "Just  like  the  cross_val_score()  function,  cross_val_predict()  performs  k-fold\n",
            "cross-validation, but instead of returning the evaluation scores, it returns the predic‐\n",
            "tions  made  on  each  test  fold.  This  means  that  you  get  a  clean  prediction  for  each\n",
            "instance  in  the  training  set  (by  “clean”  I  mean  “out-of-sample”:  the  model  makes\n",
            "predictions on data that it never saw during training).\n",
            "\n",
            "Now  you  are  ready  to  get  the  confusion  matrix  using  the  confusion_matrix()\n",
            "function.  Just  pass  it  the  target  classes  (y_train_5)  and  the  predicted  classes\n",
            "(y_train_pred):\n",
            "\n",
            ">>> from sklearn.metrics import confusion_matrix\n",
            ">>> cm = confusion_matrix(y_train_5, y_train_pred)\n",
            ">>> cm\n",
            "array([[53892,   687],\n",
            "       [ 1891,  3530]])\n",
            "\n",
            "Each  row  in  a  confusion  matrix  represents  an  actual  class,  while  each  column  rep‐\n",
            "resents  a  predicted  class.  The  first  row  of  this  matrix  considers  non-5  images  (the\n",
            "negative class): 53,892 of them were correctly classified as non-5s (they are called true\n",
            "negatives), while the remaining 687 were wrongly classified as 5s (false positives, also\n",
            "called  type  I  errors).  The  second  row  considers  the  images  of  5s  (the  positive  class):\n",
            "1,891  were  wrongly  classified  as  non-5s  (false  negatives,  also  called  type  II  errors),\n",
            "while  the  remaining  3,530  were  correctly  classified  as  5s  (true  positives).  A  perfect\n",
            "classifier  would  only  have  true  positives  and  true  negatives,  so  its  confusion  matrix\n",
            "would have nonzero values only on its main diagonal (top left to bottom right):\n",
            "\n",
            ">>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
            ">>> confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
            "array([[54579,     0],\n",
            "       [    0,  5421]])\n",
            "\n",
            "The confusion matrix gives you a lot of information, but sometimes you may prefer\n",
            "a  more  concise  metric.  An  interesting  one  to  look  at  is  the  accuracy  of  the  positive\n",
            "predictions; this is called the precision of the classifier (Equation 3-1).\n",
            "\n",
            "Equation 3-1. Precision\n",
            "\n",
            "precision =\n",
            "\n",
            "TP\n",
            "TP + FP\n",
            "\n",
            "TP is the number of true positives, and FP is the number of false positives.\n",
            "\n",
            "A  trivial  way  to  have  perfect  precision  is  to  create  a  classifier  that  always  makes\n",
            "negative  predictions,  except  for  one  single  positive  prediction  on  the  instance  it’s\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "109\n",
            "\n",
            "\fmost  confident  about.  If  this  one  prediction  is  correct,  then  the  classifier  has  100%\n",
            "precision  (precision  =  1/1  =  100%).  Obviously,  such  a  classifier  would  not  be  very\n",
            "useful,  since  it  would  ignore  all  but  one  positive  instance.  So,  precision  is  typically\n",
            "used along with another metric named recall, also called sensitivity or the true positive\n",
            "rate  (TPR):  this  is  the  ratio  of  positive  instances  that  are  correctly  detected  by  the\n",
            "classifier (Equation 3-2).\n",
            "\n",
            "Equation 3-2. Recall\n",
            "\n",
            "recall =\n",
            "\n",
            "TP\n",
            "TP + FN\n",
            "\n",
            "FN is, of course, the number of false negatives.\n",
            "\n",
            "If you are confused about the confusion matrix, Figure 3-3 may help.\n",
            "\n",
            "Figure 3-3. An illustrated confusion matrix showing examples of true negatives (top left),\n",
            "false positives (top right), false negatives (lower left), and true positives (lower right)\n",
            "\n",
            "Precision and Recall\n",
            "Scikit-Learn provides several functions to compute classifier metrics, including preci‐\n",
            "sion and recall:\n",
            "\n",
            ">>> from sklearn.metrics import precision_score, recall_score\n",
            ">>> precision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)\n",
            "0.8370879772350012\n",
            ">>> recall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530)\n",
            "0.6511713705958311\n",
            "\n",
            "110 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fNow our 5-detector does not look as shiny as it did when we looked at its accuracy.\n",
            "When it claims an image represents a 5, it is correct only 83.7% of the time. More‐\n",
            "over, it only detects 65.1% of the 5s.\n",
            "\n",
            "It is often convenient to combine precision and recall into a single metric called the\n",
            "F1  score,  especially  when  you  need  a  single  metric  to  compare  two  classifiers.  The\n",
            "F1  score  is  the  harmonic  mean  of  precision  and  recall  (Equation  3-3).  Whereas  the\n",
            "regular mean treats all values equally, the harmonic mean gives much more weight to\n",
            "low  values.  As  a  result,  the  classifier  will  only  get  a  high  F1  score  if  both  recall  and\n",
            "precision are high.\n",
            "\n",
            "Equation 3-3. F1 score\n",
            "\n",
            "F1 =\n",
            "\n",
            "2\n",
            "1\n",
            "precision +\n",
            "\n",
            "1\n",
            "recall\n",
            "\n",
            "= 2 ×\n",
            "\n",
            "precision × recall\n",
            "precision + recall\n",
            "\n",
            "=\n",
            "\n",
            "TP\n",
            "FN + FP\n",
            "2\n",
            "\n",
            "TP +\n",
            "\n",
            "To compute the F1 score, simply call the f1_score() function:\n",
            "\n",
            ">>> from sklearn.metrics import f1_score\n",
            ">>> f1_score(y_train_5, y_train_pred)\n",
            "0.7325171197343846\n",
            "\n",
            "The F1 score favors classifiers that have similar precision and recall. This is not always\n",
            "what  you  want:  in  some  contexts  you  mostly  care  about  precision,  and  in  other\n",
            "contexts you really care about recall. For example, if you trained a classifier to detect\n",
            "videos that are safe for kids, you would probably prefer a classifier that rejects many\n",
            "good  videos  (low  recall)  but  keeps  only  safe  ones  (high  precision),  rather  than  a\n",
            "classifier  that  has  a  much  higher  recall  but  lets  a  few  really  bad  videos  show  up  in\n",
            "your  product  (in  such  cases,  you  may  even  want  to  add  a  human  pipeline  to  check\n",
            "the  classifier’s  video  selection).  On  the  other  hand,  suppose  you  train  a  classifier  to\n",
            "detect shoplifters in surveillance images: it is probably fine if your classifier only has\n",
            "30% precision as long as it has 99% recall (sure, the security guards will get a few false\n",
            "alerts, but almost all shoplifters will get caught).\n",
            "\n",
            "Unfortunately,  you  can’t  have  it  both  ways:  increasing  precision  reduces  recall,  and\n",
            "vice versa. This is called the precision/recall trade-off.\n",
            "\n",
            "The Precision/Recall Trade-off\n",
            "To understand this trade-off, let’s look at how the SGDClassifier makes its classifica‐\n",
            "tion  decisions.  For  each  instance,  it  computes  a  score  based  on  a  decision  function.\n",
            "If  that  score  is  greater  than  a  threshold,  it  assigns  the  instance  to  the  positive  class;\n",
            "otherwise it assigns it to the negative class. Figure 3-4 shows a few digits positioned\n",
            "from  the  lowest  score  on  the  left  to  the  highest  score  on  the  right.  Suppose  the\n",
            "decision  threshold  is  positioned  at  the  central  arrow  (between  the  two  5s):  you  will\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "111\n",
            "\n",
            "\ffind  4  true  positives  (actual  5s)  on  the  right  of  that  threshold,  and  1  false  positive\n",
            "(actually  a  6).  Therefore,  with  that  threshold,  the  precision  is  80%  (4  out  of  5).  But\n",
            "out  of  6  actual  5s,  the  classifier  only  detects  4,  so  the  recall  is  67%  (4  out  of  6).  If\n",
            "you  raise  the  threshold  (move  it  to  the  arrow  on  the  right),  the  false  positive  (the\n",
            "6)  becomes  a  true  negative,  thereby  increasing  the  precision  (up  to  100%  in  this\n",
            "case), but one true positive becomes a false negative, decreasing recall down to 50%.\n",
            "Conversely, lowering the threshold increases recall and reduces precision.\n",
            "\n",
            "Figure 3-4. The precision/recall trade-off: images are ranked by their classifier score,\n",
            "and those above the chosen decision threshold are considered positive; the higher the\n",
            "threshold, the lower the recall, but (in general) the higher the precision\n",
            "\n",
            "Scikit-Learn does not let you set the threshold directly, but it does give you access to\n",
            "the decision scores that it uses to make predictions. Instead of calling the classifier’s\n",
            "predict()  method,  you  can  call  its  decision_function()  method,  which  returns  a\n",
            "score  for  each  instance,  and  then  use  any  threshold  you  want  to  make  predictions\n",
            "based on those scores:\n",
            "\n",
            ">>> y_scores = sgd_clf.decision_function([some_digit])\n",
            ">>> y_scores\n",
            "array([2164.22030239])\n",
            ">>> threshold = 0\n",
            ">>> y_some_digit_pred = (y_scores > threshold)\n",
            "array([ True])\n",
            "\n",
            "The  SGDClassifier  uses  a  threshold  equal  to  0,  so  the  preceding  code  returns  the\n",
            "same result as the predict() method (i.e., True). Let’s raise the threshold:\n",
            "\n",
            ">>> threshold = 3000\n",
            ">>> y_some_digit_pred = (y_scores > threshold)\n",
            ">>> y_some_digit_pred\n",
            "array([False])\n",
            "\n",
            "This  confirms  that  raising  the  threshold  decreases  recall.  The  image  actually  repre‐\n",
            "sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
            "threshold is increased to 3,000.\n",
            "\n",
            "112 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fHow  do  you  decide  which  threshold  to  use?  First,  use  the  cross_val_predict()\n",
            "function to get the scores of all instances in the training set, but this time specify that\n",
            "you want to return decision scores instead of predictions:\n",
            "\n",
            "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
            "                             method=\"decision_function\")\n",
            "\n",
            "With these scores, use the precision_recall_curve() function to compute precision\n",
            "and recall for all possible thresholds (the function adds a last precision of 0 and a last\n",
            "recall of 1, corresponding to an infinite threshold):\n",
            "\n",
            "from sklearn.metrics import precision_recall_curve\n",
            "\n",
            "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
            "\n",
            "Finally, use Matplotlib to plot precision and recall as functions of the threshold value\n",
            "(Figure 3-5). Let’s show the threshold of 3,000 we selected:\n",
            "\n",
            "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
            "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
            "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
            "[...]  # beautify the figure: add grid, legend, axis, labels, and circles\n",
            "plt.show()\n",
            "\n",
            "Figure 3-5. Precision and recall versus the decision threshold\n",
            "\n",
            "You may wonder why the precision curve is bumpier than the recall\n",
            "curve in Figure 3-5. The reason is that precision may sometimes go\n",
            "down when you raise the threshold (although in general it will go\n",
            "up).  To  understand  why,  look  back  at  Figure  3-4  and  notice  what\n",
            "happens  when  you  start  from  the  central  threshold  and  move  it\n",
            "just one digit to the right: precision goes from 4/5 (80%) down to\n",
            "3/4  (75%).  On  the  other  hand,  recall  can  only  go  down  when  the\n",
            "threshold is increased, which explains why its curve looks smooth.\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "113\n",
            "\n",
            "\fAt this threshold value, precision is near 90% and recall is around 50%. Another way\n",
            "to select a good precision/recall trade-off is to plot precision directly against recall, as\n",
            "shown in Figure 3-6 (the same threshold is shown):\n",
            "\n",
            "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
            "[...]  # beautify the figure: add labels, grid, legend, arrow, and text\n",
            "plt.show()\n",
            "\n",
            "Figure 3-6. Precision versus recall\n",
            "\n",
            "You  can  see  that  precision  really  starts  to  fall  sharply  at  around  80%  recall.  You\n",
            "will  probably  want  to  select  a  precision/recall  trade-off  just  before  that  drop—for\n",
            "example, at around 60% recall. But of course, the choice depends on your project.\n",
            "\n",
            "Suppose you decide to aim for 90% precision. You could use the first plot to find the\n",
            "threshold  you  need  to  use,  but  that’s  not  very  precise.  Alternatively,  you  can  search\n",
            "for the lowest threshold that gives you at least 90% precision. For this, you can use the\n",
            "NumPy array’s argmax() method. This returns the first index of the maximum value,\n",
            "which in this case means the first True value:\n",
            "\n",
            ">>> idx_for_90_precision = (precisions >= 0.90).argmax()\n",
            ">>> threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
            ">>> threshold_for_90_precision\n",
            "3370.0194991439557\n",
            "\n",
            "114 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fTo  make  predictions  (on  the  training  set  for  now),  instead  of  calling  the  classifier’s\n",
            "predict() method, you can run this code:\n",
            "\n",
            "y_train_pred_90 = (y_scores >= threshold_for_90_precision)\n",
            "\n",
            "Let’s check these predictions’ precision and recall:\n",
            "\n",
            ">>> precision_score(y_train_5, y_train_pred_90)\n",
            "0.9000345901072293\n",
            ">>> recall_at_90_precision = recall_score(y_train_5, y_train_pred_90)\n",
            ">>> recall_at_90_precision\n",
            "0.4799852425751706\n",
            "\n",
            "Great, you have a 90% precision classifier! As you can see, it is fairly easy to create a\n",
            "classifier with virtually any precision you want: just set a high enough threshold, and\n",
            "you’re  done.  But  wait,  not  so  fast–a  high-precision  classifier  is  not  very  useful  if  its\n",
            "recall is too low! For many applications, 48% recall wouldn’t be great at all.\n",
            "\n",
            "If  someone  says,  “Let’s  reach  99%  precision”,  you  should  ask,  “At\n",
            "what recall?”\n",
            "\n",
            "The ROC Curve\n",
            "The receiver operating characteristic (ROC) curve is another common tool used with\n",
            "binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\n",
            "ting precision versus recall, the ROC curve plots the true positive rate (another name\n",
            "for recall) against the false positive rate (FPR). The FPR (also called the fall-out) is the\n",
            "ratio of negative instances that are incorrectly classified as positive. It is equal to 1 –\n",
            "the true negative rate (TNR), which is the ratio of negative instances that are correctly\n",
            "classified as negative. The TNR is also called specificity. Hence, the ROC curve plots\n",
            "sensitivity (recall) versus 1 – specificity.\n",
            "\n",
            "To plot the ROC curve, you first use the roc_curve() function to compute the TPR\n",
            "and FPR for various threshold values:\n",
            "\n",
            "from sklearn.metrics import roc_curve\n",
            "\n",
            "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
            "\n",
            "Then  you  can  plot  the  FPR  against  the  TPR  using  Matplotlib.  The  following  code\n",
            "produces the plot in Figure 3-7. To find the point that corresponds to 90% precision,\n",
            "we need to look for the index of the desired threshold. Since thresholds are listed in\n",
            "decreasing order in this case, we use <= instead of >= on the first line:\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "115\n",
            "\n",
            "\fidx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
            "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
            "\n",
            "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
            "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
            "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
            "[...]  # beautify the figure: add labels, grid, legend, arrow, and text\n",
            "plt.show()\n",
            "\n",
            "Figure 3-7. A ROC curve plotting the false positive rate against the true positive rate for\n",
            "all possible thresholds; the black circle highlights the chosen ratio (at 90% precision and\n",
            "48% recall)\n",
            "\n",
            "Once again there is a trade-off: the higher the recall (TPR), the more false positives\n",
            "(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\n",
            "random classifier; a good classifier stays as far away from that line as possible (toward\n",
            "the top-left corner).\n",
            "\n",
            "One  way  to  compare  classifiers  is  to  measure  the  area  under  the  curve  (AUC).  A\n",
            "perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier\n",
            "will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to estimate the\n",
            "ROC AUC:\n",
            "\n",
            ">>> from sklearn.metrics import roc_auc_score\n",
            ">>> roc_auc_score(y_train_5, y_scores)\n",
            "0.9604938554008616\n",
            "\n",
            "116 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fSince  the  ROC  curve  is  so  similar  to  the  precision/recall  (PR)\n",
            "curve, you may wonder how to decide which one to use. As a rule\n",
            "of  thumb,  you  should  prefer  the  PR  curve  whenever  the  positive\n",
            "class is rare or when you care more about the false positives than\n",
            "the  false  negatives.  Otherwise,  use  the  ROC  curve.  For  example,\n",
            "looking  at  the  previous  ROC  curve  (and  the  ROC  AUC  score),\n",
            "you  may  think  that  the  classifier  is  really  good.  But  this  is  mostly\n",
            "because  there  are  few  positives  (5s)  compared  to  the  negatives\n",
            "(non-5s). In contrast, the PR curve makes it clear that the classifier\n",
            "has room for improvement: the curve could really be closer to the\n",
            "top-right corner (see Figure 3-6 again).\n",
            "\n",
            "Let’s  now  create  a  RandomForestClassifier,  whose  PR  curve  and  F1  score  we  can\n",
            "compare to those of the SGDClassifier:\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "forest_clf = RandomForestClassifier(random_state=42)\n",
            "\n",
            "The  precision_recall_curve()  function  expects  labels  and  scores  for  each\n",
            "instance,  so  we  need  to  train  the  random  forest  classifier  and  make  it  assign  a\n",
            "score  to  each  instance.  But  the  RandomForestClassifier  class  does  not  have  a\n",
            "decision_function() method, due to the way it works (we will cover this in Chap‐\n",
            "ter 7). Luckily, it has a  predict_proba() method that returns class probabilities for\n",
            "each instance, and we can just use the probability of the positive class as a score, so\n",
            "it will work fine.4 We can call the cross_val_predict() function to train the Random\n",
            "ForestClassifier  using  cross-validation  and  make  it  predict  class  probabilities  for\n",
            "every image as follows:\n",
            "\n",
            "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
            "                                    method=\"predict_proba\")\n",
            "\n",
            "Let’s look at the class probabilities for the first two images in the training set:\n",
            "\n",
            ">>> y_probas_forest[:2]\n",
            "array([[0.11, 0.89],\n",
            "       [0.99, 0.01]])\n",
            "\n",
            "The  model  predicts  that  the  first  image  is  positive  with  89%  probability,  and  it\n",
            "predicts that the second image is negative with 99% probability. Since each image is\n",
            "either positive or negative, the probabilities in each row add up to 100%.\n",
            "\n",
            "4 Scikit-Learn classifiers always have either a decision_function() method or a predict_proba() method, or\n",
            "\n",
            "sometimes both.\n",
            "\n",
            "Performance Measures \n",
            "\n",
            "| \n",
            "\n",
            "117\n",
            "\n",
            "\fThese  are  estimated  probabilities,  not  actual  probabilities.  For\n",
            "example,  if  you  look  at  all  the  images  that  the  model  classified\n",
            "as  positive  with  an  estimated  probability  between  50%  and  60%,\n",
            "roughly 94% of them are actually positive. So, the model’s estima‐\n",
            "ted probabilities were much too low in this case—but models can\n",
            "be overconfident as well. The sklearn.calibration package con‐\n",
            "tains  tools  to  calibrate  the  estimated  probabilities  and  make  them\n",
            "much closer to actual probabilities. See the extra material section in\n",
            "this chapter’s notebook for more details.\n",
            "\n",
            "The second column contains the estimated probabilities for the positive class, so let’s\n",
            "pass them to the precision_recall_curve() function:\n",
            "\n",
            "y_scores_forest = y_probas_forest[:, 1]\n",
            "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n",
            "    y_train_5, y_scores_forest)\n",
            "\n",
            "Now we’re ready to plot the PR curve. It is useful to plot the first PR curve as well to\n",
            "see how they compare (Figure 3-8):\n",
            "\n",
            "plt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\n",
            "         label=\"Random Forest\")\n",
            "plt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\n",
            "[...]  # beautify the figure: add labels, grid, and legend\n",
            "plt.show()\n",
            "\n",
            "Figure 3-8. Comparing PR curves: the random forest classifier is superior to the SGD\n",
            "classifier because its PR curve is much closer to the top-right corner, and it has a greater\n",
            "AUC\n",
            "\n",
            "118 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fAs  you  can  see  in  Figure  3-8,  the  RandomForestClassifier’s  PR  curve  looks  much\n",
            "better than the SGDClassifier’s: it comes much closer to the top-right corner. Its F1\n",
            "score and ROC AUC score are also significantly better:\n",
            "\n",
            ">>> y_train_pred_forest = y_probas_forest[:, 1] >= 0.5  # positive proba ≥ 50%\n",
            ">>> f1_score(y_train_5, y_pred_forest)\n",
            "0.9242275142688446\n",
            ">>> roc_auc_score(y_train_5, y_scores_forest)\n",
            "0.9983436731328145\n",
            "\n",
            "Try measuring the precision and recall scores: you should find about 99.1% precision\n",
            "and 86.6% recall. Not too bad!\n",
            "\n",
            "You  now  know  how  to  train  binary  classifiers,  choose  the  appropriate  metric  for\n",
            "your  task,  evaluate  your  classifiers  using  cross-validation,  select  the  precision/recall\n",
            "trade-off that fits your needs, and use several metrics and curves to compare various\n",
            "models. You’re ready to try to detect more than just the 5s.\n",
            "\n",
            "Multiclass Classification\n",
            "Whereas binary classifiers distinguish between two classes, multiclass classifiers (also\n",
            "called multinomial classifiers) can distinguish between more than two classes.\n",
            "\n",
            "Some  Scikit-Learn  classifiers  (e.g.,  LogisticRegression,  RandomForestClassifier,\n",
            "and GaussianNB) are capable of handling multiple classes natively. Others are strictly\n",
            "binary classifiers (e.g., SGDClassifier and SVC). However, there are various strategies\n",
            "that you can use to perform multiclass classification with multiple binary classifiers.\n",
            "\n",
            "One way to create a system that can classify the digit images into 10 classes (from 0\n",
            "to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a\n",
            "2-detector, and so on). Then when you want to classify an image, you get the decision\n",
            "score  from  each  classifier  for  that  image  and  you  select  the  class  whose  classifier\n",
            "outputs  the  highest  score.  This  is  called  the  one-versus-the-rest  (OvR)  strategy,  or\n",
            "sometimes one-versus-all (OvA).\n",
            "\n",
            "Another strategy is to train a binary classifier for every pair of digits: one to distin‐\n",
            "guish  0s  and  1s,  another  to  distinguish  0s  and  2s,  another  for  1s  and  2s,  and  so\n",
            "on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need\n",
            "to  train  N  ×  (N  –  1)  /  2  classifiers.  For  the  MNIST  problem,  this  means  training\n",
            "45  binary  classifiers!  When  you  want  to  classify  an  image,  you  have  to  run  the\n",
            "image  through  all  45  classifiers  and  see  which  class  wins  the  most  duels.  The  main\n",
            "advantage  of  OvO  is  that  each  classifier  only  needs  to  be  trained  on  the  part  of  the\n",
            "training set containing the two classes that it must distinguish.\n",
            "\n",
            "Some  algorithms  (such  as  support  vector  machine  classifiers)  scale  poorly  with  the\n",
            "size  of  the  training  set.  For  these  algorithms  OvO  is  preferred  because  it  is  faster\n",
            "\n",
            "Multiclass Classification \n",
            "\n",
            "| \n",
            "\n",
            "119\n",
            "\n",
            "\fto  train  many  classifiers  on  small  training  sets  than  to  train  few  classifiers  on  large\n",
            "training sets. For most binary classification algorithms, however, OvR is preferred.\n",
            "\n",
            "Scikit-Learn  detects  when  you  try  to  use  a  binary  classification  algorithm  for  a\n",
            "multiclass  classification  task,  and  it  automatically  runs  OvR  or  OvO,  depending\n",
            "on  the  algorithm.  Let’s  try  this  with  a  support  vector  machine  classifier  using  the\n",
            "sklearn.svm.SVC class (see Chapter 5). We’ll only train on the first 2,000 images, or\n",
            "else it will take a very long time:\n",
            "\n",
            "from sklearn.svm import SVC\n",
            "\n",
            "svm_clf = SVC(random_state=42)\n",
            "svm_clf.fit(X_train[:2000], y_train[:2000])  # y_train, not y_train_5\n",
            "\n",
            "That  was  easy!  We  trained  the  SVC  using  the  original  target  classes  from  0  to  9\n",
            "(y_train), instead of the 5-versus-the-rest target classes (y_train_5). Since there are\n",
            "10  classes  (i.e.,  more  than  2),  Scikit-Learn  used  the  OvO  strategy  and  trained  45\n",
            "binary classifiers. Now let’s make a prediction on an image:\n",
            "\n",
            ">>> svm_clf.predict([some_digit])\n",
            "array(['5'], dtype=object)\n",
            "\n",
            "That’s correct! This code actually made 45 predictions—one per pair of classes—and\n",
            "it  selected  the  class  that  won  the  most  duels.  If  you  call  the  decision_function()\n",
            "method, you will see that it returns 10 scores per instance: one per class. Each class\n",
            "gets  a  score  equal  to  the  number  of  won  duels  plus  or  minus  a  small  tweak  (max\n",
            "±0.33) to break ties, based on the classifier scores:\n",
            "\n",
            ">>> some_digit_scores = svm_clf.decision_function([some_digit])\n",
            ">>> some_digit_scores.round(2)\n",
            "array([[ 3.79,  0.73,  6.06,  8.3 , -0.29,  9.3 ,  1.75,  2.77,  7.21,\n",
            "         4.82]])\n",
            "\n",
            "The highest score is 9.3, and it’s indeed the one corresponding to class 5:\n",
            "\n",
            ">>> class_id = some_digit_scores.argmax()\n",
            ">>> class_id\n",
            "5\n",
            "\n",
            "When a classifier is trained, it stores the list of target classes in its classes_ attribute,\n",
            "ordered by value. In the case of MNIST, the index of each class in the classes_ array\n",
            "conveniently matches the class itself (e.g., the class at index 5 happens to be class '5'),\n",
            "but in general you won’t be so lucky; you will need to look up the class label like this:\n",
            "\n",
            ">>> svm_clf.classes_\n",
            "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)\n",
            ">>> svm_clf.classes_[class_id]\n",
            "'5'\n",
            "\n",
            "If  you  want  to  force  Scikit-Learn  to  use  one-versus-one  or  one-versus-the-rest,  you\n",
            "can  use  the  OneVsOneClassifier  or  OneVsRestClassifier  classes.  Simply  create\n",
            "\n",
            "120 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fan  instance  and  pass  a  classifier  to  its  constructor  (it  doesn’t  even  have  to  be  a\n",
            "binary classifier). For example, this code creates a multiclass classifier using the OvR\n",
            "strategy, based on an SVC:\n",
            "\n",
            "from sklearn.multiclass import OneVsRestClassifier\n",
            "\n",
            "ovr_clf = OneVsRestClassifier(SVC(random_state=42))\n",
            "ovr_clf.fit(X_train[:2000], y_train[:2000])\n",
            "\n",
            "Let’s make a prediction, and check the number of trained classifiers:\n",
            "\n",
            ">>> ovr_clf.predict([some_digit])\n",
            "array(['5'], dtype='<U1')\n",
            ">>> len(ovr_clf.estimators_)\n",
            "10\n",
            "\n",
            "Training an SGDClassifier on a multiclass dataset and using it to make predictions is\n",
            "just as easy:\n",
            "\n",
            ">>> sgd_clf = SGDClassifier(random_state=42)\n",
            ">>> sgd_clf.fit(X_train, y_train)\n",
            ">>> sgd_clf.predict([some_digit])\n",
            "array(['3'], dtype='<U1')\n",
            "\n",
            "Oops,  that’s  incorrect.  Prediction  errors  do  happen!  This  time  Scikit-Learn  used\n",
            "the  OvR  strategy  under  the  hood:  since  there  are  10  classes,  it  trained  10  binary\n",
            "classifiers. The decision_function() method now returns one value per class. Let’s\n",
            "look at the scores that the SGD classifier assigned to each class:\n",
            "\n",
            ">>> sgd_clf.decision_function([some_digit]).round()\n",
            "array([[-31893., -34420.,  -9531.,   1824., -22320.,  -1386., -26189.,\n",
            "        -16148.,  -4604., -12051.]])\n",
            "\n",
            "You  can  see  that  the  classifier  is  not  very  confident  about  its  prediction:  almost  all\n",
            "scores are very negative, while class 3 has a score of +1,824, and class 5 is not too far\n",
            "behind at –1,386. Of course, you’ll want to evaluate this classifier on more than one\n",
            "image. Since there are roughly the same number of images in each class, the accuracy\n",
            "metric is fine. As usual, you can use the cross_val_score() function to evaluate the\n",
            "model:\n",
            "\n",
            ">>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
            "array([0.87365, 0.85835, 0.8689 ])\n",
            "\n",
            "It  gets  over  85.8%  on  all  test  folds.  If  you  used  a  random  classifier,  you  would  get\n",
            "10% accuracy, so this is not such a bad score, but you can still do much better. Simply\n",
            "scaling the inputs (as discussed in Chapter 2) increases accuracy above 89.1%:\n",
            "\n",
            ">>> from sklearn.preprocessing import StandardScaler\n",
            ">>> scaler = StandardScaler()\n",
            ">>> X_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\n",
            ">>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
            "array([0.8983, 0.891 , 0.9018])\n",
            "\n",
            "Multiclass Classification \n",
            "\n",
            "| \n",
            "\n",
            "121\n",
            "\n",
            "\fError Analysis\n",
            "If this were a real project, you would now follow the steps in your machine learning\n",
            "project  checklist  (see  Appendix  A).  You’d  explore  data  preparation  options,  try  out\n",
            "multiple models, shortlist the best ones, fine-tune their hyperparameters using Grid\n",
            "SearchCV,  and  automate  as  much  as  possible.  Here,  we  will  assume  that  you  have\n",
            "found  a  promising  model  and  you  want  to  find  ways  to  improve  it.  One  way  to  do\n",
            "this is to analyze the types of errors it makes.\n",
            "\n",
            "First, look at the confusion matrix. For this, you first need to make predictions using\n",
            "the  cross_val_predict() function; then you can pass the labels and predictions to\n",
            "the confusion_matrix() function, just like you did earlier. However, since there are\n",
            "now 10 classes instead of 2, the confusion matrix will contain quite a lot of numbers,\n",
            "and it may be hard to read.\n",
            "\n",
            "A colored diagram of the confusion matrix is much easier to analyze. To plot such a\n",
            "diagram, use the ConfusionMatrixDisplay.from_predictions() function like this:\n",
            "\n",
            "from sklearn.metrics import ConfusionMatrixDisplay\n",
            "\n",
            "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
            "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
            "plt.show()\n",
            "\n",
            "This  produces  the  left  diagram  in  Figure  3-9.  This  confusion  matrix  looks  pretty\n",
            "good: most images are on the main diagonal, which means that they were classified\n",
            "correctly. Notice that the cell on the diagonal in row #5 and column #5 looks slightly\n",
            "darker  than  the  other  digits.  This  could  be  because  the  model  made  more  errors\n",
            "on  5s,  or  because  there  are  fewer  5s  in  the  dataset  than  the  other  digits.  That’s  why\n",
            "it’s important to normalize the confusion matrix by dividing each value by the total\n",
            "number  of  images  in  the  corresponding  (true)  class  (i.e.,  divide  by  the  row’s  sum).\n",
            "This  can  be  done  simply  by  setting  normalize=\"true\".  We  can  also  specify  the  val\n",
            "ues_format=\".0%\"  argument  to  show  percentages  with  no  decimals.  The  following\n",
            "code produces the diagram on the right in Figure 3-9:\n",
            "\n",
            "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
            "                                        normalize=\"true\", values_format=\".0%\")\n",
            "plt.show()\n",
            "\n",
            "Now we can easily see that only 82% of the images of 5s were classified correctly. The\n",
            "most common error the model made with images of 5s was to misclassify them as 8s:\n",
            "this happened for 10% of all 5s. But only 2% of 8s got misclassified as 5s; confusion\n",
            "matrices  are  generally  not  symmetrical!  If  you  look  carefully,  you  will  notice  that\n",
            "many digits have been misclassified as 8s, but this is not immediately obvious from\n",
            "this diagram. If you want to make the errors stand out more, you can try putting zero\n",
            "weight on the correct predictions. The following code does just that and produces the\n",
            "diagram on the left in Figure 3-10:\n",
            "\n",
            "122 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fsample_weight = (y_train_pred != y_train)\n",
            "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
            "                                        sample_weight=sample_weight,\n",
            "                                        normalize=\"true\", values_format=\".0%\")\n",
            "plt.show()\n",
            "\n",
            "Figure 3-9. Confusion matrix (left) and the same CM normalized by row (right)\n",
            "\n",
            "Figure 3-10. Confusion matrix with errors only, normalized by row (left) and by column\n",
            "(right)\n",
            "\n",
            "Now  you  can  see  much  more  clearly  the  kinds  of  errors  the  classifier  makes.  The\n",
            "column for class 8 is now really bright, which confirms that many images got misclas‐\n",
            "sified  as  8s.  In  fact  this  is  the  most  common  misclassification  for  almost  all  classes.\n",
            "But  be  careful  how  you  interpret  the  percentages  in  this  diagram:  remember  that\n",
            "we’ve excluded the correct predictions. For example, the 36% in row #7, column #9\n",
            "does not mean that 36% of all images of 7s were misclassified as 9s. It means that 36%\n",
            "of the errors the model made on images of 7s were misclassifications as 9s. In reality,\n",
            "\n",
            "Error Analysis \n",
            "\n",
            "| \n",
            "\n",
            "123\n",
            "\n",
            "\fonly 3% of images of 7s were misclassified as 9s, as you can see in the diagram on the\n",
            "right in Figure 3-9.\n",
            "\n",
            "It is also possible to normalize the confusion matrix by column rather than by row:\n",
            "if  you  set  normalize=\"pred\",  you  get  the  diagram  on  the  right  in  Figure  3-10.  For\n",
            "example, you can see that 56% of misclassified 7s are actually 9s.\n",
            "\n",
            "Analyzing  the  confusion  matrix  often  gives  you  insights  into  ways  to  improve  your\n",
            "classifier. Looking at these plots, it seems that your efforts should be spent on reduc‐\n",
            "ing the false 8s. For example, you could try to gather more training data for digits that\n",
            "look like 8s (but are not) so that the classifier can learn to distinguish them from real\n",
            "8s. Or you could engineer new features that would help the classifier—for example,\n",
            "writing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one,\n",
            "5 has none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or\n",
            "OpenCV) to make some patterns, such as closed loops, stand out more.\n",
            "\n",
            "Analyzing  individual  errors  can  also  be  a  good  way  to  gain  insights  into  what  your\n",
            "classifier is doing and why it is failing. For example, let’s plot examples of 3s and 5s in\n",
            "a confusion matrix style (Figure 3-11):\n",
            "\n",
            "cl_a, cl_b = '3', '5'\n",
            "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
            "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
            "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
            "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
            "[...]  # plot all images in X_aa, X_ab, X_ba, X_bb in a confusion matrix style\n",
            "\n",
            "Figure 3-11. Some images of 3s and 5s organized like a confusion matrix\n",
            "\n",
            "124 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fAs you can see, some of the digits that the classifier gets wrong (i.e., in the bottom-left\n",
            "and  top-right  blocks)  are  so  badly  written  that  even  a  human  would  have  trouble\n",
            "classifying them. However, most misclassified images seem like obvious errors to us.\n",
            "It may be hard to understand why the classifier made the mistakes it did, but remem‐\n",
            "ber  that  the  human  brain  is  a  fantastic  pattern  recognition  system,  and  our  visual\n",
            "system does a lot of complex preprocessing before any information even reaches our\n",
            "consciousness. So, the fact that this task feels simple does not mean that it is. Recall\n",
            "that we used a simple SGDClassifier, which is just a linear model: all it does is assign\n",
            "a  weight  per  class  to  each  pixel,  and  when  it  sees  a  new  image  it  just  sums  up  the\n",
            "weighted pixel intensities to get a score for each class. Since 3s and 5s differ by only a\n",
            "few pixels, this model will easily confuse them.\n",
            "\n",
            "The main difference between 3s and 5s is the position of the small line that joins the\n",
            "top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\n",
            "the classifier might classify it as a 5, and vice versa. In other words, this classifier is\n",
            "quite sensitive to image shifting and rotation. One way to reduce the 3/5 confusion\n",
            "is to preprocess the images to ensure that they are well centered and not too rotated.\n",
            "However, this may not be easy since it requires predicting the correct rotation of each\n",
            "image. A much simpler approach consists of augmenting the training set with slightly\n",
            "shifted and rotated variants of the training images. This will force the model to learn\n",
            "to be more tolerant to such variations. This is called data augmentation (we’ll cover\n",
            "this in Chapter 14; also see exercise 2 at the end of this chapter).\n",
            "\n",
            "Multilabel Classification\n",
            "Until now, each instance has always been assigned to just one class. But in some cases\n",
            "you  may  want  your  classifier  to  output  multiple  classes  for  each  instance.  Consider\n",
            "a  face-recognition  classifier:  what  should  it  do  if  it  recognizes  several  people  in  the\n",
            "same  picture?  It  should  attach  one  tag  per  person  it  recognizes.  Say  the  classifier\n",
            "has  been  trained  to  recognize  three  faces:  Alice,  Bob,  and  Charlie.  Then  when  the\n",
            "classifier  is  shown  a  picture  of  Alice  and  Charlie,  it  should  output  [True,  False,\n",
            "True]  (meaning  “Alice  yes,  Bob  no,  Charlie  yes”).  Such  a  classification  system  that\n",
            "outputs multiple binary tags is called a multilabel classification system.\n",
            "\n",
            "We won’t go into face recognition just yet, but let’s look at a simpler example, just for\n",
            "illustration purposes:\n",
            "\n",
            "import numpy as np\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "\n",
            "y_train_large = (y_train >= '7')\n",
            "y_train_odd = (y_train.astype('int8') % 2 == 1)\n",
            "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
            "\n",
            "Multilabel Classification \n",
            "\n",
            "| \n",
            "\n",
            "125\n",
            "\n",
            "\fknn_clf = KNeighborsClassifier()\n",
            "knn_clf.fit(X_train, y_multilabel)\n",
            "\n",
            "This  code  creates  a  y_multilabel  array  containing  two  target  labels  for  each  digit\n",
            "image: the first indicates whether or not the digit is large (7, 8, or 9), and the second\n",
            "indicates  whether  or  not  it  is  odd.  Then  the  code  creates  a  KNeighborsClassifier\n",
            "instance,  which  supports  multilabel  classification  (not  all  classifiers  do),  and  trains\n",
            "this  model  using  the  multiple  targets  array.  Now  you  can  make  a  prediction,  and\n",
            "notice that it outputs two labels:\n",
            "\n",
            ">>> knn_clf.predict([some_digit])\n",
            "array([[False,  True]])\n",
            "\n",
            "And it gets it right! The digit 5 is indeed not large (False) and odd (True).\n",
            "\n",
            "There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
            "really  depends  on  your  project.  One  approach  is  to  measure  the  F1  score  for  each\n",
            "individual label (or any other binary classifier metric discussed earlier), then simply\n",
            "compute the average score. The following code computes the average F1 score across\n",
            "all labels:\n",
            "\n",
            ">>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
            ">>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
            "0.976410265560605\n",
            "\n",
            "This  approach  assumes  that  all  labels  are  equally  important,  which  may  not  be  the\n",
            "case. In particular, if you have many more pictures of Alice than of Bob or Charlie,\n",
            "you may want to give more weight to the classifier’s score on pictures of Alice. One\n",
            "simple  option  is  to  give  each  label  a  weight  equal  to  its  support  (i.e.,  the  number  of\n",
            "instances  with  that  target  label).  To  do  this,  simply  set  average=\"weighted\"  when\n",
            "calling the f1_score() function.5\n",
            "\n",
            "If you wish to use a classifier that does not natively support multilabel classification,\n",
            "such  as  SVC,  one  possible  strategy  is  to  train  one  model  per  label.  However,  this\n",
            "strategy  may  have  a  hard  time  capturing  the  dependencies  between  the  labels.  For\n",
            "example,  a  large  digit  (7,  8,  or  9)  is  twice  more  likely  to  be  odd  than  even,  but  the\n",
            "classifier  for  the  “odd”  label  does  not  know  what  the  classifier  for  the  “large”  label\n",
            "predicted. To solve this issue, the models can be organized in a chain: when a model\n",
            "makes  a  prediction,  it  uses  the  input  features  plus  all  the  predictions  of  the  models\n",
            "that come before it in the chain.\n",
            "\n",
            "The  good  news  is  that  Scikit-Learn  has  a  class  called  ChainClassifier  that  does\n",
            "just  that!  By  default  it  will  use  the  true  labels  for  training,  feeding  each  model  the\n",
            "appropriate  labels  depending  on  their  position  in  the  chain.  But  if  you  set  the  cv\n",
            "\n",
            "5 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\n",
            "\n",
            "more details.\n",
            "\n",
            "126 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fhyperparameter, it will use cross-validation to get “clean” (out-of-sample) predictions\n",
            "from each trained model for every instance in the training set, and these predictions\n",
            "will then be used to train all the models later in the chain. Here’s an example showing\n",
            "how  to  create  and  train  a  ChainClassifier  using  the  cross-validation  strategy.  As\n",
            "earlier, we’ll just use the first 2,000 images in the training set to speed things up:\n",
            "\n",
            "from sklearn.multioutput import ClassifierChain\n",
            "\n",
            "chain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\n",
            "chain_clf.fit(X_train[:2000], y_multilabel[:2000])\n",
            "\n",
            "Now we can use this ChainClassifier to make predictions:\n",
            "\n",
            ">>> chain_clf.predict([some_digit])\n",
            "array([[0., 1.]])\n",
            "\n",
            "Multioutput Classification\n",
            "The  last  type  of  classification  task  we’ll  discuss  here  is  called  multioutput–multiclass\n",
            "classification  (or  just  multioutput  classification).  It  is  a  generalization  of  multilabel\n",
            "classification  where  each  label  can  be  multiclass  (i.e.,  it  can  have  more  than  two\n",
            "possible values).\n",
            "\n",
            "To  illustrate  this,  let’s  build  a  system  that  removes  noise  from  images.  It  will  take\n",
            "as  input  a  noisy  digit  image,  and  it  will  (hopefully)  output  a  clean  digit  image,\n",
            "represented as an array of pixel intensities, just like the MNIST images. Notice that\n",
            "the  classifier’s  output  is  multilabel  (one  label  per  pixel)  and  each  label  can  have\n",
            "multiple  values  (pixel  intensity  ranges  from  0  to  255).  This  is  thus  an  example  of  a\n",
            "multioutput classification system.\n",
            "\n",
            "The line between classification and regression is sometimes blurry,\n",
            "such as in this example. Arguably, predicting pixel intensity is more\n",
            "akin  to  regression  than  to  classification.  Moreover,  multioutput\n",
            "systems are not limited to classification tasks; you could even have\n",
            "a  system  that  outputs  multiple  labels  per  instance,  including  both\n",
            "class labels and value labels.\n",
            "\n",
            "Let’s  start  by  creating  the  training  and  test  sets  by  taking  the  MNIST  images  and\n",
            "adding noise to their pixel intensities with NumPy’s randint() function. The target\n",
            "images will be the original images:\n",
            "\n",
            "np.random.seed(42)  # to make this code example reproducible\n",
            "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
            "X_train_mod = X_train + noise\n",
            "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
            "X_test_mod = X_test + noise\n",
            "\n",
            "Multioutput Classification \n",
            "\n",
            "| \n",
            "\n",
            "127\n",
            "\n",
            "\fy_train_mod = X_train\n",
            "y_test_mod = X_test\n",
            "\n",
            "Let’s take a peek at the first image from the test set (Figure 3-12). Yes, we’re snooping\n",
            "on the test data, so you should be frowning right now.\n",
            "\n",
            "Figure 3-12. A noisy image (left) and the target clean image (right)\n",
            "\n",
            "On the left is the noisy input image, and on the right is the clean target image. Now\n",
            "let’s train the classifier and make it clean up this image (Figure 3-13):\n",
            "\n",
            "knn_clf = KNeighborsClassifier()\n",
            "knn_clf.fit(X_train_mod, y_train_mod)\n",
            "clean_digit = knn_clf.predict([X_test_mod[0]])\n",
            "plot_digit(clean_digit)\n",
            "plt.show()\n",
            "\n",
            "Figure 3-13. The cleaned-up image\n",
            "\n",
            "Looks close enough to the target! This concludes our tour of classification. You now\n",
            "know how to select good metrics for classification tasks, pick the appropriate preci‐\n",
            "sion/recall trade-off, compare classifiers, and more generally build good classification\n",
            "systems for a variety of tasks. In the next chapters, you’ll learn how all these machine\n",
            "learning models you’ve been using actually work.\n",
            "\n",
            "128 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fExercises\n",
            "\n",
            "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
            "1.\n",
            "on  the  test  set.  Hint:  the  KNeighborsClassifier  works  quite  well  for  this  task;\n",
            "you  just  need  to  find  good  hyperparameter  values  (try  a  grid  search  on  the\n",
            "weights and n_neighbors hyperparameters).\n",
            "\n",
            "2. Write a function that can shift an MNIST image in any direction (left, right, up,\n",
            "2.\n",
            "or  down)  by  one  pixel.6  Then,  for  each  image  in  the  training  set,  create  four\n",
            "shifted copies (one per direction) and add them to the training set. Finally, train\n",
            "your best model on this expanded training set and measure its accuracy on the\n",
            "test  set.  You  should  observe  that  your  model  performs  even  better  now!  This\n",
            "technique  of  artificially  growing  the  training  set  is  called  data  augmentation  or\n",
            "training set expansion.\n",
            "\n",
            "3. Tackle the Titanic dataset. A great place to start is on Kaggle. Alternatively, you\n",
            "3.\n",
            "can  download  the  data  from  https://homl.info/titanic.tgz  and  unzip  this  tarball\n",
            "like you did for the housing data in Chapter 2. This will give you two CSV files,\n",
            "train.csv  and  test.csv,  which  you  can  load  using  pandas.read_csv().  The  goal\n",
            "is  to  train  a  classifier  that  can  predict  the  Survived  column  based  on  the  other\n",
            "columns.\n",
            "\n",
            "4.\n",
            "4. Build a spam classifier (a more challenging exercise):\n",
            "\n",
            "a.\n",
            "a. Download  examples  of  spam  and  ham  from  Apache  SpamAssassin’s  public\n",
            "\n",
            "datasets.\n",
            "\n",
            "b.\n",
            "b. Unzip the datasets and familiarize yourself with the data format.\n",
            "\n",
            "c.\n",
            "c. Split the data into a training set and a test set.\n",
            "\n",
            "d.\n",
            "d. Write a data preparation pipeline to convert each email into a feature vector.\n",
            "Your  preparation  pipeline  should  transform  an  email  into  a  (sparse)  vector\n",
            "that indicates the presence or absence of each possible word. For example, if\n",
            "all  emails  only  ever  contain  four  words,  “Hello”,  “how”,  “are”,  “you”,  then  the\n",
            "email  “Hello  you  Hello  Hello  you”  would  be  converted  into  a  vector  [1,  0,\n",
            "0,  1]  (meaning  [“Hello”  is  present,  “how”  is  absent,  “are”  is  absent,  “you”  is\n",
            "present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
            "each word.\n",
            "\n",
            "You may want to add hyperparameters to your preparation pipeline to control\n",
            "whether  or  not  to  strip  off  email  headers,  convert  each  email  to  lowercase,\n",
            "remove  punctuation,  replace  all  URLs  with  “URL”,  replace  all  numbers  with\n",
            "\n",
            "6 You can use the shift() function from the scipy.ndimage.interpolation module. For example,\n",
            "\n",
            "shift(image, [2, 1], cval=0) shifts the image two pixels down and one pixel to the right.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "129\n",
            "\n",
            "\f“NUMBER”, or even perform stemming (i.e., trim off word endings; there are\n",
            "Python libraries available to do this).\n",
            "\n",
            "e.\n",
            "e. Finally, try out several classifiers and see if you can build a great spam classi‐\n",
            "\n",
            "fier, with both high recall and high precision.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "130 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 3: Classification\n",
            "\n",
            "\fCHAPTER 4\n",
            "Training Models\n",
            "\n",
            "So far we have treated machine learning models and their training algorithms mostly\n",
            "like black boxes. If you went through some of the exercises in the previous chapters,\n",
            "you may have been surprised by how much you can get done without knowing any‐\n",
            "thing about what’s under the hood: you optimized a regression system, you improved\n",
            "a digit image classifier, and you even built a spam classifier from scratch, all without\n",
            "knowing how they actually work. Indeed, in many situations you don’t really need to\n",
            "know the implementation details.\n",
            "\n",
            "However,  having  a  good  understanding  of  how  things  work  can  help  you  quickly\n",
            "home  in  on  the  appropriate  model,  the  right  training  algorithm  to  use,  and  a  good\n",
            "set of hyperparameters for your task. Understanding what’s under the hood will also\n",
            "help  you  debug  issues  and  perform  error  analysis  more  efficiently.  Lastly,  most  of\n",
            "the topics discussed in this chapter will be essential in understanding, building, and\n",
            "training neural networks (discussed in Part II of this book).\n",
            "\n",
            "In  this  chapter  we  will  start  by  looking  at  the  linear  regression  model,  one  of  the\n",
            "simplest models there is. We will discuss two very different ways to train it:\n",
            "\n",
            "• Using  a  “closed-form”  equation1  that  directly  computes  the  model  parameters\n",
            "•\n",
            "that best fit the model to the training set (i.e., the model parameters that mini‐\n",
            "mize the cost function over the training set).\n",
            "\n",
            "•\n",
            "• Using an iterative optimization approach called gradient descent (GD) that grad‐\n",
            "ually tweaks the model parameters to minimize the cost function over the train‐\n",
            "ing set, eventually converging to the same set of parameters as the first method.\n",
            "We  will  look  at  a  few  variants  of  gradient  descent  that  we  will  use  again  and\n",
            "\n",
            "1 A closed-form equation is only composed of a finite number of constants, variables, and standard operations:\n",
            "\n",
            "for example, a = sin(b – c). No infinite sums, no limits, no integrals, etc.\n",
            "\n",
            "131\n",
            "\n",
            "\fagain when we study neural networks in Part II: batch GD, mini-batch GD, and\n",
            "stochastic GD.\n",
            "\n",
            "Next  we  will  look  at  polynomial  regression,  a  more  complex  model  that  can  fit\n",
            "nonlinear  datasets.  Since  this  model  has  more  parameters  than  linear  regression,\n",
            "it  is  more  prone  to  overfitting  the  training  data.  We  will  explore  how  to  detect\n",
            "whether or not this is the case using learning curves, and then we will look at several\n",
            "regularization techniques that can reduce the risk of overfitting the training set.\n",
            "\n",
            "Finally, we will examine two more models that are commonly used for classification\n",
            "tasks: logistic regression and softmax regression.\n",
            "\n",
            "There will be quite a few math equations in this chapter, using basic\n",
            "notions  of  linear  algebra  and  calculus.  To  understand  these  equa‐\n",
            "tions, you will need to know what vectors and matrices are; how to\n",
            "transpose them, multiply them, and inverse them; and what partial\n",
            "derivatives  are.  If  you  are  unfamiliar  with  these  concepts,  please\n",
            "go  through  the  linear  algebra  and  calculus  introductory  tutorials\n",
            "available as Jupyter notebooks in the online supplemental material.\n",
            "For those who are truly allergic to mathematics, you should still go\n",
            "through this chapter and simply skip the equations; hopefully, the\n",
            "text will be sufficient to help you understand most of the concepts.\n",
            "\n",
            "Linear Regression\n",
            "In Chapter 1 we looked at a simple regression model of life satisfaction:\n",
            "\n",
            "life_satisfaction = θ0 + θ1 × GDP_per_capita\n",
            "\n",
            "This model is just a linear function of the input feature GDP_per_capita. θ0 and θ1 are\n",
            "the model’s parameters.\n",
            "\n",
            "More generally, a linear model makes a prediction by simply computing a weighted\n",
            "sum of the input features, plus a constant called the bias term (also called the intercept\n",
            "term), as shown in Equation 4-1.\n",
            "\n",
            "Equation 4-1. Linear regression model prediction\n",
            "\n",
            "y = θ0 + θ1x1 + θ2x2 + ⋯ + θnxn\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• ŷ is the predicted value.\n",
            "•\n",
            "\n",
            "•\n",
            "• n is the number of features.\n",
            "\n",
            "132 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\f• xi is the ith feature value.\n",
            "•\n",
            "• θj is the jth model parameter, including the bias term θ0 and the feature weights θ1,\n",
            "•\n",
            "\n",
            "θ2, ⋯, θn.\n",
            "\n",
            "This  can  be  written  much  more  concisely  using  a  vectorized  form,  as  shown  in\n",
            "Equation 4-2.\n",
            "\n",
            "Equation 4-2. Linear regression model prediction (vectorized form)\n",
            "\n",
            "y = ℎθ x = θ · x\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• hθ is the hypothesis function, using the model parameters θ.\n",
            "•\n",
            "•\n",
            "• θ  is  the  model’s  parameter  vector,  containing  the  bias  term  θ0  and  the  feature\n",
            "\n",
            "weights θ1 to θn.\n",
            "\n",
            "• x is the instance’s feature vector, containing x0 to xn, with x0 always equal to 1.\n",
            "•\n",
            "•\n",
            "• θ · x is the dot product of the vectors θ and x, which is equal to θ0x0 + θ1x1 + θ2x2\n",
            "\n",
            "+ ... + θnxn.\n",
            "\n",
            "In  machine  learning,  vectors  are  often  represented  as  column  vec‐\n",
            "tors,  which  are  2D  arrays  with  a  single  column.  If  θ  and  x  are\n",
            "column  vectors,  then  the  prediction  is  y = θ⊺x,  where  θ⊺\n",
            "  is  the\n",
            "transpose  of  θ  (a  row  vector  instead  of  a  column  vector)  and  θ⊺x\n",
            "is  the  matrix  multiplication  of  θ⊺\n",
            "  and  x.  It  is  of  course  the  same\n",
            "prediction, except that it is now represented as a single-cell matrix\n",
            "rather  than  a  scalar  value.  In  this  book  I  will  use  this  notation  to\n",
            "avoid switching between dot products and matrix multiplications.\n",
            "\n",
            "OK,  that’s  the  linear  regression  model—but  how  do  we  train  it?  Well,  recall  that\n",
            "training a model means setting its parameters so that the model best fits the training\n",
            "set. For this purpose, we first need a measure of how well (or poorly) the model fits\n",
            "the training data. In Chapter 2 we saw that the most common performance measure\n",
            "of  a  regression  model  is  the  root  mean  square  error  (Equation  2-1).  Therefore,  to\n",
            "train  a  linear  regression  model,  we  need  to  find  the  value  of  θ  that  minimizes  the\n",
            "RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than the\n",
            "RMSE,  and  it  leads  to  the  same  result  (because  the  value  that  minimizes  a  positive\n",
            "function also minimizes its square root).\n",
            "\n",
            "Linear Regression \n",
            "\n",
            "| \n",
            "\n",
            "133\n",
            "\n",
            "\fLearning  algorithms  will  often  optimize  a  different  loss  function\n",
            "during  training  than  the  performance  measure  used  to  evaluate\n",
            "the final model. This is generally because the function is easier to\n",
            "optimize and/or because it has extra terms needed during training\n",
            "only  (e.g.,  for  regularization).  A  good  performance  metric  is  as\n",
            "close  as  possible  to  the  final  business  objective.  A  good  training\n",
            "loss is easy to optimize and strongly correlated with the metric. For\n",
            "example, classifiers are often trained using a cost function such as\n",
            "the log loss (as you will see later in this chapter) but evaluated using\n",
            "precision/recall. The log loss is easy to minimize, and doing so will\n",
            "usually improve precision/recall.\n",
            "\n",
            "The MSE of a linear regression hypothesis hθ on a training set X is calculated using\n",
            "Equation 4-3.\n",
            "\n",
            "Equation 4-3. MSE cost function for a linear regression model\n",
            "\n",
            "MSE X, ℎθ =\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "θ⊺x i − y i 2\n",
            "\n",
            "Most  of  these  notations  were  presented  in  Chapter  2  (see  “Notations”  on  page  44).\n",
            "The only difference is that we write hθ instead of just h to make it clear that the model\n",
            "is  parametrized  by  the  vector  θ.  To  simplify  notations,  we  will  just  write  MSE(θ)\n",
            "instead of MSE(X, hθ).\n",
            "\n",
            "The Normal Equation\n",
            "To find the value of θ that minimizes the MSE, there exists a closed-form solution—in\n",
            "other words, a mathematical equation that gives the result directly. This is called the\n",
            "Normal equation (Equation 4-4).\n",
            "\n",
            "Equation 4-4. Normal equation\n",
            "\n",
            "θ = X⊺X\n",
            "\n",
            "−1\n",
            "\n",
            "  X⊺\n",
            "\n",
            "  y\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• θ is the value of θ that minimizes the cost function.\n",
            "•\n",
            "• y is the vector of target values containing y(1) to y(m).\n",
            "•\n",
            "\n",
            "134 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fLet’s generate some linear-looking data to test this equation on (Figure 4-1):\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "np.random.seed(42)  # to make this code example reproducible\n",
            "m = 100  # number of instances\n",
            "X = 2 * np.random.rand(m, 1)  # column vector\n",
            "y = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n",
            "\n",
            "Figure 4-1. A randomly generated linear dataset\n",
            "\n",
            "Now let’s compute θ using the Normal equation. We will use the inv() function from\n",
            "NumPy’s linear algebra module (np.linalg) to compute the inverse of a matrix, and\n",
            "the dot() method for matrix multiplication:\n",
            "\n",
            "from sklearn.preprocessing import add_dummy_feature\n",
            "\n",
            "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance\n",
            "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
            "\n",
            "The  @  operator  performs  matrix  multiplication.  If  A  and  B  are\n",
            "NumPy arrays, then A @ B is equivalent to np.matmul(A, B). Many\n",
            "other  libraries,  like  TensorFlow,  PyTorch,  and  JAX,  support  the  @\n",
            "operator as well. However, you cannot use @ on pure Python arrays\n",
            "(i.e., lists of lists).\n",
            "\n",
            "Linear Regression \n",
            "\n",
            "| \n",
            "\n",
            "135\n",
            "\n",
            "\fThe function that we used to generate the data is y = 4 + 3x1 + Gaussian noise. Let’s\n",
            "see what the equation found:\n",
            "\n",
            ">>> theta_best\n",
            "array([[4.21509616],\n",
            "       [2.77011339]])\n",
            "\n",
            "We  would  have  hoped  for  θ0  =  4  and  θ1  =  3  instead  of  θ0  =  4.215  and  θ1  =  2.770.\n",
            "Close enough, but the noise made it impossible to recover the exact parameters of the\n",
            "original function. The smaller and noisier the dataset, the harder it gets.\n",
            "\n",
            "Now we can make predictions using θ:\n",
            "\n",
            ">>> X_new = np.array([[0], [2]])\n",
            ">>> X_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\n",
            ">>> y_predict = X_new_b @ theta_best\n",
            ">>> y_predict\n",
            "array([[4.21509616],\n",
            "       [9.75532293]])\n",
            "\n",
            "Let’s plot this model’s predictions (Figure 4-2):\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
            "plt.plot(X, y, \"b.\")\n",
            "[...]  # beautify the figure: add labels, axis, grid, and legend\n",
            "plt.show()\n",
            "\n",
            "Figure 4-2. Linear regression model predictions\n",
            "\n",
            "136 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fPerforming linear regression using Scikit-Learn is relatively straightforward:\n",
            "\n",
            ">>> from sklearn.linear_model import LinearRegression\n",
            ">>> lin_reg = LinearRegression()\n",
            ">>> lin_reg.fit(X, y)\n",
            ">>> lin_reg.intercept_, lin_reg.coef_\n",
            "(array([4.21509616]), array([[2.77011339]]))\n",
            ">>> lin_reg.predict(X_new)\n",
            "array([[4.21509616],\n",
            "       [9.75532293]])\n",
            "\n",
            "Notice  that  Scikit-Learn  separates  the  bias  term  (intercept_)  from  the  feature\n",
            "weights (coef_). The LinearRegression class is based on the scipy.linalg.lstsq()\n",
            "function (the name stands for “least squares”), which you could call directly:\n",
            "\n",
            ">>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
            ">>> theta_best_svd\n",
            "array([[4.21509616],\n",
            "       [2.77011339]])\n",
            "\n",
            "This  function  computes  θ = X+y,  where  X+  is  the  pseudoinverse  of  X  (specifi‐\n",
            "cally,  the  Moore–Penrose  inverse).  You  can  use  np.linalg.pinv()  to  compute  the\n",
            "pseudoinverse directly:\n",
            "\n",
            ">>> np.linalg.pinv(X_b) @ y\n",
            "array([[4.21509616],\n",
            "       [2.77011339]])\n",
            "\n",
            "The pseudoinverse itself is computed using a standard matrix factorization technique\n",
            "called singular value decomposition (SVD) that can decompose the training set matrix\n",
            "X into the matrix multiplication of three matrices U Σ V⊺ (see numpy.linalg.svd()).\n",
            "The  pseudoinverse  is  computed  as  X+ = VΣ+U⊺\n",
            ".  To  compute  the  matrix  Σ+,  the\n",
            "algorithm  takes  Σ  and  sets  to  zero  all  values  smaller  than  a  tiny  threshold  value,\n",
            "then  it  replaces  all  the  nonzero  values  with  their  inverse,  and  finally  it  transposes\n",
            "the  resulting  matrix.  This  approach  is  more  efficient  than  computing  the  Normal\n",
            "equation,  plus  it  handles  edge  cases  nicely:  indeed,  the  Normal  equation  may  not\n",
            "work  if  the  matrix  X⊺X  is  not  invertible  (i.e.,  singular),  such  as  if  m  <  n  or  if  some\n",
            "features are redundant, but the pseudoinverse is always defined.\n",
            "\n",
            "Computational Complexity\n",
            "The  Normal  equation  computes  the  inverse  of  X⊺  X,  which  is  an  (n  +  1)  ×  (n  +  1)\n",
            "matrix (where n is the number of features). The computational complexity of inverting\n",
            "such a matrix is typically about O(n2.4) to O(n3), depending on the implementation.\n",
            "In other words, if you double the number of features, you multiply the computation\n",
            "time by roughly 22.4 = 5.3 to 23 = 8.\n",
            "\n",
            "Linear Regression \n",
            "\n",
            "| \n",
            "\n",
            "137\n",
            "\n",
            "\fThe SVD approach used by Scikit-Learn’s LinearRegression class is about O(n2). If\n",
            "you double the number of features, you multiply the computation time by roughly 4.\n",
            "\n",
            "Both  the  Normal  equation  and  the  SVD  approach  get  very  slow\n",
            "when  the  number  of  features  grows  large  (e.g.,  100,000).  On  the\n",
            "positive side, both are linear with regard to the number of instances\n",
            "in  the  training  set  (they  are  O(m)),  so  they  handle  large  training\n",
            "sets efficiently, provided they can fit in memory.\n",
            "\n",
            "Also, once you have trained your linear regression model (using the Normal equation\n",
            "or  any  other  algorithm),  predictions  are  very  fast:  the  computational  complexity  is\n",
            "linear with regard to both the number of instances you want to make predictions on\n",
            "and  the  number  of  features.  In  other  words,  making  predictions  on  twice  as  many\n",
            "instances (or twice as many features) will take roughly twice as much time.\n",
            "\n",
            "Now we will look at a very different way to train a linear regression model, which is\n",
            "better suited for cases where there are a large number of features or too many training\n",
            "instances to fit in memory.\n",
            "\n",
            "Gradient Descent\n",
            "Gradient descent is a generic optimization algorithm capable of finding optimal solu‐\n",
            "tions  to  a  wide  range  of  problems.  The  general  idea  of  gradient  descent  is  to  tweak\n",
            "parameters iteratively in order to minimize a cost function.\n",
            "\n",
            "Suppose you are lost in the mountains in a dense fog, and you can only feel the slope\n",
            "of  the  ground  below  your  feet.  A  good  strategy  to  get  to  the  bottom  of  the  valley\n",
            "quickly  is  to  go  downhill  in  the  direction  of  the  steepest  slope.  This  is  exactly  what\n",
            "gradient descent does: it measures the local gradient of the error function with regard\n",
            "to the parameter vector θ, and it goes in the direction of descending gradient. Once\n",
            "the gradient is zero, you have reached a minimum!\n",
            "\n",
            "In  practice,  you  start  by  filling  θ  with  random  values  (this  is  called  random  initiali‐\n",
            "zation).  Then  you  improve  it  gradually,  taking  one  baby  step  at  a  time,  each  step\n",
            "attempting to decrease the cost function (e.g., the MSE), until the algorithm converges\n",
            "to a minimum (see Figure 4-3).\n",
            "\n",
            "138 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFigure 4-3. In this depiction of gradient descent, the model parameters are initialized\n",
            "randomly and get tweaked repeatedly to minimize the cost function; the learning step\n",
            "size is proportional to the slope of the cost function, so the steps gradually get smaller as\n",
            "the cost approaches the minimum\n",
            "\n",
            "An  important  parameter  in  gradient  descent  is  the  size  of  the  steps,  determined  by\n",
            "the learning rate hyperparameter. If the learning rate is too small, then the algorithm\n",
            "will have to go through many iterations to converge, which will take a long time (see\n",
            "Figure 4-4).\n",
            "\n",
            "Figure 4-4. Learning rate too small\n",
            "\n",
            "Gradient Descent \n",
            "\n",
            "| \n",
            "\n",
            "139\n",
            "\n",
            "\fOn the other hand, if the learning rate is too high, you might jump across the valley\n",
            "and  end  up  on  the  other  side,  possibly  even  higher  up  than  you  were  before.  This\n",
            "might make the algorithm diverge, with larger and larger values, failing to find a good\n",
            "solution (see Figure 4-5).\n",
            "\n",
            "Figure 4-5. Learning rate too high\n",
            "\n",
            "Additionally,  not  all  cost  functions  look  like  nice,  regular  bowls.  There  may  be\n",
            "holes,  ridges,  plateaus,  and  all  sorts  of  irregular  terrain,  making  convergence  to  the\n",
            "minimum difficult. Figure 4-6 shows the two main challenges with gradient descent.\n",
            "If the random initialization starts the algorithm on the left, then it will converge to a\n",
            "local minimum, which is not as good as the global minimum. If it starts on the right,\n",
            "then it will take a very long time to cross the plateau. And if you stop too early, you\n",
            "will never reach the global minimum.\n",
            "\n",
            "Figure 4-6. Gradient descent pitfalls\n",
            "\n",
            "140 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFortunately,  the  MSE  cost  function  for  a  linear  regression  model  happens  to  be  a\n",
            "convex function, which means that if you pick any two points on the curve, the line\n",
            "segment joining them is never below the curve. This implies that there are no local\n",
            "minima, just one global minimum. It is also a continuous function with a slope that\n",
            "never changes abruptly.2 These two facts have a great consequence: gradient descent\n",
            "is  guaranteed  to  approach  arbitrarily  closely  the  global  minimum  (if  you  wait  long\n",
            "enough and if the learning rate is not too high).\n",
            "\n",
            "While  the  cost  function  has  the  shape  of  a  bowl,  it  can  be  an  elongated  bowl  if  the\n",
            "features have very different scales. Figure 4-7 shows gradient descent on a training set\n",
            "where features 1 and 2 have the same scale (on the left), and on a training set where\n",
            "feature 1 has much smaller values than feature 2 (on the right).3\n",
            "\n",
            "Figure 4-7. Gradient descent with (left) and without (right) feature scaling\n",
            "\n",
            "As  you  can  see,  on  the  left  the  gradient  descent  algorithm  goes  straight  toward  the\n",
            "minimum, thereby reaching it quickly, whereas on the right it first goes in a direction\n",
            "almost  orthogonal  to  the  direction  of  the  global  minimum,  and  it  ends  with  a  long\n",
            "march  down  an  almost  flat  valley.  It  will  eventually  reach  the  minimum,  but  it  will\n",
            "take a long time.\n",
            "\n",
            "When  using  gradient  descent,  you  should  ensure  that  all  features\n",
            "have  a  similar  scale  (e.g.,  using  Scikit-Learn’s  StandardScaler\n",
            "class), or else it will take much longer to converge.\n",
            "\n",
            "2 Technically speaking, its derivative is Lipschitz continuous.\n",
            "\n",
            "3 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is\n",
            "\n",
            "elongated along the θ1 axis.\n",
            "\n",
            "Gradient Descent \n",
            "\n",
            "| \n",
            "\n",
            "141\n",
            "\n",
            "\fThis  diagram  also  illustrates  the  fact  that  training  a  model  means  searching  for  a\n",
            "combination of model parameters that minimizes a cost function (over the training\n",
            "set).  It  is  a  search  in  the  model’s  parameter  space.  The  more  parameters  a  model\n",
            "has,  the  more  dimensions  this  space  has,  and  the  harder  the  search  is:  searching\n",
            "for  a  needle  in  a  300-dimensional  haystack  is  much  trickier  than  in  3  dimensions.\n",
            "Fortunately,  since  the  cost  function  is  convex  in  the  case  of  linear  regression,  the\n",
            "needle is simply at the bottom of the bowl.\n",
            "\n",
            "Batch Gradient Descent\n",
            "To implement gradient descent, you need to compute the gradient of the cost func‐\n",
            "tion  with  regard  to  each  model  parameter  θj.  In  other  words,  you  need  to  calculate\n",
            "how much the cost function will change if you change θj just a little bit. This is called\n",
            "a  partial  derivative.  It  is  like  asking,  “What  is  the  slope  of  the  mountain  under  my\n",
            "feet if I face east”? and then asking the same question facing north (and so on for all\n",
            "other dimensions, if you can imagine a universe with more than three dimensions).\n",
            "Equation 4-5 computes the partial derivative of the MSE with regard to parameter θj,\n",
            "noted ∂ MSE(θ) / ∂θj.\n",
            "\n",
            "Equation 4-5. Partial derivatives of the cost function\n",
            "\n",
            "∂\n",
            "∂θj\n",
            "\n",
            "MSE θ =\n",
            "\n",
            "m\n",
            "\n",
            "2\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "θ⊺x i − y i xj\n",
            "\n",
            "i\n",
            "\n",
            "Instead of computing these partial derivatives individually, you can use Equation 4-6\n",
            "to compute them all in one go. The gradient vector, noted ∇θMSE(θ), contains all the\n",
            "partial derivatives of the cost function (one for each model parameter).\n",
            "\n",
            "Equation 4-6. Gradient vector of the cost function\n",
            "\n",
            "∇θ MSE θ =\n",
            "\n",
            "∂\n",
            "∂θ0\n",
            "∂\n",
            "∂θ1\n",
            "\n",
            "∂\n",
            "∂θn\n",
            "\n",
            "MSE θ\n",
            "\n",
            "MSE θ\n",
            "\n",
            "⋮\n",
            "\n",
            "MSE θ\n",
            "\n",
            "=\n",
            "\n",
            "2\n",
            "m\n",
            "\n",
            "X⊺ Xθ − y\n",
            "\n",
            "142 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fNotice that this formula involves calculations over the full training\n",
            "set  X,  at  each  gradient  descent  step!  This  is  why  the  algorithm  is\n",
            "called  batch  gradient  descent:  it  uses  the  whole  batch  of  training\n",
            "data  at  every  step  (actually,  full  gradient  descent  would  probably\n",
            "be  a  better  name).  As  a  result,  it  is  terribly  slow  on  very  large\n",
            "training  sets  (we  will  look  at  some  much  faster  gradient  descent\n",
            "algorithms shortly). However, gradient descent scales well with the\n",
            "number of features; training a linear regression model when there\n",
            "are hundreds of thousands of features is much faster using gradient\n",
            "descent than using the Normal equation or SVD decomposition.\n",
            "\n",
            "Once  you  have  the  gradient  vector,  which  points  uphill,  just  go  in  the  opposite\n",
            "direction to go downhill. This means subtracting ∇θMSE(θ) from θ. This is where the\n",
            "learning rate η comes into play:4 multiply the gradient vector by η to determine the\n",
            "size of the downhill step (Equation 4-7).\n",
            "\n",
            "Equation 4-7. Gradient descent step\n",
            "\n",
            "θ next step = θ − η ∇θ MSE θ\n",
            "\n",
            "Let’s look at a quick implementation of this algorithm:\n",
            "\n",
            "eta = 0.1  # learning rate\n",
            "n_epochs = 1000\n",
            "m = len(X_b)  # number of instances\n",
            "\n",
            "np.random.seed(42)\n",
            "theta = np.random.randn(2, 1)  # randomly initialized model parameters\n",
            "\n",
            "for epoch in range(n_epochs):\n",
            "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
            "    theta = theta - eta * gradients\n",
            "\n",
            "That wasn’t too hard! Each iteration over the training set is called an epoch. Let’s look\n",
            "at the resulting theta:\n",
            "\n",
            ">>> theta\n",
            "array([[4.21509616],\n",
            "       [2.77011339]])\n",
            "\n",
            "Hey,  that’s  exactly  what  the  Normal  equation  found!  Gradient  descent  worked  per‐\n",
            "fectly.  But  what  if  you  had  used  a  different  learning  rate  (eta)?  Figure  4-8  shows\n",
            "the  first  20  steps  of  gradient  descent  using  three  different  learning  rates.  The  line\n",
            "at the bottom of each plot represents the random starting point, then each epoch is\n",
            "represented by a darker and darker line.\n",
            "\n",
            "4 Eta (η) is the seventh letter of the Greek alphabet.\n",
            "\n",
            "Gradient Descent \n",
            "\n",
            "| \n",
            "\n",
            "143\n",
            "\n",
            "\fFigure 4-8. Gradient descent with various learning rates\n",
            "\n",
            "On  the  left,  the  learning  rate  is  too  low:  the  algorithm  will  eventually  reach  the\n",
            "solution,  but  it  will  take  a  long  time.  In  the  middle,  the  learning  rate  looks  pretty\n",
            "good:  in  just  a  few  epochs,  it  has  already  converged  to  the  solution.  On  the  right,\n",
            "the learning rate is too high: the algorithm diverges, jumping all over the place and\n",
            "actually getting further and further away from the solution at every step.\n",
            "\n",
            "To find a good learning rate, you can use grid search (see Chapter 2). However, you\n",
            "may want to limit the number of epochs so that grid search can eliminate models that\n",
            "take too long to converge.\n",
            "\n",
            "You  may  wonder  how  to  set  the  number  of  epochs.  If  it  is  too  low,  you  will  still  be\n",
            "far  away  from  the  optimal  solution  when  the  algorithm  stops;  but  if  it  is  too  high,\n",
            "you  will  waste  time  while  the  model  parameters  do  not  change  anymore.  A  simple\n",
            "solution is to set a very large number of epochs but to interrupt the algorithm when\n",
            "the  gradient  vector  becomes  tiny—that  is,  when  its  norm  becomes  smaller  than  a\n",
            "tiny number ϵ (called the tolerance)—because this happens when gradient descent has\n",
            "(almost) reached the minimum.\n",
            "\n",
            "Convergence Rate\n",
            "When  the  cost  function  is  convex  and  its  slope  does  not  change  abruptly  (as  is  the\n",
            "case for the MSE cost function), batch gradient descent with a fixed learning rate will\n",
            "eventually converge to the optimal solution, but you may have to wait a while: it can\n",
            "take  O(1/ϵ)  iterations  to  reach  the  optimum  within  a  range  of ϵ,  depending  on  the\n",
            "shape of the cost function. If you divide the tolerance by 10 to have a more precise\n",
            "solution, then the algorithm may have to run about 10 times longer.\n",
            "\n",
            "144 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fStochastic Gradient Descent\n",
            "The  main  problem  with  batch  gradient  descent  is  the  fact  that  it  uses  the  whole\n",
            "training  set  to  compute  the  gradients  at  every  step,  which  makes  it  very  slow  when\n",
            "the  training  set  is  large.  At  the  opposite  extreme,  stochastic  gradient  descent  picks  a\n",
            "random instance in the training set at every step and computes the gradients based\n",
            "only on that single instance. Obviously, working on a single instance at a time makes\n",
            "the  algorithm  much  faster  because  it  has  very  little  data  to  manipulate  at  every\n",
            "iteration.  It  also  makes  it  possible  to  train  on  huge  training  sets,  since  only  one\n",
            "instance needs to be in memory at each iteration (stochastic GD can be implemented\n",
            "as an out-of-core algorithm; see Chapter 1).\n",
            "\n",
            "On  the  other  hand,  due  to  its  stochastic  (i.e.,  random)  nature,  this  algorithm  is\n",
            "much  less  regular  than  batch  gradient  descent:  instead  of  gently  decreasing  until  it\n",
            "reaches  the  minimum,  the  cost  function  will  bounce  up  and  down,  decreasing  only\n",
            "on average. Over time it will end up very close to the minimum, but once it gets there\n",
            "it  will  continue  to  bounce  around,  never  settling  down  (see  Figure  4-9).  Once  the\n",
            "algorithm stops, the final parameter values will be good, but not optimal.\n",
            "\n",
            "Figure 4-9. With stochastic gradient descent, each training step is much faster but also\n",
            "much more stochastic than when using batch gradient descent\n",
            "\n",
            "When  the  cost  function  is  very  irregular  (as  in  Figure  4-6),  this  can  actually  help\n",
            "the  algorithm  jump  out  of  local  minima,  so  stochastic  gradient  descent  has  a  better\n",
            "chance of finding the global minimum than batch gradient descent does.\n",
            "\n",
            "Therefore, randomness is good to escape from local optima, but bad because it means\n",
            "that  the  algorithm  can  never  settle  at  the  minimum.  One  solution  to  this  dilemma\n",
            "is to gradually reduce the learning rate. The steps start out large (which helps make\n",
            "quick  progress  and  escape  local  minima),  then  get  smaller  and  smaller,  allowing\n",
            "the  algorithm  to  settle  at  the  global  minimum.  This  process  is  akin  to  simulated\n",
            "annealing,  an  algorithm  inspired  by  the  process  in  metallurgy  of  annealing,  where\n",
            "molten metal is slowly cooled down. The function that determines the learning rate\n",
            "\n",
            "Gradient Descent \n",
            "\n",
            "| \n",
            "\n",
            "145\n",
            "\n",
            "\fat  each  iteration  is  called  the  learning  schedule.  If  the  learning  rate  is  reduced  too\n",
            "quickly,  you  may  get  stuck  in  a  local  minimum,  or  even  end  up  frozen  halfway  to\n",
            "the  minimum.  If  the  learning  rate  is  reduced  too  slowly,  you  may  jump  around  the\n",
            "minimum for a long time and end up with a suboptimal solution if you halt training\n",
            "too early.\n",
            "\n",
            "This code implements stochastic gradient descent using a simple learning schedule:\n",
            "\n",
            "n_epochs = 50\n",
            "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
            "\n",
            "def learning_schedule(t):\n",
            "    return t0 / (t + t1)\n",
            "\n",
            "np.random.seed(42)\n",
            "theta = np.random.randn(2, 1)  # random initialization\n",
            "\n",
            "for epoch in range(n_epochs):\n",
            "    for iteration in range(m):\n",
            "        random_index = np.random.randint(m)\n",
            "        xi = X_b[random_index : random_index + 1]\n",
            "        yi = y[random_index : random_index + 1]\n",
            "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
            "        eta = learning_schedule(epoch * m + iteration)\n",
            "        theta = theta - eta * gradients\n",
            "\n",
            "By convention we iterate by rounds of m iterations; each round is called an epoch, as\n",
            "earlier. While the batch gradient descent code iterated 1,000 times through the whole\n",
            "training set, this code goes through the training set only 50 times and reaches a pretty\n",
            "good solution:\n",
            "\n",
            ">>> theta\n",
            "array([[4.21076011],\n",
            "       [2.74856079]])\n",
            "\n",
            "Figure 4-10 shows the first 20 steps of training (notice how irregular the steps are).\n",
            "\n",
            "Note that since instances are picked randomly, some instances may be picked several\n",
            "times per epoch, while others may not be picked at all. If you want to be sure that the\n",
            "algorithm goes through every instance at each epoch, another approach is to shuffle\n",
            "the training set (making sure to shuffle the input features and the labels jointly), then\n",
            "go  through  it  instance  by  instance,  then  shuffle  it  again,  and  so  on.  However,  this\n",
            "approach is more complex, and it generally does not improve the result.\n",
            "\n",
            "146 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFigure 4-10. The first 20 steps of stochastic gradient descent\n",
            "\n",
            "When  using  stochastic  gradient  descent,  the  training  instances\n",
            "must  be  independent  and  identically  distributed  (IID)  to  ensure\n",
            "that  the  parameters  get  pulled  toward  the  global  optimum,  on\n",
            "average.  A  simple  way  to  ensure  this  is  to  shuffle  the  instances\n",
            "during  training  (e.g.,  pick  each  instance  randomly,  or  shuffle  the\n",
            "training set at the beginning of each epoch). If you do not shuffle\n",
            "the  instances—for  example,  if  the  instances  are  sorted  by  label—\n",
            "then SGD will start by optimizing for one label, then the next, and\n",
            "so on, and it will not settle close to the global minimum.\n",
            "\n",
            "To  perform  linear  regression  using  stochastic  GD  with  Scikit-Learn,  you  can  use\n",
            "the  SGDRegressor  class,  which  defaults  to  optimizing  the  MSE  cost  function.  The\n",
            "following code runs for maximum 1,000 epochs (max_iter) or until the loss drops by\n",
            "less than 10–5 (tol) during 100 epochs (n_iter_no_change). It starts with a learning\n",
            "rate  of  0.01  (eta0),  using  the  default  learning  schedule  (different  from  the  one  we\n",
            "used). Lastly, it does not use any regularization (penalty=None; more details on this\n",
            "shortly):\n",
            "\n",
            "from sklearn.linear_model import SGDRegressor\n",
            "\n",
            "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
            "                       n_iter_no_change=100, random_state=42)\n",
            "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n",
            "\n",
            "Gradient Descent \n",
            "\n",
            "| \n",
            "\n",
            "147\n",
            "\n",
            "\fOnce  again,  you  find  a  solution  quite  close  to  the  one  returned  by  the  Normal\n",
            "equation:\n",
            "\n",
            ">>> sgd_reg.intercept_, sgd_reg.coef_\n",
            "(array([4.21278812]), array([2.77270267]))\n",
            "\n",
            "All Scikit-Learn estimators can be trained using the fit() method,\n",
            "but  some  estimators  also  have  a  partial_fit()  method  that  you\n",
            "can call to run a single round of training on one or more instances\n",
            "(it ignores hyperparameters like max_iter or tol). Repeatedly call‐\n",
            "ing  partial_fit()  will  gradually  train  the  model.  This  is  useful\n",
            "when  you  need  more  control  over  the  training  process.  Other\n",
            "models have a warm_start hyperparameter instead (and some have\n",
            "both): if you set warm_start=True, calling the fit() method on a\n",
            "trained model will not reset the model; it will just continue training\n",
            "where it left off, respecting hyperparameters like max_iter and tol.\n",
            "Note  that  fit()  resets  the  iteration  counter  used  by  the  learning\n",
            "schedule, while partial_fit() does not.\n",
            "\n",
            "Mini-Batch Gradient Descent\n",
            "The  last  gradient  descent  algorithm  we  will  look  at  is  called  mini-batch  gradient\n",
            "descent.  It  is  straightforward  once  you  know  batch  and  stochastic  gradient  descent:\n",
            "at each step, instead of computing the gradients based on the full training set (as in\n",
            "batch GD) or based on just one instance (as in stochastic GD), mini-batch GD com‐\n",
            "putes the gradients on small random sets of instances called mini-batches. The main\n",
            "advantage  of  mini-batch  GD  over  stochastic  GD  is  that  you  can  get  a  performance\n",
            "boost from hardware optimization of matrix operations, especially when using GPUs.\n",
            "\n",
            "The  algorithm’s  progress  in  parameter  space  is  less  erratic  than  with  stochastic  GD,\n",
            "especially  with  fairly  large  mini-batches.  As  a  result,  mini-batch  GD  will  end  up\n",
            "walking  around  a  bit  closer  to  the  minimum  than  stochastic  GD—but  it  may  be\n",
            "harder  for  it  to  escape  from  local  minima  (in  the  case  of  problems  that  suffer  from\n",
            "local  minima,  unlike  linear  regression  with  the  MSE  cost  function).  Figure  4-11\n",
            "shows  the  paths  taken  by  the  three  gradient  descent  algorithms  in  parameter  space\n",
            "during  training.  They  all  end  up  near  the  minimum,  but  batch  GD’s  path  actually\n",
            "stops  at  the  minimum,  while  both  stochastic  GD  and  mini-batch  GD  continue  to\n",
            "walk  around.  However,  don’t  forget  that  batch  GD  takes  a  lot  of  time  to  take  each\n",
            "step,  and  stochastic  GD  and  mini-batch  GD  would  also  reach  the  minimum  if  you\n",
            "used a good learning schedule.\n",
            "\n",
            "148 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFigure 4-11. Gradient descent paths in parameter space\n",
            "\n",
            "Table 4-1 compares the algorithms we’ve discussed so far for linear regression5 (recall\n",
            "that m is the number of training instances and n is the number of features).\n",
            "\n",
            "Table 4-1. Comparison of algorithms for linear regression\n",
            "\n",
            "Large m Out-of-core support\n",
            "\n",
            "Algorithm\n",
            "Normal equation Fast\n",
            "\n",
            "SVD\n",
            "\n",
            "Batch GD\n",
            "\n",
            "Stochastic GD\n",
            "\n",
            "Mini-batch GD\n",
            "\n",
            "Fast\n",
            "\n",
            "Slow\n",
            "\n",
            "Fast\n",
            "\n",
            "Fast\n",
            "\n",
            "No\n",
            "\n",
            "No\n",
            "\n",
            "No\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "Large n Hyperparams\n",
            "0\n",
            "Slow\n",
            "\n",
            "Scaling required Scikit-Learn\n",
            "No\n",
            "\n",
            "N/A\n",
            "\n",
            "Slow\n",
            "\n",
            "Fast\n",
            "\n",
            "Fast\n",
            "\n",
            "Fast\n",
            "\n",
            "0\n",
            "\n",
            "2\n",
            "\n",
            "≥2\n",
            "\n",
            "≥2\n",
            "\n",
            "No\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "LinearRegression\n",
            "\n",
            "N/A\n",
            "\n",
            "SGDRegressor\n",
            "\n",
            "N/A\n",
            "\n",
            "There  is  almost  no  difference  after  training:  all  these  algorithms  end  up  with  very\n",
            "similar models and make predictions in exactly the same way.\n",
            "\n",
            "Polynomial Regression\n",
            "What if your data is more complex than a straight line? Surprisingly, you can use a\n",
            "linear model to fit nonlinear data. A simple way to do this is to add powers of each\n",
            "feature as new features, then train a linear model on this extended set of features. This\n",
            "technique is called polynomial regression.\n",
            "\n",
            "5 While the Normal equation can only perform linear regression, the gradient descent algorithms can be used\n",
            "\n",
            "to train many other models, as you’ll see.\n",
            "\n",
            "Polynomial Regression \n",
            "\n",
            "| \n",
            "\n",
            "149\n",
            "\n",
            "\fLet’s look at an example. First, we’ll generate some nonlinear data (see Figure 4-12),\n",
            "based on a simple quadratic equation—that’s an equation of the form y = ax² + bx +\n",
            "c—plus some noise:\n",
            "\n",
            "np.random.seed(42)\n",
            "m = 100\n",
            "X = 6 * np.random.rand(m, 1) - 3\n",
            "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n",
            "\n",
            "Figure 4-12. Generated nonlinear and noisy dataset\n",
            "\n",
            "Clearly,  a  straight  line  will  never  fit  this  data  properly.  So  let’s  use  Scikit-Learn’s\n",
            "PolynomialFeatures class to transform our training data, adding the square (second-\n",
            "degree  polynomial)  of  each  feature  in  the  training  set  as  a  new  feature  (in  this  case\n",
            "there is just one feature):\n",
            "\n",
            ">>> from sklearn.preprocessing import PolynomialFeatures\n",
            ">>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
            ">>> X_poly = poly_features.fit_transform(X)\n",
            ">>> X[0]\n",
            "array([-0.75275929])\n",
            ">>> X_poly[0]\n",
            "array([-0.75275929,  0.56664654])\n",
            "\n",
            "X_poly now contains the original feature of X plus the square of this feature. Now we\n",
            "can fit a LinearRegression model to this extended training data (Figure 4-13):\n",
            "\n",
            ">>> lin_reg = LinearRegression()\n",
            ">>> lin_reg.fit(X_poly, y)\n",
            ">>> lin_reg.intercept_, lin_reg.coef_\n",
            "(array([1.78134581]), array([[0.93366893, 0.56456263]]))\n",
            "\n",
            "150 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFigure 4-13. Polynomial regression model predictions\n",
            "\n",
            "Not  bad:  the  model  estimates  y = 0.56x1\n",
            "function was y = 0.5x1\n",
            "\n",
            "2 + 1.0x1 + 2.0 + Gaussian noise.\n",
            "\n",
            "2 + 0.93x1 + 1.78  when  in  fact  the  original\n",
            "\n",
            "Note  that  when  there  are  multiple  features,  polynomial  regression  is  capable  of\n",
            "finding  relationships  between  features,  which  is  something  a  plain  linear  regression\n",
            "model  cannot  do.  This  is  made  possible  by  the  fact  that  PolynomialFeatures  also\n",
            "adds all combinations of features up to the given degree. For example, if there were\n",
            "two  features  a  and  b,  PolynomialFeatures  with  degree=3  would  not  only  add  the\n",
            "features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\n",
            "\n",
            "PolynomialFeatures(degree=d) transforms an array containing n\n",
            "features into an array containing (n + d)! / d!n! features, where n!\n",
            "is  the  factorial  of  n,  equal  to  1  ×  2  ×  3  ×  ⋯  ×  n.  Beware  of  the\n",
            "combinatorial explosion of the number of features!\n",
            "\n",
            "Learning Curves\n",
            "If you perform high-degree polynomial regression, you will likely fit the training data\n",
            "much  better  than  with  plain  linear  regression.  For  example,  Figure  4-14  applies  a\n",
            "300-degree polynomial model to the preceding training data, and compares the result\n",
            "with a pure linear model and a quadratic model (second-degree polynomial). Notice\n",
            "how the 300-degree polynomial model wiggles around to get as close as possible to\n",
            "the training instances.\n",
            "\n",
            "Learning Curves \n",
            "\n",
            "| \n",
            "\n",
            "151\n",
            "\n",
            "\fFigure 4-14. High-degree polynomial regression\n",
            "\n",
            "This  high-degree  polynomial  regression  model  is  severely  overfitting  the  training\n",
            "data, while the linear model is underfitting it. The model that will generalize best in\n",
            "this case is the quadratic model, which makes sense because the data was generated\n",
            "using a quadratic model. But in general you won’t know what function generated the\n",
            "data, so how can you decide how complex your model should be? How can you tell\n",
            "that your model is overfitting or underfitting the data?\n",
            "\n",
            "In Chapter 2 you used cross-validation to get an estimate of a model’s generalization\n",
            "performance.  If  a  model  performs  well  on  the  training  data  but  generalizes  poorly\n",
            "according  to  the  cross-validation  metrics,  then  your  model  is  overfitting.  If  it  per‐\n",
            "forms poorly on both, then it is underfitting. This is one way to tell when a model is\n",
            "too simple or too complex.\n",
            "\n",
            "Another  way  to  tell  is  to  look  at  the  learning  curves,  which  are  plots  of  the  model’s\n",
            "training error and validation error as a function of the training iteration: just evaluate\n",
            "the  model  at  regular  intervals  during  training  on  both  the  training  set  and  the\n",
            "validation set, and plot the results. If the model cannot be trained incrementally (i.e.,\n",
            "if it does not support partial_fit() or warm_start), then you must train it several\n",
            "times on gradually larger subsets of the training set.\n",
            "\n",
            "Scikit-Learn  has  a  useful  learning_curve()  function  to  help  with  this:  it  trains\n",
            "and  evaluates  the  model  using  cross-validation.  By  default  it  retrains  the  model  on\n",
            "growing  subsets  of  the  training  set,  but  if  the  model  supports  incremental  learning\n",
            "you  can  set  exploit_incremental_learning=True  when  calling  learning_curve()\n",
            "and  it  will  train  the  model  incrementally  instead.  The  function  returns  the  training\n",
            "set  sizes  at  which  it  evaluated  the  model,  and  the  training  and  validation  scores  it\n",
            "\n",
            "152 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fmeasured  for  each  size  and  for  each  cross-validation  fold.  Let’s  use  this  function  to\n",
            "look at the learning curves of the plain linear regression model (see Figure 4-15):\n",
            "\n",
            "from sklearn.model_selection import learning_curve\n",
            "\n",
            "train_sizes, train_scores, valid_scores = learning_curve(\n",
            "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
            "    scoring=\"neg_root_mean_squared_error\")\n",
            "train_errors = -train_scores.mean(axis=1)\n",
            "valid_errors = -valid_scores.mean(axis=1)\n",
            "\n",
            "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
            "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
            "[...]  # beautify the figure: add labels, axis, grid, and legend\n",
            "plt.show()\n",
            "\n",
            "Figure 4-15. Learning curves\n",
            "\n",
            "This model is underfitting. To see why, first let’s look at the training error. When there\n",
            "are  just  one  or  two  instances  in  the  training  set,  the  model  can  fit  them  perfectly,\n",
            "which is why the curve starts at zero. But as new instances are added to the training\n",
            "set, it becomes impossible for the model to fit the training data perfectly, both because\n",
            "the  data  is  noisy  and  because  it  is  not  linear  at  all.  So  the  error  on  the  training\n",
            "data  goes  up  until  it  reaches  a  plateau,  at  which  point  adding  new  instances  to  the\n",
            "training  set  doesn’t  make  the  average  error  much  better  or  worse.  Now  let’s  look  at\n",
            "the validation error. When the model is trained on very few training instances, it is\n",
            "incapable of generalizing properly, which is why the validation error is initially quite\n",
            "large.  Then,  as  the  model  is  shown  more  training  examples,  it  learns,  and  thus  the\n",
            "validation  error  slowly  goes  down.  However,  once  again  a  straight  line  cannot  do  a\n",
            "good  job  of  modeling  the  data,  so  the  error  ends  up  at  a  plateau,  very  close  to  the\n",
            "other curve.\n",
            "\n",
            "Learning Curves \n",
            "\n",
            "| \n",
            "\n",
            "153\n",
            "\n",
            "\fThese  learning  curves  are  typical  of  a  model  that’s  underfitting.  Both  curves  have\n",
            "reached a plateau; they are close and fairly high.\n",
            "\n",
            "If your model is underfitting the training data, adding more train‐\n",
            "ing examples will not help. You need to use a better model or come\n",
            "up with better features.\n",
            "\n",
            "Now let’s look at the learning curves of a 10th-degree polynomial model on the same\n",
            "data (Figure 4-16):\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "polynomial_regression = make_pipeline(\n",
            "    PolynomialFeatures(degree=10, include_bias=False),\n",
            "    LinearRegression())\n",
            "\n",
            "train_sizes, train_scores, valid_scores = learning_curve(\n",
            "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
            "    scoring=\"neg_root_mean_squared_error\")\n",
            "[...]  # same as earlier\n",
            "\n",
            "Figure 4-16. Learning curves for the 10th-degree polynomial model\n",
            "\n",
            "These  learning  curves  look  a  bit  like  the  previous  ones,  but  there  are  two  very\n",
            "important differences:\n",
            "\n",
            "•\n",
            "• The error on the training data is much lower than before.\n",
            "\n",
            "• There  is  a  gap  between  the  curves.  This  means  that  the  model  performs  signif‐\n",
            "•\n",
            "icantly  better  on  the  training  data  than  on  the  validation  data,  which  is  the\n",
            "\n",
            "154 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fhallmark of an overfitting model. If you used a much larger training set, however,\n",
            "the two curves would continue to get closer.\n",
            "\n",
            "One way to improve an overfitting model is to feed it more training\n",
            "data until the validation error reaches the training error.\n",
            "\n",
            "The Bias/Variance Trade-Off\n",
            "An  important  theoretical  result  of  statistics  and  machine  learning  is  the  fact  that\n",
            "a  model’s  generalization  error  can  be  expressed  as  the  sum  of  three  very  different\n",
            "errors:\n",
            "\n",
            "Bias\n",
            "\n",
            "This  part  of  the  generalization  error  is  due  to  wrong  assumptions,  such  as\n",
            "assuming that the data is linear when it is actually quadratic. A high-bias model\n",
            "is most likely to underfit the training data.6\n",
            "\n",
            "Variance\n",
            "\n",
            "This  part  is  due  to  the  model’s  excessive  sensitivity  to  small  variations  in  the\n",
            "training  data.  A  model  with  many  degrees  of  freedom  (such  as  a  high-degree\n",
            "polynomial  model)  is  likely  to  have  high  variance  and  thus  overfit  the  training\n",
            "data.\n",
            "\n",
            "Irreducible error\n",
            "\n",
            "This  part  is  due  to  the  noisiness  of  the  data  itself.  The  only  way  to  reduce  this\n",
            "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
            "sensors, or detect and remove outliers).\n",
            "\n",
            "Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
            "Conversely, reducing a model’s complexity increases its bias and reduces its variance.\n",
            "This is why it is called a trade-off.\n",
            "\n",
            "Regularized Linear Models\n",
            "As you saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\n",
            "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
            "for it to overfit the data. A simple way to regularize a polynomial model is to reduce\n",
            "the number of polynomial degrees.\n",
            "\n",
            "6 This notion of bias is not to be confused with the bias term of linear models.\n",
            "\n",
            "Regularized Linear Models \n",
            "\n",
            "| \n",
            "\n",
            "155\n",
            "\n",
            "\fFor  a  linear  model,  regularization  is  typically  achieved  by  constraining  the  weights\n",
            "of the model. We will now look at ridge regression, lasso regression, and elastic net\n",
            "regression, which implement three different ways to constrain the weights.\n",
            "\n",
            "Ridge Regression\n",
            "Ridge regression (also called Tikhonov regularization) is a regularized version of linear\n",
            "α\n",
            "n\n",
            "2 is added to the MSE. This forces\n",
            "regression: a regularization term equal to \n",
            "m ∑i = 1\n",
            "the  learning  algorithm  to  not  only  fit  the  data  but  also  keep  the  model  weights\n",
            "as  small  as  possible.  Note  that  the  regularization  term  should  only  be  added  to\n",
            "the  cost  function  during  training.  Once  the  model  is  trained,  you  want  to  use  the\n",
            "unregularized MSE (or the RMSE) to evaluate the model’s performance.\n",
            "\n",
            "θi\n",
            "\n",
            "The hyperparameter α controls how much you want to regularize the model. If α =\n",
            "0,  then  ridge  regression  is  just  linear  regression.  If  α  is  very  large,  then  all  weights\n",
            "end up very close to zero and the result is a flat line going through the data’s mean.\n",
            "Equation 4-8 presents the ridge regression cost function.7\n",
            "\n",
            "Equation 4-8. Ridge regression cost function\n",
            "\n",
            "J θ = MSE θ +\n",
            "\n",
            "α\n",
            "n\n",
            "m ∑i = 1\n",
            "\n",
            "2\n",
            "\n",
            "θi\n",
            "\n",
            "Note  that  the  bias  term  θ0  is  not  regularized  (the  sum  starts  at  i  =  1,  not  0).  If  we\n",
            "define  w  as  the  vector  of  feature  weights  (θ1  to  θn),  then  the  regularization  term  is\n",
            "equal  to  α(∥  w  ∥2)2  /  m,  where  ∥  w  ∥2  represents  the  ℓ2  norm  of  the  weight  vector.8\n",
            "For batch gradient descent, just add 2αw / m to the part of the MSE gradient vector\n",
            "that  corresponds  to  the  feature  weights,  without  adding  anything  to  the  gradient  of\n",
            "the bias term (see Equation 4-6).\n",
            "\n",
            "It  is  important  to  scale  the  data  (e.g.,  using  a  StandardScaler)\n",
            "before performing ridge regression, as it is sensitive to the scale of\n",
            "the input features. This is true of most regularized models.\n",
            "\n",
            "Figure  4-17  shows  several  ridge  models  that  were  trained  on  some  very  noisy\n",
            "linear  data  using  different  α  values.  On  the  left,  plain  ridge  models  are  used,\n",
            "leading  to  linear  predictions.  On  the  right,  the  data  is  first  expanded  using\n",
            "\n",
            "7 It is common to use the notation J(θ) for cost functions that don’t have a short name; I’ll often use this\n",
            "notation throughout the rest of this book. The context will make it clear which cost function is being\n",
            "discussed.\n",
            "\n",
            "8 Norms are discussed in Chapter 2.\n",
            "\n",
            "156 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fPolynomialFeatures(degree=10),  then  it  is  scaled  using  a  StandardScaler,  and\n",
            "finally  the  ridge  models  are  applied  to  the  resulting  features:  this  is  polynomial\n",
            "regression  with  ridge  regularization.  Note  how  increasing  α  leads  to  flatter  (i.e.,\n",
            "less  extreme,  more  reasonable)  predictions,  thus  reducing  the  model’s  variance  but\n",
            "increasing its bias.\n",
            "\n",
            "Figure 4-17. Linear (left) and a polynomial (right) models, both with various levels of\n",
            "ridge regularization\n",
            "\n",
            "As  with  linear  regression,  we  can  perform  ridge  regression  either  by  computing  a\n",
            "closed-form equation or by performing gradient descent. The pros and cons are the\n",
            "same. Equation 4-9 shows the closed-form solution, where A is the (n + 1) × (n + 1)\n",
            "identity matrix,9 except with a 0 in the top-left cell, corresponding to the bias term.\n",
            "\n",
            "Equation 4-9. Ridge regression closed-form solution\n",
            "\n",
            "θ = X⊺X + αA\n",
            "\n",
            "−1\n",
            "\n",
            "  X⊺\n",
            "\n",
            "  y\n",
            "\n",
            "Here  is  how  to  perform  ridge  regression  with  Scikit-Learn  using  a  closed-form\n",
            "solution  (a  variant  of  Equation  4-9  that  uses  a  matrix  factorization  technique  by\n",
            "André-Louis Cholesky):\n",
            "\n",
            ">>> from sklearn.linear_model import Ridge\n",
            ">>> ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n",
            ">>> ridge_reg.fit(X, y)\n",
            ">>> ridge_reg.predict([[1.5]])\n",
            "array([[1.55325833]])\n",
            "\n",
            "9 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).\n",
            "\n",
            "Regularized Linear Models \n",
            "\n",
            "| \n",
            "\n",
            "157\n",
            "\n",
            "\fAnd using stochastic gradient descent:10\n",
            "\n",
            ">>> sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n",
            "...                        max_iter=1000, eta0=0.01, random_state=42)\n",
            "...\n",
            ">>> sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n",
            ">>> sgd_reg.predict([[1.5]])\n",
            "array([1.55302613])\n",
            "\n",
            "The  penalty  hyperparameter  sets  the  type  of  regularization  term  to  use.  Specifying\n",
            "\"l2\"  indicates  that  you  want  SGD  to  add  a  regularization  term  to  the  MSE  cost\n",
            "function equal to alpha times the square of the ℓ2 norm of the weight vector. This is\n",
            "just  like  ridge  regression,  except  there’s  no  division  by  m  in  this  case;  that’s  why  we\n",
            "passed alpha=0.1 / m, to get the same result as Ridge(alpha=0.1).\n",
            "\n",
            "The  RidgeCV  class  also  performs  ridge  regression,  but  it  automat‐\n",
            "ically  tunes  hyperparameters  using  cross-validation.  It’s  roughly\n",
            "equivalent  to  using  GridSearchCV,  but  it’s  optimized  for  ridge\n",
            "regression  and  runs  much  faster.  Several  other  estimators  (mostly\n",
            "linear)  also  have  efficient  CV  variants,  such  as  LassoCV  and\n",
            "ElasticNetCV.\n",
            "\n",
            "Lasso Regression\n",
            "Least  absolute  shrinkage  and  selection  operator  regression  (usually  simply  called  lasso\n",
            "regression)  is  another  regularized  version  of  linear  regression:  just  like  ridge  regres‐\n",
            "sion,  it  adds  a  regularization  term  to  the  cost  function,  but  it  uses  the  ℓ1  norm  of\n",
            "the  weight  vector  instead  of  the  square  of  the  ℓ2  norm  (see  Equation  4-10).  Notice\n",
            "that  the  ℓ1  norm  is  multiplied  by  2α,  whereas  the  ℓ2  norm  was  multiplied  by  α  /  m\n",
            "in ridge regression. These factors were chosen to ensure that the optimal α value is\n",
            "independent from the training set size: different norms lead to different factors (see\n",
            "Scikit-Learn issue #15657 for more details).\n",
            "\n",
            "Equation 4-10. Lasso regression cost function\n",
            "\n",
            "J θ = MSE θ + 2α∑i = 1\n",
            "\n",
            "n\n",
            "\n",
            "θi\n",
            "\n",
            "Figure 4-18 shows the same thing as Figure 4-17 but replaces the ridge models with\n",
            "lasso models and uses different α values.\n",
            "\n",
            "10 Alternatively, you can use the Ridge class with the \"sag\" solver. Stochastic average GD is a variant of\n",
            "\n",
            "stochastic GD. For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average\n",
            "Gradient Algorithm” by Mark Schmidt et al. from the University of British Columbia.\n",
            "\n",
            "158 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fFigure 4-18. Linear (left) and polynomial (right) models, both using various levels of\n",
            "lasso regularization\n",
            "\n",
            "An important characteristic of lasso regression is that it tends to eliminate the weights\n",
            "of  the  least  important  features  (i.e.,  set  them  to  zero).  For  example,  the  dashed  line\n",
            "in  the  righthand  plot  in  Figure  4-18  (with  α  =  0.01)  looks  roughly  cubic:  all  the\n",
            "weights  for  the  high-degree  polynomial  features  are  equal  to  zero.  In  other  words,\n",
            "lasso regression automatically performs feature selection and outputs a sparse model\n",
            "with few nonzero feature weights.\n",
            "\n",
            "You  can  get  a  sense  of  why  this  is  the  case  by  looking  at  Figure  4-19:  the  axes\n",
            "represent  two  model  parameters,  and  the  background  contours  represent  different\n",
            "loss  functions.  In  the  top-left  plot,  the  contours  represent  the  ℓ1  loss  (|θ1|  +  |θ2|),\n",
            "which  drops  linearly  as  you  get  closer  to  any  axis.  For  example,  if  you  initialize  the\n",
            "model  parameters  to  θ1  =  2  and  θ2  =  0.5,  running  gradient  descent  will  decrement\n",
            "both parameters equally (as represented by the dashed yellow line); therefore θ2 will\n",
            "reach 0 first (since it was closer to 0 to begin with). After that, gradient descent will\n",
            "roll  down  the  gutter  until  it  reaches  θ1  =  0  (with  a  bit  of  bouncing  around,  since\n",
            "the gradients of ℓ1 never get close to 0: they are either –1 or 1 for each parameter).\n",
            "In  the  top-right  plot,  the  contours  represent  lasso  regression’s  cost  function  (i.e.,  an\n",
            "MSE cost function plus an ℓ1 loss). The small white circles show the path that gradi‐\n",
            "ent  descent  takes  to  optimize  some  model  parameters  that  were  initialized  around\n",
            "θ1  =  0.25  and  θ2  =  –1:  notice  once  again  how  the  path  quickly  reaches  θ2  =  0,  then\n",
            "rolls down the gutter and ends up bouncing around the global optimum (represented\n",
            "by the red square). If we increased α, the global optimum would move left along the\n",
            "dashed yellow line, while if we decreased α, the global optimum would move right (in\n",
            "this example, the optimal parameters for the unregularized MSE are θ1 = 2 and θ2 =\n",
            "0.5).\n",
            "\n",
            "Regularized Linear Models \n",
            "\n",
            "| \n",
            "\n",
            "159\n",
            "\n",
            "\fFigure 4-19. Lasso versus ridge regularization\n",
            "\n",
            "The  two  bottom  plots  show  the  same  thing  but  with  an  ℓ2  penalty  instead.  In  the\n",
            "bottom-left plot, you can see that the ℓ2 loss decreases as we get closer to the origin, so\n",
            "gradient descent just takes a straight path toward that point. In the bottom-right plot,\n",
            "the contours represent ridge regression’s cost function (i.e., an MSE cost function plus\n",
            "an ℓ2 loss). As you can see, the gradients get smaller as the parameters approach the\n",
            "global optimum, so gradient descent naturally slows down. This limits the bouncing\n",
            "around,  which  helps  ridge  converge  faster  than  lasso  regression.  Also  note  that  the\n",
            "optimal parameters (represented by the red square) get closer and closer to the origin\n",
            "when you increase α, but they never get eliminated entirely.\n",
            "\n",
            "To  keep  gradient  descent  from  bouncing  around  the  optimum  at\n",
            "the end when using lasso regression, you need to gradually reduce\n",
            "the  learning  rate  during  training.  It  will  still  bounce  around  the\n",
            "optimum,  but  the  steps  will  get  smaller  and  smaller,  so  it  will\n",
            "converge.\n",
            "\n",
            "160 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fThe  lasso  cost  function  is  not  differentiable  at  θi  =  0  (for  i  =  1,  2,  ⋯,  n),  but\n",
            "gradient descent still works if you use a subgradient vector g11 instead when any θi = 0.\n",
            "Equation 4-11 shows a subgradient vector equation you can use for gradient descent\n",
            "with the lasso cost function.\n",
            "\n",
            "Equation 4-11. Lasso regression subgradient vector\n",
            "\n",
            "g θ, J = ∇θ MSE θ + 2α\n",
            "\n",
            "sign θ1\n",
            "sign θ2\n",
            "⋮\n",
            "sign θn\n",
            "\n",
            "   where  sign θi =\n",
            "\n",
            "−1 if θi < 0\n",
            "0 if θi = 0\n",
            "+1 if θi > 0\n",
            "\n",
            "Here is a small Scikit-Learn example using the Lasso class:\n",
            "\n",
            ">>> from sklearn.linear_model import Lasso\n",
            ">>> lasso_reg = Lasso(alpha=0.1)\n",
            ">>> lasso_reg.fit(X, y)\n",
            ">>> lasso_reg.predict([[1.5]])\n",
            "array([1.53788174])\n",
            "\n",
            "Note that you could instead use SGDRegressor(penalty=\"l1\", alpha=0.1).\n",
            "\n",
            "Elastic Net Regression\n",
            "Elastic  net  regression  is  a  middle  ground  between  ridge  regression  and  lasso  regres‐\n",
            "sion. The regularization term is a weighted sum of both ridge and lasso’s regulariza‐\n",
            "tion terms, and you can control the mix ratio r. When r = 0, elastic net is equivalent\n",
            "to  ridge  regression,  and  when  r  =  1,  it  is  equivalent  to  lasso  regression  (Equation\n",
            "4-12).\n",
            "\n",
            "Equation 4-12. Elastic net cost function\n",
            "\n",
            "J θ = MSE θ + r 2α∑i = 1\n",
            "\n",
            "n\n",
            "\n",
            "θi + 1 − r α\n",
            "\n",
            "n\n",
            "m ∑i = 1\n",
            "\n",
            "2\n",
            "θi\n",
            "\n",
            "So when should you use elastic net regression, or ridge, lasso, or plain linear regres‐\n",
            "sion (i.e., without any regularization)? It is almost always preferable to have at least\n",
            "a  little  bit  of  regularization,  so  generally  you  should  avoid  plain  linear  regression.\n",
            "Ridge  is  a  good  default,  but  if  you  suspect  that  only  a  few  features  are  useful,  you\n",
            "should  prefer  lasso  or  elastic  net  because  they  tend  to  reduce  the  useless  features’\n",
            "weights  down  to  zero,  as  discussed  earlier.  In  general,  elastic  net  is  preferred  over\n",
            "\n",
            "11 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the\n",
            "\n",
            "gradient vectors around that point.\n",
            "\n",
            "Regularized Linear Models \n",
            "\n",
            "| \n",
            "\n",
            "161\n",
            "\n",
            "\flasso because lasso may behave erratically when the number of features is greater than\n",
            "the number of training instances or when several features are strongly correlated.\n",
            "\n",
            "Here is a short example that uses Scikit-Learn’s ElasticNet (l1_ratio corresponds to\n",
            "the mix ratio r):\n",
            "\n",
            ">>> from sklearn.linear_model import ElasticNet\n",
            ">>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
            ">>> elastic_net.fit(X, y)\n",
            ">>> elastic_net.predict([[1.5]])\n",
            "array([1.54333232])\n",
            "\n",
            "Early Stopping\n",
            "A  very  different  way  to  regularize  iterative  learning  algorithms  such  as  gradient\n",
            "descent is to stop training as soon as the validation error reaches a minimum. This is\n",
            "called early stopping. Figure 4-20 shows a complex model (in this case, a high-degree\n",
            "polynomial  regression  model)  being  trained  with  batch  gradient  descent  on  the\n",
            "quadratic dataset we used earlier. As the epochs go by, the algorithm learns, and its\n",
            "prediction  error  (RMSE)  on  the  training  set  goes  down,  along  with  its  prediction\n",
            "error on the validation set. After a while, though, the validation error stops decreas‐\n",
            "ing and starts to go back up. This indicates that the model has started to overfit the\n",
            "training  data.  With  early  stopping  you  just  stop  training  as  soon  as  the  validation\n",
            "error reaches the minimum. It is such a simple and efficient regularization technique\n",
            "that Geoffrey Hinton called it a “beautiful free lunch”.\n",
            "\n",
            "Figure 4-20. Early stopping regularization\n",
            "\n",
            "162 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fWith  stochastic  and  mini-batch  gradient  descent,  the  curves  are\n",
            "not  so  smooth,  and  it  may  be  hard  to  know  whether  you  have\n",
            "reached the minimum or not. One solution is to stop only after the\n",
            "validation error has been above the minimum for some time (when\n",
            "you are confident that the model will not do any better), then roll\n",
            "back the model parameters to the point where the validation error\n",
            "was at a minimum.\n",
            "\n",
            "Here is a basic implementation of early stopping:\n",
            "\n",
            "from copy import deepcopy\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "X_train, y_train, X_valid, y_valid = [...]  # split the quadratic dataset\n",
            "\n",
            "preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n",
            "                              StandardScaler())\n",
            "X_train_prep = preprocessing.fit_transform(X_train)\n",
            "X_valid_prep = preprocessing.transform(X_valid)\n",
            "sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\n",
            "n_epochs = 500\n",
            "best_valid_rmse = float('inf')\n",
            "\n",
            "for epoch in range(n_epochs):\n",
            "    sgd_reg.partial_fit(X_train_prep, y_train)\n",
            "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
            "    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n",
            "    if val_error < best_valid_rmse:\n",
            "        best_valid_rmse = val_error\n",
            "        best_model = deepcopy(sgd_reg)\n",
            "\n",
            "This  code  first  adds  the  polynomial  features  and  scales  all  the  input  features,  both\n",
            "for  the  training  set  and  for  the  validation  set  (the  code  assumes  that  you  have  split\n",
            "the  original  training  set  into  a  smaller  training  set  and  a  validation  set).  Then  it\n",
            "creates an SGDRegressor model with no regularization and a small learning rate. In\n",
            "the  training  loop,  it  calls  partial_fit()  instead  of  fit(),  to  perform  incremental\n",
            "learning.  At  each  epoch,  it  measures  the  RMSE  on  the  validation  set.  If  it  is  lower\n",
            "than  the  lowest  RMSE  seen  so  far,  it  saves  a  copy  of  the  model  in  the  best_model\n",
            "variable. This implementation does not actually stop training, but it lets you revert to\n",
            "the best model after training. Note that the model is copied using copy.deepcopy(),\n",
            "because  it  copies  both  the  model’s  hyperparameters  and  the  learned  parameters.  In\n",
            "contrast, sklearn.base.clone() only copies the model’s hyperparameters.\n",
            "\n",
            "Regularized Linear Models \n",
            "\n",
            "| \n",
            "\n",
            "163\n",
            "\n",
            "\fLogistic Regression\n",
            "As discussed in Chapter 1, some regression algorithms can be used for classification\n",
            "(and vice versa). Logistic regression (also called logit regression) is commonly used to\n",
            "estimate  the  probability  that  an  instance  belongs  to  a  particular  class  (e.g.,  what  is\n",
            "the probability that this email is spam?). If the estimated probability is greater than a\n",
            "given threshold (typically 50%), then the model predicts that the instance belongs to\n",
            "that class (called the positive class, labeled “1”), and otherwise it predicts that it does\n",
            "not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier.\n",
            "\n",
            "Estimating Probabilities\n",
            "So  how  does  logistic  regression  work?  Just  like  a  linear  regression  model,  a  logistic\n",
            "regression model computes a weighted sum of the input features (plus a bias term),\n",
            "but instead of outputting the result directly like the linear regression model does, it\n",
            "outputs the logistic of this result (see Equation 4-13).\n",
            "\n",
            "Equation 4-13. Logistic regression model estimated probability (vectorized form)\n",
            "p = ℎθ x = σ θ⊺x\n",
            "\n",
            "The logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs a number\n",
            "between 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.\n",
            "\n",
            "Equation 4-14. Logistic function\n",
            "\n",
            "σ t =\n",
            "\n",
            "1\n",
            "1 + exp − t\n",
            "\n",
            "Figure 4-21. Logistic function\n",
            "\n",
            "164 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fOnce  the  logistic  regression  model  has  estimated  the  probability  p  =  hθ(x)  that  an\n",
            "instance  x  belongs  to  the  positive  class,  it  can  make  its  prediction  ŷ  easily  (see\n",
            "Equation 4-15).\n",
            "\n",
            "Equation 4-15. Logistic regression model prediction using a 50% threshold\n",
            "probability\n",
            "\n",
            "y =\n",
            "\n",
            "0 if p < 0.5\n",
            "1 if p ≥ 0.5\n",
            "\n",
            "Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a logistic regression\n",
            "model using the default threshold of 50% probability predicts 1 if θ⊺ x is positive and\n",
            "0 if it is negative.\n",
            "\n",
            "The  score  t  is  often  called  the  logit.  The  name  comes  from  the\n",
            "fact  that  the  logit  function,  defined  as  logit(p)  =  log(p  /  (1  –  p)),\n",
            "is  the  inverse  of  the  logistic  function.  Indeed,  if  you  compute  the\n",
            "logit  of  the  estimated  probability  p,  you  will  find  that  the  result\n",
            "is  t.  The  logit  is  also  called  the  log-odds,  since  it  is  the  log  of  the\n",
            "ratio  between  the  estimated  probability  for  the  positive  class  and\n",
            "the estimated probability for the negative class.\n",
            "\n",
            "Training and Cost Function\n",
            "Now  you  know  how  a  logistic  regression  model  estimates  probabilities  and  makes\n",
            "predictions.  But  how  is  it  trained?  The  objective  of  training  is  to  set  the  parameter\n",
            "vector θ so that the model estimates high probabilities for positive instances (y = 1)\n",
            "and low probabilities for negative instances (y = 0). This idea is captured by the cost\n",
            "function shown in Equation 4-16 for a single training instance x.\n",
            "\n",
            "Equation 4-16. Cost function of a single training instance\n",
            "\n",
            "c θ =\n",
            "\n",
            "−log p\n",
            "\n",
            "if y = 1\n",
            "−log 1 − p if y = 0\n",
            "\n",
            "This cost function makes sense because –log(t) grows very large when t approaches 0,\n",
            "so the cost will be large if the model estimates a probability close to 0 for a positive\n",
            "instance, and it will also be large if the model estimates a probability close to 1 for a\n",
            "negative instance. On the other hand, –log(t) is close to 0 when t is close to 1, so the\n",
            "cost will be close to 0 if the estimated probability is close to 0 for a negative instance\n",
            "or close to 1 for a positive instance, which is precisely what we want.\n",
            "\n",
            "Logistic Regression \n",
            "\n",
            "| \n",
            "\n",
            "165\n",
            "\n",
            "\fThe  cost  function  over  the  whole  training  set  is  the  average  cost  over  all  training\n",
            "instances.  It  can  be  written  in  a  single  expression  called  the  log  loss,  shown  in\n",
            "Equation 4-17.\n",
            "\n",
            "Equation 4-17. Logistic regression cost function (log loss)\n",
            "\n",
            "J θ = −\n",
            "\n",
            "m y i log p i\n",
            "\n",
            "1\n",
            "m ∑i = 1\n",
            "\n",
            "+ 1 − y i\n",
            "\n",
            "log 1 − p i\n",
            "\n",
            "The  log  loss  was  not  just  pulled  out  of  a  hat.  It  can  be  shown\n",
            "mathematically  (using  Bayesian  inference)  that  minimizing  this\n",
            "loss will result in the model with the maximum likelihood of being\n",
            "optimal, assuming that the instances follow a Gaussian distribution\n",
            "around  the  mean  of  their  class.  When  you  use  the  log  loss,  this\n",
            "is  the  implicit  assumption  you  are  making.  The  more  wrong  this\n",
            "assumption is, the more biased the model will be. Similarly, when\n",
            "we used the MSE to train linear regression models, we were implic‐\n",
            "itly  assuming  that  the  data  was  purely  linear,  plus  some  Gaussian\n",
            "noise.  So,  if  the  data  is  not  linear  (e.g.,  if  it’s  quadratic)  or  if  the\n",
            "noise  is  not  Gaussian  (e.g.,  if  outliers  are  not  exponentially  rare),\n",
            "then the model will be biased.\n",
            "\n",
            "The bad news is that there is no known closed-form equation to compute the value of\n",
            "θ that minimizes this cost function (there is no equivalent of the Normal equation).\n",
            "But  the  good  news  is  that  this  cost  function  is  convex,  so  gradient  descent  (or  any\n",
            "other  optimization  algorithm)  is  guaranteed  to  find  the  global  minimum  (if  the\n",
            "learning rate is not too large and you wait long enough). The partial derivatives of the\n",
            "cost function with regard to the jth model parameter θj are given by Equation 4-18.\n",
            "\n",
            "Equation 4-18. Logistic cost function partial derivatives\n",
            "\n",
            "∂\n",
            "∂θj\n",
            "\n",
            "J θ =\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "σ θ⊺x i − y i xj\n",
            "\n",
            "i\n",
            "\n",
            "This equation looks very much like Equation 4-5: for each instance it computes the\n",
            "prediction  error  and  multiplies  it  by  the  jth  feature  value,  and  then  it  computes  the\n",
            "average  over  all  training  instances.  Once  you  have  the  gradient  vector  containing\n",
            "all  the  partial  derivatives,  you  can  use  it  in  the  batch  gradient  descent  algorithm.\n",
            "That’s it: you now know how to train a logistic regression model. For stochastic GD\n",
            "you  would  take  one  instance  at  a  time,  and  for  mini-batch  GD  you  would  use  a\n",
            "mini-batch at a time.\n",
            "\n",
            "166 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fDecision Boundaries\n",
            "We  can  use  the  iris  dataset  to  illustrate  logistic  regression.  This  is  a  famous  dataset\n",
            "that contains the sepal and petal length and width of 150 iris flowers of three different\n",
            "species: Iris setosa, Iris versicolor, and Iris virginica (see Figure 4-22).\n",
            "\n",
            "Figure 4-22. Flowers of three iris plant species12\n",
            "\n",
            "Let’s  try  to  build  a  classifier  to  detect  the  Iris  virginica  type  based  only  on  the  petal\n",
            "width feature. The first step is to load the data and take a quick peek:\n",
            "\n",
            ">>> from sklearn.datasets import load_iris\n",
            ">>> iris = load_iris(as_frame=True)\n",
            ">>> list(iris)\n",
            "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names',\n",
            " 'filename', 'data_module']\n",
            ">>> iris.data.head(3)\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            ">>> iris.target.head(3)  # note that the instances are not shuffled\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "Name: target, dtype: int64\n",
            ">>> iris.target_names\n",
            "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n",
            "\n",
            "12 Photos reproduced from the corresponding Wikipedia pages. Iris virginica photo by Frank Mayfield (Creative\n",
            "Commons BY-SA 2.0), Iris versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0), Iris\n",
            "setosa photo public domain.\n",
            "\n",
            "Logistic Regression \n",
            "\n",
            "| \n",
            "\n",
            "167\n",
            "\n",
            "\fNext we’ll split the data and train a logistic regression model on the training set:\n",
            "\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "X = iris.data[[\"petal width (cm)\"]].values\n",
            "y = iris.target_names[iris.target] == 'virginica'\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
            "\n",
            "log_reg = LogisticRegression(random_state=42)\n",
            "log_reg.fit(X_train, y_train)\n",
            "\n",
            "Let’s look at the model’s estimated probabilities for flowers with petal widths varying\n",
            "from 0 cm to 3 cm (Figure 4-23):13\n",
            "\n",
            "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\n",
            "y_proba = log_reg.predict_proba(X_new)\n",
            "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
            "\n",
            "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n",
            "         label=\"Not Iris virginica proba\")\n",
            "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\n",
            "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n",
            "         label=\"Decision boundary\")\n",
            "[...] # beautify the figure: add grid, labels, axis, legend, arrows, and samples\n",
            "plt.show()\n",
            "\n",
            "Figure 4-23. Estimated probabilities and decision boundary\n",
            "\n",
            "The  petal  width  of  Iris  virginica  flowers  (represented  as  triangles)  ranges  from  1.4\n",
            "cm  to  2.5  cm,  while  the  other  iris  flowers  (represented  by  squares)  generally  have\n",
            "a  smaller  petal  width,  ranging  from  0.1  cm  to  1.8  cm.  Notice  that  there  is  a  bit  of\n",
            "overlap. Above about 2 cm the classifier is highly confident that the flower is an Iris\n",
            "virginica  (it  outputs  a  high  probability  for  that  class),  while  below  1  cm  it  is  highly\n",
            "\n",
            "13 NumPy’s reshape() function allows one dimension to be –1, which means “automatic”: the value is inferred\n",
            "\n",
            "from the length of the array and the remaining dimensions.\n",
            "\n",
            "168 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fconfident that it is not an Iris virginica (high probability for the “Not Iris virginica”\n",
            "class).  In  between  these  extremes,  the  classifier  is  unsure.  However,  if  you  ask  it\n",
            "to  predict  the  class  (using  the  predict()  method  rather  than  the  predict_proba()\n",
            "method), it will return whichever class is the most likely. Therefore, there is a decision\n",
            "boundary  at  around  1.6  cm  where  both  probabilities  are  equal  to  50%:  if  the  petal\n",
            "width  is  greater  than  1.6  cm  the  classifier  will  predict  that  the  flower  is  an  Iris\n",
            "virginica, and otherwise it will predict that it is not (even if it is not very confident):\n",
            "\n",
            ">>> decision_boundary\n",
            "1.6516516516516517\n",
            ">>> log_reg.predict([[1.7], [1.5]])\n",
            "array([ True, False])\n",
            "\n",
            "Figure 4-24 shows the same dataset, but this time displaying two features: petal width\n",
            "and  length.  Once  trained,  the  logistic  regression  classifier  can,  based  on  these  two\n",
            "features, estimate the probability that a new flower is an Iris virginica. The dashed line\n",
            "represents the points where the model estimates a 50% probability: this is the model’s\n",
            "decision boundary. Note that it is a linear boundary.14 Each parallel line represents the\n",
            "points where the model outputs a specific probability, from 15% (bottom left) to 90%\n",
            "(top right). All the flowers beyond the top-right line have over 90% chance of being\n",
            "Iris virginica, according to the model.\n",
            "\n",
            "Figure 4-24. Linear decision boundary\n",
            "\n",
            "The  hyperparameter  controlling  the  regularization  strength  of  a\n",
            "Scikit-Learn LogisticRegression model is not alpha (as in other\n",
            "linear models), but its inverse: C. The higher the value of C, the less\n",
            "the model is regularized.\n",
            "\n",
            "14 It is the set of points x such that θ0 + θ1x1 + θ2x2 = 0, which defines a straight line.\n",
            "\n",
            "Logistic Regression \n",
            "\n",
            "| \n",
            "\n",
            "169\n",
            "\n",
            "\fJust like the other linear models, logistic regression models can be regularized using\n",
            "ℓ1 or ℓ2 penalties. Scikit-Learn actually adds an ℓ2 penalty by default.\n",
            "\n",
            "Softmax Regression\n",
            "The logistic regression model can be generalized to support multiple classes directly,\n",
            "without  having  to  train  and  combine  multiple  binary  classifiers  (as  discussed  in\n",
            "Chapter 3). This is called softmax regression, or multinomial logistic regression.\n",
            "\n",
            "The  idea  is  simple:  when  given  an  instance  x,  the  softmax  regression  model  first\n",
            "computes a score sk(x) for each class k, then estimates the probability of each class by\n",
            "applying the softmax function (also called the normalized exponential) to the scores.\n",
            "The equation to compute sk(x) should look familiar, as it is just like the equation for\n",
            "linear regression prediction (see Equation 4-19).\n",
            "\n",
            "Equation 4-19. Softmax score for class k\n",
            "sk x = θ k ⊺\n",
            "\n",
            "x\n",
            "\n",
            "Note that each class has its own dedicated parameter vector θ(k). All these vectors are\n",
            "typically stored as rows in a parameter matrix Θ.\n",
            "\n",
            "Once you have computed the score of every class for the instance x, you can estimate\n",
            "the probability p k that the instance belongs to class k by running the scores through\n",
            "the  softmax  function  (Equation  4-20).  The  function  computes  the  exponential  of\n",
            "every score, then normalizes them (dividing by the sum of all the exponentials). The\n",
            "scores  are  generally  called  logits  or  log-odds  (although  they  are  actually  unnormal‐\n",
            "ized log-odds).\n",
            "\n",
            "Equation 4-20. Softmax function\n",
            "\n",
            "p k = σ s x k =\n",
            "\n",
            "exp sk x\n",
            "\n",
            "K\n",
            "∑j = 1\n",
            "\n",
            "exp sj x\n",
            "\n",
            "In this equation:\n",
            "\n",
            "•\n",
            "• K is the number of classes.\n",
            "\n",
            "• s(x) is a vector containing the scores of each class for the instance x.\n",
            "•\n",
            "\n",
            "•\n",
            "• σ(s(x))k is the estimated probability that the instance x belongs to class k, given\n",
            "\n",
            "the scores of each class for that instance.\n",
            "\n",
            "170 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fJust  like  the  logistic  regression  classifier,  by  default  the  softmax  regression  classifier\n",
            "predicts  the  class  with  the  highest  estimated  probability  (which  is  simply  the  class\n",
            "with the highest score), as shown in Equation 4-21.\n",
            "\n",
            "Equation 4-21. Softmax regression classifier prediction\n",
            "\n",
            "y = argmax\n",
            "\n",
            "k\n",
            "\n",
            "σ s x k = argmax\n",
            "\n",
            "k\n",
            "\n",
            "sk x = argmax\n",
            "\n",
            "k\n",
            "\n",
            "θ k ⊺\n",
            "x\n",
            "\n",
            "The argmax operator returns the value of a variable that maximizes a function. In this\n",
            "equation, it returns the value of k that maximizes the estimated probability σ(s(x))k.\n",
            "\n",
            "The  softmax  regression  classifier  predicts  only  one  class  at  a  time\n",
            "(i.e.,  it  is  multiclass,  not  multioutput),  so  it  should  be  used  only\n",
            "with mutually exclusive classes, such as different species of plants.\n",
            "You cannot use it to recognize multiple people in one picture.\n",
            "\n",
            "Now  that  you  know  how  the  model  estimates  probabilities  and  makes  predictions,\n",
            "let’s  take  a  look  at  training.  The  objective  is  to  have  a  model  that  estimates  a  high\n",
            "probability  for  the  target  class  (and  consequently  a  low  probability  for  the  other\n",
            "classes).  Minimizing  the  cost  function  shown  in  Equation  4-22,  called  the  cross\n",
            "entropy, should lead to this objective because it penalizes the model when it estimates\n",
            "a low probability for a target class. Cross entropy is frequently used to measure how\n",
            "well a set of estimated class probabilities matches the target classes.\n",
            "\n",
            "Equation 4-22. Cross entropy cost function\n",
            "\n",
            "J Θ = −\n",
            "\n",
            "m\n",
            "1\n",
            "m ∑i = 1\n",
            "\n",
            "K\n",
            "∑k = 1\n",
            "\n",
            "i\n",
            "yk\n",
            "\n",
            "i\n",
            "log p k\n",
            "\n",
            "i\n",
            "In this equation, yk\n",
            " is the target probability that the ith instance belongs to class k. In\n",
            "general, it is either equal to 1 or 0, depending on whether the instance belongs to the\n",
            "class or not.\n",
            "\n",
            "Notice that when there are just two classes (K = 2), this cost function is equivalent to\n",
            "the logistic regression cost function (log loss; see Equation 4-17).\n",
            "\n",
            "Cross Entropy\n",
            "Cross  entropy  originated  from  Claude  Shannon’s  information  theory.  Suppose  you\n",
            "want to efficiently transmit information about the weather every day. If there are eight\n",
            "options (sunny, rainy, etc.), you could encode each option using 3 bits, because 23 =\n",
            "8.  However,  if  you  think  it  will  be  sunny  almost  every  day,  it  would  be  much  more\n",
            "\n",
            "Logistic Regression \n",
            "\n",
            "| \n",
            "\n",
            "171\n",
            "\n",
            "\fefficient to code “sunny” on just one bit (0) and the other seven options on four bits\n",
            "(starting with a 1). Cross entropy measures the average number of bits you actually\n",
            "send per option. If your assumption about the weather is perfect, cross entropy will\n",
            "be equal to the entropy of the weather itself (i.e., its intrinsic unpredictability). But if\n",
            "your assumption is wrong (e.g., if it rains often), cross entropy will be greater by an\n",
            "amount called the Kullback–Leibler (KL) divergence.\n",
            "\n",
            "The cross entropy between two probability distributions p and q is defined as H(p,q)\n",
            "=  –Σx p(x)  log  q(x)  (at  least  when  the  distributions  are  discrete).  For  more  details,\n",
            "check out my video on the subject.\n",
            "\n",
            "The gradient vector of this cost function with regard to θ(k) is given by Equation 4-23.\n",
            "\n",
            "Equation 4-23. Cross entropy gradient vector for class k\n",
            "\n",
            "∇θ\n",
            "\n",
            "k J Θ =\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "m ∑\n",
            "i = 1\n",
            "\n",
            "i\n",
            "p k\n",
            "\n",
            "i x i\n",
            "\n",
            "− yk\n",
            "\n",
            "Now you can compute the gradient vector for every class, then use gradient descent\n",
            "(or any other optimization algorithm) to find the parameter matrix Θ that minimizes\n",
            "the cost function.\n",
            "\n",
            "Let’s  use  softmax  regression  to  classify  the  iris  plants  into  all  three  classes.  Scikit-\n",
            "Learn’s  LogisticRegression  classifier  uses  softmax  regression  automatically  when\n",
            "you train it on more than two classes (assuming you use  solver=\"lbfgs\", which is\n",
            "the default). It also applies ℓ2 regularization by default, which you can control using\n",
            "the hyperparameter C, as mentioned earlier:\n",
            "\n",
            "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
            "y = iris[\"target\"]\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
            "\n",
            "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
            "softmax_reg.fit(X_train, y_train)\n",
            "\n",
            "So the next time you find an iris with petals that are 5 cm long and 2 cm wide, you\n",
            "can ask your model to tell you what type of iris it is, and it will answer Iris virginica\n",
            "(class 2) with 96% probability (or Iris versicolor with 4% probability):\n",
            "\n",
            ">>> softmax_reg.predict([[5, 2]])\n",
            "array([2])\n",
            ">>> softmax_reg.predict_proba([[5, 2]]).round(2)\n",
            "array([[0.  , 0.04, 0.96]])\n",
            "\n",
            "Figure 4-25 shows the resulting decision boundaries, represented by the background\n",
            "colors.  Notice  that  the  decision  boundaries  between  any  two  classes  are  linear.  The\n",
            "figure  also  shows  the  probabilities  for  the  Iris  versicolor  class,  represented  by  the\n",
            "\n",
            "172 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fcurved  lines  (e.g.,  the  line  labeled  with  0.30  represents  the  30%  probability  bound‐\n",
            "ary). Notice that the model can predict a class that has an estimated probability below\n",
            "50%. For example, at the point where all decision boundaries meet, all classes have an\n",
            "equal estimated probability of 33%.\n",
            "\n",
            "Figure 4-25. Softmax regression decision boundaries\n",
            "\n",
            "In this chapter, you learned various ways to train linear models, both for regression\n",
            "and  for  classification.  You  used  a  closed-form  equation  to  solve  linear  regression,\n",
            "as  well  as  gradient  descent,  and  you  learned  how  various  penalties  can  be  added  to\n",
            "the  cost  function  during  training  to  regularize  the  model.  Along  the  way,  you  also\n",
            "learned how to plot learning curves and analyze them, and how to implement early\n",
            "stopping.  Finally,  you  learned  how  logistic  regression  and  softmax  regression  work.\n",
            "We’ve opened up the first machine learning black boxes! In the next chapters we will\n",
            "open many more, starting with support vector machines.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1. Which linear regression training algorithm can you use if you have a training set\n",
            "1.\n",
            "\n",
            "with millions of features?\n",
            "\n",
            "2. Suppose the features in your training set have very different scales. Which algo‐\n",
            "2.\n",
            "\n",
            "rithms might suffer from this, and how? What can you do about it?\n",
            "\n",
            "3.\n",
            "3. Can  gradient  descent  get  stuck  in  a  local  minimum  when  training  a  logistic\n",
            "\n",
            "regression model?\n",
            "\n",
            "4.\n",
            "4. Do all gradient descent algorithms lead to the same model, provided you let them\n",
            "\n",
            "run long enough?\n",
            "\n",
            "5. Suppose you use batch gradient descent and you plot the validation error at every\n",
            "5.\n",
            "epoch. If you notice that the validation error consistently goes up, what is likely\n",
            "going on? How can you fix this?\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "173\n",
            "\n",
            "\f6.\n",
            "6. Is  it  a  good  idea  to  stop  mini-batch  gradient  descent  immediately  when  the\n",
            "\n",
            "validation error goes up?\n",
            "\n",
            "7. Which  gradient  descent  algorithm  (among  those  we  discussed)  will  reach  the\n",
            "7.\n",
            "vicinity  of  the  optimal  solution  the  fastest?  Which  will  actually  converge?  How\n",
            "can you make the others converge as well?\n",
            "\n",
            "8. Suppose you are using polynomial regression. You plot the learning curves and\n",
            "8.\n",
            "you notice that there is a large gap between the training error and the validation\n",
            "error. What is happening? What are three ways to solve this?\n",
            "\n",
            "9. Suppose  you  are  using  ridge  regression  and  you  notice  that  the  training  error\n",
            "9.\n",
            "and  the  validation  error  are  almost  equal  and  fairly  high.  Would  you  say  that\n",
            "the  model  suffers  from  high  bias  or  high  variance?  Should  you  increase  the\n",
            "regularization hyperparameter α or reduce it?\n",
            "\n",
            "10.\n",
            "10. Why would you want to use:\n",
            "\n",
            "a.\n",
            "a. Ridge  regression  instead  of  plain  linear  regression  (i.e.,  without  any\n",
            "\n",
            "regularization)?\n",
            "\n",
            "b.\n",
            "b. Lasso instead of ridge regression?\n",
            "\n",
            "c.\n",
            "c. Elastic net instead of lasso regression?\n",
            "\n",
            "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
            "11.\n",
            "Should you implement two logistic regression classifiers or one softmax regres‐\n",
            "sion classifier?\n",
            "\n",
            "12.\n",
            "12. Implement  batch  gradient  descent  with  early  stopping  for  softmax  regression\n",
            "without  using  Scikit-Learn,  only  NumPy.  Use  it  on  a  classification  task  such  as\n",
            "the iris dataset.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "174 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 4: Training Models\n",
            "\n",
            "\fCHAPTER 5\n",
            "Support Vector Machines\n",
            "\n",
            "A support vector machine (SVM) is a powerful and versatile machine learning model,\n",
            "capable of performing linear or nonlinear classification, regression, and even novelty\n",
            "detection. SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds\n",
            "to thousands of instances), especially for classification tasks. However, they don’t scale\n",
            "very well to very large datasets, as you will see.\n",
            "\n",
            "This chapter will explain the core concepts of SVMs, how to use them, and how they\n",
            "work. Let’s jump right in!\n",
            "\n",
            "Linear SVM Classification\n",
            "The fundamental idea behind SVMs is best explained with some visuals. Figure 5-1\n",
            "shows  part  of  the  iris  dataset  that  was  introduced  at  the  end  of  Chapter  4.  The  two\n",
            "classes can clearly be separated easily with a straight line (they are linearly separable).\n",
            "The  left  plot  shows  the  decision  boundaries  of  three  possible  linear  classifiers.  The\n",
            "model  whose  decision  boundary  is  represented  by  the  dashed  line  is  so  bad  that  it\n",
            "does  not  even  separate  the  classes  properly.  The  other  two  models  work  perfectly\n",
            "on  this  training  set,  but  their  decision  boundaries  come  so  close  to  the  instances\n",
            "that  these  models  will  probably  not  perform  as  well  on  new  instances.  In  contrast,\n",
            "the  solid  line  in  the  plot  on  the  right  represents  the  decision  boundary  of  an  SVM\n",
            "classifier; this line not only separates the two classes but also stays as far away from\n",
            "the  closest  training  instances  as  possible.  You  can  think  of  an  SVM  classifier  as\n",
            "fitting the widest possible street (represented by the parallel dashed lines) between the\n",
            "classes. This is called large margin classification.\n",
            "\n",
            "175\n",
            "\n",
            "\fFigure 5-1. Large margin classification\n",
            "\n",
            "Notice that adding more training instances “off the street” will not affect the decision\n",
            "boundary at all: it is fully determined (or “supported”) by the instances located on the\n",
            "edge  of  the  street.  These  instances  are  called  the  support  vectors  (they  are  circled  in\n",
            "Figure 5-1).\n",
            "\n",
            "SVMs  are  sensitive  to  the  feature  scales,  as  you  can  see  in  Fig‐\n",
            "ure  5-2.  In  the  left  plot,  the  vertical  scale  is  much  larger  than  the\n",
            "horizontal scale, so the widest possible street is close to horizontal.\n",
            "After  feature  scaling  (e.g.,  using  Scikit-Learn’s  StandardScaler),\n",
            "the decision boundary in the right plot looks much better.\n",
            "\n",
            "Figure 5-2. Sensitivity to feature scales\n",
            "\n",
            "Soft Margin Classification\n",
            "If  we  strictly  impose  that  all  instances  must  be  off  the  street  and  on  the  correct\n",
            "side,  this  is  called  hard  margin  classification.  There  are  two  main  issues  with  hard\n",
            "margin classification. First, it only works if the data is linearly separable. Second, it is\n",
            "sensitive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier:\n",
            "on the left, it is impossible to find a hard margin; on the right, the decision boundary\n",
            "ends up very different from the one we saw in Figure 5-1 without the outlier, and the\n",
            "model will probably not generalize as well.\n",
            "\n",
            "176 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fFigure 5-3. Hard margin sensitivity to outliers\n",
            "\n",
            "To avoid these issues, we need to use a more flexible model. The objective is to find a\n",
            "good balance between keeping the street as large as possible and limiting the margin\n",
            "violations (i.e., instances that end up in the middle of the street or even on the wrong\n",
            "side). This is called soft margin classification.\n",
            "\n",
            "When creating an SVM model using Scikit-Learn, you can specify several hyperpara‐\n",
            "meters,  including  the  regularization  hyperparameter  C.  If  you  set  it  to  a  low  value,\n",
            "then you end up with the model on the left of Figure 5-4. With a high value, you get\n",
            "the model on the right. As you can see, reducing C makes the street larger, but it also\n",
            "leads to more margin violations. In other words, reducing C results in more instances\n",
            "supporting the street, so there’s less risk of overfitting. But if you reduce it too much,\n",
            "then  the  model  ends  up  underfitting,  as  seems  to  be  the  case  here:  the  model  with\n",
            "C=100 looks like it will generalize better than the one with C=1.\n",
            "\n",
            "Figure 5-4. Large margin (left) versus fewer margin violations (right)\n",
            "\n",
            "If  your  SVM  model  is  overfitting,  you  can  try  regularizing  it  by\n",
            "reducing C.\n",
            "\n",
            "The following Scikit-Learn code loads the iris dataset and trains a linear SVM classi‐\n",
            "fier  to  detect  Iris  virginica  flowers.  The  pipeline  first  scales  the  features,  then  uses  a\n",
            "LinearSVC with C=1:\n",
            "\n",
            "Linear SVM Classification \n",
            "\n",
            "| \n",
            "\n",
            "177\n",
            "\n",
            "\ffrom sklearn.datasets import load_iris\n",
            "from sklearn.pipeline import make_pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.svm import LinearSVC\n",
            "\n",
            "iris = load_iris(as_frame=True)\n",
            "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
            "y = (iris.target == 2)  # Iris virginica\n",
            "\n",
            "svm_clf = make_pipeline(StandardScaler(),\n",
            "                        LinearSVC(C=1, random_state=42))\n",
            "svm_clf.fit(X, y)\n",
            "\n",
            "The resulting model is represented on the left in Figure 5-4.\n",
            "\n",
            "Then, as usual, you can use the model to make predictions:\n",
            "\n",
            ">>> X_new = [[5.5, 1.7], [5.0, 1.5]]\n",
            ">>> svm_clf.predict(X_new)\n",
            "array([ True, False])\n",
            "\n",
            "The  first  plant  is  classified  as  an  Iris  virginica,  while  the  second  is  not.  Let’s  look  at\n",
            "the  scores  that  the  SVM  used  to  make  these  predictions.  These  measure  the  signed\n",
            "distance between each instance and the decision boundary:\n",
            "\n",
            ">>> svm_clf.decision_function(X_new)\n",
            "array([ 0.66163411, -0.22036063])\n",
            "\n",
            "Unlike LogisticRegression, LinearSVC doesn’t have a predict_proba() method to\n",
            "estimate the class probabilities. That said, if you use the SVC class (discussed shortly)\n",
            "instead  of  LinearSVC,  and  if  you  set  its  probability  hyperparameter  to  True,  then\n",
            "the  model  will  fit  an  extra  model  at  the  end  of  training  to  map  the  SVM  decision\n",
            "function scores to estimated probabilities. Under the hood, this requires using 5-fold\n",
            "cross-validation to generate out-of-sample predictions for every instance in the train‐\n",
            "ing  set,  then  training  a  LogisticRegression  model,  so  it  will  slow  down  training\n",
            "considerably.  After  that,  the  predict_proba()  and  predict_log_proba()  methods\n",
            "will be available.\n",
            "\n",
            "Nonlinear SVM Classification\n",
            "Although linear SVM classifiers are efficient and often work surprisingly well, many\n",
            "datasets  are  not  even  close  to  being  linearly  separable.  One  approach  to  handling\n",
            "nonlinear datasets is to add more features, such as polynomial features (as we did in\n",
            "Chapter 4); in some cases this can result in a linearly separable dataset. Consider the\n",
            "lefthand  plot  in  Figure  5-5:  it  represents  a  simple  dataset  with  just  one  feature,  x1.\n",
            "This dataset is not linearly separable, as you can see. But if you add a second feature\n",
            "x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\n",
            "\n",
            "178 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fFigure 5-5. Adding features to make a dataset linearly separable\n",
            "\n",
            "To  implement  this  idea  using  Scikit-Learn,  you  can  create  a  pipeline  containing  a\n",
            "PolynomialFeatures  transformer  (discussed  in  “Polynomial  Regression”  on  page\n",
            "149), followed by a StandardScaler and a LinearSVC classifier. Let’s test this on the\n",
            "moons  dataset,  a  toy  dataset  for  binary  classification  in  which  the  data  points  are\n",
            "shaped  as  two  interleaving  crescent  moons  (see  Figure  5-6).  You  can  generate  this\n",
            "dataset using the make_moons() function:\n",
            "\n",
            "from sklearn.datasets import make_moons\n",
            "from sklearn.preprocessing import PolynomialFeatures\n",
            "\n",
            "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
            "\n",
            "polynomial_svm_clf = make_pipeline(\n",
            "    PolynomialFeatures(degree=3),\n",
            "    StandardScaler(),\n",
            "    LinearSVC(C=10, max_iter=10_000, random_state=42)\n",
            ")\n",
            "polynomial_svm_clf.fit(X, y)\n",
            "\n",
            "Figure 5-6. Linear SVM classifier using polynomial features\n",
            "\n",
            "Nonlinear SVM Classification \n",
            "\n",
            "| \n",
            "\n",
            "179\n",
            "\n",
            "\fPolynomial Kernel\n",
            "Adding polynomial features is simple to implement and can work great with all sorts\n",
            "of  machine  learning  algorithms  (not  just  SVMs).  That  said,  at  a  low  polynomial\n",
            "degree this method cannot deal with very complex datasets, and with a high polyno‐\n",
            "mial degree it creates a huge number of features, making the model too slow.\n",
            "\n",
            "Fortunately,  when  using  SVMs  you  can  apply  an  almost  miraculous  mathematical\n",
            "technique called the kernel trick (which is explained later in this chapter). The kernel\n",
            "trick makes it possible to get the same result as if you had added many polynomial\n",
            "features,  even  with  a  very  high  degree,  without  actually  having  to  add  them.  This\n",
            "means  there’s  no  combinatorial  explosion  of  the  number  of  features.  This  trick  is\n",
            "implemented by the SVC class. Let’s test it on the moons dataset:\n",
            "\n",
            "from sklearn.svm import SVC\n",
            "\n",
            "poly_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
            "                                    SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
            "poly_kernel_svm_clf.fit(X, y)\n",
            "\n",
            "This  code  trains  an  SVM  classifier  using  a  third-degree  polynomial  kernel,  repre‐\n",
            "sented  on  the  left  in  Figure  5-7.  On  the  right  is  another  SVM  classifier  using  a\n",
            "10th-degree  polynomial  kernel.  Obviously,  if  your  model  is  overfitting,  you  might\n",
            "want  to  reduce  the  polynomial  degree.  Conversely,  if  it  is  underfitting,  you  can  try\n",
            "increasing it. The hyperparameter coef0 controls how much the model is influenced\n",
            "by high-degree terms versus low-degree terms.\n",
            "\n",
            "Figure 5-7. SVM classifiers with a polynomial kernel\n",
            "\n",
            "Although  hyperparameters  will  generally  be  tuned  automatically\n",
            "(e.g.,  using  randomized  search),  it’s  good  to  have  a  sense  of  what\n",
            "each  hyperparameter  actually  does  and  how  it  may  interact  with\n",
            "other  hyperparameters:  this  way,  you  can  narrow  the  search  to  a\n",
            "much smaller space.\n",
            "\n",
            "180 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fSimilarity Features\n",
            "Another technique to tackle nonlinear problems is to add features computed using a\n",
            "similarity function, which measures how much each instance resembles a particular\n",
            "landmark, as we did in Chapter 2 when we added the geographic similarity features.\n",
            "For example, let’s take the 1D dataset from earlier and add two landmarks to it at x1 =\n",
            "–2 and x1 = 1 (see the left plot in Figure 5-8). Next, we’ll define the similarity function\n",
            "to be the Gaussian RBF with γ = 0.3. This is a bell-shaped function varying from 0\n",
            "(very far away from the landmark) to 1 (at the landmark).\n",
            "\n",
            "Now we are ready to compute the new features. For example, let’s look at the instance\n",
            "x1 = –1: it is located at a distance of 1 from the first landmark and 2 from the second\n",
            "landmark. Therefore, its new features are x2 = exp(–0.3 × 12) ≈ 0.74 and x3 = exp(–0.3\n",
            "×  22)  ≈  0.30.  The  plot  on  the  right  in  Figure  5-8  shows  the  transformed  dataset\n",
            "(dropping the original features). As you can see, it is now linearly separable.\n",
            "\n",
            "Figure 5-8. Similarity features using the Gaussian RBF\n",
            "\n",
            "You  may  wonder  how  to  select  the  landmarks.  The  simplest  approach  is  to  create  a\n",
            "landmark at the location of each and every instance in the dataset. Doing that creates\n",
            "many  dimensions  and  thus  increases  the  chances  that  the  transformed  training  set\n",
            "will  be  linearly  separable.  The  downside  is  that  a  training  set  with  m  instances  and\n",
            "n  features  gets  transformed  into  a  training  set  with  m  instances  and  m  features\n",
            "(assuming you drop the original features). If your training set is very large, you end\n",
            "up with an equally large number of features.\n",
            "\n",
            "Gaussian RBF Kernel\n",
            "Just like the polynomial features method, the similarity features method can be useful\n",
            "with  any  machine  learning  algorithm,  but  it  may  be  computationally  expensive  to\n",
            "compute all the additional features (especially on large training sets). Once again the\n",
            "kernel trick does its SVM magic, making it possible to obtain a similar result as if you\n",
            "\n",
            "Nonlinear SVM Classification \n",
            "\n",
            "| \n",
            "\n",
            "181\n",
            "\n",
            "\fhad  added  many  similarity  features,  but  without  actually  doing  so.  Let’s  try  the  SVC\n",
            "class with the Gaussian RBF kernel:\n",
            "\n",
            "rbf_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
            "                                   SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
            "rbf_kernel_svm_clf.fit(X, y)\n",
            "\n",
            "This  model  is  represented  at  the  bottom  left  in  Figure  5-9.  The  other  plots  show\n",
            "models trained with different values of hyperparameters gamma (γ) and C. Increasing\n",
            "gamma  makes  the  bell-shaped  curve  narrower  (see  the  lefthand  plots  in  Figure  5-8).\n",
            "As a result, each instance’s range of influence is smaller: the decision boundary ends\n",
            "up  being  more  irregular,  wiggling  around  individual  instances.  Conversely,  a  small\n",
            "gamma value makes the bell-shaped curve wider: instances have a larger range of influ‐\n",
            "ence,  and  the  decision  boundary  ends  up  smoother.  So  γ  acts  like  a  regularization\n",
            "hyperparameter: if your model is overfitting, you should reduce γ; if it is underfitting,\n",
            "you should increase γ (similar to the C hyperparameter).\n",
            "\n",
            "Figure 5-9. SVM classifiers using an RBF kernel\n",
            "\n",
            "Other  kernels  exist  but  are  used  much  more  rarely.  Some  kernels  are  specialized\n",
            "for  specific  data  structures.  String  kernels  are  sometimes  used  when  classifying  text\n",
            "documents  or  DNA  sequences  (e.g.,  using  the  string  subsequence  kernel  or  kernels\n",
            "based on the Levenshtein distance).\n",
            "\n",
            "182 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fWith  so  many  kernels  to  choose  from,  how  can  you  decide\n",
            "which  one  to  use?  As  a  rule  of  thumb,  you  should  always  try\n",
            "the  linear  kernel  first.  The  LinearSVC  class  is  much  faster  than\n",
            "SVC(kernel=\"linear\"),  especially  if  the  training  set  is  very  large.\n",
            "If it is not too large, you should also try kernelized SVMs, starting\n",
            "with  the  Gaussian  RBF  kernel;  it  often  works  really  well.  Then,  if\n",
            "you  have  spare  time  and  computing  power,  you  can  experiment\n",
            "with a few other kernels using hyperparameter search. If there are\n",
            "kernels specialized for your training set’s data structure, make sure\n",
            "to give them a try too.\n",
            "\n",
            "SVM Classes and Computational Complexity\n",
            "The  LinearSVC  class  is  based  on  the  liblinear  library,  which  implements  an  opti‐\n",
            "mized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales\n",
            "almost  linearly  with  the  number  of  training  instances  and  the  number  of  features.\n",
            "Its training time complexity is roughly O(m × n). The algorithm takes longer if you\n",
            "require  very  high  precision.  This  is  controlled  by  the  tolerance  hyperparameter  ϵ\n",
            "(called tol in Scikit-Learn). In most classification tasks, the default tolerance is fine.\n",
            "\n",
            "The  SVC  class  is  based  on  the  libsvm  library,  which  implements  an  algorithm  that\n",
            "supports  the  kernel  trick.2  The  training  time  complexity  is  usually  between  O(m2  ×\n",
            "n)  and  O(m3  ×  n).  Unfortunately,  this  means  that  it  gets  dreadfully  slow  when  the\n",
            "number of training instances gets large (e.g., hundreds of thousands of instances), so\n",
            "this algorithm is best for small or medium-sized nonlinear training sets. It scales well\n",
            "with the number of features, especially with sparse features (i.e., when each instance\n",
            "has few nonzero features). In this case, the algorithm scales roughly with the average\n",
            "number of nonzero features per instance.\n",
            "\n",
            "The SGDClassifier class also performs large margin classification by default, and its\n",
            "hyperparameters–especially the regularization hyperparameters (alpha and penalty)\n",
            "and  the  learning_rate–can  be  adjusted  to  produce  similar  results  as  the  linear\n",
            "SVMs. For training it uses stochastic gradient descent (see Chapter 4), which allows\n",
            "incremental  learning  and  uses  little  memory,  so  you  can  use  it  to  train  a  model  on\n",
            "a large dataset that does not fit in RAM (i.e., for out-of-core learning). Moreover, it\n",
            "scales  very  well,  as  its  computational  complexity  is  O(m  ×  n).  Table  5-1  compares\n",
            "Scikit-Learn’s SVM classification classes.\n",
            "\n",
            "1 Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM”, Proceedings of the 25th\n",
            "\n",
            "International Conference on Machine Learning (2008): 408–415.\n",
            "\n",
            "2 John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines”\n",
            "\n",
            "(Microsoft Research technical report, April 21, 1998).\n",
            "\n",
            "Nonlinear SVM Classification \n",
            "\n",
            "| \n",
            "\n",
            "183\n",
            "\n",
            "\fTable 5-1. Comparison of Scikit-Learn classes for SVM classification\n",
            "\n",
            "Class\n",
            "\n",
            "LinearSVC\n",
            "\n",
            "SVC\n",
            "\n",
            "Time complexity\n",
            "O(m × n)\n",
            "\n",
            "Out-of-core support\n",
            "No\n",
            "\n",
            "Scaling required Kernel trick\n",
            "Yes\n",
            "\n",
            "No\n",
            "\n",
            "O(m² × n) to O(m³ × n) No\n",
            "\n",
            "SGDClassifier O(m × n)\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "Yes\n",
            "\n",
            "No\n",
            "\n",
            "Now  let’s  see  how  the  SVM  algorithms  can  also  be  used  for  linear  and  nonlinear\n",
            "regression.\n",
            "\n",
            "SVM Regression\n",
            "To use SVMs for regression instead of classification, the trick is to tweak the objec‐\n",
            "tive:  instead  of  trying  to  fit  the  largest  possible  street  between  two  classes  while\n",
            "limiting margin violations, SVM regression tries to fit as many instances as possible\n",
            "on the street while limiting margin violations (i.e., instances off the street). The width\n",
            "of the street is controlled by a hyperparameter, ϵ. Figure 5-10 shows two linear SVM\n",
            "regression models trained on some linear data, one with a small margin (ϵ = 0.5) and\n",
            "the other with a larger margin (ϵ = 1.2).\n",
            "\n",
            "Figure 5-10. SVM regression\n",
            "\n",
            "Reducing ϵ  increases  the  number  of  support  vectors,  which  regularizes  the  model.\n",
            "Moreover, if you add more training instances within the margin, it will not affect the\n",
            "model’s predictions; thus, the model is said to be ϵ-insensitive.\n",
            "\n",
            "You  can  use  Scikit-Learn’s  LinearSVR  class  to  perform  linear  SVM  regression.  The\n",
            "following code produces the model represented on the left in Figure 5-10:\n",
            "\n",
            "184 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\ffrom sklearn.svm import LinearSVR\n",
            "\n",
            "X, y = [...]  # a linear dataset\n",
            "svm_reg = make_pipeline(StandardScaler(),\n",
            "                        LinearSVR(epsilon=0.5, random_state=42))\n",
            "svm_reg.fit(X, y)\n",
            "\n",
            "To  tackle  nonlinear  regression  tasks,  you  can  use  a  kernelized  SVM  model.  Fig‐\n",
            "ure 5-11 shows SVM regression on a random quadratic training set, using a second-\n",
            "degree polynomial kernel. There is some regularization in the left plot (i.e., a small C\n",
            "value), and much less in the right plot (i.e., a large C value).\n",
            "\n",
            "Figure 5-11. SVM regression using a second-degree polynomial kernel\n",
            "\n",
            "The following code uses Scikit-Learn’s SVR class (which supports the kernel trick) to\n",
            "produce the model represented on the left in Figure 5-11:\n",
            "\n",
            "from sklearn.svm import SVR\n",
            "\n",
            "X, y = [...]  # a quadratic dataset\n",
            "svm_poly_reg = make_pipeline(StandardScaler(),\n",
            "                             SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1))\n",
            "svm_poly_reg.fit(X, y)\n",
            "\n",
            "The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is\n",
            "the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly\n",
            "with the size of the training set (just like the LinearSVC class), while the SVR class gets\n",
            "much too slow when the training set grows very large (just like the SVC class).\n",
            "\n",
            "SVMs  can  also  be  used  for  novelty  detection,  as  you  will  see  in\n",
            "Chapter 9.\n",
            "\n",
            "SVM Regression \n",
            "\n",
            "| \n",
            "\n",
            "185\n",
            "\n",
            "\fThe rest of this chapter explains how SVMs make predictions and how their training\n",
            "algorithms  work,  starting  with  linear  SVM  classifiers.  If  you  are  just  getting  started\n",
            "with  machine  learning,  you  can  safely  skip  this  and  go  straight  to  the  exercises\n",
            "at  the  end  of  this  chapter,  and  come  back  later  when  you  want  to  get  a  deeper\n",
            "understanding of SVMs.\n",
            "\n",
            "Under the Hood of Linear SVM Classifiers\n",
            "A linear SVM classifier predicts the class of a new instance x by first computing the\n",
            "decision function θ⊺ x = θ0 x0 + ⋯ + θn xn, where x0 is the bias feature (always equal to\n",
            "1). If the result is positive, then the predicted class ŷ is the positive class (1); otherwise\n",
            "it  is  the  negative  class  (0).  This  is  exactly  like  LogisticRegression  (discussed  in\n",
            "Chapter 4).\n",
            "\n",
            "Up  to  now,  I  have  used  the  convention  of  putting  all  the  model\n",
            "parameters  in  one  vector  θ,  including  the  bias  term  θ0  and  the\n",
            "input  feature  weights  θ1  to  θn.  This  required  adding  a  bias  input\n",
            "x0  =  1  to  all  instances.  Another  very  common  convention  is  to\n",
            "separate the bias term b (equal to θ0) and the feature weights vector\n",
            "w  (containing  θ1  to  θn).  In  this  case,  no  bias  feature  needs  to  be\n",
            "added  to  the  input  feature  vectors,  and  the  linear  SVM’s  decision\n",
            "function is equal to w⊺ x + b = w1 x1 + ⋯ + wn xn + b. I will use this\n",
            "convention throughout the rest of this book.\n",
            "\n",
            "So,  making  predictions  with  a  linear  SVM  classifier  is  quite  straightforward.  How\n",
            "about  training?  This  requires  finding  the  weights  vector  w  and  the  bias  term  b  that\n",
            "make the street, or margin, as wide as possible while limiting the number of margin\n",
            "violations. Let’s start with the width of the street: to make it larger, we need to make w\n",
            "smaller. This may be easier to visualize in 2D, as shown in Figure 5-12. Let’s define the\n",
            "borders of the street as the points where the decision function is equal to –1 or +1. In\n",
            "the left plot the weight w1 is 1, so the points at which w1 x1 = –1 or +1 are x1 = –1 and\n",
            "+1: therefore the margin’s size is 2. In the right plot the weight is 0.5, so the points at\n",
            "which w1 x1 = –1 or +1 are x1 = –2 and +2: the margin’s size is 4. So, we need to keep\n",
            "w as small as possible. Note that the bias term b has no influence on the size of the\n",
            "margin: tweaking it just shifts the margin around, without affecting its size.\n",
            "\n",
            "186 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fFigure 5-12. A smaller weight vector results in a larger margin\n",
            "\n",
            "We  also  want  to  avoid  margin  violations,  so  we  need  the  decision  function  to  be\n",
            "greater  than  1  for  all  positive  training  instances  and  lower  than  –1  for  negative\n",
            "training instances. If we define t(i) = –1 for negative instances (when y(i) = 0) and t(i) =\n",
            "1 for positive instances (when y(i) = 1), then we can write this constraint as t(i)(w⊺ x(i) +\n",
            "b) ≥ 1 for all instances.\n",
            "\n",
            "We  can  therefore  express  the  hard  margin  linear  SVM  classifier  objective  as  the\n",
            "constrained optimization problem in Equation 5-1.\n",
            "\n",
            "Equation 5-1. Hard margin linear SVM classifier objective\n",
            "\n",
            "minimize\n",
            "w, b\n",
            "\n",
            "w⊺w\n",
            "\n",
            "1\n",
            "2\n",
            "\n",
            "subject to t i w⊺x i + b ≥ 1\n",
            "\n",
            "for i = 1, 2, ⋯, m\n",
            "\n",
            "We are minimizing ½ w⊺ w, which is equal to ½∥ w ∥2, rather than\n",
            "minimizing  ∥  w  ∥  (the  norm  of  w).  Indeed,  ½∥  w  ∥2  has  a  nice,\n",
            "simple  derivative  (it  is  just  w),  while  ∥  w  ∥  is  not  differentiable\n",
            "at  w  =  0.  Optimization  algorithms  often  work  much  better  on\n",
            "differentiable functions.\n",
            "\n",
            "Under the Hood of Linear SVM Classifiers \n",
            "\n",
            "| \n",
            "\n",
            "187\n",
            "\n",
            "\fTo get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0 for each\n",
            "instance:3  ζ(i)  measures  how  much  the  ith  instance  is  allowed  to  violate  the  margin.\n",
            "We now have two conflicting objectives: make the slack variables as small as possible\n",
            "to  reduce  the  margin  violations,  and  make  ½  w⊺  w  as  small  as  possible  to  increase\n",
            "the margin. This is where the C hyperparameter comes in: it allows us to define the\n",
            "trade-off  between  these  two  objectives.  This  gives  us  the  constrained  optimization\n",
            "problem in Equation 5-2.\n",
            "\n",
            "Equation 5-2. Soft margin linear SVM classifier objective\n",
            "\n",
            "minimize\n",
            "w, b, ζ\n",
            "\n",
            "m\n",
            "w⊺w + C ∑\n",
            "i = 1\n",
            "\n",
            "1\n",
            "2\n",
            "\n",
            "ζ i\n",
            "\n",
            "subject to t i w⊺x i + b ≥ 1 − ζ i\n",
            "\n",
            "and ζ i\n",
            "\n",
            "≥ 0\n",
            "\n",
            "for i = 1, 2, ⋯, m\n",
            "\n",
            "The hard margin and soft margin problems are both convex quadratic optimization\n",
            "problems  with  linear  constraints.  Such  problems  are  known  as  quadratic  program‐\n",
            "ming (QP) problems. Many off-the-shelf solvers are available to solve QP problems by\n",
            "using a variety of techniques that are outside the scope of this book.4\n",
            "\n",
            "Using a QP solver is one way to train an SVM. Another is to use gradient descent to\n",
            "minimize the hinge loss or the squared hinge loss (see Figure 5-13). Given an instance\n",
            "x  of  the  positive  class  (i.e.,  with  t  =  1),  the  loss  is  0  if  the  output  s  of  the  decision\n",
            "function (s = w⊺ x + b) is greater than or equal to 1. This happens when the instance\n",
            "is off the street and on the positive side. Given an instance of the negative class (i.e.,\n",
            "with t = –1), the loss is 0 if s ≤ –1. This happens when the instance is off the street\n",
            "and on the negative side. The further away an instance is from the correct side of the\n",
            "margin, the higher the loss: it grows linearly for the hinge loss, and quadratically for\n",
            "the squared hinge loss. This makes the squared hinge loss more sensitive to outliers.\n",
            "However, if the dataset is clean, it tends to converge faster. By default, LinearSVC uses\n",
            "the squared hinge loss, while SGDClassifier uses the hinge loss. Both classes let you\n",
            "choose the loss by setting the loss hyperparameter to \"hinge\" or \"squared_hinge\".\n",
            "The  SVC  class’s  optimization  algorithm  finds  a  similar  solution  as  minimizing  the\n",
            "hinge loss.\n",
            "\n",
            "3 Zeta (ζ) is the sixth letter of the Greek alphabet.\n",
            "\n",
            "4 To learn more about quadratic programming, you can start by reading Stephen Boyd and Lieven Vandenber‐\n",
            "ghe’s book Convex Optimization (Cambridge University Press) or watching Richard Brown’s series of video\n",
            "lectures.\n",
            "\n",
            "188 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fFigure 5-13. The hinge loss (left) and the squared hinge loss (right)\n",
            "\n",
            "Next, we’ll look at yet another way to train a linear SVM classifier: solving the dual\n",
            "problem.\n",
            "\n",
            "The Dual Problem\n",
            "Given a constrained optimization problem, known as the primal problem, it is possi‐\n",
            "ble  to  express  a  different  but  closely  related  problem,  called  its  dual  problem.  The\n",
            "solution  to  the  dual  problem  typically  gives  a  lower  bound  to  the  solution  of  the\n",
            "primal  problem,  but  under  some  conditions  it  can  have  the  same  solution  as  the\n",
            "primal problem. Luckily, the SVM problem happens to meet these conditions,5 so you\n",
            "can choose to solve the primal problem or the dual problem; both will have the same\n",
            "solution.  Equation  5-3  shows  the  dual  form  of  the  linear  SVM  objective.  If  you  are\n",
            "interested in knowing how to derive the dual problem from the primal problem, see\n",
            "the extra material section in this chapter’s notebook.\n",
            "\n",
            "Equation 5-3. Dual form of the linear SVM objective\n",
            "\n",
            "minimize \n",
            "α\n",
            "\n",
            "m\n",
            "\n",
            "1\n",
            "2 ∑\n",
            "i = 1\n",
            "\n",
            "m\n",
            "∑\n",
            "j = 1\n",
            "\n",
            "α i α j t i t j x i ⊺\n",
            "\n",
            "x j\n",
            "\n",
            "subject to α i ≥ 0 for all i = 1, 2, …, m and  ∑\n",
            "i = 1\n",
            "\n",
            "α i\n",
            "\n",
            "m\n",
            "   −    ∑\n",
            "i = 1\n",
            "m\n",
            "\n",
            "α i t i\n",
            "\n",
            "= 0\n",
            "\n",
            "5 The objective function is convex, and the inequality constraints are continuously differentiable and convex\n",
            "\n",
            "functions.\n",
            "\n",
            "The Dual Problem \n",
            "\n",
            "| \n",
            "\n",
            "189\n",
            "\n",
            " \n",
            "\fOnce  you  find  the  vector  α  that  minimizes  this  equation  (using  a  QP  solver),  use\n",
            "Equation  5-4  to  compute  the  w  and  b   that  minimize  the  primal  problem.  In  this\n",
            "equation, ns represents the number of support vectors.\n",
            "\n",
            "Equation 5-4. From the dual solution to the primal solution\n",
            "\n",
            "m\n",
            "w = ∑\n",
            "i = 1\n",
            "\n",
            "α i t i x i\n",
            "\n",
            "b =\n",
            "\n",
            "1\n",
            "ns\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "i\n",
            "\n",
            "> 0\n",
            "\n",
            "α\n",
            "\n",
            "t i\n",
            "\n",
            "− w⊺x i\n",
            "\n",
            "The dual problem is faster to solve than the primal one when the number of training\n",
            "instances is smaller than the number of features. More importantly, the dual problem\n",
            "makes  the  kernel  trick  possible,  while  the  primal  problem  does  not.  So  what  is  this\n",
            "kernel trick, anyway?\n",
            "\n",
            "Kernelized SVMs\n",
            "Suppose  you  want  to  apply  a  second-degree  polynomial  transformation  to  a  two-\n",
            "dimensional  training  set  (such  as  the  moons  training  set),  then  train  a  linear  SVM\n",
            "classifier  on  the  transformed  training  set.  Equation  5-5  shows  the  second-degree\n",
            "polynomial mapping function ϕ that you want to apply.\n",
            "\n",
            "Equation 5-5. Second-degree polynomial mapping\n",
            "\n",
            "ϕ x = ϕ\n",
            "\n",
            "x1\n",
            "x2\n",
            "\n",
            "=\n",
            "\n",
            "2\n",
            "\n",
            "x1\n",
            "2 x1x2\n",
            "2\n",
            "x2\n",
            "\n",
            "Notice  that  the  transformed  vector  is  3D  instead  of  2D.  Now  let’s  look  at  what\n",
            "happens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial\n",
            "mapping and then compute the dot product6 of the transformed vectors (see Equa‐\n",
            "tion 5-6).\n",
            "\n",
            "6 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in\n",
            "\n",
            "machine learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\n",
            "dot product is achieved by computing a⊺b. To remain consistent with the rest of the book, we will use this\n",
            "notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.\n",
            "\n",
            "190 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fEquation 5-6. Kernel trick for a second-degree polynomial mapping\n",
            "\n",
            "ϕ a ⊺ϕ b\n",
            "\n",
            "=\n",
            "\n",
            "⊺\n",
            "\n",
            "2\n",
            "a1\n",
            "2 a1a2\n",
            "2\n",
            "a2\n",
            "\n",
            "2\n",
            "\n",
            "b1\n",
            "2 b1b2\n",
            "2\n",
            "\n",
            "b2\n",
            "\n",
            "= a1\n",
            "\n",
            "2b1\n",
            "\n",
            "2 + 2a1b1a2b2 + a2\n",
            "\n",
            "2b2\n",
            "\n",
            "2\n",
            "\n",
            "= a1b1 + a2b2\n",
            "\n",
            "2\n",
            "\n",
            "=\n",
            "\n",
            "2\n",
            "\n",
            "a1\n",
            "a2\n",
            "\n",
            "⊺ b1\n",
            "b2\n",
            "\n",
            "= a⊺b 2\n",
            "\n",
            "How about that? The dot product of the transformed vectors is equal to the square of\n",
            "the dot product of the original vectors: ϕ(a)⊺ ϕ(b) = (a⊺ b)2.\n",
            "\n",
            "Here  is  the  key  insight:  if  you  apply  the  transformation  ϕ  to  all  training  instances,\n",
            "then the dual problem (see Equation 5-3) will contain the dot product ϕ(x(i))⊺ ϕ(x(j)).\n",
            "But  if  ϕ  is  the  second-degree  polynomial  transformation  defined  in  Equation  5-5,\n",
            "2\n",
            "then  you  can  replace  this  dot  product  of  transformed  vectors  simply  by  x i ⊺\n",
            ".\n",
            "So,  you  don’t  need  to  transform  the  training  instances  at  all;  just  replace  the  dot\n",
            "product  by  its  square  in  Equation  5-3.  The  result  will  be  strictly  the  same  as  if  you\n",
            "had gone through the trouble of transforming the training set and then fitting a linear\n",
            "SVM algorithm, but this trick makes the whole process much more computationally\n",
            "efficient.\n",
            "\n",
            "x j\n",
            "\n",
            "The  function  K(a,  b)  =  (a⊺  b)2  is  a  second-degree  polynomial  kernel.  In  machine\n",
            "learning,  a  kernel  is  a  function  capable  of  computing  the  dot  product  ϕ(a)⊺  ϕ(b),\n",
            "based  only  on  the  original  vectors  a  and  b,  without  having  to  compute  (or  even  to\n",
            "know  about)  the  transformation  ϕ.  Equation  5-7  lists  some  of  the  most  commonly\n",
            "used kernels.\n",
            "\n",
            "Equation 5-7. Common kernels\n",
            "Linear: K a, b = a⊺b\n",
            "Polynomial: K a, b = γa⊺b + r d\n",
            "\n",
            "Gaussian RBF: K a, b = exp −γ∥ a − b ∥2\n",
            "Sigmoid: K a, b = tanh γa⊺b + r\n",
            "\n",
            "The Dual Problem \n",
            "\n",
            "| \n",
            "\n",
            "191\n",
            "\n",
            "\fMercer’s Theorem\n",
            "According  to  Mercer’s  theorem,  if  a  function  K(a,  b)  respects  a  few  mathematical\n",
            "conditions called Mercer’s conditions (e.g., K must be continuous and symmetric in its\n",
            "arguments so that K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a\n",
            "and b into another space (possibly with much higher dimensions) such that K(a, b)\n",
            "= ϕ(a)⊺ ϕ(b). You can use K as a kernel because you know ϕ exists, even if you don’t\n",
            "know what ϕ is. In the case of the Gaussian RBF kernel, it can be shown that ϕ maps\n",
            "each training instance to an infinite-dimensional space, so it’s a good thing you don’t\n",
            "need to actually perform the mapping!\n",
            "\n",
            "Note that some frequently used kernels (such as the sigmoid kernel) don’t respect all\n",
            "of Mercer’s conditions, yet they generally work well in practice.\n",
            "\n",
            "There  is  still  one  loose  end  we  must  tie  up.  Equation  5-4  shows  how  to  go  from\n",
            "the  dual  solution  to  the  primal  solution  in  the  case  of  a  linear  SVM  classifier.  But\n",
            "if you apply the kernel trick, you end up with equations that include ϕ(x(i)). In fact,\n",
            "w must have the same number of dimensions as ϕ(x(i)), which may be huge or even\n",
            "infinite, so you can’t compute it. But how can you make predictions without knowing\n",
            "w?  Well,  the  good  news  is  that  you  can  plug  the  formula  for  w  from  Equation  5-4\n",
            "into the decision function for a new instance x(n), and you get an equation with only\n",
            "dot  products  between  input  vectors.  This  makes  it  possible  to  use  the  kernel  trick\n",
            "(Equation 5-8).\n",
            "\n",
            "Equation 5-8. Making predictions with a kernelized SVM\n",
            "\n",
            "ℎw, b ϕ x n\n",
            "\n",
            "= w⊺ϕ x n + b = ∑\n",
            "i = 1\n",
            "\n",
            "m\n",
            "\n",
            "α i t i ϕ x i\n",
            "\n",
            "⊺\n",
            "\n",
            "ϕ x n + b\n",
            "\n",
            "α i t i ϕ x i ⊺\n",
            "\n",
            "ϕ x n\n",
            "\n",
            "+ b\n",
            "\n",
            "α i t i K x i , x n + b\n",
            "\n",
            "m\n",
            "= ∑\n",
            "i = 1\n",
            "m\n",
            "= ∑\n",
            "i = 1\n",
            "i\n",
            "\n",
            "α\n",
            "\n",
            "> 0\n",
            "\n",
            "Note that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐\n",
            "ing the dot product of the new input vector x(n) with only the support vectors, not all\n",
            "the training instances. Of course, you need to use the same trick to compute the bias\n",
            "term b  (Equation 5-9).\n",
            "\n",
            "192 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fEquation 5-9. Using the kernel trick to compute the bias term\n",
            "\n",
            "b =\n",
            "\n",
            "1\n",
            "ns\n",
            "\n",
            "=\n",
            "\n",
            "1\n",
            "ns\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "i\n",
            "\n",
            "> 0\n",
            "\n",
            "α\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "i\n",
            "\n",
            "> 0\n",
            "\n",
            "α\n",
            "\n",
            "t i\n",
            "\n",
            "− w⊺ϕ x i\n",
            "\n",
            "=\n",
            "\n",
            "1\n",
            "ns\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "i\n",
            "\n",
            "> 0\n",
            "\n",
            "α\n",
            "\n",
            "t i\n",
            "\n",
            "m\n",
            "− ∑\n",
            "j = 1\n",
            "\n",
            "α j t j ϕ x j\n",
            "\n",
            "⊺\n",
            "\n",
            "ϕ x i\n",
            "\n",
            "t i\n",
            "\n",
            "m\n",
            "− ∑\n",
            "j = 1\n",
            "j\n",
            "\n",
            "α\n",
            "\n",
            "> 0\n",
            "\n",
            "α j t j K x i , x j\n",
            "\n",
            "If you are starting to get a headache, that’s perfectly normal: it’s an unfortunate side\n",
            "effect of the kernel trick.\n",
            "\n",
            "It is also possible to implement online kernelized SVMs, capable of\n",
            "incremental learning, as described in the papers “Incremental and\n",
            "Decremental Support Vector Machine Learning”7 and “Fast Kernel\n",
            "Classifiers  with  Online  and  Active  Learning”.8  These  kernelized\n",
            "SVMs  are  implemented  in  Matlab  and  C++.  But  for  large-scale\n",
            "nonlinear problems, you may want to consider using random for‐\n",
            "ests (see Chapter 7) or neural networks (see Part II).\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. What is the fundamental idea behind support vector machines?\n",
            "\n",
            "2.\n",
            "2. What is a support vector?\n",
            "\n",
            "3.\n",
            "3. Why is it important to scale the inputs when using SVMs?\n",
            "\n",
            "4.\n",
            "4. Can an SVM classifier output a confidence score when it classifies an instance?\n",
            "\n",
            "What about a probability?\n",
            "\n",
            "5. How can you choose between LinearSVC, SVC, and SGDClassifier?\n",
            "5.\n",
            "\n",
            "6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit\n",
            "6.\n",
            "\n",
            "the training set. Should you increase or decrease γ (gamma)? What about C?\n",
            "\n",
            "7. What does it mean for a model to be ϵ-insensitive?\n",
            "7.\n",
            "\n",
            "8. What is the point of using the kernel trick?\n",
            "8.\n",
            "\n",
            "7 Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector Machine Learning”,\n",
            "Proceedings of the 13th International Conference on Neural Information Processing Systems (2000): 388–394.\n",
            "\n",
            "8 Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning”, Journal of Machine Learning\n",
            "\n",
            "Research 6 (2005): 1579–1619.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "193\n",
            "\n",
            "\f9. Train  a  LinearSVC  on  a  linearly  separable  dataset.  Then  train  an  SVC  and  a\n",
            "9.\n",
            "SGDClassifier on the same dataset. See if you can get them to produce roughly\n",
            "the same model.\n",
            "\n",
            "10.\n",
            "10. Train  an  SVM  classifier  on  the  wine  dataset,  which  you  can  load  using\n",
            "sklearn.datasets.load_wine().  This  dataset  contains  the  chemical  analyses\n",
            "of  178  wine  samples  produced  by  3  different  cultivators:  the  goal  is  to  train\n",
            "a  classification  model  capable  of  predicting  the  cultivator  based  on  the  wine’s\n",
            "chemical  analysis.  Since  SVM  classifiers  are  binary  classifiers,  you  will  need  to\n",
            "use one-versus-all to classify all three classes. What accuracy can you reach?\n",
            "\n",
            "11. Train and fine-tune an SVM regressor on the California housing dataset. You can\n",
            "11.\n",
            "use  the  original  dataset  rather  than  the  tweaked  version  we  used  in  Chapter  2,\n",
            "which  you  can  load  using  sklearn.datasets.fetch_california_housing().\n",
            "The  targets  represent  hundreds  of  thousands  of  dollars.  Since  there  are  over\n",
            "20,000  instances,  SVMs  can  be  slow,  so  for  hyperparameter  tuning  you  should\n",
            "use far fewer instances (e.g., 2,000) to test many more hyperparameter combina‐\n",
            "tions. What is your best model’s RMSE?\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "194 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 5: Support Vector Machines\n",
            "\n",
            "\fCHAPTER 6\n",
            "Decision Trees\n",
            "\n",
            "Decision trees are versatile machine learning algorithms that can perform both clas‐\n",
            "sification and regression tasks, and even multioutput tasks. They are powerful algo‐\n",
            "rithms, capable of fitting complex datasets. For example, in Chapter 2 you trained a\n",
            "DecisionTreeRegressor model on the California housing dataset, fitting it perfectly\n",
            "(actually, overfitting it).\n",
            "\n",
            "Decision  trees  are  also  the  fundamental  components  of  random  forests  (see  Chap‐\n",
            "ter  7),  which  are  among  the  most  powerful  machine  learning  algorithms  available\n",
            "today.\n",
            "\n",
            "In this chapter we will start by discussing how to train, visualize, and make predic‐\n",
            "tions  with  decision  trees.  Then  we  will  go  through  the  CART  training  algorithm\n",
            "used  by  Scikit-Learn,  and  we  will  explore  how  to  regularize  trees  and  use  them  for\n",
            "regression tasks. Finally, we will discuss some of the limitations of decision trees.\n",
            "\n",
            "Training and Visualizing a Decision Tree\n",
            "To understand decision trees, let’s build one and take a look at how it makes predic‐\n",
            "tions.  The  following  code  trains  a  DecisionTreeClassifier  on  the  iris  dataset  (see\n",
            "Chapter 4):\n",
            "\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "iris = load_iris(as_frame=True)\n",
            "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
            "y_iris = iris.target\n",
            "\n",
            "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
            "tree_clf.fit(X_iris, y_iris)\n",
            "\n",
            "195\n",
            "\n",
            "\fYou  can  visualize  the  trained  decision  tree  by  first  using  the  export_graphviz()\n",
            "function to output a graph definition file called iris_tree.dot:\n",
            "\n",
            "from sklearn.tree import export_graphviz\n",
            "\n",
            "export_graphviz(\n",
            "        tree_clf,\n",
            "        out_file=\"iris_tree.dot\",\n",
            "        feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
            "        class_names=iris.target_names,\n",
            "        rounded=True,\n",
            "        filled=True\n",
            "    )\n",
            "\n",
            "Then  you  can  use  graphviz.Source.from_file()  to  load  and  display  the  file  in  a\n",
            "Jupyter notebook:\n",
            "\n",
            "from graphviz import Source\n",
            "\n",
            "Source.from_file(\"iris_tree.dot\")\n",
            "\n",
            "Graphviz  is  an  open  source  graph  visualization  software  package.  It  also  includes  a\n",
            "dot  command-line  tool  to  convert  .dot  files  to  a  variety  of  formats,  such  as  PDF  or\n",
            "PNG.\n",
            "\n",
            "Your first decision tree looks like Figure 6-1.\n",
            "\n",
            "Figure 6-1. Iris decision tree\n",
            "\n",
            "196 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fMaking Predictions\n",
            "Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find\n",
            "an iris flower and you want to classify it based on its petals. You start at the root node\n",
            "(depth 0, at the top): this node asks whether the flower’s petal length is smaller than\n",
            "2.45 cm. If it is, then you move down to the root’s left child node (depth 1, left). In\n",
            "this  case,  it  is  a  leaf  node  (i.e.,  it  does  not  have  any  child  nodes),  so  it  does  not  ask\n",
            "any questions: simply look at the predicted class for that node, and the decision tree\n",
            "predicts that your flower is an Iris setosa (class=setosa).\n",
            "\n",
            "Now suppose you find another flower, and this time the petal length is greater than\n",
            "2.45 cm. You again start at the root but now move down to its right child node (depth\n",
            "1,  right).  This  is  not  a  leaf  node,  it’s  a  split  node,  so  it  asks  another  question:  is  the\n",
            "petal  width  smaller  than  1.75  cm?  If  it  is,  then  your  flower  is  most  likely  an  Iris\n",
            "versicolor (depth 2, left). If not, it is likely an Iris virginica (depth 2, right). It’s really\n",
            "that simple.\n",
            "\n",
            "One of the many qualities of decision trees is that they require very\n",
            "little data preparation. In fact, they don’t require feature scaling or\n",
            "centering at all.\n",
            "\n",
            "A  node’s  samples  attribute  counts  how  many  training  instances  it  applies  to.  For\n",
            "example,  100  training  instances  have  a  petal  length  greater  than  2.45  cm  (depth  1,\n",
            "right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A\n",
            "node’s value attribute tells you how many training instances of each class this node\n",
            "applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor,\n",
            "and 45 Iris virginica. Finally, a node’s gini attribute measures its Gini impurity: a node\n",
            "is “pure” (gini=0) if all training instances it applies to belong to the same class. For\n",
            "example,  since  the  depth-1  left  node  applies  only  to  Iris  setosa  training  instances,  it\n",
            "is  pure  and  its  Gini  impurity  is  0.  Equation  6-1  shows  how  the  training  algorithm\n",
            "computes  the  Gini  impurity  Gi  of  the  ith  node.  The  depth-2  left  node  has  a  Gini\n",
            "impurity equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168.\n",
            "\n",
            "Equation 6-1. Gini impurity\n",
            "\n",
            "n\n",
            "Gi = 1 − ∑\n",
            "k = 1\n",
            "\n",
            "2\n",
            "\n",
            "pi, k\n",
            "\n",
            "Making Predictions \n",
            "\n",
            "| \n",
            "\n",
            "197\n",
            "\n",
            "\fIn this equation:\n",
            "\n",
            "• Gi is the Gini impurity of the ith node.\n",
            "•\n",
            "• pi,k is the ratio of class k instances among the training instances in the ith node.\n",
            "•\n",
            "\n",
            "Scikit-Learn uses the CART algorithm, which produces only binary\n",
            "trees,  meaning  trees  where  split  nodes  always  have  exactly  two\n",
            "children (i.e., questions only have yes/no answers). However, other\n",
            "algorithms, such as ID3, can produce decision trees with nodes that\n",
            "have more than two children.\n",
            "\n",
            "Figure  6-2  shows  this  decision  tree’s  decision  boundaries.  The  thick  vertical  line\n",
            "represents  the  decision  boundary  of  the  root  node  (depth  0):  petal  length  =  2.45\n",
            "cm.  Since  the  lefthand  area  is  pure  (only  Iris  setosa),  it  cannot  be  split  any  further.\n",
            "However,  the  righthand  area  is  impure,  so  the  depth-1  right  node  splits  it  at  petal\n",
            "width = 1.75 cm (represented by the dashed line). Since max_depth was set to 2, the\n",
            "decision tree stops right there. If you set max_depth to 3, then the two depth-2 nodes\n",
            "would  each  add  another  decision  boundary  (represented  by  the  two  vertical  dotted\n",
            "lines).\n",
            "\n",
            "Figure 6-2. Decision tree decision boundaries\n",
            "\n",
            "The  tree  structure,  including  all  the  information  shown  in  Fig‐\n",
            "ure  6-1,  is  available  via  the  classifier’s  tree_  attribute.  Type\n",
            "help(tree_clf.tree_) for details, and see the this chapter’s note‐\n",
            "book for an example.\n",
            "\n",
            "198 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fModel Interpretation: White Box Versus Black Box\n",
            "Decision trees are intuitive, and their decisions are easy to interpret. Such models are\n",
            "often called white box models. In contrast, as you will see, random forests and neural\n",
            "networks  are  generally  considered  black  box  models.  They  make  great  predictions,\n",
            "and you can easily check the calculations that they performed to make these predic‐\n",
            "tions; nevertheless, it is usually hard to explain in simple terms why the predictions\n",
            "were made. For example, if a neural network says that a particular person appears in\n",
            "a picture, it is hard to know what contributed to this prediction: Did the model recog‐\n",
            "nize  that  person’s  eyes?  Their  mouth?  Their  nose?  Their  shoes?  Or  even  the  couch\n",
            "that they were sitting on? Conversely, decision trees provide nice, simple classification\n",
            "rules that can even be applied manually if need be (e.g., for flower classification). The\n",
            "field of interpretable ML aims at creating ML systems that can explain their decisions\n",
            "in a way humans can understand. This is important in many domains—for example,\n",
            "to ensure the system does not make unfair decisions.\n",
            "\n",
            "Estimating Class Probabilities\n",
            "A decision tree can also estimate the probability that an instance belongs to a partic‐\n",
            "ular  class  k.  First  it  traverses  the  tree  to  find  the  leaf  node  for  this  instance,  and\n",
            "then  it  returns  the  ratio  of  training  instances  of  class  k  in  this  node.  For  example,\n",
            "suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The\n",
            "corresponding  leaf  node  is  the  depth-2  left  node,  so  the  decision  tree  outputs  the\n",
            "following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54), and\n",
            "9.3%  for  Iris  virginica  (5/54).  And  if  you  ask  it  to  predict  the  class,  it  outputs  Iris\n",
            "versicolor (class 1) because it has the highest probability. Let’s check this:\n",
            "\n",
            ">>> tree_clf.predict_proba([[5, 1.5]]).round(3)\n",
            "array([[0.   , 0.907, 0.093]])\n",
            ">>> tree_clf.predict([[5, 1.5]])\n",
            "array([1])\n",
            "\n",
            "Perfect!  Notice  that  the  estimated  probabilities  would  be  identical  anywhere  else  in\n",
            "the bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long\n",
            "and 1.5 cm wide (even though it seems obvious that it would most likely be an Iris\n",
            "virginica in this case).\n",
            "\n",
            "The CART Training Algorithm\n",
            "Scikit-Learn  uses  the  Classification  and  Regression  Tree  (CART)  algorithm  to  train\n",
            "decision  trees  (also  called  “growing”  trees).  The  algorithm  works  by  first  splitting\n",
            "the  training  set  into  two  subsets  using  a  single  feature  k  and  a  threshold  tk  (e.g.,\n",
            "“petal length ≤ 2.45 cm”). How does it choose k and tk? It searches for the pair (k, tk)\n",
            "\n",
            "Estimating Class Probabilities \n",
            "\n",
            "| \n",
            "\n",
            "199\n",
            "\n",
            "\fthat produces the purest subsets, weighted by their size. Equation 6-2 gives the cost\n",
            "function that the algorithm tries to minimize.\n",
            "\n",
            "Equation 6-2. CART cost function for classification\n",
            "\n",
            "J k, tk =\n",
            "\n",
            "mleft\n",
            "m\n",
            "\n",
            "Gleft +\n",
            "\n",
            "mright\n",
            "m\n",
            "\n",
            "Gright\n",
            "\n",
            "where\n",
            "\n",
            "Gleft/right measures the impurity of the left/right subset\n",
            "mleft/right is the number of instances in the left/right subset\n",
            "\n",
            "Once  the  CART  algorithm  has  successfully  split  the  training  set  in  two,  it  splits\n",
            "the  subsets  using  the  same  logic,  then  the  sub-subsets,  and  so  on,  recursively.  It\n",
            "stops  recursing  once  it  reaches  the  maximum  depth  (defined  by  the  max_depth\n",
            "hyperparameter),  or  if  it  cannot  find  a  split  that  will  reduce  impurity.  A  few\n",
            "other  hyperparameters  (described  in  a  moment)  control  additional  stopping  con‐\n",
            "ditions:  min_samples_split,  min_samples_leaf,  min_weight_fraction_leaf,  and\n",
            "max_leaf_nodes.\n",
            "\n",
            "As you can see, the CART algorithm is a greedy algorithm: it greed‐\n",
            "ily  searches  for  an  optimum  split  at  the  top  level,  then  repeats\n",
            "the  process  at  each  subsequent  level.  It  does  not  check  whether\n",
            "or  not  the  split  will  lead  to  the  lowest  possible  impurity  several\n",
            "levels  down.  A  greedy  algorithm  often  produces  a  solution  that’s\n",
            "reasonably good but not guaranteed to be optimal.\n",
            "\n",
            "Unfortunately,  finding  the  optimal  tree  is  known  to  be  an  NP-\n",
            "complete  problem.1  It  requires  O(exp(m))  time,  making  the  prob‐\n",
            "lem  intractable  even  for  small  training  sets.  This  is  why  we  must\n",
            "settle for a “reasonably good” solution when training decision trees.\n",
            "\n",
            "Computational Complexity\n",
            "Making  predictions  requires  traversing  the  decision  tree  from  the  root  to  a  leaf.\n",
            "Decision  trees  generally  are  approximately  balanced,  so  traversing  the  decision  tree\n",
            "requires going through roughly O(log2(m)) nodes, where log2(m) is the binary loga‐\n",
            "rithm  of  m,  equal  to  log(m)  /  log(2).  Since  each  node  only  requires  checking  the\n",
            "\n",
            "1 P is the set of problems that can be solved in polynomial time (i.e., a polynomial of the dataset size). NP is\n",
            "\n",
            "the set of problems whose solutions can be verified in polynomial time. An NP-hard problem is a problem\n",
            "that can be reduced to a known NP-hard problem in polynomial time. An NP-complete problem is both NP\n",
            "and NP-hard. A major open mathematical question is whether or not P = NP. If P ≠ NP (which seems likely),\n",
            "then no polynomial algorithm will ever be found for any NP-complete problem (except perhaps one day on a\n",
            "quantum computer).\n",
            "\n",
            "200 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fvalue of one feature, the overall prediction complexity is O(log2(m)), independent of\n",
            "the  number  of  features.  So  predictions  are  very  fast,  even  when  dealing  with  large\n",
            "training sets.\n",
            "\n",
            "The  training  algorithm  compares  all  features  (or  less  if  max_features  is  set)  on  all\n",
            "samples at each node. Comparing all features on all samples at each node results in a\n",
            "training complexity of O(n × m log2(m)).\n",
            "\n",
            "Gini Impurity or Entropy?\n",
            "By  default,  the  DecisionTreeClassifier  class  uses  the  Gini  impurity  measure,  but\n",
            "you can select the entropy impurity measure instead by setting the criterion hyper‐\n",
            "parameter  to  \"entropy\".  The  concept  of  entropy  originated  in  thermodynamics  as\n",
            "a  measure  of  molecular  disorder:  entropy  approaches  zero  when  molecules  are  still\n",
            "and  well  ordered.  Entropy  later  spread  to  a  wide  variety  of  domains,  including  in\n",
            "Shannon’s information theory, where it measures the average information content of\n",
            "a message, as we saw in Chapter 4. Entropy is zero when all messages are identical. In\n",
            "machine learning, entropy is frequently used as an impurity measure: a set’s entropy\n",
            "is zero when it contains instances of only one class. Equation 6-3 shows the definition\n",
            "of the entropy of the ith node. For example, the depth-2 left node in Figure 6-1 has an\n",
            "entropy equal to –(49/54) log2 (49/54) – (5/54) log2 (5/54) ≈ 0.445.\n",
            "\n",
            "Equation 6-3. Entropy\n",
            "\n",
            "n\n",
            "H i = − ∑\n",
            "k = 1\n",
            "pi, k ≠ 0\n",
            "\n",
            "pi, k log2 pi, k\n",
            "\n",
            "So, should you use Gini impurity or entropy? The truth is, most of the time it does\n",
            "not make a big difference: they lead to similar trees. Gini impurity is slightly faster to\n",
            "compute,  so  it  is  a  good  default.  However,  when  they  differ,  Gini  impurity  tends  to\n",
            "isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
            "produce slightly more balanced trees.2\n",
            "\n",
            "Regularization Hyperparameters\n",
            "Decision trees make very few assumptions about the training data (as opposed to lin‐\n",
            "ear models, which assume that the data is linear, for example). If left unconstrained,\n",
            "the tree structure will adapt itself to the training data, fitting it very closely—indeed,\n",
            "most  likely  overfitting  it.  Such  a  model  is  often  called  a  nonparametric  model,  not\n",
            "\n",
            "2 See Sebastian Raschka’s interesting analysis for more details.\n",
            "\n",
            "Gini Impurity or Entropy? \n",
            "\n",
            "| \n",
            "\n",
            "201\n",
            "\n",
            "\fbecause it does not have any parameters (it often has a lot) but because the number\n",
            "of  parameters  is  not  determined  prior  to  training,  so  the  model  structure  is  free  to\n",
            "stick closely to the data. In contrast, a parametric model, such as a linear model, has\n",
            "a predetermined number of parameters, so its degree of freedom is limited, reducing\n",
            "the risk of overfitting (but increasing the risk of underfitting).\n",
            "\n",
            "To avoid overfitting the training data, you need to restrict the decision tree’s freedom\n",
            "during  training.  As  you  know  by  now,  this  is  called  regularization.  The  regulariza‐\n",
            "tion  hyperparameters  depend  on  the  algorithm  used,  but  generally  you  can  at  least\n",
            "restrict  the  maximum  depth  of  the  decision  tree.  In  Scikit-Learn,  this  is  controlled\n",
            "by the max_depth hyperparameter. The default value is None, which means unlimited.\n",
            "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
            "\n",
            "The DecisionTreeClassifier class has a few other parameters that similarly restrict\n",
            "the shape of the decision tree:\n",
            "\n",
            "max_features\n",
            "\n",
            "Maximum number of features that are evaluated for splitting at each node\n",
            "\n",
            "max_leaf_nodes\n",
            "\n",
            "Maximum number of leaf nodes\n",
            "\n",
            "min_samples_split\n",
            "\n",
            "Minimum number of samples a node must have before it can be split\n",
            "\n",
            "min_samples_leaf\n",
            "\n",
            "Minimum number of samples a leaf node must have to be created\n",
            "\n",
            "min_weight_fraction_leaf\n",
            "\n",
            "Same  as  min_samples_leaf  but  expressed  as  a  fraction  of  the  total  number  of\n",
            "weighted instances\n",
            "\n",
            "Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize\n",
            "the model.\n",
            "\n",
            "Other  algorithms  work  by  first  training  the  decision  tree  without\n",
            "restrictions,  then  pruning  (deleting)  unnecessary  nodes.  A  node\n",
            "whose  children  are  all  leaf  nodes  is  considered  unnecessary  if\n",
            "the  purity  improvement  it  provides  is  not  statistically  significant.\n",
            "Standard  statistical  tests,  such  as  the  χ2  test  (chi-squared  test),  are\n",
            "used  to  estimate  the  probability  that  the  improvement  is  purely\n",
            "the  result  of  chance  (which  is  called  the  null  hypothesis).  If  this\n",
            "probability,  called  the  p-value,  is  higher  than  a  given  threshold\n",
            "(typically  5%,  controlled  by  a  hyperparameter),  then  the  node  is\n",
            "considered  unnecessary  and  its  children  are  deleted.  The  pruning\n",
            "continues until all unnecessary nodes have been pruned.\n",
            "\n",
            "202 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fLet’s  test  regularization  on  the  moons  dataset,  introduced  in  Chapter  5.  We’ll  train\n",
            "one  decision  tree  without  regularization,  and  another  with  min_samples_leaf=5.\n",
            "Here’s the code; Figure 6-3 shows the decision boundaries of each tree:\n",
            "\n",
            "from sklearn.datasets import make_moons\n",
            "\n",
            "X_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n",
            "\n",
            "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
            "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
            "tree_clf1.fit(X_moons, y_moons)\n",
            "tree_clf2.fit(X_moons, y_moons)\n",
            "\n",
            "Figure 6-3. Decision boundaries of an unregularized tree (left) and a regularized tree\n",
            "(right)\n",
            "\n",
            "The unregularized model on the left is clearly overfitting, and the regularized model\n",
            "on  the  right  will  probably  generalize  better.  We  can  verify  this  by  evaluating  both\n",
            "trees on a test set generated using a different random seed:\n",
            "\n",
            ">>> X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2,\n",
            "...                                         random_state=43)\n",
            "...\n",
            ">>> tree_clf1.score(X_moons_test, y_moons_test)\n",
            "0.898\n",
            ">>> tree_clf2.score(X_moons_test, y_moons_test)\n",
            "0.92\n",
            "\n",
            "Indeed, the second tree has a better accuracy on the test set.\n",
            "\n",
            "Regularization Hyperparameters \n",
            "\n",
            "| \n",
            "\n",
            "203\n",
            "\n",
            "\fRegression\n",
            "Decision trees are also capable of performing regression tasks. Let’s build a regression\n",
            "tree  using  Scikit-Learn’s  DecisionTreeRegressor  class,  training  it  on  a  noisy  quad‐\n",
            "ratic dataset with max_depth=2:\n",
            "\n",
            "import numpy as np\n",
            "from sklearn.tree import DecisionTreeRegressor\n",
            "\n",
            "np.random.seed(42)\n",
            "X_quad = np.random.rand(200, 1) - 0.5  # a single random input feature\n",
            "y_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\n",
            "\n",
            "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
            "tree_reg.fit(X_quad, y_quad)\n",
            "\n",
            "The resulting tree is represented in Figure 6-4.\n",
            "\n",
            "Figure 6-4. A decision tree for regression\n",
            "\n",
            "This  tree  looks  very  similar  to  the  classification  tree  you  built  earlier.  The  main\n",
            "difference  is  that  instead  of  predicting  a  class  in  each  node,  it  predicts  a  value.  For\n",
            "example, suppose you want to make a prediction for a new instance with x1 = 0.2. The\n",
            "root node asks whether x1 ≤ 0.197. Since it is not, the algorithm goes to the right child\n",
            "node, which asks whether x1 ≤ 0.772. Since it is, the algorithm goes to the left child\n",
            "node. This is a leaf node, and it predicts value=0.111. This prediction is the average\n",
            "target value of the 110 training instances associated with this leaf node, and it results\n",
            "in a mean squared error equal to 0.015 over these 110 instances.\n",
            "\n",
            "204 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fThis  model’s  predictions  are  represented  on  the  left  in  Figure  6-5.  If  you  set\n",
            "max_depth=3,  you  get  the  predictions  represented  on  the  right.  Notice  how  the\n",
            "predicted  value  for  each  region  is  always  the  average  target  value  of  the  instances\n",
            "in  that  region.  The  algorithm  splits  each  region  in  a  way  that  makes  most  training\n",
            "instances as close as possible to that predicted value.\n",
            "\n",
            "Figure 6-5. Predictions of two decision tree regression models\n",
            "\n",
            "The CART algorithm works as described earlier, except that instead of trying to split\n",
            "the  training  set  in  a  way  that  minimizes  impurity,  it  now  tries  to  split  the  training\n",
            "set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the\n",
            "algorithm tries to minimize.\n",
            "\n",
            "Equation 6-4. CART cost function for regression\n",
            "\n",
            "J k, tk =\n",
            "\n",
            "mleft\n",
            "m MSEleft +\n",
            "\n",
            "mright\n",
            "\n",
            "m MSEright where\n",
            "\n",
            "MSEnode =\n",
            "\n",
            "∑i ∈ node y node − y i 2\n",
            "mnode\n",
            "∑i ∈ node y i\n",
            "mnode\n",
            "\n",
            "y node =\n",
            "\n",
            "Just like for classification tasks, decision trees are prone to overfitting when dealing\n",
            "with  regression  tasks.  Without  any  regularization  (i.e.,  using  the  default  hyperpara‐\n",
            "meters),  you  get  the  predictions  on  the  left  in  Figure  6-6.  These  predictions  are\n",
            "obviously  overfitting  the  training  set  very  badly.  Just  setting  min_samples_leaf=10\n",
            "results in a much more reasonable model, represented on the right in Figure 6-6.\n",
            "\n",
            "Regression \n",
            "\n",
            "| \n",
            "\n",
            "205\n",
            "\n",
            "\fFigure 6-6. Predictions of an unregularized regression tree (left) and a regularized tree\n",
            "(right)\n",
            "\n",
            "Sensitivity to Axis Orientation\n",
            "Hopefully  by  now  you  are  convinced  that  decision  trees  have  a  lot  going  for  them:\n",
            "they  are  relatively  easy  to  understand  and  interpret,  simple  to  use,  versatile,  and\n",
            "powerful.  However,  they  do  have  a  few  limitations.  First,  as  you  may  have  noticed,\n",
            "decision trees love orthogonal decision boundaries (all splits are perpendicular to an\n",
            "axis), which makes them sensitive to the data’s orientation. For example, Figure 6-7\n",
            "shows a simple linearly separable dataset: on the left, a decision tree can split it easily,\n",
            "while  on  the  right,  after  the  dataset  is  rotated  by  45°,  the  decision  boundary  looks\n",
            "unnecessarily convoluted. Although both decision trees fit the training set perfectly, it\n",
            "is very likely that the model on the right will not generalize well.\n",
            "\n",
            "Figure 6-7. Sensitivity to training set rotation\n",
            "\n",
            "One way to limit this problem is to scale the data, then apply a principal component\n",
            "analysis  transformation.  We  will  look  at  PCA  in  detail  in  Chapter  8,  but  for  now\n",
            "\n",
            "206 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fyou only need to know that it rotates the data in a way that reduces the correlation\n",
            "between the features, which often (not always) makes things easier for trees.\n",
            "\n",
            "Let’s create a small pipeline that scales the data and rotates it using PCA, then train a\n",
            "DecisionTreeClassifier on that data. Figure 6-8 shows the decision boundaries of\n",
            "that tree: as you can see, the rotation makes it possible to fit the dataset pretty well\n",
            "using only one feature, z1, which is a linear function of the original petal length and\n",
            "width. Here’s the code:\n",
            "\n",
            "from sklearn.decomposition import PCA\n",
            "from sklearn.pipeline import make_pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "pca_pipeline = make_pipeline(StandardScaler(), PCA())\n",
            "X_iris_rotated = pca_pipeline.fit_transform(X_iris)\n",
            "tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
            "tree_clf_pca.fit(X_iris_rotated, y_iris)\n",
            "\n",
            "Figure 6-8. A tree’s decision boundaries on the scaled and PCA-rotated iris dataset\n",
            "\n",
            "Decision Trees Have a High Variance\n",
            "More  generally,  the  main  issue  with  decision  trees  is  that  they  have  quite  a  high\n",
            "variance:  small  changes  to  the  hyperparameters  or  to  the  data  may  produce  very\n",
            "different models. In fact, since the training algorithm used by Scikit-Learn is stochas‐\n",
            "tic—it randomly selects the set of features to evaluate at each node—even retraining\n",
            "the  same  decision  tree  on  the  exact  same  data  may  produce  a  very  different  model,\n",
            "such  as  the  one  represented  in  Figure  6-9  (unless  you  set  the  random_state  hyper‐\n",
            "parameter).  As  you  can  see,  it  looks  very  different  from  the  previous  decision  tree\n",
            "(Figure 6-2).\n",
            "\n",
            "Decision Trees Have a High Variance \n",
            "\n",
            "| \n",
            "\n",
            "207\n",
            "\n",
            "\fFigure 6-9. Retraining the same model on the same data may produce a very different\n",
            "model\n",
            "\n",
            "Luckily,  by  averaging  predictions  over  many  trees,  it’s  possible  to  reduce  variance\n",
            "significantly.  Such  an  ensemble  of  trees  is  called  a  random  forest,  and  it’s  one  of  the\n",
            "most powerful types of models available today, as you will see in the next chapter.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. What  is  the  approximate  depth  of  a  decision  tree  trained  (without  restrictions)\n",
            "\n",
            "on a training set with one million instances?\n",
            "\n",
            "2.\n",
            "2. Is a node’s Gini impurity generally lower or higher than its parent’s? Is it generally\n",
            "\n",
            "lower/higher, or always lower/higher?\n",
            "\n",
            "3. If a decision tree is overfitting the training set, is it a good idea to try decreasing\n",
            "3.\n",
            "\n",
            "max_depth?\n",
            "\n",
            "4.\n",
            "4. If a decision tree is underfitting the training set, is it a good idea to try scaling the\n",
            "\n",
            "input features?\n",
            "\n",
            "5. If  it  takes  one  hour  to  train  a  decision  tree  on  a  training  set  containing  one\n",
            "5.\n",
            "million instances, roughly how much time will it take to train another decision\n",
            "tree on a training set containing ten million instances? Hint: consider the CART\n",
            "algorithm’s computational complexity.\n",
            "\n",
            "6. If it takes one hour to train a decision tree on a given training set, roughly how\n",
            "6.\n",
            "\n",
            "much time will it take if you double the number of features?\n",
            "\n",
            "7. Train  and  fine-tune  a  decision  tree  for  the  moons  dataset  by  following  these\n",
            "7.\n",
            "\n",
            "steps:\n",
            "\n",
            "208 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 6: Decision Trees\n",
            "\n",
            "\fa. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
            "a.\n",
            "b. Use train_test_split() to split the dataset into a training set and a test set.\n",
            "b.\n",
            "c. Use  grid  search  with  cross-validation  (with  the  help  of  the  GridSearchCV\n",
            "c.\n",
            "class)  to  find  good  hyperparameter  values  for  a  DecisionTreeClassifier.\n",
            "Hint: try various values for max_leaf_nodes.\n",
            "\n",
            "d. Train  it  on  the  full  training  set  using  these  hyperparameters,  and  measure\n",
            "d.\n",
            "your model’s performance on the test set. You should get roughly 85% to 87%\n",
            "accuracy.\n",
            "\n",
            "8. Grow a forest by following these steps:\n",
            "8.\n",
            "\n",
            "a.\n",
            "a. Continuing  the  previous  exercise,  generate  1,000  subsets  of  the  training  set,\n",
            "each  containing  100  instances  selected  randomly.  Hint:  you  can  use  Scikit-\n",
            "Learn’s ShuffleSplit class for this.\n",
            "\n",
            "b.\n",
            "b. Train one decision tree on each subset, using the best hyperparameter values\n",
            "found in the previous exercise. Evaluate these 1,000 decision trees on the test\n",
            "set.  Since  they  were  trained  on  smaller  sets,  these  decision  trees  will  likely\n",
            "perform worse than the first decision tree, achieving only about 80% accuracy.\n",
            "\n",
            "c.\n",
            "c. Now comes the magic. For each test set instance, generate the predictions of\n",
            "the 1,000 decision trees, and keep only the most frequent prediction (you can\n",
            "use  SciPy’s  mode()  function  for  this).  This  approach  gives  you  majority-vote\n",
            "predictions over the test set.\n",
            "\n",
            "d.\n",
            "d. Evaluate these predictions on the test set: you should obtain a slightly higher\n",
            "accuracy  than  your  first  model  (about  0.5  to  1.5%  higher).  Congratulations,\n",
            "you have trained a random forest classifier!\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "209\n",
            "\n",
            "\f\fCHAPTER 7\n",
            "Ensemble Learning and Random Forests\n",
            "\n",
            "Suppose you pose a complex question to thousands of random people, then aggregate\n",
            "their answers. In many cases you will find that this aggregated answer is better than\n",
            "an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\n",
            "the  predictions  of  a  group  of  predictors  (such  as  classifiers  or  regressors),  you  will\n",
            "often  get  better  predictions  than  with  the  best  individual  predictor.  A  group  of\n",
            "predictors is called an ensemble; thus, this technique is called ensemble learning, and\n",
            "an ensemble learning algorithm is called an ensemble method.\n",
            "\n",
            "As  an  example  of  an  ensemble  method,  you  can  train  a  group  of  decision  tree\n",
            "classifiers, each on a different random subset of the training set. You can then obtain\n",
            "the  predictions  of  all  the  individual  trees,  and  the  class  that  gets  the  most  votes  is\n",
            "the  ensemble’s  prediction  (see  the  last  exercise  in  Chapter  6).  Such  an  ensemble  of\n",
            "decision  trees  is  called  a  random  forest,  and  despite  its  simplicity,  this  is  one  of  the\n",
            "most powerful machine learning algorithms available today.\n",
            "\n",
            "As  discussed  in  Chapter  2,  you  will  often  use  ensemble  methods  near  the  end  of\n",
            "a  project,  once  you  have  already  built  a  few  good  predictors,  to  combine  them\n",
            "into  an  even  better  predictor.  In  fact,  the  winning  solutions  in  machine  learning\n",
            "competitions often involve several ensemble methods—most famously in the Netflix\n",
            "Prize competition.\n",
            "\n",
            "In  this  chapter  we  will  examine  the  most  popular  ensemble  methods,  including\n",
            "voting classifiers, bagging and pasting ensembles, random forests, and boosting, and\n",
            "stacking ensembles.\n",
            "\n",
            "211\n",
            "\n",
            "\fVoting Classifiers\n",
            "Suppose  you  have  trained  a  few  classifiers,  each  one  achieving  about  80%  accuracy.\n",
            "You  may  have  a  logistic  regression  classifier,  an  SVM  classifier,  a  random  forest\n",
            "classifier, a k-nearest neighbors classifier, and perhaps a few more (see Figure 7-1).\n",
            "\n",
            "Figure 7-1. Training diverse classifiers\n",
            "\n",
            "A  very  simple  way  to  create  an  even  better  classifier  is  to  aggregate  the  predictions\n",
            "of each classifier: the class that gets the most votes is the ensemble’s prediction. This\n",
            "majority-vote classifier is called a hard voting classifier (see Figure 7-2).\n",
            "\n",
            "Figure 7-2. Hard voting classifier predictions\n",
            "\n",
            "212 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fSomewhat  surprisingly,  this  voting  classifier  often  achieves  a  higher  accuracy  than\n",
            "the  best  classifier  in  the  ensemble.  In  fact,  even  if  each  classifier  is  a  weak  learner\n",
            "(meaning it does only slightly better than random guessing), the ensemble can still be\n",
            "a strong learner (achieving high accuracy), provided there are a sufficient number of\n",
            "weak learners in the ensemble and they are sufficiently diverse.\n",
            "\n",
            "How is this possible? The following analogy can help shed some light on this mystery.\n",
            "Suppose  you  have  a  slightly  biased  coin  that  has  a  51%  chance  of  coming  up  heads\n",
            "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
            "more  or  less  510  heads  and  490  tails,  and  hence  a  majority  of  heads.  If  you  do  the\n",
            "math, you will find that the probability of obtaining a majority of heads after 1,000\n",
            "tosses  is  close  to  75%.  The  more  you  toss  the  coin,  the  higher  the  probability  (e.g.,\n",
            "with  10,000  tosses,  the  probability  climbs  over  97%).  This  is  due  to  the  law  of  large\n",
            "numbers:  as  you  keep  tossing  the  coin,  the  ratio  of  heads  gets  closer  and  closer  to\n",
            "the probability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You\n",
            "can  see  that  as  the  number  of  tosses  increases,  the  ratio  of  heads  approaches  51%.\n",
            "Eventually all 10 series end up so close to 51% that they are consistently above 50%.\n",
            "\n",
            "Figure 7-3. The law of large numbers\n",
            "\n",
            "Similarly,  suppose  you  build  an  ensemble  containing  1,000  classifiers  that  are  indi‐\n",
            "vidually  correct  only  51%  of  the  time  (barely  better  than  random  guessing).  If  you\n",
            "predict the majority voted class, you can hope for up to 75% accuracy! However, this\n",
            "is  only  true  if  all  classifiers  are  perfectly  independent,  making  uncorrelated  errors,\n",
            "which is clearly not the case because they are trained on the same data. They are likely\n",
            "to make the same types of errors, so there will be many majority votes for the wrong\n",
            "class, reducing the ensemble’s accuracy.\n",
            "\n",
            "Voting Classifiers \n",
            "\n",
            "| \n",
            "\n",
            "213\n",
            "\n",
            "\fEnsemble methods work best when the predictors are as independ‐\n",
            "ent from one another as possible. One way to get diverse classifiers\n",
            "is to train them using very different algorithms. This increases the\n",
            "chance that they will make very different types of errors, improving\n",
            "the ensemble’s accuracy.\n",
            "\n",
            "Scikit-Learn  provides  a  VotingClassifier  class  that’s  quite  easy  to  use:  just  give\n",
            "it  a  list  of  name/predictor  pairs,  and  use  it  like  a  normal  classifier.  Let’s  try  it  on\n",
            "the  moons  dataset  (introduced  in  Chapter  5).  We  will  load  and  split  the  moons\n",
            "dataset into a training set and a test set, then we’ll create and train a voting classifier\n",
            "composed of three diverse classifiers:\n",
            "\n",
            "from sklearn.datasets import make_moons\n",
            "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import SVC\n",
            "\n",
            "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
            "\n",
            "voting_clf = VotingClassifier(\n",
            "    estimators=[\n",
            "        ('lr', LogisticRegression(random_state=42)),\n",
            "        ('rf', RandomForestClassifier(random_state=42)),\n",
            "        ('svc', SVC(random_state=42))\n",
            "    ]\n",
            ")\n",
            "voting_clf.fit(X_train, y_train)\n",
            "\n",
            "When you fit a VotingClassifier, it clones every estimator and fits the clones. The\n",
            "original estimators are available via the estimators attribute, while the fitted clones\n",
            "are available via the estimators_ attribute. If you prefer a dict rather than a list, you\n",
            "can use named_estimators or named_estimators_ instead. To begin, let’s look at each\n",
            "fitted classifier’s accuracy on the test set:\n",
            "\n",
            ">>> for name, clf in voting_clf.named_estimators_.items():\n",
            "...     print(name, \"=\", clf.score(X_test, y_test))\n",
            "...\n",
            "lr = 0.864\n",
            "rf = 0.896\n",
            "svc = 0.896\n",
            "\n",
            "When you call the voting classifier’s predict() method, it performs hard voting. For\n",
            "example,  the  voting  classifier  predicts  class  1  for  the  first  instance  of  the  test  set,\n",
            "because two out of three classifiers predict that class:\n",
            "\n",
            "214 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\f>>> voting_clf.predict(X_test[:1])\n",
            "array([1])\n",
            ">>> [clf.predict(X_test[:1]) for clf in voting_clf.estimators_]\n",
            "[array([1]), array([1]), array([0])]\n",
            "\n",
            "Now let’s look at the performance of the voting classifier on the test set:\n",
            "\n",
            ">>> voting_clf.score(X_test, y_test)\n",
            "0.912\n",
            "\n",
            "There you have it! The voting classifier outperforms all the individual classifiers.\n",
            "\n",
            "If  all  classifiers  are  able  to  estimate  class  probabilities  (i.e.,  if  they  all  have  a\n",
            "predict_proba()  method),  then  you  can  tell  Scikit-Learn  to  predict  the  class  with\n",
            "the highest class probability, averaged over all the individual classifiers. This is called\n",
            "soft  voting.  It  often  achieves  higher  performance  than  hard  voting  because  it  gives\n",
            "more  weight  to  highly  confident  votes.  All  you  need  to  do  is  set  the  voting  classi‐\n",
            "fier’s  voting  hyperparameter  to  \"soft\",  and  ensure  that  all  classifiers  can  estimate\n",
            "class  probabilities.  This  is  not  the  case  for  the  SVC  class  by  default,  so  you  need\n",
            "to  set  its  probability  hyperparameter  to  True  (this  will  make  the  SVC  class  use\n",
            "cross-validation to estimate class probabilities, slowing down training, and it will add\n",
            "a predict_proba() method). Let’s try that:\n",
            "\n",
            ">>> voting_clf.voting = \"soft\"\n",
            ">>> voting_clf.named_estimators[\"svc\"].probability = True\n",
            ">>> voting_clf.fit(X_train, y_train)\n",
            ">>> voting_clf.score(X_test, y_test)\n",
            "0.92\n",
            "\n",
            "We reach 92% accuracy simply by using soft voting—not bad!\n",
            "\n",
            "Bagging and Pasting\n",
            "One way to get a diverse set of classifiers is to use very different training algorithms,\n",
            "as just discussed. Another approach is to use the same training algorithm for every\n",
            "predictor  but  train  them  on  different  random  subsets  of  the  training  set.  When\n",
            "sampling  is  performed  with  replacement,1  this  method  is  called  bagging2  (short  for\n",
            "bootstrap aggregating3). When sampling is performed without replacement, it is called\n",
            "pasting.4\n",
            "\n",
            "1 Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in the deck\n",
            "\n",
            "before picking the next card: the same card could be sampled multiple times.\n",
            "\n",
            "2 Leo Breiman, “Bagging Predictors”, Machine Learning 24, no. 2 (1996): 123–140.\n",
            "\n",
            "3 In statistics, resampling with replacement is called bootstrapping.\n",
            "\n",
            "4 Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”, Machine Learning 36,\n",
            "\n",
            "no. 1–2 (1999): 85–103.\n",
            "\n",
            "Bagging and Pasting \n",
            "\n",
            "| \n",
            "\n",
            "215\n",
            "\n",
            "\fIn  other  words,  both  bagging  and  pasting  allow  training  instances  to  be  sampled\n",
            "several times across multiple predictors, but only bagging allows training instances to\n",
            "be sampled several times for the same predictor. This sampling and training process\n",
            "is represented in Figure 7-4.\n",
            "\n",
            "Figure 7-4. Bagging and pasting involve training several predictors on different random\n",
            "samples of the training set\n",
            "\n",
            "Once  all  predictors  are  trained,  the  ensemble  can  make  a  prediction  for  a  new\n",
            "instance  by  simply  aggregating  the  predictions  of  all  predictors.  The  aggregation\n",
            "function  is  typically  the  statistical  mode  for  classification  (i.e.,  the  most  frequent\n",
            "prediction, just like with a hard voting classifier), or the average for regression. Each\n",
            "individual predictor has a higher bias than if it were trained on the original training\n",
            "set, but aggregation reduces both bias and variance.5 Generally, the net result is that\n",
            "the ensemble has a similar bias but a lower variance than a single predictor trained on\n",
            "the original training set.\n",
            "\n",
            "As  you  can  see  in  Figure  7-4,  predictors  can  all  be  trained  in  parallel,  via  different\n",
            "CPU  cores  or  even  different  servers.  Similarly,  predictions  can  be  made  in  parallel.\n",
            "This is one of the reasons bagging and pasting are such popular methods: they scale\n",
            "very well.\n",
            "\n",
            "5 Bias and variance were introduced in Chapter 4.\n",
            "\n",
            "216 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fBagging and Pasting in Scikit-Learn\n",
            "Scikit-Learn  offers  a  simple  API  for  both  bagging  and  pasting:  BaggingClassifier\n",
            "class  (or  BaggingRegressor  for  regression).  The  following  code  trains  an  ensemble\n",
            "of  500  decision  tree  classifiers:6  each  is  trained  on  100  training  instances  randomly\n",
            "sampled  from  the  training  set  with  replacement  (this  is  an  example  of  bagging,  but\n",
            "if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\n",
            "tells Scikit-Learn the number of CPU cores to use for training and predictions, and\n",
            "–1 tells Scikit-Learn to use all available cores:\n",
            "\n",
            "from sklearn.ensemble import BaggingClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
            "                            max_samples=100, n_jobs=-1, random_state=42)\n",
            "bag_clf.fit(X_train, y_train)\n",
            "\n",
            "A  BaggingClassifier  automatically  performs  soft  voting  instead\n",
            "of hard voting if the base classifier can estimate class probabilities\n",
            "(i.e.,  if  it  has  a  predict_proba()  method),  which  is  the  case  with\n",
            "decision tree classifiers.\n",
            "\n",
            "Figure 7-5 compares the decision boundary of a single decision tree with the decision\n",
            "boundary of a bagging ensemble of 500 trees (from the preceding code), both trained\n",
            "on the moons dataset. As you can see, the ensemble’s predictions will likely generalize\n",
            "much better than the single decision tree’s predictions: the ensemble has a compara‐\n",
            "ble bias but a smaller variance (it makes roughly the same number of errors on the\n",
            "training set, but the decision boundary is less irregular).\n",
            "\n",
            "Bagging introduces a bit more diversity in the subsets that each predictor is trained\n",
            "on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity\n",
            "also means that the predictors end up being less correlated, so the ensemble’s variance\n",
            "is  reduced.  Overall,  bagging  often  results  in  better  models,  which  explains  why\n",
            "it’s  generally  preferred.  But  if  you  have  spare  time  and  CPU  power,  you  can  use\n",
            "cross-validation to evaluate both bagging and pasting and select the one that works\n",
            "best.\n",
            "\n",
            "6 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of sampled\n",
            "\n",
            "instances is equal to the size of the training set times max_samples.\n",
            "\n",
            "Bagging and Pasting \n",
            "\n",
            "| \n",
            "\n",
            "217\n",
            "\n",
            "\fFigure 7-5. A single decision tree (left) versus a bagging ensemble of 500 trees (right)\n",
            "\n",
            "Out-of-Bag Evaluation\n",
            "With  bagging,  some  training  instances  may  be  sampled  several  times  for  any  given\n",
            "predictor,  while  others  may  not  be  sampled  at  all.  By  default  a  BaggingClassifier\n",
            "samples  m  training  instances  with  replacement  (bootstrap=True),  where  m  is  the\n",
            "size of the training set. With this process, it can be shown mathematically that only\n",
            "about 63% of the training instances are sampled on average for each predictor.7 The\n",
            "remaining  37%  of  the  training  instances  that  are  not  sampled  are  called  out-of-bag\n",
            "(OOB) instances. Note that they are not the same 37% for all predictors.\n",
            "\n",
            "A  bagging  ensemble  can  be  evaluated  using  OOB  instances,  without  the  need  for\n",
            "a  separate  validation  set:  indeed,  if  there  are  enough  estimators,  then  each  instance\n",
            "in  the  training  set  will  likely  be  an  OOB  instance  of  several  estimators,  so  these\n",
            "estimators  can  be  used  to  make  a  fair  ensemble  prediction  for  that  instance.  Once\n",
            "you have a prediction for each instance, you can compute the ensemble’s prediction\n",
            "accuracy (or any other metric).\n",
            "\n",
            "In  Scikit-Learn,  you  can  set  oob_score=True  when  creating  a  BaggingClassifier\n",
            "to request an automatic OOB evaluation after training. The following code demon‐\n",
            "strates this. The resulting evaluation score is available in the oob_score_ attribute:\n",
            "\n",
            ">>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
            "...                             oob_score=True, n_jobs=-1, random_state=42)\n",
            "...\n",
            ">>> bag_clf.fit(X_train, y_train)\n",
            ">>> bag_clf.oob_score_\n",
            "0.896\n",
            "\n",
            "7 As m grows, this ratio approaches 1 – exp(–1) ≈ 63%.\n",
            "\n",
            "218 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fAccording to this OOB evaluation, this BaggingClassifier is likely to achieve about\n",
            "89.6% accuracy on the test set. Let’s verify this:\n",
            "\n",
            ">>> from sklearn.metrics import accuracy_score\n",
            ">>> y_pred = bag_clf.predict(X_test)\n",
            ">>> accuracy_score(y_test, y_pred)\n",
            "0.92\n",
            "\n",
            "We get 92% accuracy on the test. The OOB evaluation was a bit too pessimistic, just\n",
            "over 2% too low.\n",
            "\n",
            "The  OOB  decision  function  for  each  training  instance  is  also  available  through  the\n",
            "oob_decision_function_ attribute. Since the base estimator has a predict_proba()\n",
            "method,  the  decision  function  returns  the  class  probabilities  for  each  training\n",
            "instance. For example, the OOB evaluation estimates that the first training instance\n",
            "has a 67.6% probability of belonging to the positive class and a 32.4% probability of\n",
            "belonging to the negative class:\n",
            "\n",
            ">>> bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances\n",
            "array([[0.32352941, 0.67647059],\n",
            "       [0.3375    , 0.6625    ],\n",
            "       [1.        , 0.        ]])\n",
            "\n",
            "Random Patches and Random Subspaces\n",
            "The  BaggingClassifier  class  supports  sampling  the  features  as  well.  Sampling  is\n",
            "controlled  by  two  hyperparameters:  max_features  and  bootstrap_features.  They\n",
            "work the same way as max_samples and bootstrap, but for feature sampling instead\n",
            "of instance sampling. Thus, each predictor will be trained on a random subset of the\n",
            "input features.\n",
            "\n",
            "This  technique  is  particularly  useful  when  you  are  dealing  with  high-dimensional\n",
            "inputs  (such  as  images),  as  it  can  considerably  speed  up  training.  Sampling  both\n",
            "training  instances  and  features  is  called  the  random  patches  method.8  Keeping  all\n",
            "training instances (by setting bootstrap=False and max_samples=1.0) but sampling\n",
            "features  (by  setting  bootstrap_features  to  True  and/or  max_features  to  a  value\n",
            "smaller than 1.0) is called the random subspaces method.9\n",
            "\n",
            "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
            "a lower variance.\n",
            "\n",
            "8 Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches”, Lecture Notes in Computer Science 7523\n",
            "\n",
            "(2012): 346–361.\n",
            "\n",
            "9 Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests”, IEEE Transactions on\n",
            "\n",
            "Pattern Analysis and Machine Intelligence 20, no. 8 (1998): 832–844.\n",
            "\n",
            "Bagging and Pasting \n",
            "\n",
            "| \n",
            "\n",
            "219\n",
            "\n",
            "\fRandom Forests\n",
            "As  we  have  discussed,  a  random  forest10  is  an  ensemble  of  decision  trees,  generally\n",
            "trained via the bagging method (or sometimes pasting), typically with  max_samples\n",
            "set  to  the  size  of  the  training  set.  Instead  of  building  a  BaggingClassifier  and\n",
            "passing  it  a  DecisionTreeClassifier,  you  can  use  the  RandomForestClassifier\n",
            "class,  which  is  more  convenient  and  optimized  for  decision  trees11  (similarly,  there\n",
            "is a  RandomForestRegressor class for regression tasks). The following code trains a\n",
            "random forest classifier with 500 trees, each limited to maximum 16 leaf nodes, using\n",
            "all available CPU cores:\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
            "                                 n_jobs=-1, random_state=42)\n",
            "rnd_clf.fit(X_train, y_train)\n",
            "\n",
            "y_pred_rf = rnd_clf.predict(X_test)\n",
            "\n",
            "With  a  few  exceptions,  a  RandomForestClassifier  has  all  the  hyperparameters  of\n",
            "a DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐\n",
            "meters of a BaggingClassifier to control the ensemble itself.\n",
            "\n",
            "The  random  forest  algorithm  introduces  extra  randomness  when  growing  trees;\n",
            "instead of searching for the very best feature when splitting a node (see Chapter 6),\n",
            "it  searches  for  the  best  feature  among  a  random  subset  of  features.  By  default,  it\n",
            "samples  n features (where n is the total number of features). The algorithm results\n",
            "in  greater  tree  diversity,  which  (again)  trades  a  higher  bias  for  a  lower  variance,\n",
            "generally  yielding  an  overall  better  model.  So,  the  following  BaggingClassifier  is\n",
            "equivalent to the previous RandomForestClassifier:\n",
            "\n",
            "bag_clf = BaggingClassifier(\n",
            "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
            "    n_estimators=500, n_jobs=-1, random_state=42)\n",
            "\n",
            "Extra-Trees\n",
            "When you are growing a tree in a random forest, at each node only a random subset\n",
            "of the features is considered for splitting (as discussed earlier). It is possible to make\n",
            "trees even more random by also using random thresholds for each feature rather than\n",
            "searching  for  the  best  possible  thresholds  (like  regular  decision  trees  do).  For  this,\n",
            "simply set splitter=\"random\" when creating a DecisionTreeClassifier.\n",
            "\n",
            "10 Tin Kam Ho, “Random Decision Forests”, Proceedings of the Third International Conference on Document\n",
            "\n",
            "Analysis and Recognition 1 (1995): 278.\n",
            "\n",
            "11 The BaggingClassifier class remains useful if you want a bag of something other than decision trees.\n",
            "\n",
            "220 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fA forest of such extremely random trees is called an extremely randomized trees12 (or\n",
            "extra-trees  for  short)  ensemble.  Once  again,  this  technique  trades  more  bias  for  a\n",
            "lower variance. It also makes extra-trees classifiers much faster to train than regular\n",
            "random forests, because finding the best possible threshold for each feature at every\n",
            "node is one of the most time-consuming tasks of growing a tree.\n",
            "\n",
            "You  can  create  an  extra-trees  classifier  using  Scikit-Learn’s  ExtraTreesClassifier\n",
            "class.  Its  API  is  identical  to  the  RandomForestClassifier  class,  except  bootstrap\n",
            "defaults to False. Similarly, the ExtraTreesRegressor class has the same API as the\n",
            "RandomForestRegressor class, except bootstrap defaults to False.\n",
            "\n",
            "It  is  hard  to  tell  in  advance  whether  a  RandomForestClassifier\n",
            "will perform better or worse than an ExtraTreesClassifier. Gen‐\n",
            "erally, the only way to know is to try both and compare them using\n",
            "cross-validation.\n",
            "\n",
            "Feature Importance\n",
            "Yet another great quality of random forests is that they make it easy to measure the\n",
            "relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
            "looking at how much the tree nodes that use that feature reduce impurity on average,\n",
            "across  all  trees  in  the  forest.  More  precisely,  it  is  a  weighted  average,  where  each\n",
            "node’s  weight  is  equal  to  the  number  of  training  samples  that  are  associated  with  it\n",
            "(see Chapter 6).\n",
            "\n",
            "Scikit-Learn  computes  this  score  automatically  for  each  feature  after  training,  then\n",
            "it  scales  the  results  so  that  the  sum  of  all  importances  is  equal  to  1.  You  can  access\n",
            "the result using the feature_importances_ variable. For example, the following code\n",
            "trains a  RandomForestClassifier on the iris dataset (introduced in Chapter 4) and\n",
            "outputs  each  feature’s  importance.  It  seems  that  the  most  important  features  are\n",
            "the  petal  length  (44%)  and  width  (42%),  while  sepal  length  and  width  are  rather\n",
            "unimportant in comparison (11% and 2%, respectively):\n",
            "\n",
            ">>> from sklearn.datasets import load_iris\n",
            ">>> iris = load_iris(as_frame=True)\n",
            ">>> rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
            ">>> rnd_clf.fit(iris.data, iris.target)\n",
            ">>> for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
            "...     print(round(score, 2), name)\n",
            "...\n",
            "0.11 sepal length (cm)\n",
            "0.02 sepal width (cm)\n",
            "\n",
            "12 Pierre Geurts et al., “Extremely Randomized Trees”, Machine Learning 63, no. 1 (2006): 3–42.\n",
            "\n",
            "Random Forests \n",
            "\n",
            "| \n",
            "\n",
            "221\n",
            "\n",
            "\f0.44 petal length (cm)\n",
            "0.42 petal width (cm)\n",
            "\n",
            "Similarly,  if  you  train  a  random  forest  classifier  on  the  MNIST  dataset  (introduced\n",
            "in  Chapter  3)  and  plot  each  pixel’s  importance,  you  get  the  image  represented  in\n",
            "Figure 7-6.\n",
            "\n",
            "Figure 7-6. MNIST pixel importance (according to a random forest classifier)\n",
            "\n",
            "Random forests are very handy to get a quick understanding of what features actually\n",
            "matter, in particular if you need to perform feature selection.\n",
            "\n",
            "Boosting\n",
            "Boosting  (originally  called  hypothesis  boosting)  refers  to  any  ensemble  method  that\n",
            "can  combine  several  weak  learners  into  a  strong  learner.  The  general  idea  of  most\n",
            "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
            "cessor.  There  are  many  boosting  methods  available,  but  by  far  the  most  popular\n",
            "are  AdaBoost13  (short  for  adaptive  boosting)  and  gradient  boosting.  Let’s  start  with\n",
            "AdaBoost.\n",
            "\n",
            "AdaBoost\n",
            "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
            "to the training instances that the predecessor underfit. This results in new predictors\n",
            "focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
            "\n",
            "13 Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an\n",
            "\n",
            "Application to Boosting”, Journal of Computer and System Sciences 55, no. 1 (1997): 119–139.\n",
            "\n",
            "222 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fFor  example,  when  training  an  AdaBoost  classifier,  the  algorithm  first  trains  a  base\n",
            "classifier (such as a decision tree) and uses it to make predictions on the training set.\n",
            "The  algorithm  then  increases  the  relative  weight  of  misclassified  training  instances.\n",
            "Then it trains a second classifier, using the updated weights, and again makes predic‐\n",
            "tions on the training set, updates the instance weights, and so on (see Figure 7-7).\n",
            "\n",
            "Figure  7-8  shows  the  decision  boundaries  of  five  consecutive  predictors  on  the\n",
            "moons dataset (in this example, each predictor is a highly regularized SVM classifier\n",
            "with an RBF kernel).14 The first classifier gets many instances wrong, so their weights\n",
            "get boosted. The second classifier therefore does a better job on these instances, and\n",
            "so on. The plot on the right represents the same sequence of predictors, except that\n",
            "the learning rate is halved (i.e., the misclassified instance weights are boosted much\n",
            "less  at  every  iteration).  As  you  can  see,  this  sequential  learning  technique  has  some\n",
            "similarities with gradient descent, except that instead of tweaking a single predictor’s\n",
            "parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\n",
            "gradually making it better.\n",
            "\n",
            "Figure 7-7. AdaBoost sequential training with instance weight updates\n",
            "\n",
            "14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost; they are slow\n",
            "\n",
            "and tend to be unstable with it.\n",
            "\n",
            "Boosting \n",
            "\n",
            "| \n",
            "\n",
            "223\n",
            "\n",
            "\fOnce  all  predictors  are  trained,  the  ensemble  makes  predictions  very  much  like\n",
            "bagging or pasting, except that predictors have different weights depending on their\n",
            "overall accuracy on the weighted training set.\n",
            "\n",
            "Figure 7-8. Decision boundaries of consecutive predictors\n",
            "\n",
            "There is one important drawback to this sequential learning techni‐\n",
            "que:  training  cannot  be  parallelized  since  each  predictor  can  only\n",
            "be trained after the previous predictor has been trained and evalu‐\n",
            "ated. As a result, it does not scale as well as bagging or pasting.\n",
            "\n",
            "Let’s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\n",
            "set to 1/m. A first predictor is trained, and its weighted error rate r1 is computed on\n",
            "the training set; see Equation 7-1.\n",
            "\n",
            "Equation 7-1. Weighted error rate of the jth predictor\n",
            "\n",
            "rj =\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "i\n",
            "≠ y i\n",
            "y j\n",
            "\n",
            "i\n",
            "w i where y j\n",
            "\n",
            "is the jth predictor’s prediction for the ith instance\n",
            "\n",
            "The  predictor’s  weight  αj  is  then  computed  using  Equation  7-2,  where  η  is  the\n",
            "learning rate hyperparameter (defaults to 1).15 The more accurate the predictor is, the\n",
            "higher its weight will be. If it is just guessing randomly, then its weight will be close\n",
            "to zero. However, if it is most often wrong (i.e., less accurate than random guessing),\n",
            "then its weight will be negative.\n",
            "\n",
            "15 The original AdaBoost algorithm does not use a learning rate hyperparameter.\n",
            "\n",
            "224 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fEquation 7-2. Predictor weight\n",
            "\n",
            "αj = η log\n",
            "\n",
            "1 − rj\n",
            "rj\n",
            "\n",
            "Next,  the  AdaBoost  algorithm  updates  the  instance  weights,  using  Equation  7-3,\n",
            "which boosts the weights of the misclassified instances.\n",
            "\n",
            "Equation 7-3. Weight update rule\n",
            "\n",
            "for i = 1, 2, ⋯, m\n",
            "w i\n",
            "\n",
            "w i\n",
            "\n",
            "if yj\n",
            "\n",
            "i\n",
            "\n",
            "i\n",
            "\n",
            "= y i\n",
            "≠ y i\n",
            "\n",
            "w i exp αj\n",
            "\n",
            "if yj\n",
            "\n",
            "Then all the instance weights are normalized (i.e., divided by ∑i = 1\n",
            "\n",
            "m w i ).\n",
            "\n",
            "Finally, a new predictor is trained using the updated weights, and the whole process\n",
            "is repeated: the new predictor’s weight is computed, the instance weights are updated,\n",
            "then another predictor is trained, and so on. The algorithm stops when the desired\n",
            "number of predictors is reached, or when a perfect predictor is found.\n",
            "\n",
            "To make predictions, AdaBoost simply computes the predictions of all the predictors\n",
            "and  weighs  them  using  the  predictor  weights  αj.  The  predicted  class  is  the  one  that\n",
            "receives the majority of weighted votes (see Equation 7-4).\n",
            "\n",
            "Equation 7-4. AdaBoost predictions\n",
            "\n",
            "y x = argmax\n",
            "\n",
            "k\n",
            "\n",
            "N\n",
            "∑\n",
            "j = 1\n",
            "y j x = k\n",
            "\n",
            "αj where N is the number of predictors\n",
            "\n",
            "Scikit-Learn uses a multiclass version of AdaBoost called SAMME16 (which stands for\n",
            "Stagewise Additive Modeling using a Multiclass Exponential loss function). When there\n",
            "are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate\n",
            "class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use\n",
            "a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class\n",
            "probabilities rather than predictions and generally performs better.\n",
            "\n",
            "16 For more details, see Ji Zhu et al., “Multi-Class AdaBoost”, Statistics and Its Interface 2, no. 3 (2009): 349–360.\n",
            "\n",
            "Boosting \n",
            "\n",
            "| \n",
            "\n",
            "225\n",
            "\n",
            "\fThe  following  code  trains  an  AdaBoost  classifier  based  on  30  decision  stumps  using\n",
            "Scikit-Learn’s  AdaBoostClassifier  class  (as  you  might  expect,  there  is  also  an\n",
            "AdaBoostRegressor class). A decision stump is a decision tree with max_depth=1—in\n",
            "other words, a tree composed of a single decision node plus two leaf nodes. This is\n",
            "the default base estimator for the AdaBoostClassifier class:\n",
            "\n",
            "from sklearn.ensemble import AdaBoostClassifier\n",
            "\n",
            "ada_clf = AdaBoostClassifier(\n",
            "    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
            "    learning_rate=0.5, random_state=42)\n",
            "ada_clf.fit(X_train, y_train)\n",
            "\n",
            "If your AdaBoost ensemble is overfitting the training set, you can\n",
            "try reducing the number of estimators or more strongly regulariz‐\n",
            "ing the base estimator.\n",
            "\n",
            "Gradient Boosting\n",
            "Another  very  popular  boosting  algorithm  is  gradient  boosting.17  Just  like  AdaBoost,\n",
            "gradient boosting works by sequentially adding predictors to an ensemble, each one\n",
            "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
            "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
            "errors made by the previous predictor.\n",
            "\n",
            "Let’s go through a simple regression example, using decision trees as the base predic‐\n",
            "tors; this is called gradient tree boosting, or gradient boosted regression trees (GBRT).\n",
            "First, let’s generate a noisy quadratic dataset and fit a DecisionTreeRegressor to it:\n",
            "\n",
            "import numpy as np\n",
            "from sklearn.tree import DecisionTreeRegressor\n",
            "\n",
            "np.random.seed(42)\n",
            "X = np.random.rand(100, 1) - 0.5\n",
            "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
            "\n",
            "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
            "tree_reg1.fit(X, y)\n",
            "\n",
            "17 Gradient boosting was first introduced in Leo Breiman’s 1997 paper “Arcing the Edge” and was further\n",
            "\n",
            "developed in the 1999 paper “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome H.\n",
            "Friedman.\n",
            "\n",
            "226 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fNext, we’ll train a second DecisionTreeRegressor on the residual errors made by the\n",
            "first predictor:\n",
            "\n",
            "y2 = y - tree_reg1.predict(X)\n",
            "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
            "tree_reg2.fit(X, y2)\n",
            "\n",
            "And  then  we’ll  train  a  third  regressor  on  the  residual  errors  made  by  the  second\n",
            "predictor:\n",
            "\n",
            "y3 = y2 - tree_reg2.predict(X)\n",
            "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
            "tree_reg3.fit(X, y3)\n",
            "\n",
            "Now we have an ensemble containing three trees. It can make predictions on a new\n",
            "instance simply by adding up the predictions of all the trees:\n",
            "\n",
            ">>> X_new = np.array([[-0.4], [0.], [0.5]])\n",
            ">>> sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
            "array([0.49484029, 0.04021166, 0.75026781])\n",
            "\n",
            "Figure 7-9 represents the predictions of these three trees in the left column, and the\n",
            "ensemble’s predictions in the right column. In the first row, the ensemble has just one\n",
            "tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
            "row, a new tree is trained on the residual errors of the first tree. On the right you can\n",
            "see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
            "two trees. Similarly, in the third row another tree is trained on the residual errors of\n",
            "the  second  tree.  You  can  see  that  the  ensemble’s  predictions  gradually  get  better  as\n",
            "trees are added to the ensemble.\n",
            "\n",
            "You can use Scikit-Learn’s GradientBoostingRegressor class to train GBRT ensem‐\n",
            "bles  more  easily  (there’s  also  a  GradientBoostingClassifier  class  for  classifica‐\n",
            "tion).  Much  like  the  RandomForestRegressor  class,  it  has  hyperparameters  to\n",
            "control  the  growth  of  decision  trees  (e.g.,  max_depth,  min_samples_leaf),  as  well\n",
            "as  hyperparameters  to  control  the  ensemble  training,  such  as  the  number  of  trees\n",
            "(n_estimators). The following code creates the same ensemble as the previous one:\n",
            "\n",
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "\n",
            "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
            "                                 learning_rate=1.0, random_state=42)\n",
            "gbrt.fit(X, y)\n",
            "\n",
            "Boosting \n",
            "\n",
            "| \n",
            "\n",
            "227\n",
            "\n",
            "\fFigure 7-9. In this depiction of gradient boosting, the first predictor (top left) is trained\n",
            "normally, then each consecutive predictor (middle left and lower left) is trained on the\n",
            "previous predictor’s residuals; the right column shows the resulting ensemble’s predictions\n",
            "\n",
            "The  learning_rate  hyperparameter  scales  the  contribution  of  each  tree.  If  you  set\n",
            "it  to  a  low  value,  such  as  0.05,  you  will  need  more  trees  in  the  ensemble  to  fit  the\n",
            "training set, but the predictions will usually generalize better. This is a regularization\n",
            "technique  called  shrinkage.  Figure  7-10  shows  two  GBRT  ensembles  trained  with\n",
            "different  hyperparameters:  the  one  on  the  left  does  not  have  enough  trees  to  fit  the\n",
            "training set, while the one on the right has about the right amount. If we added more\n",
            "trees, the GBRT would start to overfit the training set.\n",
            "\n",
            "228 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fFigure 7-10. GBRT ensembles with not enough predictors (left) and just enough (right)\n",
            "\n",
            "To  find  the  optimal  number  of  trees,  you  could  perform  cross-validation  using\n",
            "GridSearchCV or RandomizedSearchCV, as usual, but there’s a simpler way: if you set\n",
            "the n_iter_no_change hyperparameter to an integer value, say 10, then the Gradient\n",
            "BoostingRegressor  will  automatically  stop  adding  more  trees  during  training  if  it\n",
            "sees  that  the  last  10  trees  didn’t  help.  This  is  simply  early  stopping  (introduced  in\n",
            "Chapter 4), but with a little bit of patience: it tolerates having no progress for a few\n",
            "iterations before it stops. Let’s train the ensemble using early stopping:\n",
            "\n",
            "gbrt_best = GradientBoostingRegressor(\n",
            "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
            "    n_iter_no_change=10, random_state=42)\n",
            "gbrt_best.fit(X, y)\n",
            "\n",
            "If you set n_iter_no_change too low, training may stop too early and the model will\n",
            "underfit.  But  if  you  set  it  too  high,  it  will  overfit  instead.  We  also  set  a  fairly  small\n",
            "learning rate and a high number of estimators, but the actual number of estimators in\n",
            "the trained ensemble is much lower, thanks to early stopping:\n",
            "\n",
            ">>> gbrt_best.n_estimators_\n",
            "92\n",
            "\n",
            "When  n_iter_no_change  is  set,  the  fit()  method  automatically  splits  the  training\n",
            "set  into  a  smaller  training  set  and  a  validation  set:  this  allows  it  to  evaluate  the\n",
            "model’s  performance  each  time  it  adds  a  new  tree.  The  size  of  the  validation  set  is\n",
            "controlled  by  the  validation_fraction  hyperparameter,  which  is  10%  by  default.\n",
            "The  tol  hyperparameter  determines  the  maximum  performance  improvement  that\n",
            "still counts as negligible. It defaults to 0.0001.\n",
            "\n",
            "Boosting \n",
            "\n",
            "| \n",
            "\n",
            "229\n",
            "\n",
            "\fThe  GradientBoostingRegressor  class  also  supports  a  subsample  hyperparameter,\n",
            "which  specifies  the  fraction  of  training  instances  to  be  used  for  training  each  tree.\n",
            "For  example,  if  subsample=0.25,  then  each  tree  is  trained  on  25%  of  the  training\n",
            "instances,  selected  randomly.  As  you  can  probably  guess  by  now,  this  technique\n",
            "trades a higher bias for a lower variance. It also speeds up training considerably. This\n",
            "is called stochastic gradient boosting.\n",
            "\n",
            "Histogram-Based Gradient Boosting\n",
            "Scikit-Learn also provides another GBRT implementation, optimized for large data‐\n",
            "sets: histogram-based gradient boosting (HGB). It works by binning the input features,\n",
            "replacing  them  with  integers.  The  number  of  bins  is  controlled  by  the  max_bins\n",
            "hyperparameter,  which  defaults  to  255  and  cannot  be  set  any  higher  than  this.  Bin‐\n",
            "ning can greatly reduce the number of possible thresholds that the training algorithm\n",
            "needs to evaluate. Moreover, working with integers makes it possible to use faster and\n",
            "more  memory-efficient  data  structures.  And  the  way  the  bins  are  built  removes  the\n",
            "need for sorting the features when training each tree.\n",
            "\n",
            "As a result, this implementation has a computational complexity of O(b×m) instead\n",
            "of  O(n×m×log(m)),  where  b  is  the  number  of  bins,  m  is  the  number  of  training\n",
            "instances,  and  n  is  the  number  of  features.  In  practice,  this  means  that  HGB  can\n",
            "train hundreds of times faster than regular GBRT on large datasets. However, binning\n",
            "causes a precision loss, which acts as a regularizer: depending on the dataset, this may\n",
            "help reduce overfitting, or it may cause underfitting.\n",
            "\n",
            "Scikit-Learn  provides  two  classes  for  HGB:  HistGradientBoostingRegressor  and\n",
            "HistGradientBoostingClassifier.  They’re  similar  to  GradientBoostingRegressor\n",
            "and GradientBoostingClassifier, with a few notable differences:\n",
            "\n",
            "• Early  stopping  is  automatically  activated  if  the  number  of  instances  is  greater\n",
            "•\n",
            "than 10,000. You can turn early stopping always on or always off by setting the\n",
            "early_stopping hyperparameter to True or False.\n",
            "\n",
            "• Subsampling is not supported.\n",
            "•\n",
            "• n_estimators is renamed to max_iter.\n",
            "•\n",
            "• The only decision tree hyperparameters that can be tweaked are max_leaf_nodes,\n",
            "•\n",
            "\n",
            "min_samples_leaf, and max_depth.\n",
            "\n",
            "230 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fThe HGB classes also have two nice features: they support both categorical features\n",
            "and missing values. This simplifies preprocessing quite a bit. However, the categorical\n",
            "features  must  be  represented  as  integers  ranging  from  0  to  a  number  lower  than\n",
            "max_bins.  You  can  use  an  OrdinalEncoder  for  this.  For  example,  here’s  how  to\n",
            "build and train a complete pipeline for the California housing dataset introduced in\n",
            "Chapter 2:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "from sklearn.compose import make_column_transformer\n",
            "from sklearn.ensemble import HistGradientBoostingRegressor\n",
            "from sklearn.preprocessing import OrdinalEncoder\n",
            "\n",
            "hgb_reg = make_pipeline(\n",
            "    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\n",
            "                            remainder=\"passthrough\"),\n",
            "    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n",
            ")\n",
            "hgb_reg.fit(housing, housing_labels)\n",
            "\n",
            "The  whole  pipeline  is  just  as  short  as  the  imports!  No  need  for  an  imputer,  scaler,\n",
            "or  a  one-hot  encoder,  so  it’s  really  convenient.  Note  that  categorical_features\n",
            "must  be  set  to  the  categorical  column  indices  (or  a  Boolean  array).  Without  any\n",
            "hyperparameter tuning, this model yields an RMSE of about 47,600, which is not too\n",
            "bad.\n",
            "\n",
            "Several  other  optimized  implementations  of  gradient  boosting  are\n",
            "available in the Python ML ecosystem: in particular, XGBoost, Cat‐\n",
            "Boost, and LightGBM. These libraries have been around for several\n",
            "years. They are all specialized for gradient boosting, their APIs are\n",
            "very  similar  to  Scikit-Learn’s,  and  they  provide  many  additional\n",
            "features,  including  GPU  acceleration;  you  should  definitely  check\n",
            "them out! Moreover, the TensorFlow Random Forests library pro‐\n",
            "vides  optimized  implementations  of  a  variety  of  random  forest\n",
            "algorithms, including plain random forests, extra-trees, GBRT, and\n",
            "several more.\n",
            "\n",
            "Boosting \n",
            "\n",
            "| \n",
            "\n",
            "231\n",
            "\n",
            "\fStacking\n",
            "The last ensemble method we will discuss in this chapter is called stacking (short for\n",
            "stacked generalization).18 It is based on a simple idea: instead of using trivial functions\n",
            "(such  as  hard  voting)  to  aggregate  the  predictions  of  all  predictors  in  an  ensemble,\n",
            "why don’t we train a model to perform this aggregation? Figure 7-11 shows such an\n",
            "ensemble performing a regression task on a new instance. Each of the bottom three\n",
            "predictors  predicts  a  different  value  (3.1,  2.7,  and  2.9),  and  then  the  final  predictor\n",
            "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
            "final prediction (3.0).\n",
            "\n",
            "Figure 7-11. Aggregating predictions using a blending predictor\n",
            "\n",
            "To  train  the  blender,  you  first  need  to  build  the  blending  training  set.  You  can\n",
            "use  cross_val_predict()  on  every  predictor  in  the  ensemble  to  get  out-of-sample\n",
            "predictions for each instance in the original training set (Figure 7-12), and use these\n",
            "\n",
            "18 David H. Wolpert, “Stacked Generalization”, Neural Networks 5, no. 2 (1992): 241–259.\n",
            "\n",
            "232 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fcan be used as the input features to train the blender; and the targets can simply be\n",
            "copied from the original training set. Note that regardless of the number of features\n",
            "in  the  original  training  set  (just  one  in  this  example),  the  blending  training  set  will\n",
            "contain one input feature per predictor (three in this example). Once the blender is\n",
            "trained, the base predictors are retrained one last time on the full original training set.\n",
            "\n",
            "Figure 7-12. Training the blender in a stacking ensemble\n",
            "\n",
            "It is actually possible to train several different blenders this way (e.g., one using linear\n",
            "regression, another using random forest regression) to get a whole layer of blenders,\n",
            "and then add another blender on top of that to produce the final prediction, as shown\n",
            "in Figure 7-13. You may be able to squeeze out a few more drops of performance by\n",
            "doing this, but it will cost you in both training time and system complexity.\n",
            "\n",
            "Stacking \n",
            "\n",
            "| \n",
            "\n",
            "233\n",
            "\n",
            "\fFigure 7-13. Predictions in a multilayer stacking ensemble\n",
            "\n",
            "Scikit-Learn  provides  two  classes  for  stacking  ensembles:  StackingClassifier  and\n",
            "StackingRegressor. For example, we can replace the VotingClassifier we used at\n",
            "the beginning of this chapter on the moons dataset with a StackingClassifier:\n",
            "\n",
            "from sklearn.ensemble import StackingClassifier\n",
            "\n",
            "stacking_clf = StackingClassifier(\n",
            "    estimators=[\n",
            "        ('lr', LogisticRegression(random_state=42)),\n",
            "        ('rf', RandomForestClassifier(random_state=42)),\n",
            "        ('svc', SVC(probability=True, random_state=42))\n",
            "    ],\n",
            "    final_estimator=RandomForestClassifier(random_state=43),\n",
            "    cv=5  # number of cross-validation folds\n",
            ")\n",
            "stacking_clf.fit(X_train, y_train)\n",
            "\n",
            "For each predictor, the stacking classifier will call predict_proba() if available; if not\n",
            "it  will  fall  back  to  decision_function()  or,  as  a  last  resort,  call  predict().  If  you\n",
            "don’t  provide  a  final  estimator,  StackingClassifier  will  use  LogisticRegression\n",
            "and StackingRegressor will use RidgeCV.\n",
            "\n",
            "234 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fIf  you  evaluate  this  stacking  model  on  the  test  set,  you  will  find  92.8%  accuracy,\n",
            "which is a bit better than the voting classifier using soft voting, which got 92%.\n",
            "\n",
            "In  conclusion,  ensemble  methods  are  versatile,  powerful,  and  fairly  simple  to  use.\n",
            "Random forests, AdaBoost, and GBRT are among the first models you should test for\n",
            "most machine learning tasks, and they particularly shine with heterogeneous tabular\n",
            "data.  Moreover,  as  they  require  very  little  preprocessing,  they’re  great  for  getting  a\n",
            "prototype  up  and  running  quickly.  Lastly,  ensemble  methods  like  voting  classifiers\n",
            "and stacking classifiers can help push your system’s performance to its limits.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1. If  you  have  trained  five  different  models  on  the  exact  same  training  data,  and\n",
            "1.\n",
            "they  all  achieve  95%  precision,  is  there  any  chance  that  you  can  combine  these\n",
            "models to get better results? If so, how? If not, why?\n",
            "\n",
            "2.\n",
            "2. What is the difference between hard and soft voting classifiers?\n",
            "\n",
            "3. Is it possible to speed up training of a bagging ensemble by distributing it across\n",
            "3.\n",
            "multiple  servers?  What  about  pasting  ensembles,  boosting  ensembles,  random\n",
            "forests, or stacking ensembles?\n",
            "\n",
            "4.\n",
            "4. What is the benefit of out-of-bag evaluation?\n",
            "\n",
            "5. What  makes  extra-trees  ensembles  more  random  than  regular  random  forests?\n",
            "5.\n",
            "How can this extra randomness help? Are extra-trees classifiers slower or faster\n",
            "than regular random forests?\n",
            "\n",
            "6.\n",
            "6. If  your  AdaBoost  ensemble  underfits  the  training  data,  which  hyperparameters\n",
            "\n",
            "should you tweak, and how?\n",
            "\n",
            "7.\n",
            "7. If your gradient boosting ensemble overfits the training set, should you increase\n",
            "\n",
            "or decrease the learning rate?\n",
            "\n",
            "8. Load  the  MNIST  dataset  (introduced  in  Chapter  3),  and  split  it  into  a  training\n",
            "8.\n",
            "set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000\n",
            "for  validation,  and  10,000  for  testing).  Then  train  various  classifiers,  such  as  a\n",
            "random forest classifier, an extra-trees classifier, and an SVM classifier. Next, try\n",
            "to  combine  them  into  an  ensemble  that  outperforms  each  individual  classifier\n",
            "on  the  validation  set,  using  soft  or  hard  voting.  Once  you  have  found  one,  try\n",
            "it on the test set. How much better does it perform compared to the individual\n",
            "classifiers?\n",
            "\n",
            "9. Run the individual classifiers from the previous exercise to make predictions on\n",
            "9.\n",
            "the  validation  set,  and  create  a  new  training  set  with  the  resulting  predictions:\n",
            "each training instance is a vector containing the set of predictions from all your\n",
            "classifiers  for  an  image,  and  the  target  is  the  image’s  class.  Train  a  classifier\n",
            "on this new training set. Congratulations—you have just trained a blender, and\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "235\n",
            "\n",
            "\ftogether  with  the  classifiers  it  forms  a  stacking  ensemble!  Now  evaluate  the\n",
            "ensemble on the test set. For each image in the test set, make predictions with all\n",
            "your classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\n",
            "dictions.  How  does  it  compare  to  the  voting  classifier  you  trained  earlier?  Now\n",
            "try again using a StackingClassifier instead. Do you get better performance? If\n",
            "so, why?\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "236 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 7: Ensemble Learning and Random Forests\n",
            "\n",
            "\fCHAPTER 8\n",
            "Dimensionality Reduction\n",
            "\n",
            "Many machine learning problems involve thousands or even millions of features for\n",
            "each training instance. Not only do all these features make training extremely slow,\n",
            "but they can also make it much harder to find a good solution, as you will see. This\n",
            "problem is often referred to as the curse of dimensionality.\n",
            "\n",
            "Fortunately,  in  real-world  problems,  it  is  often  possible  to  reduce  the  number  of\n",
            "features considerably, turning an intractable problem into a tractable one. For exam‐\n",
            "ple, consider the MNIST images (introduced in Chapter 3): the pixels on the image\n",
            "borders are almost always white, so you could completely drop these pixels from the\n",
            "training  set  without  losing  much  information.  As  we  saw  in  the  previous  chapter,\n",
            "(Figure 7-6) confirms that these pixels are utterly unimportant for the classification\n",
            "task.  Additionally,  two  neighboring  pixels  are  often  highly  correlated:  if  you  merge\n",
            "them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will\n",
            "not lose much information.\n",
            "\n",
            "Reducing  dimensionality  does  cause  some  information  loss,  just\n",
            "like compressing an image to JPEG can degrade its quality, so even\n",
            "though it will speed up training, it may make your system perform\n",
            "slightly worse. It also makes your pipelines a bit more complex and\n",
            "thus  harder  to  maintain.  Therefore,  I  recommend  you  first  try  to\n",
            "train  your  system  with  the  original  data  before  considering  using\n",
            "dimensionality reduction. In some cases, reducing the dimension‐\n",
            "ality of the training data may filter out some noise and unnecessary\n",
            "details  and  thus  result  in  higher  performance,  but  in  general  it\n",
            "won’t; it will just speed up training.\n",
            "\n",
            "237\n",
            "\n",
            "\fApart  from  speeding  up  training,  dimensionality  reduction  is  also  extremely  useful\n",
            "for  data  visualization.  Reducing  the  number  of  dimensions  down  to  two  (or  three)\n",
            "makes  it  possible  to  plot  a  condensed  view  of  a  high-dimensional  training  set  on  a\n",
            "graph and often gain some important insights by visually detecting patterns, such as\n",
            "clusters. Moreover, data visualization is essential to communicate your conclusions to\n",
            "people who are not data scientists—in particular, decision makers who will use your\n",
            "results.\n",
            "\n",
            "In  this  chapter  we  will  first  discuss  the  curse  of  dimensionality  and  get  a  sense\n",
            "of  what  goes  on  in  high-dimensional  space.  Then  we  will  consider  the  two  main\n",
            "approaches to dimensionality reduction (projection and manifold learning), and we\n",
            "will go through three of the most popular dimensionality reduction techniques: PCA,\n",
            "random projection, and locally linear embedding (LLE).\n",
            "\n",
            "The Curse of Dimensionality\n",
            "We are so used to living in three dimensions1 that our intuition fails us when we try\n",
            "to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\n",
            "picture in our minds (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\n",
            "1,000-dimensional space.\n",
            "\n",
            "Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\n",
            "\n",
            "It turns out that many things behave very differently in high-dimensional space. For\n",
            "example, if you pick a random point in a unit square (a 1 × 1 square), it will have only\n",
            "about a 0.4% chance of being located less than 0.001 from a border (in other words, it\n",
            "is very unlikely that a random point will be “extreme” along any dimension). But in a\n",
            "\n",
            "1 Well, four dimensions if you count time, and a few more if you are a string theorist.\n",
            "\n",
            "2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐\n",
            "\n",
            "Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.\n",
            "\n",
            "238 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\f10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most\n",
            "points in a high-dimensional hypercube are very close to the border.3\n",
            "\n",
            "1, 000, 000\n",
            "\n",
            "Here  is  a  more  troublesome  difference:  if  you  pick  two  points  randomly  in  a  unit\n",
            "square,  the  distance  between  these  two  points  will  be,  on  average,  roughly  0.52.  If\n",
            "you pick two random points in a 3D unit cube, the average distance will be roughly\n",
            "0.66.  But  what  about  two  points  picked  randomly  in  a  1,000,000-dimensional  unit\n",
            "hypercube?  The  average  distance,  believe  it  or  not,  will  be  about  408.25  (roughly\n",
            "6)! This is counterintuitive: how can two points be so far apart when they\n",
            "both  lie  within  the  same  unit  hypercube?  Well,  there’s  just  plenty  of  space  in  high\n",
            "dimensions.  As  a  result,  high-dimensional  datasets  are  at  risk  of  being  very  sparse:\n",
            "most training instances are likely to be far away from each other. This also means that\n",
            "a new instance will likely be far away from any training instance, making predictions\n",
            "much less reliable than in lower dimensions, since they will be based on much larger\n",
            "extrapolations. In short, the more dimensions the training set has, the greater the risk\n",
            "of overfitting it.\n",
            "\n",
            "In  theory,  one  solution  to  the  curse  of  dimensionality  could  be  to  increase  the  size\n",
            "of  the  training  set  to  reach  a  sufficient  density  of  training  instances.  Unfortunately,\n",
            "in  practice,  the  number  of  training  instances  required  to  reach  a  given  density\n",
            "grows exponentially with the number of dimensions. With just 100 features—signifi‐\n",
            "cantly fewer than in the MNIST problem—all ranging from 0 to 1, you would need\n",
            "more training instances than atoms in the observable universe in order for training\n",
            "instances to be within 0.1 of each other on average, assuming they were spread out\n",
            "uniformly across all dimensions.\n",
            "\n",
            "Main Approaches for Dimensionality Reduction\n",
            "Before  we  dive  into  specific  dimensionality  reduction  algorithms,  let’s  take  a  look\n",
            "at  the  two  main  approaches  to  reducing  dimensionality:  projection  and  manifold\n",
            "learning.\n",
            "\n",
            "Projection\n",
            "In most real-world problems, training instances are not spread out uniformly across\n",
            "all dimensions. Many features are almost constant, while others are highly correlated\n",
            "(as discussed earlier for MNIST). As a result, all training instances lie within (or close\n",
            "to) a much lower-dimensional subspace of the high-dimensional space. This sounds\n",
            "very  abstract,  so  let’s  look  at  an  example.  In  Figure  8-2  you  can  see  a  3D  dataset\n",
            "represented by small spheres.\n",
            "\n",
            "3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\n",
            "\n",
            "in their coffee), if you consider enough dimensions.\n",
            "\n",
            "Main Approaches for Dimensionality Reduction \n",
            "\n",
            "| \n",
            "\n",
            "239\n",
            "\n",
            "\fFigure 8-2. A 3D dataset lying close to a 2D subspace\n",
            "\n",
            "Notice  that  all  training  instances  lie  close  to  a  plane:  this  is  a  lower-dimensional\n",
            "(2D)  subspace  of  the  higher-dimensional  (3D)  space.  If  we  project  every  training\n",
            "instance  perpendicularly  onto  this  subspace  (as  represented  by  the  short  dashed\n",
            "lines  connecting  the  instances  to  the  plane),  we  get  the  new  2D  dataset  shown  in\n",
            "Figure 8-3. Ta-da! We have just reduced the dataset’s dimensionality from 3D to 2D.\n",
            "Note that the axes correspond to new features z1 and z2: they are the coordinates of\n",
            "the projections on the plane.\n",
            "\n",
            "Figure 8-3. The new 2D dataset after projection\n",
            "\n",
            "240 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fManifold Learning\n",
            "However, projection is not always the best approach to dimensionality reduction. In\n",
            "many  cases  the  subspace  may  twist  and  turn,  such  as  in  the  famous  Swiss  roll  toy\n",
            "dataset represented in Figure 8-4.\n",
            "\n",
            "Figure 8-4. Swiss roll dataset\n",
            "\n",
            "Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\n",
            "the  Swiss  roll  together,  as  shown  on  the  left  side  of  Figure  8-5.  What  you  probably\n",
            "want instead is to unroll the Swiss roll to obtain the 2D dataset on the right side of\n",
            "Figure 8-5.\n",
            "\n",
            "Figure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll\n",
            "(right)\n",
            "\n",
            "Main Approaches for Dimensionality Reduction \n",
            "\n",
            "| \n",
            "\n",
            "241\n",
            "\n",
            "\fThe  Swiss  roll  is  an  example  of  a  2D  manifold.  Put  simply,  a  2D  manifold  is  a  2D\n",
            "shape that can be bent and twisted in a higher-dimensional space. More generally, a\n",
            "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\n",
            "resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\n",
            "locally resembles a 2D plane, but it is rolled in the third dimension.\n",
            "\n",
            "Many dimensionality reduction algorithms work by modeling the manifold on which\n",
            "the  training  instances  lie;  this  is  called  manifold  learning.  It  relies  on  the  manifold\n",
            "assumption,  also  called  the  manifold  hypothesis,  which  holds  that  most  real-world\n",
            "high-dimensional  datasets  lie  close  to  a  much  lower-dimensional  manifold.  This\n",
            "assumption is very often empirically observed.\n",
            "\n",
            "Once again, think about the MNIST dataset: all handwritten digit images have some\n",
            "similarities.  They  are  made  of  connected  lines,  the  borders  are  white,  and  they  are\n",
            "more  or  less  centered.  If  you  randomly  generated  images,  only  a  ridiculously  tiny\n",
            "fraction  of  them  would  look  like  handwritten  digits.  In  other  words,  the  degrees  of\n",
            "freedom available to you if you try to create a digit image are dramatically lower than\n",
            "the degrees of freedom you have if you are allowed to generate any image you want.\n",
            "These constraints tend to squeeze the dataset into a lower-dimensional manifold.\n",
            "\n",
            "The manifold assumption is often accompanied by another implicit assumption: that\n",
            "the task at hand (e.g., classification or regression) will be simpler if expressed in the\n",
            "lower-dimensional space of the manifold. For example, in the top row of Figure 8-6\n",
            "the  Swiss  roll  is  split  into  two  classes:  in  the  3D  space  (on  the  left)  the  decision\n",
            "boundary  would  be  fairly  complex,  but  in  the  2D  unrolled  manifold  space  (on  the\n",
            "right) the decision boundary is a straight line.\n",
            "\n",
            "However, this implicit assumption does not always hold. For example, in the bottom\n",
            "row of Figure 8-6, the decision boundary is located at x1 = 5. This decision boundary\n",
            "looks  very  simple  in  the  original  3D  space  (a  vertical  plane),  but  it  looks  more\n",
            "complex in the unrolled manifold (a collection of four independent line segments).\n",
            "\n",
            "In short, reducing the dimensionality of your training set before training a model will\n",
            "usually speed up training, but it may not always lead to a better or simpler solution; it\n",
            "all depends on the dataset.\n",
            "\n",
            "Hopefully  you  now  have  a  good  sense  of  what  the  curse  of  dimensionality  is  and\n",
            "how  dimensionality  reduction  algorithms  can  fight  it,  especially  when  the  manifold\n",
            "assumption holds. The rest of this chapter will go through some of the most popular\n",
            "algorithms for dimensionality reduction.\n",
            "\n",
            "242 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fFigure 8-6. The decision boundary may not always be simpler with lower dimensions\n",
            "\n",
            "PCA\n",
            "Principal component analysis (PCA) is by far the most popular dimensionality reduc‐\n",
            "tion algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
            "it projects the data onto it, just like in Figure 8-2.\n",
            "\n",
            "Preserving the Variance\n",
            "Before  you  can  project  the  training  set  onto  a  lower-dimensional  hyperplane,  you\n",
            "first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\n",
            "sented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes).\n",
            "On  the  right  is  the  result  of  the  projection  of  the  dataset  onto  each  of  these  axes.\n",
            "As  you  can  see,  the  projection  onto  the  solid  line  preserves  the  maximum  variance\n",
            "(top), while the projection onto the dotted line preserves very little variance (bottom)\n",
            "and the projection onto the dashed line preserves an intermediate amount of variance\n",
            "(middle).\n",
            "\n",
            "PCA \n",
            "\n",
            "| \n",
            "\n",
            "243\n",
            "\n",
            "\fFigure 8-7. Selecting the subspace on which to project\n",
            "\n",
            "It  seems  reasonable  to  select  the  axis  that  preserves  the  maximum  amount  of  var‐\n",
            "iance, as it will most likely lose less information than the other projections. Another\n",
            "way  to  justify  this  choice  is  that  it  is  the  axis  that  minimizes  the  mean  squared\n",
            "distance  between  the  original  dataset  and  its  projection  onto  that  axis.  This  is  the\n",
            "rather simple idea behind PCA.4\n",
            "\n",
            "Principal Components\n",
            "PCA  identifies  the  axis  that  accounts  for  the  largest  amount  of  variance  in  the\n",
            "training set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to\n",
            "the first one, that accounts for the largest amount of the remaining variance. In this\n",
            "2D  example  there  is  no  choice:  it  is  the  dotted  line.  If  it  were  a  higher-dimensional\n",
            "dataset,  PCA  would  also  find  a  third  axis,  orthogonal  to  both  previous  axes,  and  a\n",
            "fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
            "\n",
            "The  ith  axis  is  called  the  ith  principal  component  (PC)  of  the  data.  In  Figure  8-7,  the\n",
            "first  PC  is  the  axis  on  which  vector  c1  lies,  and  the  second  PC  is  the  axis  on  which\n",
            "vector c2 lies. In Figure 8-2 the first two PCs are on the projection plane, and the third\n",
            "PC is the axis orthogonal to that plane. After the projection, in Figure 8-3, the first\n",
            "PC corresponds to the z1 axis, and the second PC corresponds to the z2 axis.\n",
            "\n",
            "4 Karl Pearson, “On Lines and Planes of Closest Fit to Systems of Points in Space”, The London, Edinburgh, and\n",
            "\n",
            "Dublin Philosophical Magazine and Journal of Science 2, no. 11 (1901): 559–572.\n",
            "\n",
            "244 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fFor  each  principal  component,  PCA  finds  a  zero-centered  unit\n",
            "vector  pointing  in  the  direction  of  the  PC.  Since  two  opposing\n",
            "unit vectors lie on the same axis, the direction of the unit vectors\n",
            "returned  by  PCA  is  not  stable:  if  you  perturb  the  training  set\n",
            "slightly and run PCA again, the unit vectors may point in the oppo‐\n",
            "site  direction  as  the  original  vectors.  However,  they  will  generally\n",
            "still lie on the same axes. In some cases, a pair of unit vectors may\n",
            "even rotate or swap (if the variances along these two axes are very\n",
            "close), but the plane they define will generally remain the same.\n",
            "\n",
            "So  how  can  you  find  the  principal  components  of  a  training  set?  Luckily,  there  is\n",
            "a standard matrix factorization technique called singular value decomposition (SVD)\n",
            "that can decompose the training set matrix X into the matrix multiplication of three\n",
            "matrices  U  Σ  V⊺,  where  V  contains  the  unit  vectors  that  define  all  the  principal\n",
            "components that you are looking for, as shown in Equation 8-1.\n",
            "\n",
            "Equation 8-1. Principal components matrix\n",
            "\n",
            "V =\n",
            "\n",
            "∣\n",
            "\n",
            "∣\n",
            "∣\n",
            "c1 c2 ⋯ cn\n",
            "∣\n",
            "∣\n",
            "\n",
            "∣\n",
            "\n",
            "The following Python code uses NumPy’s svd() function to obtain all the principal\n",
            "components of the 3D training set represented in Figure 8-2, then it extracts the two\n",
            "unit vectors that define the first two PCs:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "X = [...]  # create a small 3D dataset\n",
            "X_centered = X - X.mean(axis=0)\n",
            "U, s, Vt = np.linalg.svd(X_centered)\n",
            "c1 = Vt[0]\n",
            "c2 = Vt[1]\n",
            "\n",
            "PCA  assumes  that  the  dataset  is  centered  around  the  origin.  As\n",
            "you  will  see,  Scikit-Learn’s  PCA  classes  take  care  of  centering  the\n",
            "data for you. If you implement PCA yourself (as in the preceding\n",
            "example),  or  if  you  use  other  libraries,  don’t  forget  to  center  the\n",
            "data first.\n",
            "\n",
            "Projecting Down to d Dimensions\n",
            "Once  you  have  identified  all  the  principal  components,  you  can  reduce  the  dimen‐\n",
            "sionality  of  the  dataset  down  to  d  dimensions  by  projecting  it  onto  the  hyperplane\n",
            "defined  by  the  first  d  principal  components.  Selecting  this  hyperplane  ensures  that\n",
            "\n",
            "PCA \n",
            "\n",
            "| \n",
            "\n",
            "245\n",
            "\n",
            "\fthe projection will preserve as much variance as possible. For example, in Figure 8-2\n",
            "the 3D dataset is projected down to the 2D plane defined by the first two principal\n",
            "components,  preserving  a  large  part  of  the  dataset’s  variance.  As  a  result,  the  2D\n",
            "projection looks very much like the original 3D dataset.\n",
            "\n",
            "To project the training set onto the hyperplane and obtain a reduced dataset Xd-proj of\n",
            "dimensionality d, compute the matrix multiplication of the training set matrix X by\n",
            "the matrix Wd, defined as the matrix containing the first d columns of V, as shown in\n",
            "Equation 8-2.\n",
            "\n",
            "Equation 8-2. Projecting the training set down to d dimensions\n",
            "\n",
            "Xd‐proj = XWd\n",
            "\n",
            "The following Python code projects the training set onto the plane defined by the first\n",
            "two principal components:\n",
            "\n",
            "W2 = Vt[:2].T\n",
            "X2D = X_centered @ W2\n",
            "\n",
            "There you have it! You now know how to reduce the dimensionality of any dataset by\n",
            "projecting it down to any number of dimensions, while preserving as much variance\n",
            "as possible.\n",
            "\n",
            "Using Scikit-Learn\n",
            "Scikit-Learn’s  PCA  class  uses  SVD  to  implement  PCA,  just  like  we  did  earlier  in  this\n",
            "chapter. The following code applies PCA to reduce the dimensionality of the dataset\n",
            "down to two dimensions (note that it automatically takes care of centering the data):\n",
            "\n",
            "from sklearn.decomposition import PCA\n",
            "\n",
            "pca = PCA(n_components=2)\n",
            "X2D = pca.fit_transform(X)\n",
            "\n",
            "After  fitting  the  PCA  transformer  to  the  dataset,  its  components_  attribute  holds  the\n",
            "transpose of Wd: it contains one row for each of the first d principal components.\n",
            "\n",
            "Explained Variance Ratio\n",
            "Another  useful  piece  of  information  is  the  explained  variance  ratio  of  each  princi‐\n",
            "pal  component,  available  via  the  explained_variance_ratio_  variable.  The  ratio\n",
            "indicates the proportion of the dataset’s variance that lies along each principal com‐\n",
            "ponent.  For  example,  let’s  look  at  the  explained  variance  ratios  of  the  first  two\n",
            "components of the 3D dataset represented in Figure 8-2:\n",
            "\n",
            ">>> pca.explained_variance_ratio_\n",
            "array([0.7578477 , 0.15186921])\n",
            "\n",
            "246 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fThis output tells us that about 76% of the dataset’s variance lies along the first PC, and\n",
            "about  15%  lies  along  the  second  PC.  This  leaves  about  9%  for  the  third  PC,  so  it  is\n",
            "reasonable to assume that the third PC probably carries little information.\n",
            "\n",
            "Choosing the Right Number of Dimensions\n",
            "Instead  of  arbitrarily  choosing  the  number  of  dimensions  to  reduce  down  to,  it\n",
            "is  simpler  to  choose  the  number  of  dimensions  that  add  up  to  a  sufficiently  large\n",
            "portion of the variance—say, 95% (An exception to this rule, of course, is if you are\n",
            "reducing dimensionality for data visualization, in which case you will want to reduce\n",
            "the dimensionality down to 2 or 3).\n",
            "\n",
            "The  following  code  loads  and  splits  the  MNIST  dataset  (introduced  in  Chapter  3)\n",
            "and  performs  PCA  without  reducing  dimensionality,  then  computes  the  minimum\n",
            "number of dimensions required to preserve 95% of the training set’s variance:\n",
            "\n",
            "from sklearn.datasets import fetch_openml\n",
            "\n",
            "mnist = fetch_openml('mnist_784', as_frame=False)\n",
            "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
            "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
            "\n",
            "pca = PCA()\n",
            "pca.fit(X_train)\n",
            "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
            "d = np.argmax(cumsum >= 0.95) + 1  # d equals 154\n",
            "\n",
            "You could then set n_components=d and run PCA again, but there’s a better option.\n",
            "Instead  of  specifying  the  number  of  principal  components  you  want  to  preserve,\n",
            "you  can  set  n_components  to  be  a  float  between  0.0  and  1.0,  indicating  the  ratio  of\n",
            "variance you wish to preserve:\n",
            "\n",
            "pca = PCA(n_components=0.95)\n",
            "X_reduced = pca.fit_transform(X_train)\n",
            "\n",
            "The actual number of components is determined during training, and it is stored in\n",
            "the n_components_ attribute:\n",
            "\n",
            ">>> pca.n_components_\n",
            "154\n",
            "\n",
            "Yet  another  option  is  to  plot  the  explained  variance  as  a  function  of  the  number  of\n",
            "dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\n",
            "curve, where the explained variance stops growing fast. In this case, you can see that\n",
            "reducing the dimensionality down to about 100 dimensions wouldn’t lose too much\n",
            "explained variance.\n",
            "\n",
            "PCA \n",
            "\n",
            "| \n",
            "\n",
            "247\n",
            "\n",
            "\fFigure 8-8. Explained variance as a function of the number of dimensions\n",
            "\n",
            "Lastly, if you are using dimensionality reduction as a preprocessing step for a super‐\n",
            "vised learning task (e.g., classification), then you can tune the number of dimensions\n",
            "as you would any other hyperparameter (see Chapter 2). For example, the following\n",
            "code  example  creates  a  two-step  pipeline,  first  reducing  dimensionality  using  PCA,\n",
            "then  classifying  using  a  random  forest.  Next,  it  uses  RandomizedSearchCV  to  find\n",
            "a good combination of hyperparameters for both PCA and the random forest classi‐\n",
            "fier.  This  example  does  a  quick  search,  tuning  only  2  hyperparameters,  training  on\n",
            "just  1,000  instances,  and  running  for  just  10  iterations,  but  feel  free  to  do  a  more\n",
            "thorough search if you have the time:\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import RandomizedSearchCV\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "clf = make_pipeline(PCA(random_state=42),\n",
            "                    RandomForestClassifier(random_state=42))\n",
            "param_distrib = {\n",
            "    \"pca__n_components\": np.arange(10, 80),\n",
            "    \"randomforestclassifier__n_estimators\": np.arange(50, 500)\n",
            "}\n",
            "rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\n",
            "                                random_state=42)\n",
            "rnd_search.fit(X_train[:1000], y_train[:1000])\n",
            "\n",
            "Let’s look at the best hyperparameters found:\n",
            "\n",
            ">>> print(rnd_search.best_params_)\n",
            "{'randomforestclassifier__n_estimators': 465, 'pca__n_components': 23}\n",
            "\n",
            "It’s interesting to note how low the optimal number of components is: we reduced a\n",
            "784-dimensional dataset to just 23 dimensions! This is tied to the fact that we used a\n",
            "random forest, which is a pretty powerful model. If we used a linear model instead,\n",
            "\n",
            "248 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fsuch  as  an  SGDClassifier,  the  search  would  find  that  we  need  to  preserve  more\n",
            "dimensions (about 70).\n",
            "\n",
            "PCA for Compression\n",
            "After dimensionality reduction, the training set takes up much less space. For exam‐\n",
            "ple, after applying PCA to the MNIST dataset while preserving 95% of its variance,\n",
            "we  are  left  with  154  features,  instead  of  the  original  784  features.  So  the  dataset  is\n",
            "now less than 20% of its original size, and we only lost 5% of its variance! This is a\n",
            "reasonable  compression  ratio,  and  it’s  easy  to  see  how  such  a  size  reduction  would\n",
            "speed up a classification algorithm tremendously.\n",
            "\n",
            "It  is  also  possible  to  decompress  the  reduced  dataset  back  to  784  dimensions  by\n",
            "applying  the  inverse  transformation  of  the  PCA  projection.  This  won’t  give  you\n",
            "back  the  original  data,  since  the  projection  lost  a  bit  of  information  (within  the  5%\n",
            "variance that was dropped), but it will likely be close to the original data. The mean\n",
            "squared  distance  between  the  original  data  and  the  reconstructed  data  (compressed\n",
            "and then decompressed) is called the reconstruction error.\n",
            "\n",
            "The  inverse_transform()  method  lets  us  decompress  the  reduced  MNIST  dataset\n",
            "back to 784 dimensions:\n",
            "\n",
            "X_recovered = pca.inverse_transform(X_reduced)\n",
            "\n",
            "Figure  8-9  shows  a  few  digits  from  the  original  training  set  (on  the  left),  and  the\n",
            "corresponding digits after compression and decompression. You can see that there is\n",
            "a slight image quality loss, but the digits are still mostly intact.\n",
            "\n",
            "Figure 8-9. MNIST compression that preserves 95% of the variance\n",
            "\n",
            "PCA \n",
            "\n",
            "| \n",
            "\n",
            "249\n",
            "\n",
            "\fThe equation for the inverse transformation is shown in Equation 8-3.\n",
            "\n",
            "Equation 8-3. PCA inverse transformation, back to the original number of\n",
            "dimensions\n",
            "\n",
            "Xrecovered = Xd‐projWd\n",
            "\n",
            "⊺\n",
            "\n",
            "Randomized PCA\n",
            "If  you  set  the  svd_solver  hyperparameter  to  \"randomized\",  Scikit-Learn  uses  a\n",
            "stochastic algorithm called randomized PCA that quickly finds an approximation of\n",
            "the first d principal components. Its computational complexity is O(m × d2) + O(d3),\n",
            "instead  of  O(m  ×  n2)  +  O(n3)  for  the  full  SVD  approach,  so  it  is  dramatically  faster\n",
            "than full SVD when d is much smaller than n:\n",
            "\n",
            "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
            "X_reduced = rnd_pca.fit_transform(X_train)\n",
            "\n",
            "By default, svd_solver is actually set to \"auto\": Scikit-Learn auto‐\n",
            "matically  uses  the  randomized  PCA  algorithm  if  max(m,  n)  >\n",
            "500  and  n_components  is  an  integer  smaller  than  80%  of  min(m,\n",
            "n),  or  else  it  uses  the  full  SVD  approach.  So  the  preceding  code\n",
            "would  use  the  randomized  PCA  algorithm  even  if  you  removed\n",
            "the  svd_solver=\"randomized\"  argument,  since  154  <  0.8  ×  784.\n",
            "If  you  want  to  force  Scikit-Learn  to  use  full  SVD  for  a  slightly\n",
            "more precise result, you can set the svd_solver hyperparameter to\n",
            "\"full\".\n",
            "\n",
            "Incremental PCA\n",
            "One  problem  with  the  preceding  implementations  of  PCA  is  that  they  require  the\n",
            "whole  training  set  to  fit  in  memory  in  order  for  the  algorithm  to  run.  Fortunately,\n",
            "incremental PCA (IPCA) algorithms have been developed that allow you to split the\n",
            "training  set  into  mini-batches  and  feed  these  in  one  mini-batch  at  a  time.  This  is\n",
            "useful  for  large  training  sets  and  for  applying  PCA  online  (i.e.,  on  the  fly,  as  new\n",
            "instances arrive).\n",
            "\n",
            "The following code splits the MNIST training set into 100 mini-batches (using Num‐\n",
            "Py’s array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5\n",
            "to reduce the dimensionality of the MNIST dataset down to 154 dimensions, just like\n",
            "\n",
            "5 Scikit-Learn uses the algorithm described in David A. Ross et al., “Incremental Learning for Robust Visual\n",
            "\n",
            "Tracking”, International Journal of Computer Vision 77, no. 1–3 (2008): 125–141.\n",
            "\n",
            "250 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fbefore.  Note  that  you  must  call  the  partial_fit()  method  with  each  mini-batch,\n",
            "rather than the fit() method with the whole training set:\n",
            "\n",
            "from sklearn.decomposition import IncrementalPCA\n",
            "\n",
            "n_batches = 100\n",
            "inc_pca = IncrementalPCA(n_components=154)\n",
            "for X_batch in np.array_split(X_train, n_batches):\n",
            "    inc_pca.partial_fit(X_batch)\n",
            "\n",
            "X_reduced = inc_pca.transform(X_train)\n",
            "\n",
            "Alternatively,  you  can  use  NumPy’s  memmap  class,  which  allows  you  to  manipulate  a\n",
            "large array stored in a binary file on disk as if it were entirely in memory; the class\n",
            "loads  only  the  data  it  needs  in  memory,  when  it  needs  it.  To  demonstrate  this,  let’s\n",
            "first create a memory-mapped (memmap) file and copy the MNIST training set to it,\n",
            "then call flush() to ensure that any data still in the cache gets saved to disk. In real\n",
            "life, X_train would typically not fit in memory, so you would load it chunk by chunk\n",
            "and save each chunk to the right part of the memmap array:\n",
            "\n",
            "filename = \"my_mnist.mmap\"\n",
            "X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\n",
            "X_mmap[:] = X_train  # could be a loop instead, saving the data chunk by chunk\n",
            "X_mmap.flush()\n",
            "\n",
            "Next, we can load the memmap file and use it like a regular NumPy array. Let’s use\n",
            "the IncrementalPCA class to reduce its dimensionality. Since this algorithm uses only\n",
            "a  small  part  of  the  array  at  any  given  time,  memory  usage  remains  under  control.\n",
            "This  makes  it  possible  to  call  the  usual  fit()  method  instead  of  partial_fit(),\n",
            "which is quite convenient:\n",
            "\n",
            "X_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\n",
            "batch_size = X_mmap.shape[0] // n_batches\n",
            "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
            "inc_pca.fit(X_mmap)\n",
            "\n",
            "Only the raw binary data is saved to disk, so you need to specify the\n",
            "data type and shape of the array when you load it. If you omit the\n",
            "shape, np.memmap() returns a 1D array.\n",
            "\n",
            "For very high-dimensional datasets, PCA can be too slow. As you saw earlier, even if\n",
            "you use randomized PCA its computational complexity is still O(m × d2) + O(d3), so\n",
            "the target number of dimensions d must not be too large. If you are dealing with a\n",
            "dataset with tens of thousands of features or more (e.g., images), then training may\n",
            "become  much  too  slow:  in  this  case,  you  should  consider  using  random  projection\n",
            "instead.\n",
            "\n",
            "PCA \n",
            "\n",
            "| \n",
            "\n",
            "251\n",
            "\n",
            "\fRandom Projection\n",
            "As its name suggests, the random projection algorithm projects the data to a lower-\n",
            "dimensional  space  using  a  random  linear  projection.  This  may  sound  crazy,  but  it\n",
            "turns out that such a random projection is actually very likely to preserve distances\n",
            "fairly  well,  as  was  demonstrated  mathematically  by  William  B.  Johnson  and  Joram\n",
            "Lindenstrauss in a famous lemma. So, two similar instances will remain similar after\n",
            "the projection, and two very different instances will remain very different.\n",
            "\n",
            "Obviously,  the  more  dimensions  you  drop,  the  more  information  is  lost,  and  the\n",
            "more distances get distorted. So how can you choose the optimal number of dimen‐\n",
            "sions?  Well,  Johnson  and  Lindenstrauss  came  up  with  an  equation  that  determines\n",
            "the minimum number of dimensions to preserve in order to ensure—with high prob‐\n",
            "ability—that  distances  won’t  change  by  more  than  a  given  tolerance.  For  example,\n",
            "if  you  have  a  dataset  containing  m  =  5,000  instances  with  n  =  20,000  features  each,\n",
            "and  you  don’t  want  the  squared  distance  between  any  two  instances  to  change  by\n",
            "more  than  ε  =  10%,6  then  you  should  project  the  data  down  to  d  dimensions,  with\n",
            "d  ≥  4  log(m)  /  (½  ε²  -  ⅓  ε³),  which  is  7,300  dimensions.  That’s  quite  a  significant\n",
            "dimensionality  reduction!  Notice  that  the  equation  does  not  use  n,  it  only  relies  on\n",
            "m  and  ε.  This  equation  is  implemented  by  the  johnson_lindenstrauss_min_dim()\n",
            "function:\n",
            "\n",
            ">>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
            ">>> m, ε = 5_000, 0.1\n",
            ">>> d = johnson_lindenstrauss_min_dim(m, eps=ε)\n",
            ">>> d\n",
            "7300\n",
            "\n",
            "Now  we  can  just  generate  a  random  matrix  P  of  shape  [d,  n],  where  each  item  is\n",
            "sampled randomly from a Gaussian distribution with mean 0 and variance 1 / d, and\n",
            "use it to project a dataset from n dimensions down to d:\n",
            "\n",
            "n = 20_000\n",
            "np.random.seed(42)\n",
            "P = np.random.randn(d, n) / np.sqrt(d)  # std dev = square root of variance\n",
            "\n",
            "X = np.random.randn(m, n)  # generate a fake dataset\n",
            "X_reduced = X @ P.T\n",
            "\n",
            "That’s all there is to it! It’s simple and efficient, and no training is required: the only\n",
            "thing the algorithm needs to create the random matrix is the dataset’s shape. The data\n",
            "itself is not used at all.\n",
            "\n",
            "Scikit-Learn  offers  a  GaussianRandomProjection  class  to  do  exactly  what  we  just\n",
            "did: when you call its fit() method, it uses johnson_lindenstrauss_min_dim() to\n",
            "\n",
            "6 ε is the Greek letter epsilon, often used for tiny values.\n",
            "\n",
            "252 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fdetermine  the  output  dimensionality,  then  it  generates  a  random  matrix,  which  it\n",
            "stores  in  the  components_  attribute.  Then  when  you  call  transform(),  it  uses  this\n",
            "matrix  to  perform  the  projection.  When  creating  the  transformer,  you  can  set  eps\n",
            "if  you  want  to  tweak  ε  (it  defaults  to  0.1),  and  n_components  if  you  want  to  force  a\n",
            "specific  target  dimensionality  d.  The  following  code  example  gives  the  same  result\n",
            "as the preceding code (you can also verify that  gaussian_rnd_proj.components_ is\n",
            "equal to P):\n",
            "\n",
            "from sklearn.random_projection import GaussianRandomProjection\n",
            "\n",
            "gaussian_rnd_proj = GaussianRandomProjection(eps=ε, random_state=42)\n",
            "X_reduced = gaussian_rnd_proj.fit_transform(X)  # same result as above\n",
            "\n",
            "Scikit-Learn  also  provides  a  second  random  projection  transformer,  known  as\n",
            "SparseRandomProjection.  It  determines  the  target  dimensionality  in  the  same  way,\n",
            "generates  a  random  matrix  of  the  same  shape,  and  performs  the  projection  identi‐\n",
            "cally.  The  main  difference  is  that  the  random  matrix  is  sparse.  This  means  it  uses\n",
            "much  less  memory:  about  25  MB  instead  of  almost  1.2  GB  in  the  preceding  exam‐\n",
            "ple!  And  it’s  also  much  faster,  both  to  generate  the  random  matrix  and  to  reduce\n",
            "dimensionality:  about  50%  faster  in  this  case.  Moreover,  if  the  input  is  sparse,  the\n",
            "transformation keeps it sparse (unless you set dense_output=True). Lastly, it enjoys\n",
            "the  same  distance-preserving  property  as  the  previous  approach,  and  the  quality  of\n",
            "the dimensionality reduction is comparable. In short, it’s usually preferable to use this\n",
            "transformer instead of the first one, especially for large or sparse datasets.\n",
            "\n",
            "The  ratio  r  of  nonzero  items  in  the  sparse  random  matrix  is  called  its  density.  By\n",
            "default,  it  is  equal  to  1/ n.  With  20,000  features,  this  means  that  only  1  in  ~141\n",
            "cells  in  the  random  matrix  is  nonzero:  that’s  quite  sparse!  You  can  set  the  density\n",
            "hyperparameter to another value if you prefer. Each cell in the sparse random matrix\n",
            "has a probability r of being nonzero, and each nonzero value is either –v or +v (both\n",
            "equally likely), where v = 1/ dr.\n",
            "\n",
            "If you want to perform the inverse transform, you first need to compute the pseudo-\n",
            "inverse  of  the  components  matrix  using  SciPy’s  pinv()  function,  then  multiply  the\n",
            "reduced data by the transpose of the pseudo-inverse:\n",
            "\n",
            "components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
            "X_recovered = X_reduced @ components_pinv.T\n",
            "\n",
            "Computing  the  pseudo-inverse  may  take  a  very  long  time  if  the\n",
            "components  matrix  is  large,  as  the  computational  complexity  of\n",
            "pinv() is O(dn²) if d < n, or O(nd²) otherwise.\n",
            "\n",
            "Random Projection \n",
            "\n",
            "| \n",
            "\n",
            "253\n",
            "\n",
            "\fIn summary, random projection is a simple, fast, memory-efficient, and surprisingly\n",
            "powerful  dimensionality  reduction  algorithm  that  you  should  keep  in  mind,  espe‐\n",
            "cially when you deal with high-dimensional datasets.\n",
            "\n",
            "Random projection is not always used to reduce the dimensionality\n",
            "of  large  datasets.  For  example,  a  2017  paper7  by  Sanjoy  Dasgupta\n",
            "et al. showed that the brain of a fruit fly implements an analog of\n",
            "random projection to map dense low-dimensional olfactory inputs\n",
            "to  sparse  high-dimensional  binary  outputs:  for  each  odor,  only\n",
            "a  small  fraction  of  the  output  neurons  get  activated,  but  similar\n",
            "odors  activate  many  of  the  same  neurons.  This  is  similar  to  a\n",
            "well-known algorithm called locality sensitive hashing (LSH), which\n",
            "is typically used in search engines to group similar documents.\n",
            "\n",
            "LLE\n",
            "Locally linear embedding (LLE)8 is a nonlinear dimensionality reduction (NLDR) tech‐\n",
            "nique.  It  is  a  manifold  learning  technique  that  does  not  rely  on  projections,  unlike\n",
            "PCA and random projection. In a nutshell, LLE works by first measuring how each\n",
            "training  instance  linearly  relates  to  its  nearest  neighbors,  and  then  looking  for  a\n",
            "low-dimensional representation of the training set where these local relationships are\n",
            "best  preserved  (more  details  shortly).  This  approach  makes  it  particularly  good  at\n",
            "unrolling twisted manifolds, especially when there is not too much noise.\n",
            "\n",
            "The following code makes a Swiss roll, then uses Scikit-Learn’s LocallyLinearEmbed\n",
            "ding class to unroll it:\n",
            "\n",
            "from sklearn.datasets import make_swiss_roll\n",
            "from sklearn.manifold import LocallyLinearEmbedding\n",
            "\n",
            "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
            "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
            "X_unrolled = lle.fit_transform(X_swiss)\n",
            "\n",
            "The  variable  t  is  a  1D  NumPy  array  containing  the  position  of  each  instance  along\n",
            "the rolled axis of the Swiss roll. We don’t use it in this example, but it can be used as a\n",
            "target for a nonlinear regression task.\n",
            "\n",
            "The  resulting  2D  dataset  is  shown  in  Figure  8-10.  As  you  can  see,  the  Swiss  roll  is\n",
            "completely unrolled, and the distances between instances are locally well preserved.\n",
            "\n",
            "7 Sanjoy Dasgupta et al., “A neural algorithm for a fundamental computing problem”, Science 358, no. 6364\n",
            "\n",
            "(2017): 793–796.\n",
            "\n",
            "8 Sam T. Roweis and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear Embedding”,\n",
            "\n",
            "Science 290, no. 5500 (2000): 2323–2326.\n",
            "\n",
            "254 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fHowever, distances are not preserved on a larger scale: the unrolled Swiss roll should\n",
            "be a rectangle, not this kind of stretched and twisted band. Nevertheless, LLE did a\n",
            "pretty good job of modeling the manifold.\n",
            "\n",
            "Figure 8-10. Unrolled Swiss roll using LLE\n",
            "\n",
            "Here’s  how  LLE  works:  for  each  training  instance  x(i),  the  algorithm  identifies  its\n",
            "k-nearest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a\n",
            "linear  function  of  these  neighbors.  More  specifically,  it  tries  to  find  the  weights  wi,j\n",
            "m wi, jx j  is as small as possible,\n",
            "such that the squared distance between x(i) and ∑j = 1\n",
            "assuming wi,j = 0 if x(j) is not one of the k-nearest neighbors of x(i). Thus the first step\n",
            "of  LLE  is  the  constrained  optimization  problem  described  in  Equation  8-4,  where\n",
            "W is the weight matrix containing all the weights wi,j. The second constraint simply\n",
            "normalizes the weights for each training instance x(i).\n",
            "\n",
            "Equation 8-4. LLE step 1: linearly modeling local relationships\n",
            "\n",
            "2\n",
            "\n",
            "wi, jx j\n",
            "\n",
            "m\n",
            "x i − ∑\n",
            "j = 1\n",
            "if x j\n",
            "\n",
            "W = argmin\n",
            "\n",
            "W\n",
            "\n",
            "subject to\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "wi, j = 0\n",
            "m\n",
            "∑\n",
            "j = 1\n",
            "\n",
            "wi, j = 1 for i = 1, 2, ⋯, m\n",
            "\n",
            "is not one of the k n.n. of x i\n",
            "\n",
            "After this step, the weight matrix W (containing the weights wi, j) encodes the local\n",
            "linear  relationships  between  the  training  instances.  The  second  step  is  to  map  the\n",
            "training  instances  into  a  d-dimensional  space  (where  d  <  n)  while  preserving  these\n",
            "\n",
            "LLE \n",
            "\n",
            "| \n",
            "\n",
            "255\n",
            "\n",
            "\flocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\n",
            "m wi, jz j  to be as small\n",
            "space, then we want the squared distance between z(i) and ∑j = 1\n",
            "as  possible.  This  idea  leads  to  the  unconstrained  optimization  problem  described\n",
            "in  Equation  8-5.  It  looks  very  similar  to  the  first  step,  but  instead  of  keeping  the\n",
            "instances  fixed  and  finding  the  optimal  weights,  we  are  doing  the  reverse:  keeping\n",
            "the  weights  fixed  and  finding  the  optimal  position  of  the  instances’  images  in  the\n",
            "low-dimensional space. Note that Z is the matrix containing all z(i).\n",
            "\n",
            "Equation 8-5. LLE step 2: reducing dimensionality while preserving relationships\n",
            "\n",
            "Z = argmin\n",
            "\n",
            "Z\n",
            "\n",
            "m\n",
            "∑\n",
            "i = 1\n",
            "\n",
            "m\n",
            "z i − ∑\n",
            "j = 1\n",
            "\n",
            "wi, jz j\n",
            "\n",
            "2\n",
            "\n",
            "Scikit-Learn’s  LLE  implementation  has  the  following  computational  complexity:\n",
            "O(m log(m)n log(k)) for finding the k-nearest neighbors, O(mnk3) for optimizing the\n",
            "weights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\n",
            "nately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\n",
            "\n",
            "As  you  can  see,  LLE  is  quite  different  from  the  projection  techniques,  and  it’s  sig‐\n",
            "nificantly  more  complex,  but  it  can  also  construct  much  better  low-dimensional\n",
            "representations, especially if the data is nonlinear.\n",
            "\n",
            "Other Dimensionality Reduction Techniques\n",
            "Before  we  conclude  this  chapter,  let’s  take  a  quick  look  at  a  few  other  popular\n",
            "dimensionality reduction techniques available in Scikit-Learn:\n",
            "\n",
            "sklearn.manifold.MDS\n",
            "\n",
            "Multidimensional scaling (MDS) reduces dimensionality while trying to preserve\n",
            "the  distances  between  the  instances.  Random  projection  does  that  for  high-\n",
            "dimensional data, but it doesn’t work well on low-dimensional data.\n",
            "\n",
            "sklearn.manifold.Isomap\n",
            "\n",
            "Isomap creates a graph by connecting each instance to its nearest neighbors, then\n",
            "reduces  dimensionality  while  trying  to  preserve  the  geodesic  distances  between\n",
            "the instances. The geodesic distance between two nodes in a graph is the number\n",
            "of nodes on the shortest path between these nodes.\n",
            "\n",
            "sklearn.manifold.TSNE\n",
            "\n",
            "t-distributed stochastic neighbor embedding (t-SNE) reduces dimensionality while\n",
            "trying to keep similar instances close and dissimilar instances apart. It is mostly\n",
            "used  for  visualization,  in  particular  to  visualize  clusters  of  instances  in  high-\n",
            "dimensional  space.  For  example,  in  the  exercises  at  the  end  of  this  chapter  you\n",
            "will use t-SNE to visualize a 2D map of the MNIST images.\n",
            "\n",
            "256 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fsklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
            "\n",
            "Linear discriminant analysis (LDA) is a linear classification algorithm that, during\n",
            "training, learns the most discriminative axes between the classes. These axes can\n",
            "then be used to define a hyperplane onto which to project the data. The benefit\n",
            "of  this  approach  is  that  the  projection  will  keep  classes  as  far  apart  as  possible,\n",
            "so  LDA  is  a  good  technique  to  reduce  dimensionality  before  running  another\n",
            "classification algorithm (unless LDA alone is sufficient).\n",
            "\n",
            "Figure  8-11  shows  the  results  of  MDS,  Isomap,  and  t-SNE  on  the  Swiss  roll.  MDS\n",
            "manages  to  flatten  the  Swiss  roll  without  losing  its  global  curvature,  while  Isomap\n",
            "drops  it  entirely.  Depending  on  the  downstream  task,  preserving  the  large-scale\n",
            "structure  may  be  good  or  bad.  t-SNE  does  a  reasonable  job  of  flattening  the  Swiss\n",
            "roll, preserving a bit of curvature, and it also amplifies clusters, tearing the roll apart.\n",
            "Again, this might be good or bad, depending on the downstream task.\n",
            "\n",
            "Figure 8-11. Using various techniques to reduce the Swiss roll to 2D\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. What are the main motivations for reducing a dataset’s dimensionality? What are\n",
            "\n",
            "the main drawbacks?\n",
            "\n",
            "2.\n",
            "2. What is the curse of dimensionality?\n",
            "\n",
            "3. Once  a  dataset’s  dimensionality  has  been  reduced,  is  it  possible  to  reverse  the\n",
            "3.\n",
            "\n",
            "operation? If so, how? If not, why?\n",
            "\n",
            "4.\n",
            "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
            "\n",
            "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\n",
            "5.\n",
            "variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
            "\n",
            "6.\n",
            "6. In what cases would you use regular PCA, incremental PCA, randomized PCA,\n",
            "\n",
            "or random projection?\n",
            "\n",
            "7.\n",
            "7. How can you evaluate the performance of a dimensionality reduction algorithm\n",
            "\n",
            "on your dataset?\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "257\n",
            "\n",
            "\f8.\n",
            "8. Does  it  make  any  sense  to  chain  two  different  dimensionality  reduction\n",
            "\n",
            "algorithms?\n",
            "\n",
            "9. Load  the  MNIST  dataset  (introduced  in  Chapter  3)  and  split  it  into  a  training\n",
            "9.\n",
            "set and a test set (take the first 60,000 instances for training, and the remaining\n",
            "10,000 for testing). Train a random forest classifier on the dataset and time how\n",
            "long  it  takes,  then  evaluate  the  resulting  model  on  the  test  set.  Next,  use  PCA\n",
            "to  reduce  the  dataset’s  dimensionality,  with  an  explained  variance  ratio  of  95%.\n",
            "Train a new random forest classifier on the reduced dataset and see how long it\n",
            "takes. Was training much faster? Next, evaluate the classifier on the test set. How\n",
            "does  it  compare  to  the  previous  classifier?  Try  again  with  an  SGDClassifier.\n",
            "How much does PCA help now?\n",
            "\n",
            "10. Use  t-SNE  to  reduce  the  first  5,000  images  of  the  MNIST  dataset  down  to  2\n",
            "10.\n",
            "dimensions and plot the result using Matplotlib. You can use a scatterplot using\n",
            "10  different  colors  to  represent  each  image’s  target  class.  Alternatively,  you  can\n",
            "replace each dot in the scatterplot with the corresponding instance’s class (a digit\n",
            "from  0  to  9),  or  even  plot  scaled-down  versions  of  the  digit  images  themselves\n",
            "(if you plot all digits the visualization will be too cluttered, so you should either\n",
            "draw a random sample or plot an instance only if no other instance has already\n",
            "been  plotted  at  a  close  distance).  You  should  get  a  nice  visualization  with  well-\n",
            "separated clusters of digits. Try using other dimensionality reduction algorithms,\n",
            "such as PCA, LLE, or MDS, and compare the resulting visualizations.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "258 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 8: Dimensionality Reduction\n",
            "\n",
            "\fCHAPTER 9\n",
            "Unsupervised Learning Techniques\n",
            "\n",
            "Although most of the applications of machine learning today are based on supervised\n",
            "learning  (and  as  a  result,  this  is  where  most  of  the  investments  go  to),  the  vast\n",
            "majority  of  the  available  data  is  unlabeled:  we  have  the  input  features  X,  but  we\n",
            "do  not  have  the  labels  y.  The  computer  scientist  Yann  LeCun  famously  said  that  “if\n",
            "intelligence was a cake, unsupervised learning would be the cake, supervised learning\n",
            "would be the icing on the cake, and reinforcement learning would be the cherry on\n",
            "the cake.” In other words, there is a huge potential in unsupervised learning that we\n",
            "have only barely started to sink our teeth into.\n",
            "\n",
            "Say  you  want  to  create  a  system  that  will  take  a  few  pictures  of  each  item  on  a\n",
            "manufacturing  production  line  and  detect  which  items  are  defective.  You  can  fairly\n",
            "easily  create  a  system  that  will  take  pictures  automatically,  and  this  might  give  you\n",
            "thousands of pictures every day. You can then build a reasonably large dataset in just\n",
            "a few weeks. But wait, there are no labels! If you want to train a regular binary classi‐\n",
            "fier that will predict whether an item is defective or not, you will need to label every\n",
            "single picture as “defective” or “normal”. This will generally require human experts to\n",
            "sit down and manually go through all the pictures. This is a long, costly, and tedious\n",
            "task, so it will usually only be done on a small subset of the available pictures. As a\n",
            "result, the labeled dataset will be quite small, and the classifier’s performance will be\n",
            "disappointing. Moreover, every time the company makes any change to its products,\n",
            "the  whole  process  will  need  to  be  started  over  from  scratch.  Wouldn’t  it  be  great  if\n",
            "the algorithm could just exploit the unlabeled data without needing humans to label\n",
            "every picture? Enter unsupervised learning.\n",
            "\n",
            "259\n",
            "\n",
            "\fIn Chapter 8 we looked at the most common unsupervised learning task: dimension‐\n",
            "ality reduction. In this chapter we will look at a few more unsupervised tasks:\n",
            "\n",
            "Clustering\n",
            "\n",
            "The goal is to group similar instances together into clusters. Clustering is a great\n",
            "tool  for  data  analysis,  customer  segmentation,  recommender  systems,  search\n",
            "engines,  image  segmentation,  semi-supervised  learning,  dimensionality  reduc‐\n",
            "tion, and more.\n",
            "\n",
            "Anomaly detection (also called outlier detection)\n",
            "\n",
            "The  objective  is  to  learn  what  “normal”  data  looks  like,  and  then  use  that  to\n",
            "detect abnormal instances. These instances are called anomalies, or outliers, while\n",
            "the  normal  instances  are  called  inliers.  Anomaly  detection  is  useful  in  a  wide\n",
            "variety  of  applications,  such  as  fraud  detection,  detecting  defective  products  in\n",
            "manufacturing, identifying new trends in time series, or removing outliers from\n",
            "a  dataset  before  training  another  model,  which  can  significantly  improve  the\n",
            "performance of the resulting model.\n",
            "\n",
            "Density estimation\n",
            "\n",
            "This is the task of estimating the probability density function (PDF) of the random\n",
            "process  that  generated  the  dataset.  Density  estimation  is  commonly  used  for\n",
            "anomaly detection: instances located in very low-density regions are likely to be\n",
            "anomalies. It is also useful for data analysis and visualization.\n",
            "\n",
            "Ready  for  some  cake?  We  will  start  with  two  clustering  algorithms,  k-means  and\n",
            "DBSCAN, then we’ll discuss Gaussian mixture models and see how they can be used\n",
            "for density estimation, clustering, and anomaly detection.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN\n",
            "As  you  enjoy  a  hike  in  the  mountains,  you  stumble  upon  a  plant  you  have  never\n",
            "seen before. You look around and you notice a few more. They are not identical, yet\n",
            "they are sufficiently similar for you to know that they most likely belong to the same\n",
            "species (or at least the same genus). You may need a botanist to tell you what species\n",
            "that  is,  but  you  certainly  don’t  need  an  expert  to  identify  groups  of  similar-looking\n",
            "objects.  This  is  called  clustering:  it  is  the  task  of  identifying  similar  instances  and\n",
            "assigning them to clusters, or groups of similar instances.\n",
            "\n",
            "Just  like  in  classification,  each  instance  gets  assigned  to  a  group.  However,  unlike\n",
            "classification,  clustering  is  an  unsupervised  task.  Consider  Figure  9-1:  on  the  left  is\n",
            "the iris dataset (introduced in Chapter 4), where each instance’s species (i.e., its class)\n",
            "is represented with a different marker. It is a labeled dataset, for which classification\n",
            "algorithms  such  as  logistic  regression,  SVMs,  or  random  forest  classifiers  are  well\n",
            "suited. On the right is the same dataset, but without the labels, so you cannot use a\n",
            "classification algorithm anymore. This is where clustering algorithms step in: many of\n",
            "\n",
            "260 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fthem can easily detect the lower-left cluster. It is also quite easy to see with our own\n",
            "eyes, but it is not so obvious that the upper-right cluster is composed of two distinct\n",
            "subclusters. That said, the dataset has two additional features (sepal length and width)\n",
            "that  are  not  represented  here,  and  clustering  algorithms  can  make  good  use  of  all\n",
            "features,  so  in  fact  they  identify  the  three  clusters  fairly  well  (e.g.,  using  a  Gaussian\n",
            "mixture model, only 5 instances out of 150 are assigned to the wrong cluster).\n",
            "\n",
            "Figure 9-1. Classification (left) versus clustering (right)\n",
            "\n",
            "Clustering is used in a wide variety of applications, including:\n",
            "\n",
            "Customer segmentation\n",
            "\n",
            "You  can  cluster  your  customers  based  on  their  purchases  and  their  activity  on\n",
            "your website. This is useful to understand who your customers are and what they\n",
            "need, so you can adapt your products and marketing campaigns to each segment.\n",
            "For  example,  customer  segmentation  can  be  useful  in  recommender  systems  to\n",
            "suggest content that other users in the same cluster enjoyed.\n",
            "\n",
            "Data analysis\n",
            "\n",
            "When you analyze a new dataset, it can be helpful to run a clustering algorithm,\n",
            "and then analyze each cluster separately.\n",
            "\n",
            "Dimensionality reduction\n",
            "\n",
            "Once a dataset has been clustered, it is usually possible to measure each instance’s\n",
            "affinity with each cluster; affinity is any measure of how well an instance fits into\n",
            "a cluster. Each instance’s feature vector x can then be replaced with the vector of\n",
            "its cluster affinities. If there are k clusters, then this vector is k-dimensional. The\n",
            "new vector is typically much lower-dimensional than the original feature vector,\n",
            "but it can preserve enough information for further processing.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "261\n",
            "\n",
            "\fFeature engineering\n",
            "\n",
            "The cluster affinities can often be useful as extra features. For example, we used\n",
            "k-means in Chapter 2 to add geographic cluster affinity features to the California\n",
            "housing dataset, and they helped us get better performance.\n",
            "\n",
            "Anomaly detection (also called outlier detection)\n",
            "\n",
            "Any instance that has a low affinity to all the clusters is likely to be an anomaly.\n",
            "For  example,  if  you  have  clustered  the  users  of  your  website  based  on  their\n",
            "behavior, you can detect users with unusual behavior, such as an unusual number\n",
            "of requests per second.\n",
            "\n",
            "Semi-supervised learning\n",
            "\n",
            "If  you  only  have  a  few  labels,  you  could  perform  clustering  and  propagate  the\n",
            "labels to all the instances in the same cluster. This technique can greatly increase\n",
            "the  number  of  labels  available  for  a  subsequent  supervised  learning  algorithm,\n",
            "and thus improve its performance.\n",
            "\n",
            "Search engines\n",
            "\n",
            "Some  search  engines  let  you  search  for  images  that  are  similar  to  a  reference\n",
            "image.  To  build  such  a  system,  you  would  first  apply  a  clustering  algorithm  to\n",
            "all the images in your database; similar images would end up in the same cluster.\n",
            "Then  when  a  user  provides  a  reference  image,  all  you’d  need  to  do  is  use  the\n",
            "trained clustering model to find this image’s cluster, and you could then simply\n",
            "return all the images from this cluster.\n",
            "\n",
            "Image segmentation\n",
            "\n",
            "By  clustering  pixels  according  to  their  color,  then  replacing  each  pixel’s  color\n",
            "with  the  mean  color  of  its  cluster,  it  is  possible  to  considerably  reduce  the\n",
            "number  of  different  colors  in  an  image.  Image  segmentation  is  used  in  many\n",
            "object detection and tracking systems, as it makes it easier to detect the contour\n",
            "of each object.\n",
            "\n",
            "There is no universal definition of what a cluster is: it really depends on the context,\n",
            "and  different  algorithms  will  capture  different  kinds  of  clusters.  Some  algorithms\n",
            "look for instances centered around a particular point, called a centroid. Others look\n",
            "for  continuous  regions  of  densely  packed  instances:  these  clusters  can  take  on  any\n",
            "shape. Some algorithms are hierarchical, looking for clusters of clusters. And the list\n",
            "goes on.\n",
            "\n",
            "In  this  section,  we  will  look  at  two  popular  clustering  algorithms,  k-means  and\n",
            "DBSCAN, and explore some of their applications, such as nonlinear dimensionality\n",
            "reduction, semi-supervised learning, and anomaly detection.\n",
            "\n",
            "262 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fk-means\n",
            "Consider  the  unlabeled  dataset  represented  in  Figure  9-2:  you  can  clearly  see  five\n",
            "blobs of instances. The k-means algorithm is a simple algorithm capable of clustering\n",
            "this  kind  of  dataset  very  quickly  and  efficiently,  often  in  just  a  few  iterations.  It\n",
            "was  proposed  by  Stuart  Lloyd  at  Bell  Labs  in  1957  as  a  technique  for  pulse-code\n",
            "modulation,  but  it  was  only  published  outside  of  the  company  in  1982.1  In  1965,\n",
            "Edward  W.  Forgy  had  published  virtually  the  same  algorithm,  so  k-means  is  some‐\n",
            "times referred to as the Lloyd–Forgy algorithm.\n",
            "\n",
            "Figure 9-2. An unlabeled dataset composed of five blobs of instances\n",
            "\n",
            "Let’s train a k-means clusterer on this dataset. It will try to find each blob’s center and\n",
            "assign each instance to the closest blob:\n",
            "\n",
            "from sklearn.cluster import KMeans\n",
            "from sklearn.datasets import make_blobs\n",
            "\n",
            "X, y = make_blobs([...])  # make the blobs: y contains the cluster IDs, but we\n",
            "                          # will not use them; that's what we want to predict\n",
            "k = 5\n",
            "kmeans = KMeans(n_clusters=k, random_state=42)\n",
            "y_pred = kmeans.fit_predict(X)\n",
            "\n",
            "Note that you have to specify the number of clusters k that the algorithm must find.\n",
            "In this example, it is pretty obvious from looking at the data that k should be set to 5,\n",
            "but in general it is not that easy. We will discuss this shortly.\n",
            "\n",
            "Each instance will be assigned to one of the five clusters. In the context of clustering,\n",
            "an  instance’s  label  is  the  index  of  the  cluster  to  which  the  algorithm  assigns  this\n",
            "instance;  this  is  not  to  be  confused  with  the  class  labels  in  classification,  which  are\n",
            "used  as  targets  (remember  that  clustering  is  an  unsupervised  learning  task).  The\n",
            "\n",
            "1 Stuart P. Lloyd, “Least Squares Quantization in PCM”, IEEE Transactions on Information Theory 28, no. 2\n",
            "\n",
            "(1982): 129–137.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "263\n",
            "\n",
            "\fKMeans  instance  preserves  the  predicted  labels  of  the  instances  it  was  trained  on,\n",
            "available via the labels_ instance variable:\n",
            "\n",
            ">>> y_pred\n",
            "array([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n",
            ">>> y_pred is kmeans.labels_\n",
            "True\n",
            "\n",
            "We can also take a look at the five centroids that the algorithm found:\n",
            "\n",
            ">>> kmeans.cluster_centers_\n",
            "array([[-2.80389616,  1.80117999],\n",
            "       [ 0.20876306,  2.25551336],\n",
            "       [-2.79290307,  2.79641063],\n",
            "       [-1.46679593,  2.28585348],\n",
            "       [-2.80037642,  1.30082566]])\n",
            "\n",
            "You can easily assign new instances to the cluster whose centroid is closest:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
            ">>> kmeans.predict(X_new)\n",
            "array([1, 1, 2, 2], dtype=int32)\n",
            "\n",
            "If  you  plot  the  cluster’s  decision  boundaries,  you  get  a  Voronoi  tessellation:  see\n",
            "Figure 9-3, where each centroid is represented with an X.\n",
            "\n",
            "Figure 9-3. k-means decision boundaries (Voronoi tessellation)\n",
            "\n",
            "The  vast  majority  of  the  instances  were  clearly  assigned  to  the  appropriate  cluster,\n",
            "but a few instances were probably mislabeled, especially near the boundary between\n",
            "the  top-left  cluster  and  the  central  cluster.  Indeed,  the  k-means  algorithm  does  not\n",
            "behave  very  well  when  the  blobs  have  very  different  diameters  because  all  it  cares\n",
            "about when assigning an instance to a cluster is the distance to the centroid.\n",
            "\n",
            "Instead of assigning each instance to a single cluster, which is called hard clustering, it\n",
            "can be useful to give each instance a score per cluster, which is called soft clustering.\n",
            "The score can be the distance between the instance and the centroid or a similarity\n",
            "\n",
            "264 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fscore (or affinity), such as the Gaussian radial basis function we used in Chapter 2. In\n",
            "the KMeans class, the transform() method measures the distance from each instance\n",
            "to every centroid:\n",
            "\n",
            ">>> kmeans.transform(X_new).round(2)\n",
            "array([[2.81, 0.33, 2.9 , 1.49, 2.89],\n",
            "       [5.81, 2.8 , 5.85, 4.48, 5.84],\n",
            "       [1.21, 3.29, 0.29, 1.69, 1.71],\n",
            "       [0.73, 3.22, 0.36, 1.55, 1.22]])\n",
            "\n",
            "In  this  example,  the  first  instance  in  X_new  is  located  at  a  distance  of  about  2.81\n",
            "from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid,\n",
            "1.49  from  the  fourth  centroid,  and  2.89  from  the  fifth  centroid.  If  you  have  a  high-\n",
            "dimensional dataset and you transform it this way, you end up with a k-dimensional\n",
            "dataset:  this  transformation  can  be  a  very  efficient  nonlinear  dimensionality  reduc‐\n",
            "tion  technique.  Alternatively,  you  can  use  these  distances  as  extra  features  to  train\n",
            "another model, as in Chapter 2.\n",
            "\n",
            "The k-means algorithm\n",
            "\n",
            "So, how does the algorithm work? Well, suppose you were given the centroids. You\n",
            "could  easily  label  all  the  instances  in  the  dataset  by  assigning  each  of  them  to  the\n",
            "cluster whose centroid is closest. Conversely, if you were given all the instance labels,\n",
            "you could easily locate each cluster’s centroid by computing the mean of the instances\n",
            "in  that  cluster.  But  you  are  given  neither  the  labels  nor  the  centroids,  so  how  can\n",
            "you  proceed?  Start  by  placing  the  centroids  randomly  (e.g.,  by  picking  k  instances\n",
            "at  random  from  the  dataset  and  using  their  locations  as  centroids).  Then  label  the\n",
            "instances,  update  the  centroids,  label  the  instances,  update  the  centroids,  and  so  on\n",
            "until the centroids stop moving. The algorithm is guaranteed to converge in a finite\n",
            "number  of  steps  (usually  quite  small).  That’s  because  the  mean  squared  distance\n",
            "between the instances and their closest centroids can only go down at each step, and\n",
            "since it cannot be negative, it’s guaranteed to converge.\n",
            "\n",
            "You  can  see  the  algorithm  in  action  in  Figure  9-4:  the  centroids  are  initialized\n",
            "randomly (top left), then the instances are labeled (top right), then the centroids are\n",
            "updated (center left), the instances are relabeled (center right), and so on. As you can\n",
            "see, in just three iterations the algorithm has reached a clustering that seems close to\n",
            "optimal.\n",
            "\n",
            "The computational complexity of the algorithm is generally linear\n",
            "with regard to the number of instances m, the number of clusters\n",
            "k,  and  the  number  of  dimensions  n.  However,  this  is  only  true\n",
            "when  the  data  has  a  clustering  structure.  If  it  does  not,  then  in\n",
            "the  worst-case  scenario  the  complexity  can  increase  exponentially\n",
            "with the number of instances. In practice, this rarely happens, and\n",
            "k-means is generally one of the fastest clustering algorithms.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "265\n",
            "\n",
            ".\n",
            "\n",
            "\fFigure 9-4. The k-means algorithm\n",
            "\n",
            "Although the algorithm is guaranteed to converge, it may not converge to the right\n",
            "solution  (i.e.,  it  may  converge  to  a  local  optimum):  whether  it  does  or  not  depends\n",
            "on  the  centroid  initialization.  Figure  9-5  shows  two  suboptimal  solutions  that  the\n",
            "algorithm can converge to if you are not lucky with the random initialization step.\n",
            "\n",
            "Figure 9-5. Suboptimal solutions due to unlucky centroid initializations\n",
            "\n",
            "Let’s take a look at a few ways you can mitigate this risk by improving the centroid\n",
            "initialization.\n",
            "\n",
            "266 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fCentroid initialization methods\n",
            "\n",
            "If you happen to know approximately where the centroids should be (e.g., if you ran\n",
            "another clustering algorithm earlier), then you can set the init hyperparameter to a\n",
            "NumPy array containing the list of centroids, and set n_init to 1:\n",
            "\n",
            "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
            "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
            "kmeans.fit(X)\n",
            "\n",
            "Another  solution  is  to  run  the  algorithm  multiple  times  with  different  random\n",
            "initializations  and  keep  the  best  solution.  The  number  of  random  initializations  is\n",
            "controlled  by  the  n_init  hyperparameter:  by  default  it  is  equal  to  10,  which  means\n",
            "that  the  whole  algorithm  described  earlier  runs  10  times  when  you  call  fit(),  and\n",
            "Scikit-Learn keeps the best solution. But how exactly does it know which solution is\n",
            "the best? It uses a performance metric! That metric is called the model’s inertia, which\n",
            "is the sum of the squared distances between the instances and their closest centroids.\n",
            "It is roughly equal to 219.4 for the model on the left in Figure 9-5, 258.6 for the model\n",
            "on  the  right  in  Figure  9-5,  and  only  211.6  for  the  model  in  Figure  9-3.  The  KMeans\n",
            "class  runs  the  algorithm  n_init  times  and  keeps  the  model  with  the  lowest  inertia.\n",
            "In this example, the model in Figure 9-3 will be selected (unless we are very unlucky\n",
            "with n_init consecutive random initializations). If you are curious, a model’s inertia\n",
            "is accessible via the inertia_ instance variable:\n",
            "\n",
            ">>> kmeans.inertia_\n",
            "211.59853725816836\n",
            "\n",
            "The  score()  method  returns  the  negative  inertia  (it’s  negative  because  a  predictor’s\n",
            "score() method must always respect Scikit-Learn’s “greater is better” rule: if a predic‐\n",
            "tor is better than another, its score() method should return a greater score):\n",
            "\n",
            ">>> kmeans.score(X)\n",
            "-211.5985372581684\n",
            "\n",
            "An important improvement to the k-means algorithm, k-means++, was proposed in\n",
            "a  2006  paper  by  David  Arthur  and  Sergei  Vassilvitskii.2  They  introduced  a  smarter\n",
            "initialization  step  that  tends  to  select  centroids  that  are  distant  from  one  another,\n",
            "and this improvement makes the k-means algorithm much less likely to converge to\n",
            "a  suboptimal  solution.  The  paper  showed  that  the  additional  computation  required\n",
            "for  the  smarter  initialization  step  is  well  worth  it  because  it  makes  it  possible  to\n",
            "drastically  reduce  the  number  of  times  the  algorithm  needs  to  be  run  to  find  the\n",
            "optimal solution. The k-means++ initialization algorithm works like this:\n",
            "\n",
            "1. Take one centroid c(1), chosen uniformly at random from the dataset.\n",
            "1.\n",
            "\n",
            "2 David Arthur and Sergei Vassilvitskii, “k-Means++: The Advantages of Careful Seeding”, Proceedings of the\n",
            "\n",
            "18th Annual ACM-SIAM Symposium on Discrete Algorithms (2007): 1027–1035.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "267\n",
            "\n",
            "\f2. Take  a  new  centroid  c(i),  choosing  an  instance  x(i)  with  probability  D x i 2\n",
            "2.\n",
            "\n",
            "  /\n",
            "m D x j 2\n",
            ",  where  D(x(i))  is  the  distance  between  the  instance  x(i)  and  the\n",
            "∑j = 1\n",
            "closest  centroid  that  was  already  chosen.  This  probability  distribution  ensures\n",
            "that instances farther away from already chosen centroids are much more likely\n",
            "to be selected as centroids.\n",
            "\n",
            "3.\n",
            "3. Repeat the previous step until all k centroids have been chosen.\n",
            "\n",
            "The KMeans class uses this initialization method by default.\n",
            "\n",
            "Accelerated k-means and mini-batch k-means\n",
            "\n",
            "Another  improvement  to  the  k-means  algorithm  was  proposed  in  a  2003  paper  by\n",
            "Charles  Elkan.3  On  some  large  datasets  with  many  clusters,  the  algorithm  can  be\n",
            "accelerated by avoiding many unnecessary distance calculations. Elkan achieved this\n",
            "by exploiting the triangle inequality (i.e., that a straight line is always the shortest dis‐\n",
            "tance between two points4) and by keeping track of lower and upper bounds for dis‐\n",
            "tances between instances and centroids. However, Elkan’s algorithm does not always\n",
            "accelerate  training,  and  sometimes  it  can  even  slow  down  training  significantly;  it\n",
            "depends on the dataset. Still, if you want to give it a try, set algorithm=\"elkan\".\n",
            "\n",
            "Yet  another  important  variant  of  the  k-means  algorithm  was  proposed  in  a  2010\n",
            "paper  by  David  Sculley.5  Instead  of  using  the  full  dataset  at  each  iteration,  the  algo‐\n",
            "rithm  is  capable  of  using  mini-batches,  moving  the  centroids  just  slightly  at  each\n",
            "iteration.  This  speeds  up  the  algorithm  (typically  by  a  factor  of  three  to  four)  and\n",
            "makes  it  possible  to  cluster  huge  datasets  that  do  not  fit  in  memory.  Scikit-Learn\n",
            "implements this algorithm in the MiniBatchKMeans class, which you can use just like\n",
            "the KMeans class:\n",
            "\n",
            "from sklearn.cluster import MiniBatchKMeans\n",
            "\n",
            "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
            "minibatch_kmeans.fit(X)\n",
            "\n",
            "If the dataset does not fit in memory, the simplest option is to use the memmap class, as\n",
            "we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch\n",
            "at a time to the partial_fit() method, but this will require much more work, since\n",
            "you will need to perform multiple initializations and select the best one yourself.\n",
            "\n",
            "3 Charles Elkan, “Using the Triangle Inequality to Accelerate k-Means”, Proceedings of the 20th International\n",
            "\n",
            "Conference on Machine Learning (2003): 147–153.\n",
            "\n",
            "4 The triangle inequality is AC ≤ AB + BC, where A, B and C are three points and AB, AC, and BC are the\n",
            "\n",
            "distances between these points.\n",
            "\n",
            "5 David Sculley, “Web-Scale K-Means Clustering”, Proceedings of the 19th International Conference on World\n",
            "\n",
            "Wide Web (2010): 1177–1178.\n",
            "\n",
            "268 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fAlthough the mini-batch k-means algorithm is much faster than the regular k-means\n",
            "algorithm,  its  inertia  is  generally  slightly  worse.  You  can  see  this  in  Figure  9-6:  the\n",
            "plot  on  the  left  compares  the  inertias  of  mini-batch  k-means  and  regular  k-means\n",
            "models trained on the previous five-blobs dataset using various numbers of clusters k.\n",
            "The difference between the two curves is small, but visible. In the plot on the right,\n",
            "you can see that mini-batch k-means is roughly 3.5 times faster than regular k-means\n",
            "on this dataset.\n",
            "\n",
            "Figure 9-6. Mini-batch k-means has a higher inertia than k-means (left) but it is much\n",
            "faster (right), especially as k increases\n",
            "\n",
            "Finding the optimal number of clusters\n",
            "\n",
            "So far, we’ve set the number of clusters k to 5 because it was obvious by looking at the\n",
            "data that this was the correct number of clusters. But in general, it won’t be so easy to\n",
            "know how to set k, and the result might be quite bad if you set it to the wrong value.\n",
            "As you can see in Figure 9-7, for this dataset setting k to 3 or 8 results in fairly bad\n",
            "models.\n",
            "\n",
            "You  might  be  thinking  that  you  could  just  pick  the  model  with  the  lowest  inertia.\n",
            "Unfortunately, it is not that simple. The inertia for k=3 is about 653.2, which is much\n",
            "higher than for k=5 (211.6). But with k=8, the inertia is just 119.1. The inertia is not\n",
            "a good performance metric when trying to choose k because it keeps getting lower as\n",
            "we increase k. Indeed, the more clusters there are, the closer each instance will be to\n",
            "its closest centroid, and therefore the lower the inertia will be. Let’s plot the inertia as\n",
            "a function of k. When we do this, the curve often contains an inflexion point called\n",
            "the elbow (see Figure 9-8).\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "269\n",
            "\n",
            "\fFigure 9-7. Bad choices for the number of clusters: when k is too small, separate clusters\n",
            "get merged (left), and when k is too large, some clusters get chopped into multiple pieces\n",
            "(right)\n",
            "\n",
            "Figure 9-8. Plotting the inertia as a function of the number of clusters k\n",
            "\n",
            "As  you  can  see,  the  inertia  drops  very  quickly  as  we  increase  k  up  to  4,  but  then\n",
            "it  decreases  much  more  slowly  as  we  keep  increasing  k.  This  curve  has  roughly  the\n",
            "shape  of  an  arm,  and  there  is  an  elbow  at  k  =  4.  So,  if  we  did  not  know  better,  we\n",
            "might  think  4  was  a  good  choice:  any  lower  value  would  be  dramatic,  while  any\n",
            "higher  value  would  not  help  much,  and  we  might  just  be  splitting  perfectly  good\n",
            "clusters in half for no good reason.\n",
            "\n",
            "This  technique  for  choosing  the  best  value  for  the  number  of  clusters  is  rather\n",
            "coarse.  A  more  precise  (but  also  more  computationally  expensive)  approach  is  to\n",
            "use the silhouette score, which is the mean silhouette coefficient over all the instances.\n",
            "An  instance’s  silhouette  coefficient  is  equal  to  (b  –  a)  /  max(a,  b),  where  a  is  the\n",
            "mean distance to the other instances in the same cluster (i.e., the mean intra-cluster\n",
            "distance)  and  b  is  the  mean  nearest-cluster  distance  (i.e.,  the  mean  distance  to  the\n",
            "instances  of  the  next  closest  cluster,  defined  as  the  one  that  minimizes  b,  excluding\n",
            "the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A\n",
            "coefficient close to +1 means that the instance is well inside its own cluster and far\n",
            "from  other  clusters,  while  a  coefficient  close  to  0  means  that  it  is  close  to  a  cluster\n",
            "\n",
            "270 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fboundary;  finally,  a  coefficient  close  to  –1  means  that  the  instance  may  have  been\n",
            "assigned to the wrong cluster.\n",
            "\n",
            "To  compute  the  silhouette  score,  you  can  use  Scikit-Learn’s  silhouette_score()\n",
            "function, giving it all the instances in the dataset and the labels they were assigned:\n",
            "\n",
            ">>> from sklearn.metrics import silhouette_score\n",
            ">>> silhouette_score(X, kmeans.labels_)\n",
            "0.655517642572828\n",
            "\n",
            "Let’s compare the silhouette scores for different numbers of clusters (see Figure 9-9).\n",
            "\n",
            "Figure 9-9. Selecting the number of clusters k using the silhouette score\n",
            "\n",
            "As  you  can  see,  this  visualization  is  much  richer  than  the  previous  one:  although  it\n",
            "confirms that k = 4 is a very good choice, it also highlights the fact that k = 5 is quite\n",
            "good as well, and much better than k = 6 or 7. This was not visible when comparing\n",
            "inertias.\n",
            "\n",
            "An  even  more  informative  visualization  is  obtained  when  we  plot  every  instance’s\n",
            "silhouette coefficient, sorted by the clusters they are assigned to and by the value of\n",
            "the  coefficient.  This  is  called  a  silhouette  diagram  (see  Figure  9-10).  Each  diagram\n",
            "contains  one  knife  shape  per  cluster.  The  shape’s  height  indicates  the  number  of\n",
            "instances in the cluster, and its width represents the sorted silhouette coefficients of\n",
            "the instances in the cluster (wider is better).\n",
            "\n",
            "The  vertical  dashed  lines  represent  the  mean  silhouette  score  for  each  number  of\n",
            "clusters.  When  most  of  the  instances  in  a  cluster  have  a  lower  coefficient  than  this\n",
            "score (i.e., if many of the instances stop short of the dashed line, ending to the left of\n",
            "it), then the cluster is rather bad since this means its instances are much too close to\n",
            "other clusters. Here we can see that when k = 3 or 6, we get bad clusters. But when\n",
            "k  =  4  or  5,  the  clusters  look  pretty  good:  most  instances  extend  beyond  the  dashed\n",
            "line, to the right and closer to 1.0. When k = 4, the cluster at index 1 (the second from\n",
            "the bottom) is rather big. When k = 5, all clusters have similar sizes. So, even though\n",
            "the overall silhouette score from k = 4 is slightly greater than for k = 5, it seems like a\n",
            "good idea to use k = 5 to get clusters of similar sizes.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "271\n",
            "\n",
            "\fFigure 9-10. Analyzing the silhouette diagrams for various values of k\n",
            "\n",
            "Limits of k-means\n",
            "Despite its many merits, most notably being fast and scalable, k-means is not perfect.\n",
            "As  we  saw,  it  is  necessary  to  run  the  algorithm  several  times  to  avoid  suboptimal\n",
            "solutions,  plus  you  need  to  specify  the  number  of  clusters,  which  can  be  quite  a\n",
            "hassle. Moreover, k-means does not behave very well when the clusters have varying\n",
            "sizes, different densities, or nonspherical shapes. For example, Figure 9-11 shows how\n",
            "k-means  clusters  a  dataset  containing  three  ellipsoidal  clusters  of  different  dimen‐\n",
            "sions, densities, and orientations.\n",
            "\n",
            "As  you  can  see,  neither  of  these  solutions  is  any  good.  The  solution  on  the  left  is\n",
            "better, but it still chops off 25% of the middle cluster and assigns it to the cluster on\n",
            "the right. The solution on the right is just terrible, even though its inertia is lower. So,\n",
            "depending on the data, different clustering algorithms may perform better. On these\n",
            "types of elliptical clusters, Gaussian mixture models work great.\n",
            "\n",
            "272 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fFigure 9-11. k-means fails to cluster these ellipsoidal blobs properly\n",
            "\n",
            "It is important to scale the input features (see Chapter 2) before you\n",
            "run  k-means,  or  the  clusters  may  be  very  stretched  and  k-means\n",
            "will  perform  poorly.  Scaling  the  features  does  not  guarantee  that\n",
            "all  the  clusters  will  be  nice  and  spherical,  but  it  generally  helps\n",
            "k-means.\n",
            "\n",
            "Now let’s look at a few ways we can benefit from clustering. We will use k-means, but\n",
            "feel free to experiment with other clustering algorithms.\n",
            "\n",
            "Using Clustering for Image Segmentation\n",
            "Image segmentation is the task of partitioning an image into multiple segments. There\n",
            "are several variants:\n",
            "\n",
            "• In  color  segmentation,  pixels  with  a  similar  color  get  assigned  to  the  same  seg‐\n",
            "•\n",
            "ment. This is sufficient in many applications. For example, if you want to analyze\n",
            "satellite images to measure how much total forest area there is in a region, color\n",
            "segmentation may be just fine.\n",
            "\n",
            "• In  semantic  segmentation,  all  pixels  that  are  part  of  the  same  object  type  get\n",
            "•\n",
            "assigned to the same segment. For example, in a self-driving car’s vision system,\n",
            "all  pixels  that  are  part  of  a  pedestrian’s  image  might  be  assigned  to  the  “pedes‐\n",
            "trian” segment (there would be one segment containing all the pedestrians).\n",
            "\n",
            "• In instance segmentation, all pixels that are part of the same individual object are\n",
            "•\n",
            "assigned to the same segment. In this case there would be a different segment for\n",
            "each pedestrian.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "273\n",
            "\n",
            "\fThe  state  of  the  art  in  semantic  or  instance  segmentation  today  is  achieved  using\n",
            "complex  architectures  based  on  convolutional  neural  networks  (see  Chapter  14).  In\n",
            "this  chapter  we  are  going  to  focus  on  the  (much  simpler)  color  segmentation  task,\n",
            "using k-means.\n",
            "\n",
            "We’ll start by importing the Pillow package (successor to the Python Imaging Library,\n",
            "PIL), which we’ll then use to load the ladybug.png image (see the upper-left image in\n",
            "Figure 9-12), assuming it’s located at filepath:\n",
            "\n",
            ">>> import PIL\n",
            ">>> image = np.asarray(PIL.Image.open(filepath))\n",
            ">>> image.shape\n",
            "(533, 800, 3)\n",
            "\n",
            "The  image  is  represented  as  a  3D  array.  The  first  dimension’s  size  is  the  height;  the\n",
            "second is the width; and the third is the number of color channels, in this case red,\n",
            "green, and blue (RGB). In other words, for each pixel there is a 3D vector containing\n",
            "the intensities of red, green, and blue as unsigned 8-bit integers between 0 and 255.\n",
            "Some  images  may  have  fewer  channels  (such  as  grayscale  images,  which  only  have\n",
            "one), and some images may have more channels (such as images with an additional\n",
            "alpha channel for transparency, or satellite images, which often contain channels for\n",
            "additional light frequencies (like infrared).\n",
            "\n",
            "The  following  code  reshapes  the  array  to  get  a  long  list  of  RGB  colors,  then  it\n",
            "clusters  these  colors  using  k-means  with  eight  clusters.  It  creates  a  segmented_img\n",
            "array  containing  the  nearest  cluster  center  for  each  pixel  (i.e.,  the  mean  color  of\n",
            "each  pixel’s  cluster),  and  lastly  it  reshapes  this  array  to  the  original  image  shape.\n",
            "The  third  line  uses  advanced  NumPy  indexing;  for  example,  if  the  first  10  labels  in\n",
            "kmeans_.labels_ are equal to 1, then the first 10 colors in segmented_img are equal\n",
            "to kmeans.cluster_centers_[1]:\n",
            "\n",
            "X = image.reshape(-1, 3)\n",
            "kmeans = KMeans(n_clusters=8, random_state=42).fit(X)\n",
            "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
            "segmented_img = segmented_img.reshape(image.shape)\n",
            "\n",
            "This outputs the image shown in the upper right of Figure 9-12. You can experiment\n",
            "with various numbers of clusters, as shown in the figure. When you use fewer than\n",
            "eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own:\n",
            "it  gets  merged  with  colors  from  the  environment.  This  is  because  k-means  prefers\n",
            "clusters  of  similar  sizes.  The  ladybug  is  small—much  smaller  than  the  rest  of  the\n",
            "image—so even though its color is flashy, k-means fails to dedicate a cluster to it.\n",
            "\n",
            "274 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fFigure 9-12. Image segmentation using k-means with various numbers of color clusters\n",
            "\n",
            "That wasn’t too hard, was it? Now let’s look at another application of clustering.\n",
            "\n",
            "Using Clustering for Semi-Supervised Learning\n",
            "Another use case for clustering is in semi-supervised learning, when we have plenty\n",
            "of  unlabeled  instances  and  very  few  labeled  instances.  In  this  section,  we’ll  use  the\n",
            "digits dataset, which is a simple MNIST-like dataset containing 1,797 grayscale 8 × 8\n",
            "images representing the digits 0 to 9. First, let’s load and split the dataset (it’s already\n",
            "shuffled):\n",
            "\n",
            "from sklearn.datasets import load_digits\n",
            "\n",
            "X_digits, y_digits = load_digits(return_X_y=True)\n",
            "X_train, y_train = X_digits[:1400], y_digits[:1400]\n",
            "X_test, y_test = X_digits[1400:], y_digits[1400:]\n",
            "\n",
            "We will pretend we only have labels for 50 instances. To get a baseline performance,\n",
            "let’s train a logistic regression model on these 50 labeled instances:\n",
            "\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "\n",
            "n_labeled = 50\n",
            "log_reg = LogisticRegression(max_iter=10_000)\n",
            "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "275\n",
            "\n",
            "\fWe can then measure the accuracy of this model on the test set (note that the test set\n",
            "must be labeled):\n",
            "\n",
            ">>> log_reg.score(X_test, y_test)\n",
            "0.7481108312342569\n",
            "\n",
            "The  model’s  accuracy  is  just  74.8%.  That’s  not  great:  indeed,  if  you  try  training  the\n",
            "model on the full training set, you will find that it will reach about 90.7% accuracy.\n",
            "Let’s  see  how  we  can  do  better.  First,  let’s  cluster  the  training  set  into  50  clusters.\n",
            "Then,  for  each  cluster,  we’ll  find  the  image  closest  to  the  centroid.  We’ll  call  these\n",
            "images the representative images:\n",
            "\n",
            "k = 50\n",
            "kmeans = KMeans(n_clusters=k, random_state=42)\n",
            "X_digits_dist = kmeans.fit_transform(X_train)\n",
            "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
            "X_representative_digits = X_train[representative_digit_idx]\n",
            "\n",
            "Figure 9-13 shows the 50 representative images.\n",
            "\n",
            "Figure 9-13. Fifty representative digit images (one per cluster)\n",
            "\n",
            "Let’s look at each image and manually label them:\n",
            "\n",
            "y_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, ..., 5, 1, 9, 9, 3, 7])\n",
            "\n",
            "Now  we  have  a  dataset  with  just  50  labeled  instances,  but  instead  of  being  random\n",
            "instances, each of them is a representative image of its cluster. Let’s see if the perfor‐\n",
            "mance is any better:\n",
            "\n",
            ">>> log_reg = LogisticRegression(max_iter=10_000)\n",
            ">>> log_reg.fit(X_representative_digits, y_representative_digits)\n",
            ">>> log_reg.score(X_test, y_test)\n",
            "0.8488664987405542\n",
            "\n",
            "Wow! We jumped from 74.8% accuracy to 84.9%, although we are still only training\n",
            "the  model  on  50  instances.  Since  it  is  often  costly  and  painful  to  label  instances,\n",
            "especially  when  it  has  to  be  done  manually  by  experts,  it  is  a  good  idea  to  label\n",
            "representative instances rather than just random instances.\n",
            "\n",
            "276 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fBut  perhaps  we  can  go  one  step  further:  what  if  we  propagated  the  labels  to  all  the\n",
            "other instances in the same cluster? This is called label propagation:\n",
            "\n",
            "y_train_propagated = np.empty(len(X_train), dtype=np.int64)\n",
            "for i in range(k):\n",
            "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n",
            "\n",
            "Now let’s train the model again and look at its performance:\n",
            "\n",
            ">>> log_reg = LogisticRegression()\n",
            ">>> log_reg.fit(X_train, y_train_propagated)\n",
            ">>> log_reg.score(X_test, y_test)\n",
            "0.8942065491183879\n",
            "\n",
            "We  got  another  significant  accuracy  boost!  Let’s  see  if  we  can  do  even  better  by\n",
            "ignoring  the  1%  of  instances  that  are  farthest  from  their  cluster  center:  this  should\n",
            "eliminate  some  outliers.  The  following  code  first  computes  the  distance  from  each\n",
            "instance  to  its  closest  cluster  center,  then  for  each  cluster  it  sets  the  1%  largest\n",
            "distances  to  –1.  Lastly,  it  creates  a  set  without  these  instances  marked  with  a  –1\n",
            "distance:\n",
            "\n",
            "percentile_closest = 99\n",
            "\n",
            "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
            "for i in range(k):\n",
            "    in_cluster = (kmeans.labels_ == i)\n",
            "    cluster_dist = X_cluster_dist[in_cluster]\n",
            "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
            "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
            "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
            "\n",
            "partially_propagated = (X_cluster_dist != -1)\n",
            "X_train_partially_propagated = X_train[partially_propagated]\n",
            "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
            "\n",
            "Now  let’s  train  the  model  again  on  this  partially  propagated  dataset  and  see  what\n",
            "accuracy we get:\n",
            "\n",
            ">>> log_reg = LogisticRegression(max_iter=10_000)\n",
            ">>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
            ">>> log_reg.score(X_test, y_test)\n",
            "0.9093198992443325\n",
            "\n",
            "Nice!  With  just  50  labeled  instances  (only  5  examples  per  class  on  average!)  we\n",
            "got  90.9%  accuracy,  which  is  actually  slightly  higher  than  the  performance  we  got\n",
            "on  the  fully  labeled  digits  dataset  (90.7%).  This  is  partly  thanks  to  the  fact  that  we\n",
            "dropped  some  outliers,  and  partly  because  the  propagated  labels  are  actually  pretty\n",
            "good—their accuracy is about 97.5%, as the following code shows:\n",
            "\n",
            ">>> (y_train_partially_propagated == y_train[partially_propagated]).mean()\n",
            "0.9755555555555555\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "277\n",
            "\n",
            "\fScikit-Learn  also  offers  two  classes  that  can  propagate  labels\n",
            "automatically:  LabelSpreading  and  LabelPropagation  in  the\n",
            "sklearn.semi_supervised package. Both classes construct a simi‐\n",
            "larity  matrix  between  all  the  instances,  and  iteratively  propagate\n",
            "labels from labeled instances to similar unlabeled instances. There’s\n",
            "also  a  very  different  class  called  SelfTrainingClassifier  in  the\n",
            "same package: you give it a base classifier (such as a RandomForest\n",
            "Classifier) and it trains it on the labeled instances, then uses it to\n",
            "predict labels for the unlabeled samples. It then updates the train‐\n",
            "ing  set  with  the  labels  it  is  most  confident  about,  and  repeats  this\n",
            "process of training and labeling until it cannot add labels anymore.\n",
            "These  techniques  are  not  magic  bullets,  but  they  can  occasionally\n",
            "give your model a little boost.\n",
            "\n",
            "Active Learning\n",
            "To  continue  improving  your  model  and  your  training  set,  the  next  step  could  be  to\n",
            "do  a  few  rounds  of  active  learning,  which  is  when  a  human  expert  interacts  with\n",
            "the  learning  algorithm,  providing  labels  for  specific  instances  when  the  algorithm\n",
            "requests them. There are many different strategies for active learning, but one of the\n",
            "most common ones is called uncertainty sampling. Here is how it works:\n",
            "\n",
            "1. The model is trained on the labeled instances gathered so far, and this model is\n",
            "1.\n",
            "\n",
            "used to make predictions on all the unlabeled instances.\n",
            "\n",
            "2. The  instances  for  which  the  model  is  most  uncertain  (i.e.,  where  its  estimated\n",
            "2.\n",
            "\n",
            "probability is lowest) are given to the expert for labeling.\n",
            "\n",
            "3. You  iterate  this  process  until  the  performance  improvement  stops  being  worth\n",
            "3.\n",
            "\n",
            "the labeling effort.\n",
            "\n",
            "Other  active  learning  strategies  include  labeling  the  instances  that  would  result  in\n",
            "the  largest  model  change  or  the  largest  drop  in  the  model’s  validation  error,  or  the\n",
            "instances that different models disagree on (e.g., an SVM and a random forest).\n",
            "\n",
            "Before  we  move  on  to  Gaussian  mixture  models,  let’s  take  a  look  at  DBSCAN,\n",
            "another popular clustering algorithm that illustrates a very different approach based\n",
            "on local density estimation. This approach allows the algorithm to identify clusters of\n",
            "arbitrary shapes.\n",
            "\n",
            "278 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fDBSCAN\n",
            "The  density-based  spatial  clustering  of  applications  with  noise  (DBSCAN)  algorithm\n",
            "defines clusters as continuous regions of high density. Here is how it works:\n",
            "\n",
            "• For  each  instance,  the  algorithm  counts  how  many  instances  are  located\n",
            "•\n",
            "within  a  small  distance  ε  (epsilon)  from  it.  This  region  is  called  the  instance’s\n",
            "ε-neighborhood.\n",
            "\n",
            "• If an instance has at least min_samples instances in its ε-neighborhood (includ‐\n",
            "•\n",
            "ing itself), then it is considered a core instance. In other words, core instances are\n",
            "those that are located in dense regions.\n",
            "\n",
            "•\n",
            "• All instances in the neighborhood of a core instance belong to the same cluster.\n",
            "This neighborhood may include other core instances; therefore, a long sequence\n",
            "of neighboring core instances forms a single cluster.\n",
            "\n",
            "•\n",
            "• Any instance that is not a core instance and does not have one in its neighbor‐\n",
            "\n",
            "hood is considered an anomaly.\n",
            "\n",
            "This algorithm works well if all the clusters are well separated by low-density regions.\n",
            "The DBSCAN class in Scikit-Learn is as simple to use as you might expect. Let’s test it\n",
            "on the moons dataset, introduced in Chapter 5:\n",
            "\n",
            "from sklearn.cluster import DBSCAN\n",
            "from sklearn.datasets import make_moons\n",
            "\n",
            "X, y = make_moons(n_samples=1000, noise=0.05)\n",
            "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
            "dbscan.fit(X)\n",
            "\n",
            "The labels of all the instances are now available in the labels_ instance variable:\n",
            "\n",
            ">>> dbscan.labels_\n",
            "array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])\n",
            "\n",
            "Notice that some instances have a cluster index equal to –1, which means that they\n",
            "are  considered  as  anomalies  by  the  algorithm.  The  indices  of  the  core  instances\n",
            "are available in the core_sample_indices_ instance variable, and the core instances\n",
            "themselves are available in the components_ instance variable:\n",
            "\n",
            ">>> dbscan.core_sample_indices_\n",
            "array([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])\n",
            ">>> dbscan.components_\n",
            "array([[-0.02137124,  0.40618608],\n",
            "       [-0.84192557,  0.53058695],\n",
            "       [...],\n",
            "       [ 0.79419406,  0.60777171]])\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "279\n",
            "\n",
            "\fThis clustering is represented in the lefthand plot of Figure 9-14. As you can see, it\n",
            "identified quite a lot of anomalies, plus seven different clusters. How disappointing!\n",
            "Fortunately, if we widen each instance’s neighborhood by increasing eps to 0.2, we get\n",
            "the clustering on the right, which looks perfect. Let’s continue with this model.\n",
            "\n",
            "Figure 9-14. DBSCAN clustering using two different neighborhood radiuses\n",
            "\n",
            "Surprisingly,  the  DBSCAN  class  does  not  have  a  predict()  method,  although  it  has\n",
            "a  fit_predict()  method.  In  other  words,  it  cannot  predict  which  cluster  a  new\n",
            "instance  belongs  to.  This  decision  was  made  because  different  classification  algo‐\n",
            "rithms can be better for different tasks, so the authors decided to let the user choose\n",
            "which  one  to  use.  Moreover,  it’s  not  hard  to  implement.  For  example,  let’s  train  a\n",
            "KNeighborsClassifier:\n",
            "\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "\n",
            "knn = KNeighborsClassifier(n_neighbors=50)\n",
            "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
            "\n",
            "Now, given a few new instances, we can predict which clusters they most likely belong\n",
            "to and even estimate a probability for each cluster:\n",
            "\n",
            ">>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
            ">>> knn.predict(X_new)\n",
            "array([1, 0, 1, 0])\n",
            ">>> knn.predict_proba(X_new)\n",
            "array([[0.18, 0.82],\n",
            "       [1.  , 0.  ],\n",
            "       [0.12, 0.88],\n",
            "       [1.  , 0.  ]])\n",
            "\n",
            "Note that we only trained the classifier on the core instances, but we could also have\n",
            "chosen to train it on all the instances, or all but the anomalies: this choice depends on\n",
            "the final task.\n",
            "\n",
            "280 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fThe decision boundary is represented in Figure 9-15 (the crosses represent the four\n",
            "instances  in  X_new).  Notice  that  since  there  is  no  anomaly  in  the  training  set,  the\n",
            "classifier  always  chooses  a  cluster,  even  when  that  cluster  is  far  away.  It  is  fairly\n",
            "straightforward  to  introduce  a  maximum  distance,  in  which  case  the  two  instances\n",
            "that  are  far  away  from  both  clusters  are  classified  as  anomalies.  To  do  this,  use  the\n",
            "kneighbors()  method  of  the  KNeighborsClassifier.  Given  a  set  of  instances,  it\n",
            "returns  the  distances  and  the  indices  of  the  k-nearest  neighbors  in  the  training  set\n",
            "(two matrices, each with k columns):\n",
            "\n",
            ">>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
            ">>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
            ">>> y_pred[y_dist > 0.2] = -1\n",
            ">>> y_pred.ravel()\n",
            "array([-1,  0,  1, -1])\n",
            "\n",
            "Figure 9-15. Decision boundary between two clusters\n",
            "\n",
            "In  short,  DBSCAN  is  a  very  simple  yet  powerful  algorithm  capable  of  identifying\n",
            "any  number  of  clusters  of  any  shape.  It  is  robust  to  outliers,  and  it  has  just  two\n",
            "hyperparameters  (eps  and  min_samples).  If  the  density  varies  significantly  across\n",
            "the  clusters,  however,  or  if  there’s  no  sufficiently  low-density  region  around  some\n",
            "clusters,  DBSCAN  can  struggle  to  capture  all  the  clusters  properly.  Moreover,  its\n",
            "computational  complexity  is  roughly  O(m2n),  so  it  does  not  scale  well  to  large\n",
            "datasets.\n",
            "\n",
            "You  may  also  want  to  try  hierarchical  DBSCAN  (HDBSCAN),\n",
            "which  is  implemented  in  the  scikit-learn-contrib  project,  as\n",
            "it  is  usually  better  than  DBSCAN  at  finding  clusters  of  varying\n",
            "densities.\n",
            "\n",
            "Clustering Algorithms: k-means and DBSCAN \n",
            "\n",
            "| \n",
            "\n",
            "281\n",
            "\n",
            "\fOther Clustering Algorithms\n",
            "Scikit-Learn  implements  several  more  clustering  algorithms  that  you  should  take  a\n",
            "look at. I cannot cover them all in detail here, but here is a brief overview:\n",
            "\n",
            "Agglomerative clustering\n",
            "\n",
            "A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles\n",
            "floating  on  water  and  gradually  attaching  to  each  other  until  there’s  one  big\n",
            "group of bubbles. Similarly, at each iteration, agglomerative clustering connects\n",
            "the  nearest  pair  of  clusters  (starting  with  individual  instances).  If  you  drew  a\n",
            "tree with a branch for every pair of clusters that merged, you would get a binary\n",
            "tree of clusters, where the leaves are the individual instances. This approach can\n",
            "capture  clusters  of  various  shapes;  it  also  produces  a  flexible  and  informative\n",
            "cluster  tree  instead  of  forcing  you  to  choose  a  particular  cluster  scale,  and  it\n",
            "can  be  used  with  any  pairwise  distance.  It  can  scale  nicely  to  large  numbers\n",
            "of  instances  if  you  provide  a  connectivity  matrix,  which  is  a  sparse  m  ×  m\n",
            "matrix  that  indicates  which  pairs  of  instances  are  neighbors  (e.g.,  returned  by\n",
            "sklearn.neighbors.kneighbors_graph()).  Without  a  connectivity  matrix,  the\n",
            "algorithm does not scale well to large datasets.\n",
            "\n",
            "BIRCH\n",
            "\n",
            "The balanced iterative reducing and clustering using hierarchies (BIRCH) algo‐\n",
            "rithm was designed specifically for very large datasets, and it can be faster than\n",
            "batch k-means, with similar results, as long as the number of features is not too\n",
            "large  (<20).  During  training,  it  builds  a  tree  structure  containing  just  enough\n",
            "information  to  quickly  assign  each  new  instance  to  a  cluster,  without  having  to\n",
            "store all the instances in the tree: this approach allows it to use limited memory\n",
            "while handling huge datasets.\n",
            "\n",
            "Mean-shift\n",
            "\n",
            "This algorithm starts by placing a circle centered on each instance; then for each\n",
            "circle  it  computes  the  mean  of  all  the  instances  located  within  it,  and  it  shifts\n",
            "the circle so that it is centered on the mean. Next, it iterates this mean-shifting\n",
            "step until all the circles stop moving (i.e., until each of them is centered on the\n",
            "mean  of  the  instances  it  contains).  Mean-shift  shifts  the  circles  in  the  direction\n",
            "of higher density, until each of them has found a local density maximum. Finally,\n",
            "all  the  instances  whose  circles  have  settled  in  the  same  place  (or  close  enough)\n",
            "are  assigned  to  the  same  cluster.  Mean-shift  has  some  of  the  same  features  as\n",
            "DBSCAN, like how it can find any number of clusters of any shape, it has very\n",
            "few hyperparameters (just one—the radius of the circles, called the bandwidth),\n",
            "and it relies on local density estimation. But unlike DBSCAN, mean-shift tends\n",
            "to chop clusters into pieces when they have internal density variations. Unfortu‐\n",
            "nately,  its  computational  complexity  is  O(m2n),  so  it  is  not  suited  for  large\n",
            "datasets.\n",
            "\n",
            "282 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fAffinity propagation\n",
            "\n",
            "In this algorithm, instances repeatedly exchange messages between one another\n",
            "until every instance has elected another instance (or itself) to represent it. These\n",
            "elected  instances  are  called  exemplars.  Each  exemplar  and  all  the  instances  that\n",
            "elected  it  form  one  cluster.  In  real-life  politics,  you  typically  want  to  vote  for  a\n",
            "candidate  whose  opinions  are  similar  to  yours,  but  you  also  want  them  to  win\n",
            "the  election,  so  you  might  choose  a  candidate  you  don’t  fully  agree  with,  but\n",
            "who  is  more  popular.  You  typically  evaluate  popularity  through  polls.  Affinity\n",
            "propagation  works  in  a  similar  way,  and  it  tends  to  choose  exemplars  located\n",
            "near  the  center  of  clusters,  similar  to  k-means.  But  unlike  with  k-means,  you\n",
            "don’t  have  to  pick  a  number  of  clusters  ahead  of  time:  it  is  determined  during\n",
            "training. Moreover, affinity propagation can deal nicely with clusters of different\n",
            "sizes. Sadly, this algorithm has a computational complexity of O(m2), so it is not\n",
            "suited for large datasets.\n",
            "\n",
            "Spectral clustering\n",
            "\n",
            "This algorithm takes a similarity matrix between the instances and creates a low-\n",
            "dimensional embedding from it (i.e., it reduces the matrix’s dimensionality), then\n",
            "it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s\n",
            "implementation  uses  k-means).  Spectral  clustering  can  capture  complex  cluster\n",
            "structures,  and  it  can  also  be  used  to  cut  graphs  (e.g.,  to  identify  clusters  of\n",
            "friends on a social network). It does not scale well to large numbers of instances,\n",
            "and it does not behave well when the clusters have very different sizes.\n",
            "\n",
            "Now let’s dive into Gaussian mixture models, which can be used for density estima‐\n",
            "tion, clustering, and anomaly detection.\n",
            "\n",
            "Gaussian Mixtures\n",
            "A  Gaussian  mixture  model  (GMM)  is  a  probabilistic  model  that  assumes  that  the\n",
            "instances  were  generated  from  a  mixture  of  several  Gaussian  distributions  whose\n",
            "parameters are unknown. All the instances generated from a single Gaussian distri‐\n",
            "bution  form  a  cluster  that  typically  looks  like  an  ellipsoid.  Each  cluster  can  have\n",
            "a  different  ellipsoidal  shape,  size,  density,  and  orientation,  just  like  in  Figure  9-11.\n",
            "When you observe an instance, you know it was generated from one of the Gaussian\n",
            "distributions,  but  you  are  not  told  which  one,  and  you  do  not  know  what  the\n",
            "parameters of these distributions are.\n",
            "\n",
            "There  are  several  GMM  variants.  In  the  simplest  variant,  implemented  in  the\n",
            "GaussianMixture  class,  you  must  know  in  advance  the  number  k  of  Gaussian  dis‐\n",
            "tributions.  The  dataset  X  is  assumed  to  have  been  generated  through  the  following\n",
            "probabilistic process:\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "283\n",
            "\n",
            "\f• For  each  instance,  a  cluster  is  picked  randomly  from  among  k  clusters.  The\n",
            "•\n",
            "probability of choosing the jth cluster is the cluster’s weight ϕ(j).6 The index of the\n",
            "cluster chosen for the ith instance is noted z(i).\n",
            "\n",
            "• If the ith instance was assigned to the jth cluster (i.e., z(i) = j), then the location x(i)\n",
            "•\n",
            "of this instance is sampled randomly from the Gaussian distribution with mean\n",
            "μ(j) and covariance matrix Σ(j). This is noted x(i) ~ N(μ(j), Σ(j)).\n",
            "\n",
            "So what can you do with such a model? Well, given the dataset X, you typically want\n",
            "to start by estimating the weights ϕ and all the distribution parameters μ(1) to μ(k) and\n",
            "Σ(1) to Σ(k). Scikit-Learn’s GaussianMixture class makes this super easy:\n",
            "\n",
            "from sklearn.mixture import GaussianMixture\n",
            "\n",
            "gm = GaussianMixture(n_components=3, n_init=10)\n",
            "gm.fit(X)\n",
            "\n",
            "Let’s look at the parameters that the algorithm estimated:\n",
            "\n",
            ">>> gm.weights_\n",
            "array([0.39025715, 0.40007391, 0.20966893])\n",
            ">>> gm.means_\n",
            "array([[ 0.05131611,  0.07521837],\n",
            "       [-1.40763156,  1.42708225],\n",
            "       [ 3.39893794,  1.05928897]])\n",
            ">>> gm.covariances_\n",
            "array([[[ 0.68799922,  0.79606357],\n",
            "        [ 0.79606357,  1.21236106]],\n",
            "\n",
            "       [[ 0.63479409,  0.72970799],\n",
            "        [ 0.72970799,  1.1610351 ]],\n",
            "\n",
            "       [[ 1.14833585, -0.03256179],\n",
            "        [-0.03256179,  0.95490931]]])\n",
            "\n",
            "Great,  it  worked  fine!  Indeed,  two  of  the  three  clusters  were  generated  with  500\n",
            "instances each, while the third cluster only contains 250 instances. So the true cluster\n",
            "weights  are  0.4,  0.4,  and  0.2,  respectively,  and  that’s  roughly  what  the  algorithm\n",
            "found.  Similarly,  the  true  means  and  covariance  matrices  are  quite  close  to  those\n",
            "found by the algorithm. But how? This class relies on the expectation-maximization\n",
            "(EM) algorithm, which has many similarities with the k-means algorithm: it also ini‐\n",
            "tializes the cluster parameters randomly, then it repeats two steps until convergence,\n",
            "first  assigning  instances  to  clusters  (this  is  called  the  expectation  step)  and  then\n",
            "updating  the  clusters  (this  is  called  the  maximization  step).  Sounds  familiar,  right?\n",
            "In  the  context  of  clustering,  you  can  think  of  EM  as  a  generalization  of  k-means\n",
            "that  not  only  finds  the  cluster  centers  (μ(1)  to  μ(k)),  but  also  their  size,  shape,  and\n",
            "\n",
            "6 Phi (ϕ or φ) is the 21st letter of the Greek alphabet.\n",
            "\n",
            "284 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\forientation (Σ(1) to Σ(k)), as well as their relative weights (ϕ(1) to ϕ(k)). Unlike k-means,\n",
            "though,  EM  uses  soft  cluster  assignments,  not  hard  assignments.  For  each  instance,\n",
            "during  the  expectation  step,  the  algorithm  estimates  the  probability  that  it  belongs\n",
            "to each cluster (based on the current cluster parameters). Then, during the maximi‐\n",
            "zation  step,  each  cluster  is  updated  using  all  the  instances  in  the  dataset,  with  each\n",
            "instance  weighted  by  the  estimated  probability  that  it  belongs  to  that  cluster.  These\n",
            "probabilities are called the responsibilities of the clusters for the instances. During the\n",
            "maximization step, each cluster’s update will mostly be impacted by the instances it is\n",
            "most responsible for.\n",
            "\n",
            "Unfortunately,  just  like  k-means,  EM  can  end  up  converging  to\n",
            "poor solutions, so it needs to be run several times, keeping only the\n",
            "best solution. This is why we set n_init to 10. Be careful: by default\n",
            "n_init is set to 1.\n",
            "\n",
            "You  can  check  whether  or  not  the  algorithm  converged  and  how  many  iterations  it\n",
            "took:\n",
            "\n",
            ">>> gm.converged_\n",
            "True\n",
            ">>> gm.n_iter_\n",
            "4\n",
            "\n",
            "Now  that  you  have  an  estimate  of  the  location,  size,  shape,  orientation,  and  relative\n",
            "weight  of  each  cluster,  the  model  can  easily  assign  each  instance  to  the  most  likely\n",
            "cluster  (hard  clustering)  or  estimate  the  probability  that  it  belongs  to  a  particular\n",
            "cluster  (soft  clustering).  Just  use  the  predict()  method  for  hard  clustering,  or  the\n",
            "predict_proba() method for soft clustering:\n",
            "\n",
            ">>> gm.predict(X)\n",
            "array([0, 0, 1, ..., 2, 2, 2])\n",
            ">>> gm.predict_proba(X).round(3)\n",
            "array([[0.977, 0.   , 0.023],\n",
            "       [0.983, 0.001, 0.016],\n",
            "       [0.   , 1.   , 0.   ],\n",
            "       ...,\n",
            "       [0.   , 0.   , 1.   ],\n",
            "       [0.   , 0.   , 1.   ],\n",
            "       [0.   , 0.   , 1.   ]])\n",
            "\n",
            "A  Gaussian  mixture  model  is  a  generative  model,  meaning  you  can  sample  new\n",
            "instances from it (note that they are ordered by cluster index):\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "285\n",
            "\n",
            "\f>>> X_new, y_new = gm.sample(6)\n",
            ">>> X_new\n",
            "array([[-0.86944074, -0.32767626],\n",
            "       [ 0.29836051,  0.28297011],\n",
            "       [-2.8014927 , -0.09047309],\n",
            "       [ 3.98203732,  1.49951491],\n",
            "       [ 3.81677148,  0.53095244],\n",
            "       [ 2.84104923, -0.73858639]])\n",
            ">>> y_new\n",
            "array([0, 0, 1, 2, 2, 2])\n",
            "\n",
            "It  is  also  possible  to  estimate  the  density  of  the  model  at  any  given  location.  This\n",
            "is  achieved  using  the  score_samples()  method:  for  each  instance  it  is  given,  this\n",
            "method  estimates  the  log  of  the  probability  density  function  (PDF)  at  that  location.\n",
            "The greater the score, the higher the density:\n",
            "\n",
            ">>> gm.score_samples(X).round(2)\n",
            "array([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])\n",
            "\n",
            "If you compute the exponential of these scores, you get the value of the PDF at the\n",
            "location of the given instances. These are not probabilities, but probability densities:\n",
            "they  can  take  on  any  positive  value,  not  just  a  value  between  0  and  1.  To  estimate\n",
            "the  probability  that  an  instance  will  fall  within  a  particular  region,  you  would  have\n",
            "to integrate the PDF over that region (if you do so over the entire space of possible\n",
            "instance locations, the result will be 1).\n",
            "\n",
            "Figure 9-16 shows the cluster means, the decision boundaries (dashed lines), and the\n",
            "density contours of this model.\n",
            "\n",
            "Figure 9-16. Cluster means, decision boundaries, and density contours of a trained\n",
            "Gaussian mixture model\n",
            "\n",
            "286 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fNice! The algorithm clearly found an excellent solution. Of course, we made its task\n",
            "easy by generating the data using a set of 2D Gaussian distributions (unfortunately,\n",
            "real-life  data  is  not  always  so  Gaussian  and  low-dimensional).  We  also  gave  the\n",
            "algorithm the correct number of clusters. When there are many dimensions, or many\n",
            "clusters, or few instances, EM can struggle to converge to the optimal solution. You\n",
            "might need to reduce the difficulty of the task by limiting the number of parameters\n",
            "that the algorithm has to learn. One way to do this is to limit the range of shapes and\n",
            "orientations that the clusters can have. This can be achieved by imposing constraints\n",
            "on the covariance matrices. To do this, set the  covariance_type hyperparameter to\n",
            "one of the following values:\n",
            "\n",
            "\"spherical\"\n",
            "\n",
            "All clusters must be spherical, but they can have different diameters (i.e., differ‐\n",
            "ent variances).\n",
            "\n",
            "\"diag\"\n",
            "\n",
            "Clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s axes must\n",
            "be parallel to the coordinate axes (i.e., the covariance matrices must be diagonal).\n",
            "\n",
            "\"tied\"\n",
            "\n",
            "All  clusters  must  have  the  same  ellipsoidal  shape,  size,  and  orientation  (i.e.,  all\n",
            "clusters share the same covariance matrix).\n",
            "\n",
            "By  default,  covariance_type  is  equal  to  \"full\",  which  means  that  each  cluster\n",
            "can  take  on  any  shape,  size,  and  orientation  (it  has  its  own  unconstrained  cova‐\n",
            "riance  matrix).  Figure  9-17  plots  the  solutions  found  by  the  EM  algorithm  when\n",
            "covariance_type is set to \"tied\" or \"spherical\".\n",
            "\n",
            "Figure 9-17. Gaussian mixtures for tied clusters (left) and spherical clusters (right)\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "287\n",
            "\n",
            "\fThe  computational  complexity  of  training  a  GaussianMixture\n",
            "model  depends  on  the  number  of  instances  m,  the  number  of\n",
            "dimensions  n,  the  number  of  clusters  k,  and  the  constraints  on\n",
            "the  covariance  matrices.  If  covariance_type  is  \"spherical\"  or\n",
            "\"diag\", it is O(kmn), assuming the data has a clustering structure.\n",
            "If covariance_type is \"tied\" or \"full\", it is O(kmn2 + kn3), so it\n",
            "will not scale to large numbers of features.\n",
            "\n",
            "Gaussian  mixture  models  can  also  be  used  for  anomaly  detection.  We’ll  see  how  in\n",
            "the next section.\n",
            "\n",
            "Using Gaussian Mixtures for Anomaly Detection\n",
            "Using a Gaussian mixture model for anomaly detection is quite simple: any instance\n",
            "located  in  a  low-density  region  can  be  considered  an  anomaly.  You  must  define\n",
            "what  density  threshold  you  want  to  use.  For  example,  in  a  manufacturing  company\n",
            "that  tries  to  detect  defective  products,  the  ratio  of  defective  products  is  usually  well\n",
            "known. Say it is equal to 2%. You then set the density threshold to be the value that\n",
            "results in having 2% of the instances located in areas below that threshold density. If\n",
            "you notice that you get too many false positives (i.e., perfectly good products that are\n",
            "flagged as defective), you can lower the threshold. Conversely, if you have too many\n",
            "false negatives (i.e., defective products that the system does not flag as defective), you\n",
            "can increase the threshold. This is the usual precision/recall trade-off (see Chapter 3).\n",
            "Here is how you would identify the outliers using the fourth percentile lowest density\n",
            "as the threshold (i.e., approximately 4% of the instances will be flagged as anomalies):\n",
            "\n",
            "densities = gm.score_samples(X)\n",
            "density_threshold = np.percentile(densities, 2)\n",
            "anomalies = X[densities < density_threshold]\n",
            "\n",
            "Figure 9-18 represents these anomalies as stars.\n",
            "\n",
            "A closely related task is novelty detection: it differs from anomaly detection in that the\n",
            "algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\n",
            "whereas anomaly detection does not make this assumption. Indeed, outlier detection\n",
            "is often used to clean up a dataset.\n",
            "\n",
            "Gaussian mixture models try to fit all the data, including the outli‐\n",
            "ers;  if  you  have  too  many  of  them  this  will  bias  the  model’s  view\n",
            "of  “normality”,  and  some  outliers  may  wrongly  be  considered  as\n",
            "normal.  If  this  happens,  you  can  try  to  fit  the  model  once,  use  it\n",
            "to detect and remove the most extreme outliers, then fit the model\n",
            "again on the cleaned-up dataset. Another approach is to use robust\n",
            "covariance estimation methods (see the EllipticEnvelope class).\n",
            "\n",
            "288 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fFigure 9-18. Anomaly detection using a Gaussian mixture model\n",
            "\n",
            "Just like k-means, the GaussianMixture algorithm requires you to specify the number\n",
            "of clusters. So how can you find that number?\n",
            "\n",
            "Selecting the Number of Clusters\n",
            "With k-means, you can use the inertia or the silhouette score to select the appropriate\n",
            "number of clusters. But with Gaussian mixtures, it is not possible to use these metrics\n",
            "because  they  are  not  reliable  when  the  clusters  are  not  spherical  or  have  different\n",
            "sizes. Instead, you can try to find the model that minimizes a theoretical information\n",
            "criterion, such as the Bayesian information criterion (BIC) or the Akaike information\n",
            "criterion (AIC), defined in Equation 9-1.\n",
            "\n",
            "Equation 9-1. Bayesian information criterion (BIC) and Akaike information\n",
            "criterion (AIC)\n",
            "\n",
            "BI C =\n",
            "\n",
            "log m p − 2 log ℒ\n",
            "\n",
            "AI C = 2p − 2 log ℒ\n",
            "\n",
            "In these equations:\n",
            "\n",
            "• m is the number of instances, as always.\n",
            "•\n",
            "\n",
            "•\n",
            "• p is the number of parameters learned by the model.\n",
            "\n",
            "• ℒ  is the maximized value of the likelihood function of the model.\n",
            "•\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "289\n",
            "\n",
            "\fBoth the BIC and the AIC penalize models that have more parameters to learn (e.g.,\n",
            "more clusters) and reward models that fit the data well. They often end up selecting\n",
            "the same model. When they differ, the model selected by the BIC tends to be simpler\n",
            "(fewer  parameters)  than  the  one  selected  by  the  AIC,  but  tends  to  not  fit  the  data\n",
            "quite as well (this is especially true for larger datasets).\n",
            "\n",
            "Likelihood Function\n",
            "The terms “probability” and “likelihood” are often used interchangeably in everyday\n",
            "language, but they have very different meanings in statistics. Given a statistical model\n",
            "with  some  parameters  θ,  the  word  “probability”  is  used  to  describe  how  plausible  a\n",
            "future  outcome  x  is  (knowing  the  parameter  values  θ),  while  the  word  “likelihood”\n",
            "is used to describe how plausible a particular set of parameter values θ are, after the\n",
            "outcome x is known.\n",
            "\n",
            "Consider a 1D mixture model of two Gaussian distributions centered at –4 and +1.\n",
            "For simplicity, this toy model has a single parameter θ that controls the standard devi‐\n",
            "ations of both distributions. The top-left contour plot in Figure 9-19 shows the entire\n",
            "model f(x; θ) as a function of both x and θ. To estimate the probability distribution of\n",
            "a future outcome x, you need to set the model parameter θ. For example, if you set θ\n",
            "to 1.3 (the horizontal line), you get the probability density function f(x; θ=1.3) shown\n",
            "in the lower-left plot. Say you want to estimate the probability that x will fall between\n",
            "–2 and +2. You must calculate the integral of the PDF on this range (i.e., the surface of\n",
            "the shaded region). But what if you don’t know θ, and instead if you have observed a\n",
            "single instance x=2.5 (the vertical line in the upper-left plot)? In this case, you get the\n",
            "likelihood function ℒ(θ|x=2.5)=f(x=2.5; θ), represented in the upper-right plot.\n",
            "\n",
            "Figure 9-19. A model’s parametric function (top left), and some derived functions: a PDF\n",
            "(lower left), a likelihood function (top right), and a log likelihood function (lower right)\n",
            "\n",
            "290 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fIn short, the PDF is a function of x (with θ fixed), while the likelihood function is a\n",
            "function of θ (with x fixed). It is important to understand that the likelihood function\n",
            "is  not  a  probability  distribution:  if  you  integrate  a  probability  distribution  over  all\n",
            "possible values of x, you always get 1, but if you integrate the likelihood function over\n",
            "all possible values of θ the result can be any positive value.\n",
            "\n",
            "Given a dataset X, a common task is to try to estimate the most likely values for the\n",
            "model parameters. To do this, you must find the values that maximize the likelihood\n",
            "function, given X. In this example, if you have observed a single instance x=2.5, the\n",
            "maximum likelihood estimate (MLE) of θ is  θ=1.5. If a prior probability distribution\n",
            "g over θ exists, it is possible to take it into account by maximizing ℒ(θ|x)g(θ) rather\n",
            "than just maximizing ℒ(θ|x). This is called maximum a-posteriori (MAP) estimation.\n",
            "Since  MAP  constrains  the  parameter  values,  you  can  think  of  it  as  a  regularized\n",
            "version of MLE.\n",
            "\n",
            "Notice that maximizing the likelihood function is equivalent to maximizing its loga‐\n",
            "rithm (represented in the lower-right plot in Figure 9-19). Indeed, the logarithm is a\n",
            "strictly increasing function, so if θ maximizes the log likelihood, it also maximizes the\n",
            "likelihood. It turns out that it is generally easier to maximize the log likelihood. For\n",
            "example, if you observed several independent instances x(1) to x(m), you would need to\n",
            "find the value of θ that maximizes the product of the individual likelihood functions.\n",
            "But  it  is  equivalent,  and  much  simpler,  to  maximize  the  sum  (not  the  product)  of\n",
            "the  log  likelihood  functions,  thanks  to  the  magic  of  the  logarithm  which  converts\n",
            "products into sums: log(ab) = log(a) + log(b).\n",
            "\n",
            "Once  you  have  estimated  θ,  the  value  of  θ  that  maximizes  the  likelihood  function,\n",
            "then you are ready to compute ℒ = ℒ θ, X , which is the value used to compute the\n",
            "AIC and BIC; you can think of it as a measure of how well the model fits the data.\n",
            "\n",
            "To compute the BIC and AIC, call the bic() and aic() methods:\n",
            "\n",
            ">>> gm.bic(X)\n",
            "8189.747000497186\n",
            ">>> gm.aic(X)\n",
            "8102.521720382148\n",
            "\n",
            "Figure 9-20 shows the BIC for different numbers of clusters k. As you can see, both\n",
            "the BIC and the AIC are lowest when k=3, so it is most likely the best choice.\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "291\n",
            "\n",
            "\fFigure 9-20. AIC and BIC for different numbers of clusters k\n",
            "\n",
            "Bayesian Gaussian Mixture Models\n",
            "Rather  than  manually  searching  for  the  optimal  number  of  clusters,  you  can  use\n",
            "the  BayesianGaussianMixture  class,  which  is  capable  of  giving  weights  equal  (or\n",
            "close)  to  zero  to  unnecessary  clusters.  Set  the  number  of  clusters  n_components  to\n",
            "a  value  that  you  have  good  reason  to  believe  is  greater  than  the  optimal  number  of\n",
            "clusters (this assumes some minimal knowledge about the problem at hand), and the\n",
            "algorithm will eliminate the unnecessary clusters automatically. For example, let’s set\n",
            "the number of clusters to 10 and see what happens:\n",
            "\n",
            ">>> from sklearn.mixture import BayesianGaussianMixture\n",
            ">>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n",
            ">>> bgm.fit(X)\n",
            ">>> bgm.weights_.round(2)\n",
            "array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\n",
            "\n",
            "Perfect: the algorithm automatically detected that only three clusters are needed, and\n",
            "the resulting clusters are almost identical to the ones in Figure 9-16.\n",
            "\n",
            "A  final  note  about  Gaussian  mixture  models:  although  they  work  great  on  clusters\n",
            "with  ellipsoidal  shapes,  they  don’t  do  so  well  with  clusters  of  very  different  shapes.\n",
            "For example, let’s see what happens if we use a Bayesian Gaussian mixture model to\n",
            "cluster the moons dataset (see Figure 9-21).\n",
            "\n",
            "Oops!  The  algorithm  desperately  searched  for  ellipsoids,  so  it  found  eight  different\n",
            "clusters  instead  of  two.  The  density  estimation  is  not  too  bad,  so  this  model  could\n",
            "perhaps  be  used  for  anomaly  detection,  but  it  failed  to  identify  the  two  moons.  To\n",
            "conclude  this  chapter,  let’s  take  a  quick  look  at  a  few  algorithms  capable  of  dealing\n",
            "with arbitrarily shaped clusters.\n",
            "\n",
            "292 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fFigure 9-21. Fitting a Gaussian mixture to nonellipsoidal clusters\n",
            "\n",
            "Other Algorithms for Anomaly and Novelty Detection\n",
            "Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty\n",
            "detection:\n",
            "\n",
            "Fast-MCD (minimum covariance determinant)\n",
            "\n",
            "Implemented by the EllipticEnvelope class, this algorithm is useful for outlier\n",
            "detection, in particular to clean up a dataset. It assumes that the normal instances\n",
            "(inliers) are generated from a single Gaussian distribution (not a mixture). It also\n",
            "assumes  that  the  dataset  is  contaminated  with  outliers  that  were  not  generated\n",
            "from  this  Gaussian  distribution.  When  the  algorithm  estimates  the  parameters\n",
            "of  the  Gaussian  distribution  (i.e.,  the  shape  of  the  elliptic  envelope  around  the\n",
            "inliers),  it  is  careful  to  ignore  the  instances  that  are  most  likely  outliers.  This\n",
            "technique  gives  a  better  estimation  of  the  elliptic  envelope  and  thus  makes  the\n",
            "algorithm better at identifying the outliers.\n",
            "\n",
            "Isolation forest\n",
            "\n",
            "This is an efficient algorithm for outlier detection, especially in high-dimensional\n",
            "datasets.  The  algorithm  builds  a  random  forest  in  which  each  decision  tree  is\n",
            "grown  randomly:  at  each  node,  it  picks  a  feature  randomly,  then  it  picks  a\n",
            "random threshold value (between the min and max values) to split the dataset in\n",
            "two.  The  dataset  gradually  gets  chopped  into  pieces  this  way,  until  all  instances\n",
            "end  up  isolated  from  the  other  instances.  Anomalies  are  usually  far  from  other\n",
            "instances, so on average (across all the decision trees) they tend to get isolated in\n",
            "fewer steps than normal instances.\n",
            "\n",
            "Local outlier factor (LOF)\n",
            "\n",
            "This  algorithm  is  also  good  for  outlier  detection.  It  compares  the  density  of\n",
            "instances around a given instance to the density around its neighbors. An anom‐\n",
            "aly is often more isolated than its k-nearest neighbors.\n",
            "\n",
            "Gaussian Mixtures \n",
            "\n",
            "| \n",
            "\n",
            "293\n",
            "\n",
            "\fOne-class SVM\n",
            "\n",
            "This  algorithm  is  better  suited  for  novelty  detection.  Recall  that  a  kernelized\n",
            "SVM classifier separates two classes by first (implicitly) mapping all the instances\n",
            "to a high-dimensional space, then separating the two classes using a linear SVM\n",
            "classifier within this high-dimensional space (see Chapter 5). Since we just have\n",
            "one class of instances, the one-class SVM algorithm instead tries to separate the\n",
            "instances  in  high-dimensional  space  from  the  origin.  In  the  original  space,  this\n",
            "will  correspond  to  finding  a  small  region  that  encompasses  all  the  instances.\n",
            "If  a  new  instance  does  not  fall  within  this  region,  it  is  an  anomaly.  There  are\n",
            "a  few  hyperparameters  to  tweak:  the  usual  ones  for  a  kernelized  SVM,  plus  a\n",
            "margin  hyperparameter  that  corresponds  to  the  probability  of  a  new  instance\n",
            "being  mistakenly  considered  as  novel  when  it  is  in  fact  normal.  It  works  great,\n",
            "especially with high-dimensional datasets, but like all SVMs it does not scale to\n",
            "large datasets.\n",
            "\n",
            "PCA and other dimensionality reduction techniques with an inverse_transform()\n",
            "method\n",
            "\n",
            "If  you  compare  the  reconstruction  error  of  a  normal  instance  with  the  recon‐\n",
            "struction  error  of  an  anomaly,  the  latter  will  usually  be  much  larger.  This  is  a\n",
            "simple  and  often  quite  efficient  anomaly  detection  approach  (see  this  chapter’s\n",
            "exercises for an example).\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. How would you define clustering? Can you name a few clustering algorithms?\n",
            "\n",
            "2.\n",
            "2. What are some of the main applications of clustering algorithms?\n",
            "\n",
            "3.\n",
            "3. Describe  two  techniques  to  select  the  right  number  of  clusters  when  using\n",
            "\n",
            "k-means.\n",
            "\n",
            "4. What is label propagation? Why would you implement it, and how?\n",
            "4.\n",
            "\n",
            "5.\n",
            "5. Can you name two clustering algorithms that can scale to large datasets? And two\n",
            "\n",
            "that look for regions of high density?\n",
            "\n",
            "6. Can you think of a use case where active learning would be useful? How would\n",
            "6.\n",
            "\n",
            "you implement it?\n",
            "\n",
            "7.\n",
            "7. What is the difference between anomaly detection and novelty detection?\n",
            "\n",
            "8.\n",
            "8. What is a Gaussian mixture? What tasks can you use it for?\n",
            "\n",
            "9.\n",
            "9. Can you name two techniques to find the right number of clusters when using a\n",
            "\n",
            "Gaussian mixture model?\n",
            "\n",
            "10. The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of\n",
            "10.\n",
            "faces. Each image is flattened to a 1D vector of size 4,096. Forty different people\n",
            "were  photographed  (10  times  each),  and  the  usual  task  is  to  train  a  model  that\n",
            "\n",
            "294 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 9: Unsupervised Learning Techniques\n",
            "\n",
            "\fcan predict which person is represented in each picture. Load the dataset using\n",
            "the  sklearn.datasets.fetch_olivetti_faces()  function,  then  split  it  into  a\n",
            "training set, a validation set, and a test set (note that the dataset is already scaled\n",
            "between  0  and  1).  Since  the  dataset  is  quite  small,  you  will  probably  want  to\n",
            "use  stratified  sampling  to  ensure  that  there  are  the  same  number  of  images  per\n",
            "person in each set. Next, cluster the images using k-means, and ensure that you\n",
            "have  a  good  number  of  clusters  (using  one  of  the  techniques  discussed  in  this\n",
            "chapter). Visualize the clusters: do you see similar faces in each cluster?\n",
            "\n",
            "11. Continuing  with  the  Olivetti  faces  dataset,  train  a  classifier  to  predict  which\n",
            "11.\n",
            "person is represented in each picture, and evaluate it on the validation set. Next,\n",
            "use  k-means  as  a  dimensionality  reduction  tool,  and  train  a  classifier  on  the\n",
            "reduced  set.  Search  for  the  number  of  clusters  that  allows  the  classifier  to  get\n",
            "the best performance: what performance can you reach? What if you append the\n",
            "features  from  the  reduced  set  to  the  original  features  (again,  searching  for  the\n",
            "best number of clusters)?\n",
            "\n",
            "12. Train  a  Gaussian  mixture  model  on  the  Olivetti  faces  dataset.  To  speed  up  the\n",
            "12.\n",
            "algorithm,  you  should  probably  reduce  the  dataset’s  dimensionality  (e.g.,  use\n",
            "PCA,  preserving  99%  of  the  variance).  Use  the  model  to  generate  some  new\n",
            "faces  (using  the  sample()  method),  and  visualize  them  (if  you  used  PCA,  you\n",
            "will need to use its inverse_transform() method). Try to modify some images\n",
            "(e.g.,  rotate,  flip,  darken)  and  see  if  the  model  can  detect  the  anomalies  (i.e.,\n",
            "compare the output of the score_samples() method for normal images and for\n",
            "anomalies).\n",
            "\n",
            "13. Some dimensionality reduction techniques can also be used for anomaly detec‐\n",
            "13.\n",
            "tion. For example, take the Olivetti faces dataset and reduce it with PCA, preserv‐\n",
            "ing 99% of the variance. Then compute the reconstruction error for each image.\n",
            "Next,  take  some  of  the  modified  images  you  built  in  the  previous  exercise  and\n",
            "look  at  their  reconstruction  error:  notice  how  much  larger  it  is.  If  you  plot  a\n",
            "reconstructed image, you will see why: it tries to reconstruct a normal face.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "295\n",
            "\n",
            "\f\fPART II\n",
            "Neural Networks and Deep Learning\n",
            "\n",
            "\f\fCHAPTER 10\n",
            "Introduction to Artificial Neural\n",
            "Networks with Keras\n",
            "\n",
            "Birds  inspired  us  to  fly,  burdock  plants  inspired  Velcro,  and  nature  has  inspired\n",
            "countless more inventions. It seems only logical, then, to look at the brain’s architec‐\n",
            "ture  for  inspiration  on  how  to  build  an  intelligent  machine.  This  is  the  logic  that\n",
            "sparked artificial neural networks (ANNs), machine learning models inspired by the\n",
            "networks of biological neurons found in our brains. However, although planes were\n",
            "inspired  by  birds,  they  don’t  have  to  flap  their  wings  to  fly.  Similarly,  ANNs  have\n",
            "gradually  become  quite  different  from  their  biological  cousins.  Some  researchers\n",
            "even  argue  that  we  should  drop  the  biological  analogy  altogether  (e.g.,  by  saying\n",
            "“units” rather than “neurons”), lest we restrict our creativity to biologically plausible\n",
            "systems.1\n",
            "\n",
            "ANNs are at the very core of deep learning. They are versatile, powerful, and scalable,\n",
            "making  them  ideal  to  tackle  large  and  highly  complex  machine  learning  tasks  such\n",
            "as  classifying  billions  of  images  (e.g.,  Google  Images),  powering  speech  recognition\n",
            "services  (e.g.,  Apple’s  Siri),  recommending  the  best  videos  to  watch  to  hundreds  of\n",
            "millions of users every day (e.g., YouTube), or learning to beat the world champion at\n",
            "the game of Go (DeepMind’s AlphaGo).\n",
            "\n",
            "The  first  part  of  this  chapter  introduces  artificial  neural  networks,  starting  with  a\n",
            "quick tour of the very first ANN architectures and leading up to multilayer percep‐\n",
            "trons, which are heavily used today (other architectures will be explored in the next\n",
            "chapters).  In  the  second  part,  we  will  look  at  how  to  implement  neural  networks\n",
            "using  TensorFlow’s  Keras  API.  This  is  a  beautifully  designed  and  simple  high-level\n",
            "\n",
            "1 You can get the best of both worlds by being open to biological inspirations without being afraid to create\n",
            "\n",
            "biologically unrealistic models, as long as they work well.\n",
            "\n",
            "299\n",
            "\n",
            "\fAPI  for  building,  training,  evaluating,  and  running  neural  networks.  But  don’t  be\n",
            "fooled  by  its  simplicity:  it  is  expressive  and  flexible  enough  to  let  you  build  a  wide\n",
            "variety of neural network architectures. In fact, it will probably be sufficient for most\n",
            "of  your  use  cases.  And  should  you  ever  need  extra  flexibility,  you  can  always  write\n",
            "custom Keras components using its lower-level API, or even use TensorFlow directly,\n",
            "as you will see in Chapter 12.\n",
            "\n",
            "But first, let’s go back in time to see how artificial neural networks came to be!\n",
            "\n",
            "From Biological to Artificial Neurons\n",
            "Surprisingly,  ANNs  have  been  around  for  quite  a  while:  they  were  first  introduced\n",
            "back  in  1943  by  the  neurophysiologist  Warren  McCulloch  and  the  mathematician\n",
            "Walter  Pitts.  In  their  landmark  paper2  “A  Logical  Calculus  of  Ideas  Immanent  in\n",
            "Nervous Activity”, McCulloch and Pitts presented a simplified computational model\n",
            "of how biological neurons might work together in animal brains to perform complex\n",
            "computations  using  propositional  logic.  This  was  the  first  artificial  neural  network\n",
            "architecture. Since then many other architectures have been invented, as you will see.\n",
            "\n",
            "The  early  successes  of  ANNs  led  to  the  widespread  belief  that  we  would  soon  be\n",
            "conversing  with  truly  intelligent  machines.  When  it  became  clear  in  the  1960s  that\n",
            "this promise would go unfulfilled (at least for quite a while), funding flew elsewhere,\n",
            "and ANNs entered a long winter. In the early 1980s, new architectures were invented\n",
            "and  better  training  techniques  were  developed,  sparking  a  revival  of  interest  in\n",
            "connectionism, the study of neural networks. But progress was slow, and by the 1990s\n",
            "other  powerful  machine  learning  techniques  had  been  invented,  such  as  support\n",
            "vector  machines  (see  Chapter  5).  These  techniques  seemed  to  offer  better  results\n",
            "and  stronger  theoretical  foundations  than  ANNs,  so  once  again  the  study  of  neural\n",
            "networks was put on hold.\n",
            "\n",
            "We are now witnessing yet another wave of interest in ANNs. Will this wave die out\n",
            "like the previous ones did? Well, here are a few good reasons to believe that this time\n",
            "is different and that the renewed interest in ANNs will have a much more profound\n",
            "impact on our lives:\n",
            "\n",
            "• There  is  now  a  huge  quantity  of  data  available  to  train  neural  networks,  and\n",
            "•\n",
            "ANNs  frequently  outperform  other  ML  techniques  on  very  large  and  complex\n",
            "problems.\n",
            "\n",
            "• The  tremendous  increase  in  computing  power  since  the  1990s  now  makes  it\n",
            "•\n",
            "possible  to  train  large  neural  networks  in  a  reasonable  amount  of  time.  This  is\n",
            "\n",
            "2 Warren S. McCulloch and Walter Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity”, The\n",
            "\n",
            "Bulletin of Mathematical Biology 5, no. 4 (1943): 115–113.\n",
            "\n",
            "300 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fin  part  due  to  Moore’s  law  (the  number  of  components  in  integrated  circuits\n",
            "has  doubled  about  every  2  years  over  the  last  50  years),  but  also  thanks  to  the\n",
            "gaming  industry,  which  has  stimulated  the  production  of  powerful  GPU  cards\n",
            "by  the  millions.  Moreover,  cloud  platforms  have  made  this  power  accessible  to\n",
            "everyone.\n",
            "\n",
            "• The  training  algorithms  have  been  improved.  To  be  fair  they  are  only  slightly\n",
            "•\n",
            "different from the ones used in the 1990s, but these relatively small tweaks have\n",
            "had a huge positive impact.\n",
            "\n",
            "• Some theoretical limitations of ANNs have turned out to be benign in practice.\n",
            "•\n",
            "For example, many people thought that ANN training algorithms were doomed\n",
            "because  they  were  likely  to  get  stuck  in  local  optima,  but  it  turns  out  that  this\n",
            "is  not  a  big  problem  in  practice,  especially  for  larger  neural  networks:  the  local\n",
            "optima often perform almost as well as the global optimum.\n",
            "\n",
            "• ANNs seem to have entered a virtuous circle of funding and progress. Amazing\n",
            "•\n",
            "products  based  on  ANNs  regularly  make  the  headline  news,  which  pulls  more\n",
            "and  more  attention  and  funding  toward  them,  resulting  in  more  and  more\n",
            "progress and even more amazing products.\n",
            "\n",
            "Biological Neurons\n",
            "Before  we  discuss  artificial  neurons,  let’s  take  a  quick  look  at  a  biological  neuron\n",
            "(represented  in  Figure  10-1).  It  is  an  unusual-looking  cell  mostly  found  in  animal\n",
            "brains.  It’s  composed  of  a  cell  body  containing  the  nucleus  and  most  of  the  cell’s\n",
            "complex components, many branching extensions called dendrites, plus one very long\n",
            "extension called the axon. The axon’s length may be just a few times longer than the\n",
            "cell body, or up to tens of thousands of times longer. Near its extremity the axon splits\n",
            "off into many branches called telodendria, and at the tip of these branches are minus‐\n",
            "cule  structures  called  synaptic  terminals  (or  simply  synapses),  which  are  connected\n",
            "to  the  dendrites  or  cell  bodies  of  other  neurons.3  Biological  neurons  produce  short\n",
            "electrical  impulses  called  action  potentials  (APs,  or  just  signals),  which  travel  along\n",
            "the  axons  and  make  the  synapses  release  chemical  signals  called  neurotransmitters.\n",
            "When  a  neuron  receives  a  sufficient  amount  of  these  neurotransmitters  within  a\n",
            "few  milliseconds,  it  fires  its  own  electrical  impulses  (actually,  it  depends  on  the\n",
            "neurotransmitters, as some of them inhibit the neuron from firing).\n",
            "\n",
            "3 They are not actually attached, just so close that they can very quickly exchange chemical signals.\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "301\n",
            "\n",
            "\fFigure 10-1. A biological neuron4\n",
            "\n",
            "Thus,  individual  biological  neurons  seem  to  behave  in  a  simple  way,  but  they’re\n",
            "organized  in  a  vast  network  of  billions,  with  each  neuron  typically  connected  to\n",
            "thousands  of  other  neurons.  Highly  complex  computations  can  be  performed  by\n",
            "a  network  of  fairly  simple  neurons,  much  like  a  complex  anthill  can  emerge  from\n",
            "the  combined  efforts  of  simple  ants.  The  architecture  of  biological  neural  networks\n",
            "(BNNs)5 is the subject of active research, but some parts of the brain have been map‐\n",
            "ped. These efforts show that neurons are often organized in consecutive layers, espe‐\n",
            "cially in the cerebral cortex (the outer layer of the brain), as shown in Figure 10-2.\n",
            "\n",
            "Figure 10-2. Multiple layers in a biological neural network (human cortex)6\n",
            "\n",
            "4 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from https://en.wikipedia.org/wiki/Neuron.\n",
            "\n",
            "5 In the context of machine learning, the phrase “neural networks” generally refers to ANNs, not BNNs.\n",
            "\n",
            "6 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe\n",
            "\n",
            "dia.org/wiki/Cerebral_cortex.\n",
            "\n",
            "302 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fLogical Computations with Neurons\n",
            "McCulloch and Pitts proposed a very simple model of the biological neuron, which\n",
            "later became known as an artificial neuron: it has one or more binary (on/off) inputs\n",
            "and  one  binary  output.  The  artificial  neuron  activates  its  output  when  more  than  a\n",
            "certain number of its inputs are active. In their paper, McCulloch and Pitts showed\n",
            "that  even  with  such  a  simplified  model  it  is  possible  to  build  a  network  of  artificial\n",
            "neurons  that  can  compute  any  logical  proposition  you  want.  To  see  how  such  a\n",
            "network  works,  let’s  build  a  few  ANNs  that  perform  various  logical  computations\n",
            "(see Figure 10-3), assuming that a neuron is activated when at least two of its input\n",
            "connections are active.\n",
            "\n",
            "Figure 10-3. ANNs performing simple logical computations\n",
            "\n",
            "Let’s see what these networks do:\n",
            "\n",
            "• The  first  network  on  the  left  is  the  identity  function:  if  neuron  A  is  activated,\n",
            "•\n",
            "then  neuron  C  gets  activated  as  well  (since  it  receives  two  input  signals  from\n",
            "neuron A); but if neuron A is off, then neuron C is off as well.\n",
            "\n",
            "•\n",
            "• The  second  network  performs  a  logical  AND:  neuron  C  is  activated  only  when\n",
            "both  neurons  A  and  B  are  activated  (a  single  input  signal  is  not  enough  to\n",
            "activate neuron C).\n",
            "\n",
            "• The  third  network  performs  a  logical  OR:  neuron  C  gets  activated  if  either\n",
            "•\n",
            "\n",
            "neuron A or neuron B is activated (or both).\n",
            "\n",
            "• Finally, if we suppose that an input connection can inhibit the neuron’s activity\n",
            "•\n",
            "(which is the case with biological neurons), then the fourth network computes a\n",
            "slightly more complex logical proposition: neuron C is activated only if neuron\n",
            "A is active and neuron B is off. If neuron A is active all the time, then you get a\n",
            "logical NOT: neuron C is active when neuron B is off, and vice versa.\n",
            "\n",
            "You can imagine how these networks can be combined to compute complex logical\n",
            "expressions (see the exercises at the end of the chapter for an example).\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "303\n",
            "\n",
            "\fThe Perceptron\n",
            "The perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\n",
            "Rosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called\n",
            "a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs\n",
            "and output are numbers (instead of binary on/off values), and each input connection\n",
            "is associated with a weight. The TLU first computes a linear function of its inputs: z =\n",
            "w1 x1 + w2 x2 + ⋯ + wn xn + b = w⊺ x + b. Then it applies a step function to the result:\n",
            "hw(x)  =  step(z).  So  it’s  almost  like  logistic  regression,  except  it  uses  a  step  function\n",
            "instead of the logistic function (Chapter 4). Just like in logistic regression, the model\n",
            "parameters are the input weights w and the bias term b.\n",
            "\n",
            "Figure 10-4. TLU: an artificial neuron that computes a weighted sum of its inputs w⊺ x,\n",
            "plus a bias term b, then applies a step function\n",
            "\n",
            "The  most  common  step  function  used  in  perceptrons  is  the  Heaviside  step  function\n",
            "(see Equation 10-1). Sometimes the sign function is used instead.\n",
            "\n",
            "Equation 10-1. Common step functions used in perceptrons (assuming threshold = 0)\n",
            "\n",
            "heaviside z =\n",
            "\n",
            "0 if z < 0\n",
            "1 if z ≥ 0\n",
            "\n",
            "sgn z =\n",
            "\n",
            "−1 if z < 0\n",
            "if z = 0\n",
            "0\n",
            "+1 if z > 0\n",
            "\n",
            "304 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fA single TLU can be used for simple linear binary classification. It computes a linear\n",
            "function  of  its  inputs,  and  if  the  result  exceeds  a  threshold,  it  outputs  the  positive\n",
            "class. Otherwise, it outputs the negative class. This may remind you of logistic regres‐\n",
            "sion (Chapter 4) or linear SVM classification (Chapter 5). You could, for example, use\n",
            "a single TLU to classify iris flowers based on petal length and width. Training such a\n",
            "TLU would require finding the right values for w1, w2, and b (the training algorithm is\n",
            "discussed shortly).\n",
            "\n",
            "A  perceptron  is  composed  of  one  or  more  TLUs  organized  in  a  single  layer,  where\n",
            "every TLU is connected to every input. Such a layer is called a fully connected layer,\n",
            "or  a  dense  layer.  The  inputs  constitute  the  input  layer.  And  since  the  layer  of  TLUs\n",
            "produces  the  final  outputs,  it  is  called  the  output  layer.  For  example,  a  perceptron\n",
            "with two inputs and three outputs is represented in Figure 10-5.\n",
            "\n",
            "Figure 10-5. Architecture of a perceptron with two inputs and three output neurons\n",
            "\n",
            "This  perceptron  can  classify  instances  simultaneously  into  three  different  binary\n",
            "classes,  which  makes  it  a  multilabel  classifier.  It  may  also  be  used  for  multiclass\n",
            "classification.\n",
            "\n",
            "Thanks  to  the  magic  of  linear  algebra,  Equation  10-2  can  be  used  to  efficiently\n",
            "compute the outputs of a layer of artificial neurons for several instances at once.\n",
            "\n",
            "Equation 10-2. Computing the outputs of a fully connected layer\n",
            "\n",
            "ℎW, b X = ϕ XW + b\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "305\n",
            "\n",
            "\fIn this equation:\n",
            "\n",
            "•\n",
            "• As always, X represents the matrix of input features. It has one row per instance\n",
            "\n",
            "and one column per feature.\n",
            "\n",
            "•\n",
            "• The  weight  matrix  W  contains  all  the  connection  weights.  It  has  one  row  per\n",
            "\n",
            "input and one column per neuron.\n",
            "\n",
            "• The bias vector b contains all the bias terms: one per neuron.\n",
            "•\n",
            "\n",
            "• The  function  ϕ  is  called  the  activation  function:  when  the  artificial  neurons  are\n",
            "•\n",
            "TLUs, it is a step function (we will discuss other activation functions shortly).\n",
            "\n",
            "In  mathematics,  the  sum  of  a  matrix  and  a  vector  is  undefined.\n",
            "However, in data science, we allow “broadcasting”: adding a vector\n",
            "to a matrix means adding it to every row in the matrix. So, XW + b\n",
            "first  multiplies  X  by  W—which  results  in  a  matrix  with  one  row\n",
            "per  instance  and  one  column  per  output—then  adds  the  vector  b\n",
            "to every row of that matrix, which adds each bias term to the corre‐\n",
            "sponding  output,  for  every  instance.  Moreover,  ϕ  is  then  applied\n",
            "itemwise to each item in the resulting matrix.\n",
            "\n",
            "So,  how  is  a  perceptron  trained?  The  perceptron  training  algorithm  proposed  by\n",
            "Rosenblatt  was  largely  inspired  by  Hebb’s  rule.  In  his  1949  book  The  Organization\n",
            "of  Behavior  (Wiley),  Donald  Hebb  suggested  that  when  a  biological  neuron  triggers\n",
            "another neuron often, the connection between these two neurons grows stronger. Sie‐\n",
            "grid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together,\n",
            "wire together”; that is, the connection weight between two neurons tends to increase\n",
            "when  they  fire  simultaneously.  This  rule  later  became  known  as  Hebb’s  rule  (or\n",
            "Hebbian learning). Perceptrons are trained using a variant of this rule that takes into\n",
            "account the error made by the network when it makes a prediction; the perceptron\n",
            "learning rule reinforces connections that help reduce the error. More specifically, the\n",
            "perceptron is fed one training instance at a time, and for each instance it makes its\n",
            "predictions. For every output neuron that produced a wrong prediction, it reinforces\n",
            "the  connection  weights  from  the  inputs  that  would  have  contributed  to  the  correct\n",
            "prediction. The rule is shown in Equation 10-3.\n",
            "\n",
            "Equation 10-3. Perceptron learning rule (weight update)\n",
            "\n",
            "wi, j\n",
            "\n",
            "next step = wi, j + η yj − y j xi\n",
            "\n",
            "306 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fIn this equation:\n",
            "\n",
            "• wi, j is the connection weight between the ith input and the jth neuron.\n",
            "•\n",
            "• xi is the ith input value of the current training instance.\n",
            "•\n",
            "• y j is the output of the jth output neuron for the current training instance.\n",
            "•\n",
            "• yj is the target output of the jth output neuron for the current training instance.\n",
            "•\n",
            "•\n",
            "• η is the learning rate (see Chapter 4).\n",
            "\n",
            "The decision boundary of each output neuron is linear, so perceptrons are incapable\n",
            "of learning complex patterns (just like logistic regression classifiers). However, if the\n",
            "training instances are linearly separable, Rosenblatt demonstrated that this algorithm\n",
            "would converge to a solution.7 This is called the perceptron convergence theorem.\n",
            "\n",
            "Scikit-Learn provides a Perceptron class that can be used pretty much as you would\n",
            "expect—for example, on the iris dataset (introduced in Chapter 4):\n",
            "\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.linear_model import Perceptron\n",
            "\n",
            "iris = load_iris(as_frame=True)\n",
            "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
            "y = (iris.target == 0)  # Iris setosa\n",
            "\n",
            "per_clf = Perceptron(random_state=42)\n",
            "per_clf.fit(X, y)\n",
            "\n",
            "X_new = [[2, 0.5], [3, 1]]\n",
            "y_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers\n",
            "\n",
            "You may have noticed that the perceptron learning algorithm strongly resembles sto‐\n",
            "chastic gradient descent (introduced in Chapter 4). In fact, Scikit-Learn’s Perceptron\n",
            "class  is  equivalent  to  using  an  SGDClassifier  with  the  following  hyperparameters:\n",
            "loss=\"perceptron\",  learning_rate=\"constant\",  eta0=1  (the  learning  rate),  and\n",
            "penalty=None (no regularization).\n",
            "\n",
            "In  their  1969  monograph  Perceptrons,  Marvin  Minsky  and  Seymour  Papert  high‐\n",
            "lighted  a  number  of  serious  weaknesses  of  perceptrons—in  particular,  the  fact  that\n",
            "they  are  incapable  of  solving  some  trivial  problems  (e.g.,  the  exclusive  OR  (XOR)\n",
            "classification  problem;  see  the  left  side  of  Figure  10-6).  This  is  true  of  any  other\n",
            "linear classification model (such as logistic regression classifiers), but researchers had\n",
            "expected  much  more  from  perceptrons,  and  some  were  so  disappointed  that  they\n",
            "\n",
            "7 Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyper‐\n",
            "\n",
            "planes that can separate them.\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "307\n",
            "\n",
            "\fdropped neural networks altogether in favor of higher-level problems such as logic,\n",
            "problem solving, and search. The lack of practical applications also didn’t help.\n",
            "\n",
            "It turns out that some of the limitations of perceptrons can be eliminated by stacking\n",
            "multiple perceptrons. The resulting ANN is called a multilayer perceptron (MLP). An\n",
            "MLP can solve the XOR problem, as you can verify by computing the output of the\n",
            "MLP  represented  on  the  right  side  of  Figure  10-6:  with  inputs  (0,  0)  or  (1,  1),  the\n",
            "network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. Try verifying that this\n",
            "network indeed solves the XOR problem!8\n",
            "\n",
            "Figure 10-6. XOR classification problem and an MLP that solves it\n",
            "\n",
            "Contrary to logistic regression classifiers, perceptrons do not out‐\n",
            "put a class probability. This is one reason to prefer logistic regres‐\n",
            "sion  over  perceptrons.  Moreover,  perceptrons  do  not  use  any\n",
            "regularization by default, and training stops as soon as there are no\n",
            "more  prediction  errors  on  the  training  set,  so  the  model  typically\n",
            "does  not  generalize  as  well  as  logistic  regression  or  a  linear  SVM\n",
            "classifier. However, perceptrons may train a bit faster.\n",
            "\n",
            "8 For example, when the inputs are (0, 1) the lower-left neuron computes 0 × 1 + 1 × 1 – 3 / 2 = –1 / 2, which\n",
            "is negative, so it outputs 0. The lower-right neuron computes 0 × 1 + 1 × 1 – 1 / 2 = 1 / 2, which is positive,\n",
            "so it outputs 1. The output neuron receives the outputs of the first two neurons as its inputs, so it computes\n",
            "0 × (–1) + 1 × 1 - 1 / 2 = 1 / 2. This is positive, so it outputs 1.\n",
            "\n",
            "308 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fThe Multilayer Perceptron and Backpropagation\n",
            "An  MLP  is  composed  of  one  input  layer,  one  or  more  layers  of  TLUs  called  hidden\n",
            "layers, and one final layer of TLUs called the output layer (see Figure 10-7). The layers\n",
            "close to the input layer are usually called the lower layers, and the ones close to the\n",
            "outputs are usually called the upper layers.\n",
            "\n",
            "Figure 10-7. Architecture of a multilayer perceptron with two inputs, one hidden layer of\n",
            "four neurons, and three output neurons\n",
            "\n",
            "The  signal  flows  only  in  one  direction  (from  the  inputs  to  the\n",
            "outputs), so this architecture is an example of a feedforward neural\n",
            "network (FNN).\n",
            "\n",
            "When  an  ANN  contains  a  deep  stack  of  hidden  layers,9  it  is  called  a  deep  neural\n",
            "network  (DNN).  The  field  of  deep  learning  studies  DNNs,  and  more  generally  it  is\n",
            "interested in models containing deep stacks of computations. Even so, many people\n",
            "talk about deep learning whenever neural networks are involved (even shallow ones).\n",
            "\n",
            "9 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\n",
            "\n",
            "ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "309\n",
            "\n",
            "\fFor many years researchers struggled to find a way to train MLPs, without success. In\n",
            "the early 1960s several researchers discussed the possibility of using gradient descent\n",
            "to  train  neural  networks,  but  as  we  saw  in  Chapter  4,  this  requires  computing  the\n",
            "gradients of the model’s error with regard to the model parameters; it wasn’t clear at\n",
            "the  time  how  to  do  this  efficiently  with  such  a  complex  model  containing  so  many\n",
            "parameters, especially with the computers they had back then.\n",
            "\n",
            "Then,  in  1970,  a  researcher  named  Seppo  Linnainmaa  introduced  in  his  master’s\n",
            "thesis  a  technique  to  compute  all  the  gradients  automatically  and  efficiently.  This\n",
            "algorithm is now called reverse-mode automatic differentiation (or reverse-mode auto‐\n",
            "diff for short). In just two passes through the network (one forward, one backward),\n",
            "it  is  able  to  compute  the  gradients  of  the  neural  network’s  error  with  regard  to\n",
            "every single model parameter. In other words, it can find out how each connection\n",
            "weight  and  each  bias  should  be  tweaked  in  order  to  reduce  the  neural  network’s\n",
            "error.  These  gradients  can  then  be  used  to  perform  a  gradient  descent  step.  If  you\n",
            "repeat  this  process  of  computing  the  gradients  automatically  and  taking  a  gradient\n",
            "descent step, the neural network’s error will gradually drop until it eventually reaches\n",
            "a minimum. This combination of reverse-mode autodiff and gradient descent is now\n",
            "called backpropagation (or backprop for short).\n",
            "\n",
            "There are various autodiff techniques, with different pros and cons.\n",
            "Reverse-mode  autodiff  is  well  suited  when  the  function  to  differ‐\n",
            "entiate  has  many  variables  (e.g.,  connection  weights  and  biases)\n",
            "and  few  outputs  (e.g.,  one  loss).  If  you  want  to  learn  more  about\n",
            "autodiff, check out Appendix B.\n",
            "\n",
            "Backpropagation  can  actually  be  applied  to  all  sorts  of  computational  graphs,  not\n",
            "just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural nets,\n",
            "it  was  more  general.  It  was  several  more  years  before  backprop  started  to  be  used\n",
            "to train neural networks, but it still wasn’t mainstream. Then, in 1985, David Rumel‐\n",
            "hart,  Geoffrey  Hinton,  and  Ronald  Williams  published  a  groundbreaking  paper10\n",
            "analyzing  how  backpropagation  allowed  neural  networks  to  learn  useful  internal\n",
            "representations.  Their  results  were  so  impressive  that  backpropagation  was  quickly\n",
            "popularized  in  the  field.  Today,  it  is  by  far  the  most  popular  training  technique  for\n",
            "neural networks.\n",
            "\n",
            "10 David Rumelhart et al., “Learning Internal Representations by Error Propagation” (Defense Technical Infor‐\n",
            "\n",
            "mation Center technical report, September 1985).\n",
            "\n",
            "310 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fLet’s run through how backpropagation works again in a bit more detail:\n",
            "\n",
            "• It handles one mini-batch at a time (for example, containing 32 instances each),\n",
            "•\n",
            "and  it  goes  through  the  full  training  set  multiple  times.  Each  pass  is  called  an\n",
            "epoch.\n",
            "\n",
            "• Each  mini-batch  enters  the  network  through  the  input  layer.  The  algorithm\n",
            "•\n",
            "then  computes  the  output  of  all  the  neurons  in  the  first  hidden  layer,  for  every\n",
            "instance  in  the  mini-batch.  The  result  is  passed  on  to  the  next  layer,  its  output\n",
            "is  computed  and  passed  to  the  next  layer,  and  so  on  until  we  get  the  output  of\n",
            "the last layer, the output layer. This is the forward pass: it is exactly like making\n",
            "predictions,  except  all  intermediate  results  are  preserved  since  they  are  needed\n",
            "for the backward pass.\n",
            "\n",
            "• Next,  the  algorithm  measures  the  network’s  output  error  (i.e.,  it  uses  a  loss\n",
            "•\n",
            "function that compares the desired output and the actual output of the network,\n",
            "and returns some measure of the error).\n",
            "\n",
            "• Then it computes how much each output bias and each connection to the output\n",
            "•\n",
            "layer contributed to the error. This is done analytically by applying the chain rule\n",
            "(perhaps the most fundamental rule in calculus), which makes this step fast and\n",
            "precise.\n",
            "\n",
            "• The algorithm then measures how much of these error contributions came from\n",
            "•\n",
            "each connection in the layer below, again using the chain rule, working backward\n",
            "until  it  reaches  the  input  layer.  As  explained  earlier,  this  reverse  pass  efficiently\n",
            "measures  the  error  gradient  across  all  the  connection  weights  and  biases  in  the\n",
            "network by propagating the error gradient backward through the network (hence\n",
            "the name of the algorithm).\n",
            "\n",
            "•\n",
            "• Finally, the algorithm performs a gradient descent step to tweak all the connec‐\n",
            "\n",
            "tion weights in the network, using the error gradients it just computed.\n",
            "\n",
            "It is important to initialize all the hidden layers’ connection weights\n",
            "randomly, or else training will fail. For example, if you initialize all\n",
            "weights  and  biases  to  zero,  then  all  neurons  in  a  given  layer  will\n",
            "be  perfectly  identical,  and  thus  backpropagation  will  affect  them\n",
            "in  exactly  the  same  way,  so  they  will  remain  identical.  In  other\n",
            "words,  despite  having  hundreds  of  neurons  per  layer,  your  model\n",
            "will  act  as  if  it  had  only  one  neuron  per  layer:  it  won’t  be  too\n",
            "smart.  If  instead  you  randomly  initialize  the  weights,  you  break\n",
            "the symmetry and allow backpropagation to train a diverse team of\n",
            "neurons.\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "311\n",
            "\n",
            "\fIn short, backpropagation makes predictions for a mini-batch (forward pass), meas‐\n",
            "ures the error, then goes through each layer in reverse to measure the error contribu‐\n",
            "tion  from  each  parameter  (reverse  pass),  and  finally  tweaks  the  connection  weights\n",
            "and biases to reduce the error (gradient descent step).\n",
            "\n",
            "In  order  for  backprop  to  work  properly,  Rumelhart  and  his  colleagues  made  a  key\n",
            "change  to  the  MLP’s  architecture:  they  replaced  the  step  function  with  the  logistic\n",
            "function, σ(z) = 1 / (1 + exp(–z)), also called the sigmoid function. This was essential\n",
            "because the step function contains only flat segments, so there is no gradient to work\n",
            "with  (gradient  descent  cannot  move  on  a  flat  surface),  while  the  sigmoid  function\n",
            "has a well-defined nonzero derivative everywhere, allowing gradient descent to make\n",
            "some progress at every step. In fact, the backpropagation algorithm works well with\n",
            "many  other  activation  functions,  not  just  the  sigmoid  function.  Here  are  two  other\n",
            "popular choices:\n",
            "\n",
            "The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1\n",
            "\n",
            "Just  like  the  sigmoid  function,  this  activation  function  is  S-shaped,  continuous,\n",
            "and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in\n",
            "the case of the sigmoid function). That range tends to make each layer’s output\n",
            "more  or  less  centered  around  0  at  the  beginning  of  training,  which  often  helps\n",
            "speed up convergence.\n",
            "\n",
            "The rectified linear unit function: ReLU(z) = max(0, z)\n",
            "\n",
            "The  ReLU  function  is  continuous  but  unfortunately  not  differentiable  at  z  =  0\n",
            "(the  slope  changes  abruptly,  which  can  make  gradient  descent  bounce  around),\n",
            "and  its  derivative  is  0  for  z  <  0.  In  practice,  however,  it  works  very  well  and\n",
            "has  the  advantage  of  being  fast  to  compute,  so  it  has  become  the  default.11\n",
            "Importantly, the fact that it does not have a maximum output value helps reduce\n",
            "some issues during gradient descent (we will come back to this in Chapter 11).\n",
            "\n",
            "These  popular  activation  functions  and  their  derivatives  are  represented  in  Fig‐\n",
            "ure  10-8.  But  wait!  Why  do  we  need  activation  functions  in  the  first  place?  Well,\n",
            "if you chain several linear transformations, all you get is a linear transformation. For\n",
            "example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions\n",
            "gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t\n",
            "have some nonlinearity between layers, then even a deep stack of layers is equivalent\n",
            "to  a  single  layer,  and  you  can’t  solve  very  complex  problems  with  that.  Conversely,\n",
            "a  large  enough  DNN  with  nonlinear  activations  can  theoretically  approximate  any\n",
            "continuous function.\n",
            "\n",
            "11 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\n",
            "to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\n",
            "one of the cases where the biological analogy was perhaps misleading.\n",
            "\n",
            "312 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fFigure 10-8. Activation functions (left) and their derivatives (right)\n",
            "\n",
            "OK! You know where neural nets came from, what their architecture is, and how to\n",
            "compute their outputs. You’ve also learned about the backpropagation algorithm. But\n",
            "what exactly can you do with neural nets?\n",
            "\n",
            "Regression MLPs\n",
            "First,  MLPs  can  be  used  for  regression  tasks.  If  you  want  to  predict  a  single  value\n",
            "(e.g.,  the  price  of  a  house,  given  many  of  its  features),  then  you  just  need  a  single\n",
            "output neuron: its output is the predicted value. For multivariate regression (i.e., to\n",
            "predict multiple values at once), you need one output neuron per output dimension.\n",
            "For  example,  to  locate  the  center  of  an  object  in  an  image,  you  need  to  predict  2D\n",
            "coordinates, so you need two output neurons. If you also want to place a bounding\n",
            "box around the object, then you need two more numbers: the width and the height of\n",
            "the object. So, you end up with four output neurons.\n",
            "\n",
            "Scikit-Learn includes an MLPRegressor class, so let’s use it to build an MLP with three\n",
            "hidden  layers  composed  of  50  neurons  each,  and  train  it  on  the  California  housing\n",
            "dataset. For simplicity, we will use Scikit-Learn’s fetch_california_housing() func‐\n",
            "tion  to  load  the  data.  This  dataset  is  simpler  than  the  one  we  used  in  Chapter  2,\n",
            "since it contains only numerical features (there is no ocean_proximity feature), and\n",
            "there  are  no  missing  values.  The  following  code  starts  by  fetching  and  splitting  the\n",
            "dataset,  then  it  creates  a  pipeline  to  standardize  the  input  features  before  sending\n",
            "them to the MLPRegressor. This is very important for neural networks because they\n",
            "are  trained  using  gradient  descent,  and  as  we  saw  in  Chapter  4,  gradient  descent\n",
            "does not converge very well when the features have very different scales. Finally, the\n",
            "code  trains  the  model  and  evaluates  its  validation  error.  The  model  uses  the  ReLU\n",
            "activation  function  in  the  hidden  layers,  and  it  uses  a  variant  of  gradient  descent\n",
            "called Adam (see Chapter 11) to minimize the mean squared error, with a little bit of\n",
            "ℓ2 regularization (which you can control via the alpha hyperparameter):\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "313\n",
            "\n",
            "\ffrom sklearn.datasets import fetch_california_housing\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neural_network import MLPRegressor\n",
            "from sklearn.pipeline import make_pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "housing = fetch_california_housing()\n",
            "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
            "    housing.data, housing.target, random_state=42)\n",
            "X_train, X_valid, y_train, y_valid = train_test_split(\n",
            "    X_train_full, y_train_full, random_state=42)\n",
            "\n",
            "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
            "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
            "pipeline.fit(X_train, y_train)\n",
            "y_pred = pipeline.predict(X_valid)\n",
            "rmse = mean_squared_error(y_valid, y_pred, squared=False)  # about 0.505\n",
            "\n",
            "We get a validation RMSE of about 0.505, which is comparable to what you would get\n",
            "with a random forest classifier. Not too bad for a first try!\n",
            "\n",
            "Note that this MLP does not use any activation function for the output layer, so it’s\n",
            "free to output any value it wants. This is generally fine, but if you want to guarantee\n",
            "that  the  output  will  always  be  positive,  then  you  should  use  the  ReLU  activation\n",
            "function  in  the  output  layer,  or  the  softplus  activation  function,  which  is  a  smooth\n",
            "variant  of  ReLU:  softplus(z)  =  log(1  +  exp(z)).  Softplus  is  close  to  0  when  z  is\n",
            "negative, and close to z when z is positive. Finally, if you want to guarantee that the\n",
            "predictions  will  always  fall  within  a  given  range  of  values,  then  you  should  use  the\n",
            "sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate\n",
            "range: 0 to 1 for sigmoid and –1 to 1 for tanh. Sadly, the MLPRegressor class does not\n",
            "support activation functions in the output layer.\n",
            "\n",
            "Building  and  training  a  standard  MLP  with  Scikit-Learn  in  just  a\n",
            "few lines of code is very convenient, but the neural net features are\n",
            "limited.  This  is  why  we  will  switch  to  Keras  in  the  second  part  of\n",
            "this chapter.\n",
            "\n",
            "The MLPRegressor class uses the mean squared error, which is usually what you want\n",
            "for regression, but if you have a lot of outliers in the training set, you may prefer to\n",
            "use  the  mean  absolute  error  instead.  Alternatively,  you  may  want  to  use  the  Huber\n",
            "loss,  which  is  a  combination  of  both.  It  is  quadratic  when  the  error  is  smaller  than\n",
            "a threshold δ (typically 1) but linear when the error is larger than δ. The linear part\n",
            "makes  it  less  sensitive  to  outliers  than  the  mean  squared  error,  and  the  quadratic\n",
            "part  allows  it  to  converge  faster  and  be  more  precise  than  the  mean  absolute  error.\n",
            "However, MLPRegressor only supports the MSE.\n",
            "\n",
            "314 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fTable 10-1 summarizes the typical architecture of a regression MLP.\n",
            "\n",
            "Table 10-1. Typical regression MLP architecture\n",
            "\n",
            "Hyperparameter\n",
            "# hidden layers\n",
            "\n",
            "Typical value\n",
            "Depends on the problem, but typically 1 to 5\n",
            "\n",
            "# neurons per hidden layer Depends on the problem, but typically 10 to 100\n",
            "\n",
            "# output neurons\n",
            "\n",
            "Hidden activation\n",
            "\n",
            "Output activation\n",
            "\n",
            "Loss function\n",
            "\n",
            "1 per prediction dimension\n",
            "\n",
            "ReLU\n",
            "\n",
            "None, or ReLU/softplus (if positive outputs) or sigmoid/tanh (if bounded outputs)\n",
            "\n",
            "MSE, or Huber if outliers\n",
            "\n",
            "Classification MLPs\n",
            "MLPs  can  also  be  used  for  classification  tasks.  For  a  binary  classification  problem,\n",
            "you  just  need  a  single  output  neuron  using  the  sigmoid  activation  function:  the\n",
            "output will be a number between 0 and 1, which you can interpret as the estimated\n",
            "probability  of  the  positive  class.  The  estimated  probability  of  the  negative  class  is\n",
            "equal to one minus that number.\n",
            "\n",
            "MLPs  can  also  easily  handle  multilabel  binary  classification  tasks  (see  Chapter  3).\n",
            "For  example,  you  could  have  an  email  classification  system  that  predicts  whether\n",
            "each  incoming  email  is  ham  or  spam,  and  simultaneously  predicts  whether  it  is  an\n",
            "urgent  or  nonurgent  email.  In  this  case,  you  would  need  two  output  neurons,  both\n",
            "using the sigmoid activation function: the first would output the probability that the\n",
            "email  is  spam,  and  the  second  would  output  the  probability  that  it  is  urgent.  More\n",
            "generally, you would dedicate one output neuron for each positive class. Note that the\n",
            "output  probabilities  do  not  necessarily  add  up  to  1.  This  lets  the  model  output  any\n",
            "combination  of  labels:  you  can  have  nonurgent  ham,  urgent  ham,  nonurgent  spam,\n",
            "and perhaps even urgent spam (although that would probably be an error).\n",
            "\n",
            "If each instance can belong only to a single class, out of three or more possible classes\n",
            "(e.g.,  classes  0  through  9  for  digit  image  classification),  then  you  need  to  have  one\n",
            "output neuron per class, and you should use the softmax activation function for the\n",
            "whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)\n",
            "will  ensure  that  all  the  estimated  probabilities  are  between  0  and  1  and  that  they\n",
            "add  up  to  1,  since  the  classes  are  exclusive.  As  you  saw  in  Chapter  3,  this  is  called\n",
            "multiclass classification.\n",
            "\n",
            "Regarding  the  loss  function,  since  we  are  predicting  probability  distributions,  the\n",
            "cross-entropy  loss  (or  x-entropy  or  log  loss  for  short,  see  Chapter  4)  is  generally  a\n",
            "good choice.\n",
            "\n",
            "From Biological to Artificial Neurons \n",
            "\n",
            "| \n",
            "\n",
            "315\n",
            "\n",
            "\fFigure 10-9. A modern MLP (including ReLU and softmax) for classification\n",
            "\n",
            "Scikit-Learn  has  an  MLPClassifier  class  in  the  sklearn.neural_network  package.\n",
            "It  is  almost  identical  to  the  MLPRegressor  class,  except  that  it  minimizes  the  cross\n",
            "entropy  rather  than  the  MSE.  Give  it  a  try  now,  for  example  on  the  iris  dataset.  It’s\n",
            "almost a linear task, so a single layer with 5 to 10 neurons should suffice (make sure\n",
            "to scale the features).\n",
            "\n",
            "Table 10-2 summarizes the typical architecture of a classification MLP.\n",
            "\n",
            "Table 10-2. Typical classification MLP architecture\n",
            "\n",
            "Hyperparameter\n",
            "# hidden layers\n",
            "\n",
            "Binary classification Multilabel binary classification Multiclass classification\n",
            "Typically 1 to 5 layers, depending on the task\n",
            "\n",
            "# output neurons\n",
            "\n",
            "1\n",
            "\n",
            "1 per binary label\n",
            "\n",
            "Output layer activation Sigmoid\n",
            "\n",
            "Loss function\n",
            "\n",
            "X-entropy\n",
            "\n",
            "Sigmoid\n",
            "\n",
            "X-entropy\n",
            "\n",
            "1 per class\n",
            "\n",
            "Softmax\n",
            "\n",
            "X-entropy\n",
            "\n",
            "Before  we  go  on,  I  recommend  you  go  through  exercise  1  at  the\n",
            "end  of  this  chapter.  You  will  play  with  various  neural  network\n",
            "architectures and visualize their outputs using the TensorFlow play‐\n",
            "ground. This will be very useful to better understand MLPs, includ‐\n",
            "ing  the  effects  of  all  the  hyperparameters  (number  of  layers  and\n",
            "neurons, activation functions, and more).\n",
            "\n",
            "316 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fNow you have all the concepts you need to start implementing MLPs with Keras!\n",
            "\n",
            "Implementing MLPs with Keras\n",
            "Keras  is  TensorFlow’s  high-level  deep  learning  API:  it  allows  you  to  build,  train,\n",
            "evaluate,  and  execute  all  sorts  of  neural  networks.  The  original  Keras  library  was\n",
            "developed  by  François  Chollet  as  part  of  a  research  project12  and  was  released  as  a\n",
            "standalone open source project in March 2015. It quickly gained popularity, owing to\n",
            "its ease of use, flexibility, and beautiful design.\n",
            "\n",
            "Keras  used  to  support  multiple  backends,  including  TensorFlow,\n",
            "PlaidML,  Theano,  and  Microsoft  Cognitive  Toolkit  (CNTK)  (the\n",
            "last  two  are  sadly  deprecated),  but  since  version  2.4,  Keras  is\n",
            "TensorFlow-only.  Similarly,  TensorFlow  used  to  include  multiple\n",
            "high-level  APIs,  but  Keras  was  officially  chosen  as  its  preferred\n",
            "high-level API when TensorFlow 2 came out. Installing TensorFlow\n",
            "will  automatically  install  Keras  as  well,  and  Keras  will  not  work\n",
            "without  TensorFlow  installed.  In  short,  Keras  and  TensorFlow  fell\n",
            "in  love  and  got  married.  Other  popular  deep  learning  libraries\n",
            "include PyTorch by Facebook and JAX by Google.13\n",
            "\n",
            "Now let’s use Keras! We will start by building an MLP for image classification.\n",
            "\n",
            "Colab  runtimes  come  with  recent  versions  of  TensorFlow  and\n",
            "Keras  preinstalled.  However,  if  you  want  to  install  them  on  your\n",
            "own  machine,  please  see  the  installation  instructions  at  https://\n",
            "homl.info/install.\n",
            "\n",
            "12 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System). Chollet joined Goo‐\n",
            "\n",
            "gle in 2015, where he continues to lead the Keras project.\n",
            "\n",
            "13 PyTorch’s API is quite similar to Keras’s, so once you know Keras, it is not difficult to switch to PyTorch,\n",
            "if you ever want to. PyTorch’s popularity grew exponentially in 2018, largely thanks to its simplicity and\n",
            "excellent documentation, which were not TensorFlow 1.x’s main strengths back then. However, TensorFlow 2\n",
            "is just as simple as PyTorch, in part because it has adopted Keras as its official high-level API, and also\n",
            "because the developers have greatly simplified and cleaned up the rest of the API. The documentation has\n",
            "also been completely reorganized, and it is much easier to find what you need now. Similarly, PyTorch’s\n",
            "main weaknesses (e.g., limited portability and no computation graph analysis) have been largely addressed in\n",
            "PyTorch 1.0. Healthy competition seems beneficial to everyone.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "317\n",
            "\n",
            "\fBuilding an Image Classifier Using the Sequential API\n",
            "First,  we  need  to  load  a  dataset.  We  will  use  Fashion  MNIST,  which  is  a  drop-in\n",
            "replacement  of  MNIST  (introduced  in  Chapter  3).  It  has  the  exact  same  format  as\n",
            "MNIST  (70,000  grayscale  images  of  28  ×  28  pixels  each,  with  10  classes),  but  the\n",
            "images represent fashion items rather than handwritten digits, so each class is more\n",
            "diverse, and the problem turns out to be significantly more challenging than MNIST.\n",
            "For example, a simple linear model reaches about 92% accuracy on MNIST, but only\n",
            "about 83% on Fashion MNIST.\n",
            "\n",
            "Using Keras to load the dataset\n",
            "\n",
            "Keras provides some utility functions to fetch and load common datasets, including\n",
            "MNIST,  Fashion  MNIST,  and  a  few  more.  Let’s  load  Fashion  MNIST.  It’s  already\n",
            "shuffled  and  split  into  a  training  set  (60,000  images)  and  a  test  set  (10,000  images),\n",
            "but we’ll hold out the last 5,000 images from the training set for validation:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
            "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
            "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
            "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
            "\n",
            "TensorFlow  is  usually  imported  as  tf,  and  the  Keras  API  is  avail‐\n",
            "able via tf.keras.\n",
            "\n",
            "When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\n",
            "important difference is that every image is represented as a 28 × 28 array rather than\n",
            "a  1D  array  of  size  784.  Moreover,  the  pixel  intensities  are  represented  as  integers\n",
            "(from 0 to 255) rather than floats (from 0.0 to 255.0). Let’s take a look at the shape\n",
            "and data type of the training set:\n",
            "\n",
            ">>> X_train.shape\n",
            "(55000, 28, 28)\n",
            ">>> X_train.dtype\n",
            "dtype('uint8')\n",
            "\n",
            "For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them\n",
            "by 255.0 (this also converts them to floats):\n",
            "\n",
            "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n",
            "\n",
            "With  MNIST,  when  the  label  is  equal  to  5,  it  means  that  the  image  represents  the\n",
            "handwritten  digit  5.  Easy.  For  Fashion  MNIST,  however,  we  need  the  list  of  class\n",
            "names to know what we are dealing with:\n",
            "\n",
            "318 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
            "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
            "\n",
            "For example, the first image in the training set represents an ankle boot:\n",
            "\n",
            ">>> class_names[y_train[0]]\n",
            "'Ankle boot'\n",
            "\n",
            "Figure 10-10 shows some samples from the Fashion MNIST dataset.\n",
            "\n",
            "Figure 10-10. Samples from Fashion MNIST\n",
            "\n",
            "Creating the model using the sequential API\n",
            "\n",
            "Now  let’s  build  the  neural  network!  Here  is  a  classification  MLP  with  two  hidden\n",
            "layers:\n",
            "\n",
            "tf.random.set_seed(42)\n",
            "model = tf.keras.Sequential()\n",
            "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
            "model.add(tf.keras.layers.Flatten())\n",
            "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
            "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
            "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
            "\n",
            "Let’s go through this code line by line:\n",
            "\n",
            "• First, set TensorFlow’s random seed to make the results reproducible: the random\n",
            "•\n",
            "weights of the hidden layers and the output layer will be the same every time you\n",
            "run  the  notebook.  You  could  also  choose  to  use  the  tf.keras.utils.set_ran\n",
            "dom_seed() function, which conveniently sets the random seeds for TensorFlow,\n",
            "Python (random.seed()), and NumPy (np.random.seed()).\n",
            "\n",
            "• The  next  line  creates  a  Sequential  model.  This  is  the  simplest  kind  of  Keras\n",
            "•\n",
            "model  for  neural  networks  that  are  just  composed  of  a  single  stack  of  layers\n",
            "connected sequentially. This is called the sequential API.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "319\n",
            "\n",
            "\f• Next, we build the first layer (an Input layer) and add it to the model. We specify\n",
            "•\n",
            "the  input  shape,  which  doesn’t  include  the  batch  size,  only  the  shape  of  the\n",
            "instances.  Keras  needs  to  know  the  shape  of  the  inputs  so  it  can  determine  the\n",
            "shape of the connection weight matrix of the first hidden layer.\n",
            "\n",
            "• Then  we  add  a  Flatten  layer.  Its  role  is  to  convert  each  input  image  into  a  1D\n",
            "•\n",
            "array: for example, if it receives a batch of shape [32, 28, 28], it will reshape it to\n",
            "[32, 784]. In other words, if it receives input data X, it computes X.reshape(-1,\n",
            "784).  This  layer  doesn’t  have  any  parameters;  it’s  just  there  to  do  some  simple\n",
            "preprocessing.\n",
            "\n",
            "• Next  we  add  a  Dense  hidden  layer  with  300  neurons.  It  will  use  the  ReLU\n",
            "•\n",
            "activation function. Each Dense layer manages its own weight matrix, containing\n",
            "all the connection weights between the neurons and their inputs. It also manages\n",
            "a  vector  of  bias  terms  (one  per  neuron).  When  it  receives  some  input  data,  it\n",
            "computes Equation 10-2.\n",
            "\n",
            "• Then we add a second Dense hidden layer with 100 neurons, also using the ReLU\n",
            "•\n",
            "\n",
            "activation function.\n",
            "\n",
            "• Finally, we add a  Dense output layer with 10 neurons (one per class), using the\n",
            "•\n",
            "\n",
            "softmax activation function because the classes are exclusive.\n",
            "\n",
            "is  equivalent \n",
            "\n",
            "Specifying  activation=\"relu\" \n",
            "specifying\n",
            "activation=tf.keras.activations.relu.  Other  activation  func‐\n",
            "tions are available in the tf.keras.activations package. We will\n",
            "use many of them in this book; see https://keras.io/api/layers/activa\n",
            "tions for the full list. We will also define our own custom activation\n",
            "functions in Chapter 12.\n",
            "\n",
            "to \n",
            "\n",
            "Instead of adding the layers one by one as we just did, it’s often more convenient to\n",
            "pass a list of layers when creating the Sequential model. You can also drop the Input\n",
            "layer and instead specify the input_shape in the first layer:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
            "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "320 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fThe model’s summary() method displays all the model’s layers,14 including each layer’s\n",
            "name  (which  is  automatically  generated  unless  you  set  it  when  creating  the  layer),\n",
            "its  output  shape  (None  means  the  batch  size  can  be  anything),  and  its  number  of\n",
            "parameters.  The  summary  ends  with  the  total  number  of  parameters,  including\n",
            "trainable and non-trainable parameters. Here we only have trainable parameters (you\n",
            "will see some non-trainable parameters later in this chapter):\n",
            "\n",
            ">>> model.summary()\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #\n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0\n",
            "\n",
            " dense (Dense)               (None, 300)               235500\n",
            "\n",
            " dense_1 (Dense)             (None, 100)               30100\n",
            "\n",
            " dense_2 (Dense)             (None, 10)                1010\n",
            "\n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Note that  Dense layers often have a lot of parameters. For example, the first hidden\n",
            "layer  has  784  ×  300  connection  weights,  plus  300  bias  terms,  which  adds  up  to\n",
            "235,500 parameters! This gives the model quite a lot of flexibility to fit the training\n",
            "data, but it also means that the model runs the risk of overfitting, especially when you\n",
            "do not have a lot of training data. We will come back to this later.\n",
            "\n",
            "Each  layer  in  a  model  must  have  a  unique  name  (e.g.,  \"dense_2\").  You  can  set  the\n",
            "layer  names  explicitly  using  the  constructor’s  name  argument,  but  generally  it’s  sim‐\n",
            "pler to let Keras name the layers automatically, as we just did. Keras takes the layer’s\n",
            "class  name  and  converts  it  to  snake  case  (e.g.,  a  layer  from  the  MyCoolLayer  class\n",
            "is named \"my_cool_layer\" by default). Keras also ensures that the name is globally\n",
            "unique, even across models, by appending an index if needed, as in \"dense_2\". But\n",
            "why  does  it  bother  making  the  names  unique  across  models?  Well,  this  makes  it\n",
            "possible to merge models easily without getting name conflicts.\n",
            "\n",
            "14 You can also use tf.keras.utils.plot_model() to generate an image of your model.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "321\n",
            "\n",
            "\fAll  global  state  managed  by  Keras  is  stored  in  a  Keras  session,\n",
            "which  you  can  clear  using  tf.keras.backend.clear_session().\n",
            "In particular, this resets the name counters.\n",
            "\n",
            "You  can  easily  get  a  model’s  list  of  layers  using  the  layers  attribute,  or  use  the\n",
            "get_layer() method to access a layer by name:\n",
            "\n",
            ">>> model.layers\n",
            "[<keras.layers.core.flatten.Flatten at 0x7fa1dea02250>,\n",
            " <keras.layers.core.dense.Dense at 0x7fa1c8f42520>,\n",
            " <keras.layers.core.dense.Dense at 0x7fa188be7ac0>,\n",
            " <keras.layers.core.dense.Dense at 0x7fa188be7fa0>]\n",
            ">>> hidden1 = model.layers[1]\n",
            ">>> hidden1.name\n",
            "'dense'\n",
            ">>> model.get_layer('dense') is hidden1\n",
            "True\n",
            "\n",
            "All  the  parameters  of  a  layer  can  be  accessed  using  its  get_weights()  and\n",
            "set_weights() methods. For a Dense layer, this includes both the connection weights\n",
            "and the bias terms:\n",
            "\n",
            ">>> weights, biases = hidden1.get_weights()\n",
            ">>> weights\n",
            "array([[ 0.02448617, -0.00877795, -0.02189048, ...,  0.03859074, -0.06889391],\n",
            "       [ 0.00476504, -0.03105379, -0.0586676 , ..., -0.02763776, -0.04165364],\n",
            "       ...,\n",
            "       [ 0.07061854, -0.06960931,  0.07038955, ..., 0.00034875,  0.02878492],\n",
            "       [-0.06022581,  0.01577859, -0.02585464, ..., 0.00272203, -0.06793761]],\n",
            "       dtype=float32)\n",
            ">>> weights.shape\n",
            "(784, 300)\n",
            ">>> biases\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\n",
            ">>> biases.shape\n",
            "(300,)\n",
            "\n",
            "Notice  that  the  Dense  layer  initialized  the  connection  weights  randomly  (which  is\n",
            "needed  to  break  symmetry,  as  discussed  earlier),  and  the  biases  were  initialized  to\n",
            "zeros, which is fine. If you want to use a different initialization method, you can set\n",
            "kernel_initializer (kernel is another name for the matrix of connection weights)\n",
            "or  bias_initializer  when  creating  the  layer.  We’ll  discuss  initializers  further  in\n",
            "Chapter 11, and the full list is at https://keras.io/api/layers/initializers.\n",
            "\n",
            "322 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fThe shape of the weight matrix depends on the number of inputs,\n",
            "which  is  why  we  specified  the  input_shape  when  creating  the\n",
            "model.  If  you  do  not  specify  the  input  shape,  it’s  OK:  Keras  will\n",
            "simply wait until it knows the input shape before it actually builds\n",
            "the  model  parameters.  This  will  happen  either  when  you  feed  it\n",
            "some  data  (e.g.,  during  training),  or  when  you  call  its  build()\n",
            "method. Until the model parameters are built, you will not be able\n",
            "to do certain things, such as display the model summary or save the\n",
            "model. So, if you know the input shape when creating the model, it\n",
            "is best to specify it.\n",
            "\n",
            "Compiling the model\n",
            "\n",
            "After  a  model  is  created,  you  must  call  its  compile()  method  to  specify  the  loss\n",
            "function and the optimizer to use. Optionally, you can specify a list of extra metrics to\n",
            "compute during training and evaluation:\n",
            "\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
            "              optimizer=\"sgd\",\n",
            "              metrics=[\"accuracy\"])\n",
            "\n",
            "Using  loss=\"sparse_categorical_crossentropy\"  is  the  equiv‐\n",
            "alent  of  using  loss=tf.keras.losses.sparse_categorical_\n",
            "crossentropy.  Similarly,  using  optimizer=\"sgd\"  is  the  equiva‐\n",
            "lent  of  using  optimizer=tf.keras.optimizers.SGD(),  and  using\n",
            "is  the  equivalent  of  using  metrics=\n",
            "metrics=[\"accuracy\"] \n",
            "[tf.keras.metrics.sparse_categorical_accuracy] (when using\n",
            "this  loss).  We  will  use  many  other  losses,  optimizers,  and  metrics\n",
            "in this book; for the full lists, see https://keras.io/api/losses, https://\n",
            "keras.io/api/optimizers, and https://keras.io/api/metrics.\n",
            "\n",
            "This  code  requires  explanation.  We  use  the  \"sparse_categorical_crossentropy\"\n",
            "loss  because  we  have  sparse  labels  (i.e.,  for  each  instance,  there  is  just  a  target  class\n",
            "index,  from  0  to  9  in  this  case),  and  the  classes  are  exclusive.  If  instead  we  had\n",
            "one  target  probability  per  class  for  each  instance  (such  as  one-hot  vectors,  e.g.,\n",
            "[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would\n",
            "need to use the \"categorical_crossentropy\" loss instead. If we were doing binary\n",
            "classification  or  multilabel  binary  classification,  then  we  would  use  the  \"sigmoid\"\n",
            "activation function in the output layer instead of the \"softmax\" activation function,\n",
            "and we would use the \"binary_crossentropy\" loss.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "323\n",
            "\n",
            "\fIf you want to convert sparse labels (i.e., class indices) to one-hot\n",
            "vector  labels,  use  the  tf.keras.utils.to_categorical()  func‐\n",
            "tion.  To  go  the  other  way  round,  use  the  np.argmax()  function\n",
            "with axis=1.\n",
            "\n",
            "Regarding the optimizer, \"sgd\" means that we will train the model using stochastic\n",
            "gradient descent. In other words, Keras will perform the backpropagation algorithm\n",
            "described  earlier  (i.e.,  reverse-mode  autodiff  plus  gradient  descent).  We  will  discuss\n",
            "more efficient optimizers in Chapter 11. They improve gradient descent, not autodiff.\n",
            "\n",
            "When  using  the  SGD  optimizer,  it  is  important  to  tune  the  learn‐\n",
            "ing  rate.  So,  you  will  generally  want  to  use  optimizer=tf.keras.\n",
            "optimizers.SGD(learning_rate=__???__) to set the learning rate,\n",
            "rather than  optimizer=\"sgd\", which defaults to a learning rate of\n",
            "0.01.\n",
            "\n",
            "Finally, since this is a classifier, it’s useful to measure its accuracy during training and\n",
            "evaluation, which is why we set metrics=[\"accuracy\"].\n",
            "\n",
            "Training and evaluating the model\n",
            "\n",
            "Now  the  model  is  ready  to  be  trained.  For  this  we  simply  need  to  call  its  fit()\n",
            "method:\n",
            "\n",
            ">>> history = model.fit(X_train, y_train, epochs=30,\n",
            "...                     validation_data=(X_valid, y_valid))\n",
            "...\n",
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 2s 989us/step\n",
            "  - loss: 0.7220 - sparse_categorical_accuracy: 0.7649\n",
            "  - val_loss: 0.4959 - val_sparse_categorical_accuracy: 0.8332\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 2s 964us/step\n",
            "  - loss: 0.4825 - sparse_categorical_accuracy: 0.8332\n",
            "  - val_loss: 0.4567 - val_sparse_categorical_accuracy: 0.8384\n",
            "[...]\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 2s 963us/step\n",
            "  - loss: 0.2235 - sparse_categorical_accuracy: 0.9200\n",
            "  - val_loss: 0.3056 - val_sparse_categorical_accuracy: 0.8894\n",
            "\n",
            "We pass it the input features (X_train) and the target classes (y_train), as well as the\n",
            "number of epochs to train (or else it would default to just 1, which would definitely\n",
            "not be enough to converge to a good solution). We also pass a validation set (this is\n",
            "optional). Keras will measure the loss and the extra metrics on this set at the end of\n",
            "each  epoch,  which  is  very  useful  to  see  how  well  the  model  really  performs.  If  the\n",
            "performance on the training set is much better than on the validation set, your model\n",
            "\n",
            "324 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fis  probably  overfitting  the  training  set,  or  there  is  a  bug,  such  as  a  data  mismatch\n",
            "between the training set and the validation set.\n",
            "\n",
            "Shape  errors  are  quite  common,  especially  when  getting  started,\n",
            "so  you  should  familiarize  yourself  with  the  error  messages:\n",
            "try  fitting  a  model  with  inputs  and/or  labels  of  the  wrong\n",
            "shape,  and  see  the  errors  you  get.  Similarly,  try  compiling\n",
            "the  model  with  loss=\"categorical_crossentropy\"  instead  of\n",
            "loss=\"sparse_categorical_crossentropy\".  Or  you  can  remove\n",
            "the Flatten layer.\n",
            "\n",
            "And  that’s  it!  The  neural  network  is  trained.  At  each  epoch  during  training,  Keras\n",
            "displays the number of mini-batches processed so far on the left side of the progress\n",
            "bar. The batch size is 32 by default, and since the training set has 55,000 images, the\n",
            "model goes through 1,719 batches per epoch: 1,718 of size 32, and 1 of size 24. After\n",
            "the  progress  bar,  you  can  see  the  mean  training  time  per  sample,  and  the  loss  and\n",
            "accuracy (or any other extra metrics you asked for) on both the training set and the\n",
            "validation set. Notice that the training loss went down, which is a good sign, and the\n",
            "validation accuracy reached 88.94% after 30 epochs. That’s slightly below the training\n",
            "accuracy, so there is a little bit of overfitting going on, but not a huge amount.\n",
            "\n",
            "Instead  of  passing  a  validation  set  using  the  validation_data\n",
            "argument,  you  could  set  validation_split  to  the  ratio  of  the\n",
            "training set that you want Keras to use for validation. For example,\n",
            "validation_split=0.1  tells  Keras  to  use  the  last  10%  of  the  data\n",
            "(before shuffling) for validation.\n",
            "\n",
            "If  the  training  set  was  very  skewed,  with  some  classes  being  overrepresented  and\n",
            "others underrepresented, it would be useful to set the class_weight argument when\n",
            "calling  the  fit()  method,  to  give  a  larger  weight  to  underrepresented  classes  and\n",
            "a  lower  weight  to  overrepresented  classes.  These  weights  would  be  used  by  Keras\n",
            "when  computing  the  loss.  If  you  need  per-instance  weights,  set  the  sample_weight\n",
            "argument. If both class_weight and sample_weight are provided, then Keras multi‐\n",
            "plies them. Per-instance weights could be useful, for example, if some instances were\n",
            "labeled  by  experts  while  others  were  labeled  using  a  crowdsourcing  platform:  you\n",
            "might want to give more weight to the former. You can also provide sample weights\n",
            "(but  not  class  weights)  for  the  validation  set  by  adding  them  as  a  third  item  in  the\n",
            "validation_data tuple.\n",
            "\n",
            "The  fit()  method  returns  a  History  object  containing  the  training  parameters\n",
            "(history.params),  the  list  of  epochs  it  went  through  (history.epoch),  and  most\n",
            "importantly  a  dictionary  (history.history)  containing  the  loss  and  extra  metrics\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "325\n",
            "\n",
            "\fit  measured  at  the  end  of  each  epoch  on  the  training  set  and  on  the  validation  set\n",
            "(if any). If you use this dictionary to create a Pandas DataFrame and call its plot()\n",
            "method, you get the learning curves shown in Figure 10-11:\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "\n",
            "pd.DataFrame(history.history).plot(\n",
            "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
            "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
            "plt.show()\n",
            "\n",
            "Figure 10-11. Learning curves: the mean training loss and accuracy measured over each\n",
            "epoch, and the mean validation loss and accuracy measured at the end of each epoch\n",
            "\n",
            "You  can  see  that  both  the  training  accuracy  and  the  validation  accuracy  steadily\n",
            "increase during training, while the training loss and the validation loss decrease. This\n",
            "is good. The validation curves are relatively close to each other at first, but they get\n",
            "further  apart  over  time,  which  shows  that  there’s  a  little  bit  of  overfitting.  In  this\n",
            "particular  case,  the  model  looks  like  it  performed  better  on  the  validation  set  than\n",
            "on the training set at the beginning of training, but that’s not actually the case. The\n",
            "validation  error  is  computed  at  the  end  of  each  epoch,  while  the  training  error  is\n",
            "computed using a running mean during each epoch, so the training curve should be\n",
            "shifted by half an epoch to the left. If you do that, you will see that the training and\n",
            "validation curves overlap almost perfectly at the beginning of training.\n",
            "\n",
            "The  training  set  performance  ends  up  beating  the  validation  performance,  as  is\n",
            "generally the case when you train for long enough. You can tell that the model has not\n",
            "quite converged yet, as the validation loss is still going down, so you should probably\n",
            "\n",
            "326 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fcontinue  training.  This  is  as  simple  as  calling  the  fit()  method  again,  since  Keras\n",
            "just  continues  training  where  it  left  off:  you  should  be  able  to  reach  about  89.8%\n",
            "validation accuracy, while the training accuracy will continue to rise up to 100% (this\n",
            "is not always the case).\n",
            "\n",
            "If you are not satisfied with the performance of your model, you should go back and\n",
            "tune the hyperparameters. The first one to check is the learning rate. If that doesn’t\n",
            "help,  try  another  optimizer  (and  always  retune  the  learning  rate  after  changing\n",
            "any  hyperparameter).  If  the  performance  is  still  not  great,  then  try  tuning  model\n",
            "hyperparameters  such  as  the  number  of  layers,  the  number  of  neurons  per  layer,\n",
            "and  the  types  of  activation  functions  to  use  for  each  hidden  layer.  You  can  also  try\n",
            "tuning  other  hyperparameters,  such  as  the  batch  size  (it  can  be  set  in  the  fit()\n",
            "method  using  the  batch_size  argument,  which  defaults  to  32).  We  will  get  back  to\n",
            "hyperparameter  tuning  at  the  end  of  this  chapter.  Once  you  are  satisfied  with  your\n",
            "model’s  validation  accuracy,  you  should  evaluate  it  on  the  test  set  to  estimate  the\n",
            "generalization  error  before  you  deploy  the  model  to  production.  You  can  easily  do\n",
            "this using the evaluate() method (it also supports several other arguments, such as\n",
            "batch_size and sample_weight; please check the documentation for more details):\n",
            "\n",
            ">>> model.evaluate(X_test, y_test)\n",
            "313/313 [==============================] - 0s 626us/step\n",
            "  - loss: 0.3243 - sparse_categorical_accuracy: 0.8864\n",
            "[0.32431697845458984, 0.8863999843597412]\n",
            "\n",
            "As you saw in Chapter 2, it is common to get slightly lower performance on the test\n",
            "set than on the validation set, because the hyperparameters are tuned on the valida‐\n",
            "tion set, not the test set (however, in this example, we did not do any hyperparameter\n",
            "tuning, so the lower accuracy is just bad luck). Remember to resist the temptation to\n",
            "tweak the hyperparameters on the test set, or else your estimate of the generalization\n",
            "error will be too optimistic.\n",
            "\n",
            "Using the model to make predictions\n",
            "\n",
            "Now let’s use the model’s predict() method to make predictions on new instances.\n",
            "Since we don’t have actual new instances, we’ll just use the first three instances of the\n",
            "test set:\n",
            "\n",
            ">>> X_new = X_test[:3]\n",
            ">>> y_proba = model.predict(X_new)\n",
            ">>> y_proba.round(2)\n",
            "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97],\n",
            "       [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
            "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
            "      dtype=float32)\n",
            "\n",
            "For  each  instance  the  model  estimates  one  probability  per  class,  from  class  0  to\n",
            "class 9. This is similar to the output of the predict_proba() method in Scikit-Learn\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "327\n",
            "\n",
            "\fclassifiers.  For  example,  for  the  first  image  it  estimates  that  the  probability  of  class\n",
            "9  (ankle  boot)  is  96%,  the  probability  of  class  7  (sneaker)  is  2%,  the  probability  of\n",
            "class 5 (sandal) is 1%, and the probabilities of the other classes are negligible. In other\n",
            "words, it is highly confident that the first image is footwear, most likely ankle boots\n",
            "but  possibly  sneakers  or  sandals.  If  you  only  care  about  the  class  with  the  highest\n",
            "estimated  probability  (even  if  that  probability  is  quite  low),  then  you  can  use  the\n",
            "argmax() method to get the highest probability class index for each instance:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> y_pred = y_proba.argmax(axis=-1)\n",
            ">>> y_pred\n",
            "array([9, 2, 1])\n",
            ">>> np.array(class_names)[y_pred]\n",
            "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\n",
            "\n",
            "Here,  the  classifier  actually  classified  all  three  images  correctly  (these  images  are\n",
            "shown in Figure 10-12):\n",
            "\n",
            ">>> y_new = y_test[:3]\n",
            ">>> y_new\n",
            "array([9, 2, 1], dtype=uint8)\n",
            "\n",
            "Figure 10-12. Correctly classified Fashion MNIST images\n",
            "\n",
            "Now you know how to use the sequential API to build, train, and evaluate a classifica‐\n",
            "tion MLP. But what about regression?\n",
            "\n",
            "Building a Regression MLP Using the Sequential API\n",
            "Let’s switch back to the California housing problem and tackle it using the same MLP\n",
            "as earlier, with 3 hidden layers composed of 50 neurons each, but this time building it\n",
            "with Keras.\n",
            "\n",
            "Using the sequential API to build, train, evaluate, and use a regression MLP is quite\n",
            "similar to what we did for classification. The main differences in the following code\n",
            "example  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want\n",
            "to predict a single value) and it uses no activation function, the loss function is the\n",
            "mean squared error, the metric is the RMSE, and we’re using an Adam optimizer like\n",
            "\n",
            "328 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fScikit-Learn’s MLPRegressor did. Moreover, in this example we don’t need a Flatten\n",
            "layer,  and  instead  we’re  using  a  Normalization  layer  as  the  first  layer:  it  does  the\n",
            "same thing as Scikit-Learn’s StandardScaler, but it must be fitted to the training data\n",
            "using its adapt() method before you call the model’s fit() method. (Keras has other\n",
            "preprocessing layers, which will be covered in Chapter 13). Let’s take a look:\n",
            "\n",
            "tf.random.set_seed(42)\n",
            "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
            "model = tf.keras.Sequential([\n",
            "    norm_layer,\n",
            "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(1)\n",
            "])\n",
            "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
            "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
            "norm_layer.adapt(X_train)\n",
            "history = model.fit(X_train, y_train, epochs=20,\n",
            "                    validation_data=(X_valid, y_valid))\n",
            "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
            "X_new = X_test[:3]\n",
            "y_pred = model.predict(X_new)\n",
            "\n",
            "The  Normalization  layer  learns  the  feature  means  and  standard\n",
            "deviations in the training data when you call the adapt() method.\n",
            "Yet  when  you  display  the  model’s  summary,  these  statistics  are\n",
            "listed  as  non-trainable.  This  is  because  these  parameters  are  not\n",
            "affected by gradient descent.\n",
            "\n",
            "As  you  can  see,  the  sequential  API  is  quite  clean  and  straightforward.  However,\n",
            "although Sequential models are extremely common, it is sometimes useful to build\n",
            "neural networks with more complex topologies, or with multiple inputs or outputs.\n",
            "For this purpose, Keras offers the functional API.\n",
            "\n",
            "Building Complex Models Using the Functional API\n",
            "One  example  of  a  nonsequential  neural  network  is  a  Wide  &  Deep  neural  network.\n",
            "This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\n",
            "et  al.15  It  connects  all  or  part  of  the  inputs  directly  to  the  output  layer,  as  shown  in\n",
            "Figure 10-13. This architecture makes it possible for the neural network to learn both\n",
            "deep  patterns  (using  the  deep  path)  and  simple  rules  (through  the  short  path).16  In\n",
            "\n",
            "15 Heng-Tze Cheng et al., “Wide & Deep Learning for Recommender Systems”, Proceedings of the First Workshop\n",
            "\n",
            "on Deep Learning for Recommender Systems (2016): 7–10.\n",
            "\n",
            "16 The short path can also be used to provide manually engineered features to the neural network.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "329\n",
            "\n",
            "\fcontrast,  a  regular  MLP  forces  all  the  data  to  flow  through  the  full  stack  of  layers;\n",
            "thus,  simple  patterns  in  the  data  may  end  up  being  distorted  by  this  sequence  of\n",
            "transformations.\n",
            "\n",
            "Figure 10-13. Wide & Deep neural network\n",
            "\n",
            "Let’s build such a neural network to tackle the California housing problem:\n",
            "\n",
            "normalization_layer = tf.keras.layers.Normalization()\n",
            "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
            "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
            "concat_layer = tf.keras.layers.Concatenate()\n",
            "output_layer = tf.keras.layers.Dense(1)\n",
            "\n",
            "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
            "normalized = normalization_layer(input_)\n",
            "hidden1 = hidden_layer1(normalized)\n",
            "hidden2 = hidden_layer2(hidden1)\n",
            "concat = concat_layer([normalized, hidden2])\n",
            "output = output_layer(concat)\n",
            "\n",
            "model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
            "\n",
            "330 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fAt a high level, the first five lines create all the layers we need to build the model, the\n",
            "next six lines use these layers just like functions to go from the input to the output,\n",
            "and the last line creates a Keras Model object by pointing to the input and the output.\n",
            "Let’s go through this code in more detail:\n",
            "\n",
            "• First,  we  create  five  layers:  a  Normalization  layer  to  standardize  the  inputs,\n",
            "•\n",
            "two  Dense  layers  with  30  neurons  each,  using  the  ReLU  activation  function,  a\n",
            "Concatenate layer, and one more Dense layer with a single neuron for the output\n",
            "layer, without any activation function.\n",
            "\n",
            "• Next,  we  create  an  Input  object  (the  variable  name  input_  is  used  to  avoid\n",
            "•\n",
            "overshadowing Python’s built-in input() function). This is a specification of the\n",
            "kind  of  input  the  model  will  get,  including  its  shape  and  optionally  its  dtype,\n",
            "which defaults to 32-bit floats. A model may actually have multiple inputs, as you\n",
            "will see shortly.\n",
            "\n",
            "• Then  we  use  the  Normalization  layer  just  like  a  function,  passing  it  the  Input\n",
            "•\n",
            "object. This is why this is called the functional API. Note that we are just telling\n",
            "Keras how it should connect the layers together; no actual data is being processed\n",
            "yet, as the Input object is just a data specification. In other words, it’s a symbolic\n",
            "input.  The  output  of  this  call  is  also  symbolic:  normalized  doesn’t  store  any\n",
            "actual data, it’s just used to construct the model.\n",
            "\n",
            "• In  the  same  way,  we  then  pass  normalized  to  hidden_layer1,  which  outputs\n",
            "•\n",
            "\n",
            "hidden1, and we pass hidden1 to hidden_layer2, which outputs hidden2.\n",
            "\n",
            "• So far we’ve connected the layers sequentially, but then we use the concat_layer\n",
            "•\n",
            "to concatenate the input and the second hidden layer’s output. Again, no actual\n",
            "data is concatenated yet: it’s all symbolic, to build the model.\n",
            "\n",
            "• Then we pass concat to the output_layer, which gives us the final output.\n",
            "•\n",
            "• Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
            "•\n",
            "\n",
            "Once you have built this Keras model, everything is exactly like earlier, so there’s no\n",
            "need to repeat it here: you compile the model, adapt the Normalization layer, fit the\n",
            "model, evaluate it, and use it to make predictions.\n",
            "\n",
            "But  what  if  you  want  to  send  a  subset  of  the  features  through  the  wide  path  and\n",
            "a different subset (possibly overlapping) through the deep path, as illustrated in Fig‐\n",
            "ure 10-14? In this case, one solution is to use multiple inputs. For example, suppose\n",
            "we want to send five features through the wide path (features 0 to 4), and six features\n",
            "through the deep path (features 2 to 7). We can do this as follows:\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "331\n",
            "\n",
            "\finput_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
            "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
            "norm_layer_wide = tf.keras.layers.Normalization()\n",
            "norm_layer_deep = tf.keras.layers.Normalization()\n",
            "norm_wide = norm_layer_wide(input_wide)\n",
            "norm_deep = norm_layer_deep(input_deep)\n",
            "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
            "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
            "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
            "output = tf.keras.layers.Dense(1)(concat)\n",
            "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])\n",
            "\n",
            "Figure 10-14. Handling multiple inputs\n",
            "\n",
            "There are a few things to note in this example, compared to the previous one:\n",
            "\n",
            "• Each  Dense  layer  is  created  and  called  on  the  same  line.  This  is  a  common\n",
            "•\n",
            "practice, as it makes the code more concise without losing clarity. However, we\n",
            "can’t do this with the Normalization layer since we need a reference to the layer\n",
            "to be able to call its adapt() method before fitting the model.\n",
            "\n",
            "• We  used  tf.keras.layers.concatenate(),  which  creates  a  Concatenate  layer\n",
            "•\n",
            "\n",
            "and calls it with the given inputs.\n",
            "\n",
            "• We  specified  inputs=[input_wide,  input_deep]  when  creating  the  model,\n",
            "•\n",
            "\n",
            "since there are two inputs.\n",
            "\n",
            "332 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fNow  we  can  compile  the  model  as  usual,  but  when  we  call  the  fit()  method,\n",
            "instead  of  passing  a  single  input  matrix  X_train,  we  must  pass  a  pair  of  matrices\n",
            "(X_train_wide,  X_train_deep),  one  per  input.  The  same  is  true  for  X_valid,  and\n",
            "also for X_test and X_new when you call evaluate() or predict():\n",
            "\n",
            "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
            "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
            "\n",
            "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
            "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
            "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
            "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
            "\n",
            "norm_layer_wide.adapt(X_train_wide)\n",
            "norm_layer_deep.adapt(X_train_deep)\n",
            "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n",
            "                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
            "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
            "y_pred = model.predict((X_new_wide, X_new_deep))\n",
            "\n",
            "Instead  of  passing  a  tuple  (X_train_wide,  X_train_deep),\n",
            "you  can  pass  a  dictionary  {\"input_wide\":  X_train_wide,\n",
            "\"input_deep\":  X_train_deep},  if  you  set  name=\"input_wide\"\n",
            "and  name=\"input_deep\"  when  creating  the  inputs.  This  is  highly\n",
            "recommended when there are many inputs, to clarify the code and\n",
            "avoid getting the order wrong.\n",
            "\n",
            "There are also many use cases in which you may want to have multiple outputs:\n",
            "\n",
            "• The  task  may  demand  it.  For  instance,  you  may  want  to  locate  and  classify  the\n",
            "•\n",
            "main object in a picture. This is both a regression tasks and a classification task.\n",
            "\n",
            "• Similarly, you may have multiple independent tasks based on the same data. Sure,\n",
            "•\n",
            "you  could  train  one  neural  network  per  task,  but  in  many  cases  you  will  get\n",
            "better  results  on  all  tasks  by  training  a  single  neural  network  with  one  output\n",
            "per  task.  This  is  because  the  neural  network  can  learn  features  in  the  data  that\n",
            "are  useful  across  tasks.  For  example,  you  could  perform  multitask  classification\n",
            "on  pictures  of  faces,  using  one  output  to  classify  the  person’s  facial  expression\n",
            "(smiling, surprised, etc.) and another output to identify whether they are wearing\n",
            "glasses or not.\n",
            "\n",
            "• Another  use  case  is  as  a  regularization  technique  (i.e.,  a  training  constraint\n",
            "•\n",
            "whose objective is to reduce overfitting and thus improve the model’s ability to\n",
            "generalize).  For  example,  you  may  want  to  add  an  auxiliary  output  in  a  neural\n",
            "network architecture (see Figure 10-15) to ensure that the underlying part of the\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "333\n",
            "\n",
            "\fnetwork  learns  something  useful  on  its  own,  without  relying  on  the  rest  of  the\n",
            "network.\n",
            "\n",
            "Figure 10-15. Handling multiple outputs, in this example to add an auxiliary output for\n",
            "regularization\n",
            "\n",
            "Adding  an  extra  output  is  quite  easy:  we  just  connect  it  to  the  appropriate  layer\n",
            "and add it to the model’s list of outputs. For example, the following code builds the\n",
            "network represented in Figure 10-15:\n",
            "\n",
            "[...]  # Same as above, up to the main output layer\n",
            "output = tf.keras.layers.Dense(1)(concat)\n",
            "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
            "model = tf.keras.Model(inputs=[input_wide, input_deep],\n",
            "                       outputs=[output, aux_output])\n",
            "\n",
            "Each output will need its own loss function. Therefore, when we compile the model,\n",
            "we  should  pass  a  list  of  losses.  If  we  pass  a  single  loss,  Keras  will  assume  that  the\n",
            "same loss must be used for all outputs. By default, Keras will compute all the losses\n",
            "and simply add them up to get the final loss used for training. Since we care much\n",
            "more  about  the  main  output  than  about  the  auxiliary  output  (as  it  is  just  used  for\n",
            "regularization), we want to give the main output’s loss a much greater weight. Luckily,\n",
            "it is possible to set all the loss weights when compiling the model:\n",
            "\n",
            "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
            "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n",
            "              metrics=[\"RootMeanSquaredError\"])\n",
            "\n",
            "334 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fInstead  of  passing  a  tuple  loss=(\"mse\",  \"mse\"),  you  can  pass\n",
            "a  dictionary  loss={\"output\":  \"mse\",  \"aux_output\":  \"mse\"},\n",
            "assuming  you  created  the  output  layers  with  name=\"output\"  and\n",
            "name=\"aux_output\". Just like for the inputs, this clarifies the code\n",
            "and avoids errors when there are several outputs. You can also pass\n",
            "a dictionary for loss_weights.\n",
            "\n",
            "Now  when  we  train  the  model,  we  need  to  provide  labels  for  each  output.  In  this\n",
            "example,  the  main  output  and  the  auxiliary  output  should  try  to  predict  the  same\n",
            "thing,  so  they  should  use  the  same  labels.  So  instead  of  passing  y_train,  we  need\n",
            "to pass (y_train, y_train), or a dictionary {\"output\": y_train, \"aux_output\":\n",
            "y_train} if the outputs were named \"output\" and \"aux_output\". The same goes for\n",
            "y_valid and y_test:\n",
            "\n",
            "norm_layer_wide.adapt(X_train_wide)\n",
            "norm_layer_deep.adapt(X_train_deep)\n",
            "history = model.fit(\n",
            "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n",
            "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
            ")\n",
            "\n",
            "When we evaluate the model, Keras returns the weighted sum of the losses, as well as\n",
            "all the individual losses and metrics:\n",
            "\n",
            "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
            "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n",
            "\n",
            "If you set return_dict=True, then evaluate() will return a dictio‐\n",
            "nary instead of a big tuple.\n",
            "\n",
            "Similarly, the predict() method will return predictions for each output:\n",
            "\n",
            "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n",
            "\n",
            "The predict() method returns a tuple, and it does not have a return_dict argument\n",
            "to get a dictionary instead. However, you can create one using model.output_names:\n",
            "\n",
            "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
            "y_pred = dict(zip(model.output_names, y_pred_tuple))\n",
            "\n",
            "As you can see, you can build all sorts of architectures with the functional API. Next,\n",
            "we’ll look at one last way you can build Keras models.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "335\n",
            "\n",
            "\fUsing the Subclassing API to Build Dynamic Models\n",
            "Both the sequential API and the functional API are declarative: you start by declaring\n",
            "which  layers  you  want  to  use  and  how  they  should  be  connected,  and  only  then\n",
            "can you start feeding the model some data for training or inference. This has many\n",
            "advantages:  the  model  can  easily  be  saved,  cloned,  and  shared;  its  structure  can  be\n",
            "displayed  and  analyzed;  the  framework  can  infer  shapes  and  check  types,  so  errors\n",
            "can be caught early (i.e., before any data ever goes through the model). It’s also fairly\n",
            "straightforward  to  debug,  since  the  whole  model  is  a  static  graph  of  layers.  But  the\n",
            "flip side is just that: it’s static. Some models involve loops, varying shapes, conditional\n",
            "branching,  and  other  dynamic  behaviors.  For  such  cases,  or  simply  if  you  prefer  a\n",
            "more imperative programming style, the subclassing API is for you.\n",
            "\n",
            "With  this  approach,  you  subclass  the  Model  class,  create  the  layers  you  need  in  the\n",
            "constructor,  and  use  them  to  perform  the  computations  you  want  in  the  call()\n",
            "method. For example, creating an instance of the following WideAndDeepModel class\n",
            "gives us an equivalent model to the one we just built with the functional API:\n",
            "\n",
            "class WideAndDeepModel(tf.keras.Model):\n",
            "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
            "        super().__init__(**kwargs)  # needed to support naming the model\n",
            "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
            "        self.norm_layer_deep = tf.keras.layers.Normalization()\n",
            "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
            "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
            "        self.main_output = tf.keras.layers.Dense(1)\n",
            "        self.aux_output = tf.keras.layers.Dense(1)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        input_wide, input_deep = inputs\n",
            "        norm_wide = self.norm_layer_wide(input_wide)\n",
            "        norm_deep = self.norm_layer_deep(input_deep)\n",
            "        hidden1 = self.hidden1(norm_deep)\n",
            "        hidden2 = self.hidden2(hidden1)\n",
            "        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
            "        output = self.main_output(concat)\n",
            "        aux_output = self.aux_output(hidden2)\n",
            "        return output, aux_output\n",
            "\n",
            "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n",
            "\n",
            "This  example  looks  like  the  previous  one,  except  we  separate  the  creation  of  the\n",
            "layers17 in the constructor from their usage in the call() method. And we don’t need\n",
            "to create the Input objects: we can use the input argument to the call() method.\n",
            "\n",
            "17 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why\n",
            "\n",
            "we renamed it to main_output.\n",
            "\n",
            "336 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fNow  that  we  have  a  model  instance,  we  can  compile  it,  adapt  its  normal‐\n",
            "ization  layers  (e.g.,  using  model.norm_layer_wide.adapt(...)  and  model.norm_\n",
            "layer_deep.adapt(...)),  fit  it,  evaluate  it,  and  use  it  to  make  predictions,  exactly\n",
            "like we did with the functional API.\n",
            "\n",
            "The  big  difference  with  this  API  is  that  you  can  include  pretty  much  anything\n",
            "you  want  in  the  call()  method:  for  loops,  if  statements,  low-level  TensorFlow\n",
            "operations—your  imagination  is  the  limit  (see  Chapter  12)!  This  makes  it  a  great\n",
            "API  when  experimenting  with  new  ideas,  especially  for  researchers.  However,  this\n",
            "extra  flexibility  does  come  at  a  cost:  your  model’s  architecture  is  hidden  within  the\n",
            "call() method, so Keras cannot easily inspect it; the model cannot be cloned using\n",
            "tf.keras.models.clone_model();  and  when  you  call  the  summary()  method,  you\n",
            "only get a list of layers, without any information on how they are connected to each\n",
            "other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier\n",
            "to make mistakes. So unless you really need that extra flexibility, you should probably\n",
            "stick to the sequential API or the functional API.\n",
            "\n",
            "Keras models can be used just like regular layers, so you can easily\n",
            "combine them to build complex architectures.\n",
            "\n",
            "Now that you know how to build and train neural nets using Keras, you will want to\n",
            "save them!\n",
            "\n",
            "Saving and Restoring a Model\n",
            "Saving a trained Keras model is as simple as it gets:\n",
            "\n",
            "model.save(\"my_keras_model\", save_format=\"tf\")\n",
            "\n",
            "When you set save_format=\"tf\",18 Keras saves the model using TensorFlow’s Saved‐\n",
            "Model  format:  this  is  a  directory  (with  the  given  name)  containing  several  files  and\n",
            "subdirectories. In particular, the saved_model.pb file contains the model’s architecture\n",
            "and  logic  in  the  form  of  a  serialized  computation  graph,  so  you  don’t  need  to\n",
            "deploy  the  model’s  source  code  in  order  to  use  it  in  production;  the  SavedModel\n",
            "is sufficient (you will see how this works in Chapter 12). The keras_metadata.pb file\n",
            "contains extra information needed by Keras. The variables subdirectory contains all\n",
            "the parameter values (including the connection weights, the biases, the normalization\n",
            "statistics,  and  the  optimizer’s  parameters),  possibly  split  across  multiple  files  if  the\n",
            "\n",
            "18 This is currently the default, but the Keras team is working on a new format that may become the default in\n",
            "\n",
            "upcoming versions, so I prefer to set the format explicitly to be future-proof.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "337\n",
            "\n",
            "\fmodel  is  very  large.  Lastly,  the  assets  directory  may  contain  extra  files,  such  as  data\n",
            "samples,  feature  names,  class  names,  and  so  on.  By  default,  the  assets  directory  is\n",
            "empty. Since the optimizer is also saved, including its hyperparameters and any state\n",
            "it may have, after loading the model you can continue training if you want.\n",
            "\n",
            "If  you  set  save_format=\"h5\"  or  use  a  filename  that  ends\n",
            "with .h5, .hdf5, or .keras, then Keras will save the model to a single\n",
            "file using a Keras-specific format based on the HDF5 format. How‐\n",
            "ever,  most  TensorFlow  deployment  tools  require  the  SavedModel\n",
            "format instead.\n",
            "\n",
            "You  will  typically  have  a  script  that  trains  a  model  and  saves  it,  and  one  or  more\n",
            "scripts  (or  web  services)  that  load  the  model  and  use  it  to  evaluate  it  or  to  make\n",
            "predictions. Loading the model is just as easy as saving it:\n",
            "\n",
            "model = tf.keras.models.load_model(\"my_keras_model\")\n",
            "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n",
            "\n",
            "You  can  also  use  save_weights()  and  load_weights()  to  save  and  load  only  the\n",
            "parameter  values.  This  includes  the  connection  weights,  biases,  preprocessing  stats,\n",
            "optimizer  state,  etc.  The  parameter  values  are  saved  in  one  or  more  files  such  as\n",
            "my_weights.data-00004-of-00052, plus an index file like my_weights.index.\n",
            "\n",
            "Saving just the weights is faster and uses less disk space than saving the whole model,\n",
            "so it’s perfect to save quick checkpoints during training. If you’re training a big model,\n",
            "and  it  takes  hours  or  days,  then  you  must  save  checkpoints  regularly  in  case  the\n",
            "computer crashes. But how can you tell the fit() method to save checkpoints? Use\n",
            "callbacks.\n",
            "\n",
            "Using Callbacks\n",
            "The fit() method accepts a callbacks argument that lets you specify a list of objects\n",
            "that  Keras  will  call  before  and  after  training,  before  and  after  each  epoch,  and  even\n",
            "before  and  after  processing  each  batch.  For  example,  the  ModelCheckpoint  callback\n",
            "saves checkpoints of your model at regular intervals during training, by default at the\n",
            "end of each epoch:\n",
            "\n",
            "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\",\n",
            "                                                   save_weights_only=True)\n",
            "history = model.fit([...], callbacks=[checkpoint_cb])\n",
            "\n",
            "338 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fMoreover,  if  you  use  a  validation  set  during  training,  you  can  set  save_\n",
            "best_only=True  when  creating  the  ModelCheckpoint.  In  this  case,  it  will  only  save\n",
            "your  model  when  its  performance  on  the  validation  set  is  the  best  so  far.  This  way,\n",
            "you  do  not  need  to  worry  about  training  for  too  long  and  overfitting  the  training\n",
            "set: simply restore the last saved model after training, and this will be the best model\n",
            "on  the  validation  set.  This  is  one  way  to  implement  early  stopping  (introduced  in\n",
            "Chapter 4), but it won’t actually stop training.\n",
            "\n",
            "Another  way  is  to  use  the  EarlyStopping  callback.  It  will  interrupt  training  when\n",
            "it  measures  no  progress  on  the  validation  set  for  a  number  of  epochs  (defined  by\n",
            "the patience argument), and if you set restore_best_weights=True it will roll back\n",
            "to  the  best  model  at  the  end  of  training.  You  can  combine  both  callbacks  to  save\n",
            "checkpoints  of  your  model  in  case  your  computer  crashes,  and  interrupt  training\n",
            "early  when  there  is  no  more  progress,  to  avoid  wasting  time  and  resources  and  to\n",
            "reduce overfitting:\n",
            "\n",
            "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
            "                                                     restore_best_weights=True)\n",
            "history = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])\n",
            "\n",
            "The number of epochs can be set to a large value since training will stop automati‐\n",
            "cally  when  there  is  no  more  progress  (just  make  sure  the  learning  rate  is  not  too\n",
            "small, or else it might keep making slow progress until the end). The EarlyStopping\n",
            "callback will store the weights of the best model in RAM, and it will restore them for\n",
            "you at the end of training.\n",
            "\n",
            "Many  other  callbacks  are  available  in  the  tf.keras.callbacks\n",
            "package.\n",
            "\n",
            "If  you  need  extra  control,  you  can  easily  write  your  own  custom  callbacks.  For\n",
            "example, the following custom callback will display the ratio between the validation\n",
            "loss and the training loss during training (e.g., to detect overfitting):\n",
            "\n",
            "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
            "    def on_epoch_end(self, epoch, logs):\n",
            "        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
            "        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "339\n",
            "\n",
            "\fAs  you  might  expect,  you  can  implement  on_train_begin(),  on_train_end(),\n",
            "on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Call‐\n",
            "backs  can  also  be  used  during  evaluation  and  predictions,  should  you  ever  need\n",
            "them (e.g., for debugging). For evaluation, you should implement on_test_begin(),\n",
            "on_test_end(), on_test_batch_begin(), or on_test_batch_end(), which are called\n",
            "by  evaluate().  For  prediction,  you  should  implement  on_predict_begin(),\n",
            "on_predict_end(),  on_predict_batch_begin(),  or  on_predict_batch_end(),\n",
            "which are called by predict().\n",
            "\n",
            "Now  let’s  take  a  look  at  one  more  tool  you  should  definitely  have  in  your  toolbox\n",
            "when using Keras: TensorBoard.\n",
            "\n",
            "Using TensorBoard for Visualization\n",
            "TensorBoard  is  a  great  interactive  visualization  tool  that  you  can  use  to  view  the\n",
            "learning curves during training, compare curves and metrics between multiple runs,\n",
            "visualize  the  computation  graph,  analyze  training  statistics,  view  images  generated\n",
            "by  your  model,  visualize  complex  multidimensional  data  projected  down  to  3D\n",
            "and  automatically  clustered  for  you,  profile  your  network  (i.e.,  measure  its  speed  to\n",
            "identify bottlenecks), and more!\n",
            "\n",
            "TensorBoard  is  installed  automatically  when  you  install  TensorFlow.  However,  you\n",
            "will  need  a  TensorBoard  plug-in  to  visualize  profiling  data.  If  you  followed  the\n",
            "installation instructions at https://homl.info/install to run everything locally, then you\n",
            "already have the plug-in installed, but if you are using Colab, then you must run the\n",
            "following command:\n",
            "\n",
            "%pip install -q -U tensorboard-plugin-profile\n",
            "\n",
            "To use TensorBoard, you must modify your program so that it outputs the data you\n",
            "want to visualize to special binary logfiles called event files. Each binary data record\n",
            "is  called  a  summary.  The  TensorBoard  server  will  monitor  the  log  directory,  and  it\n",
            "will automatically pick up the changes and update the visualizations: this allows you\n",
            "to visualize live data (with a short delay), such as the learning curves during training.\n",
            "In  general,  you  want  to  point  the  TensorBoard  server  to  a  root  log  directory  and\n",
            "configure  your  program  so  that  it  writes  to  a  different  subdirectory  every  time  it\n",
            "runs. This way, the same TensorBoard server instance will allow you to visualize and\n",
            "compare data from multiple runs of your program, without getting everything mixed\n",
            "up.\n",
            "\n",
            "Let’s  name  the  root  log  directory  my_logs,  and  let’s  define  a  little  function  that\n",
            "generates the path of the log subdirectory based on the current date and time, so that\n",
            "it’s different at every run:\n",
            "\n",
            "340 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\ffrom pathlib import Path\n",
            "from time import strftime\n",
            "\n",
            "def get_run_logdir(root_logdir=\"my_logs\"):\n",
            "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
            "\n",
            "run_logdir = get_run_logdir()  # e.g., my_logs/run_2022_08_01_17_25_59\n",
            "\n",
            "The good news is that Keras provides a convenient TensorBoard() callback that will\n",
            "take  care  of  creating  the  log  directory  for  you  (along  with  its  parent  directories  if\n",
            "needed), and it will create event files and write summaries to them during training. It\n",
            "will measure your model’s training and validation loss and metrics (in this case, the\n",
            "MSE and RMSE), and it will also profile your neural network. It is straightforward to\n",
            "use:\n",
            "\n",
            "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n",
            "                                                profile_batch=(100, 200))\n",
            "history = model.fit([...], callbacks=[tensorboard_cb])\n",
            "\n",
            "That’s  all  there  is  to  it!  In  this  example,  it  will  profile  the  network  between  batches\n",
            "100  and  200  during  the  first  epoch.  Why  100  and  200?  Well,  it  often  takes  a  few\n",
            "batches for the neural network to “warm up”, so you don’t want to profile too early,\n",
            "and profiling uses resources, so it’s best not to do it for every batch.\n",
            "\n",
            "Next, try changing the learning rate from 0.001 to 0.002, and run the code again, with\n",
            "a new log subdirectory. You will end up with a directory structure similar to this one:\n",
            "\n",
            "my_logs\n",
            "├── run_2022_08_01_17_25_59\n",
            "│   ├── train\n",
            "│   │   ├── events.out.tfevents.1659331561.my_host_name.42042.0.v2\n",
            "│   │   ├── events.out.tfevents.1659331562.my_host_name.profile-empty\n",
            "│   │   └── plugins\n",
            "│   │       └── profile\n",
            "│   │           └── 2022_08_01_17_26_02\n",
            "│   │               ├── my_host_name.input_pipeline.pb\n",
            "│   │               └── [...]\n",
            "│   └── validation\n",
            "│       └── events.out.tfevents.1659331562.my_host_name.42042.1.v2\n",
            "└── run_2022_08_01_17_31_12\n",
            "    └── [...]\n",
            "\n",
            "There’s one directory per run, each containing one subdirectory for training logs and\n",
            "one  for  validation  logs.  Both  contain  event  files,  and  the  training  logs  also  include\n",
            "profiling traces.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "341\n",
            "\n",
            "\fNow  that  you  have  the  event  files  ready,  it’s  time  to  start  the  TensorBoard  server.\n",
            "This  can  be  done  directly  within  Jupyter  or  Colab  using  the  Jupyter  extension  for\n",
            "TensorBoard, which gets installed along with the TensorBoard library. This extension\n",
            "is preinstalled in Colab. The following code loads the Jupyter extension for Tensor‐\n",
            "Board,  and  the  second  line  starts  a  TensorBoard  server  for  the  my_logs  directory,\n",
            "connects to this server and displays the user interface directly inside of Jupyter. The\n",
            "server, listens on the first available TCP port greater than or equal to 6006 (or you can\n",
            "set the port you want using the --port option).\n",
            "\n",
            "%load_ext tensorboard\n",
            "%tensorboard --logdir=./my_logs\n",
            "\n",
            "If you’re running everything on your own machine, it’s possible to\n",
            "start TensorBoard by executing tensorboard --logdir=./my_logs\n",
            "in  a  terminal.  You  must  first  activate  the  Conda  environment  in\n",
            "which you installed TensorBoard, and go to the handson-ml3 direc‐\n",
            "tory. Once the server is started, visit http://localhost:6006.\n",
            "\n",
            "Now  you  should  see  TensorBoard’s  user  interface.  Click  the  SCALARS  tab  to  view\n",
            "the  learning  curves  (see  Figure  10-16).  At  the  bottom  left,  select  the  logs  you  want\n",
            "to  visualize  (e.g.,  the  training  logs  from  the  first  and  second  run),  and  click  the\n",
            "epoch_loss scalar. Notice that the training loss went down nicely during both runs,\n",
            "but in the second run it went down a bit faster thanks to the higher learning rate.\n",
            "\n",
            "Figure 10-16. Visualizing learning curves with TensorBoard\n",
            "\n",
            "342 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fYou can also visualize the whole computation graph in the GRAPHS tab, the learned\n",
            "weights  projected  to  3D  in  the  PROJECTOR  tab,  and  the  profiling  traces  in  the\n",
            "PROFILE  tab.  The  TensorBoard()  callback  has  options  to  log  extra  data  too  (see\n",
            "the documentation for more details). You can click the refresh button (⟳) at the top\n",
            "right to make TensorBoard refresh data, and you can click the settings button (⚙) to\n",
            "activate auto-refresh and specify the refresh interval.\n",
            "\n",
            "Additionally,  TensorFlow  offers  a  lower-level  API  in  the  tf.summary  package.  The\n",
            "following  code  creates  a  SummaryWriter  using  the  create_file_writer()  function,\n",
            "and it uses this writer as a Python context to log scalars, histograms, images, audio,\n",
            "and text, all of which can then be visualized using TensorBoard:\n",
            "\n",
            "test_logdir = get_run_logdir()\n",
            "writer = tf.summary.create_file_writer(str(test_logdir))\n",
            "with writer.as_default():\n",
            "    for step in range(1, 1000 + 1):\n",
            "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
            "\n",
            "        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n",
            "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
            "\n",
            "        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n",
            "        tf.summary.image(\"my_images\", images, step=step)\n",
            "\n",
            "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n",
            "        tf.summary.text(\"my_text\", texts, step=step)\n",
            "\n",
            "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
            "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
            "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n",
            "\n",
            "If you run this code and click the refresh button in TensorBoard, you will see several\n",
            "tabs  appear:  IMAGES,  AUDIO,  DISTRIBUTIONS,  HISTOGRAMS,  and  TEXT.  Try\n",
            "clicking the IMAGES tab, and use the slider above each image to view the images at\n",
            "different  time  steps.  Similarly,  go  to  the  AUDIO  tab  and  try  listening  to  the  audio\n",
            "at  different  time  steps.  As  you  can  see,  TensorBoard  is  a  useful  tool  even  beyond\n",
            "TensorFlow or deep learning.\n",
            "\n",
            "You  can  share  your  results  online  by  publishing  them  to  https://\n",
            "tensorboard.dev.  For  this,  just  run  !tensorboard  dev  upload\n",
            "--logdir  ./my_logs.  The  first  time,  it  will  ask  you  to  accept  the\n",
            "terms  and  conditions  and  authenticate.  Then  your  logs  will  be\n",
            "uploaded, and you will get a permanent link to view your results in\n",
            "a TensorBoard interface.\n",
            "\n",
            "Implementing MLPs with Keras \n",
            "\n",
            "| \n",
            "\n",
            "343\n",
            "\n",
            "\fLet’s  summarize  what  you’ve  learned  so  far  in  this  chapter:  you  now  know  where\n",
            "neural  nets  came  from,  what  an  MLP  is  and  how  you  can  use  it  for  classification\n",
            "and regression, how to use Keras’s sequential API to build MLPs, and how to use the\n",
            "functional  API  or  the  subclassing  API  to  build  more  complex  model  architectures\n",
            "(including Wide & Deep models, as well as models with multiple inputs and outputs).\n",
            "You also learned how to save and restore a model and how to use callbacks for check‐\n",
            "pointing,  early  stopping,  and  more.  Finally,  you  learned  how  to  use  TensorBoard\n",
            "for visualization. You can already go ahead and use neural networks to tackle many\n",
            "problems! However, you may wonder how to choose the number of hidden layers, the\n",
            "number  of  neurons  in  the  network,  and  all  the  other  hyperparameters.  Let’s  look  at\n",
            "this now.\n",
            "\n",
            "Fine-Tuning Neural Network Hyperparameters\n",
            "The  flexibility  of  neural  networks  is  also  one  of  their  main  drawbacks:  there  are\n",
            "many  hyperparameters  to  tweak.  Not  only  can  you  use  any  imaginable  network\n",
            "architecture,  but  even  in  a  basic  MLP  you  can  change  the  number  of  layers,  the\n",
            "number of neurons and the type of activation function to use in each layer, the weight\n",
            "initialization logic, the type of optimizer to use, its learning rate, the batch size, and\n",
            "more. How do you know what combination of hyperparameters is the best for your\n",
            "task?\n",
            "\n",
            "One  option  is  to  convert  your  Keras  model  to  a  Scikit-Learn  estimator,  and  then\n",
            "use GridSearchCV or RandomizedSearchCV to fine-tune the hyperparameters, as you\n",
            "did  in  Chapter  2.  For  this,  you  can  use  the  KerasRegressor  and  KerasClassifier\n",
            "wrapper classes from the SciKeras library (see https://github.com/adriangb/scikeras for\n",
            "more  details).  However,  there’s  a  better  way:  you  can  use  the  Keras  Tuner  library,\n",
            "which  is  a  hyperparameter  tuning  library  for  Keras  models.  It  offers  several  tuning\n",
            "strategies, it’s highly customizable, and it has excellent integration with TensorBoard.\n",
            "Let’s see how to use it.\n",
            "\n",
            "If  you  followed  the  installation  instructions  at  https://homl.info/install  to  run  every‐\n",
            "thing locally, then you already have Keras Tuner installed, but if you are using Colab,\n",
            "you’ll  need  to  run  %pip  install  -q  -U  keras-tuner.  Next,  import  keras_tuner,\n",
            "usually as kt, then write a function that builds, compiles, and returns a Keras model.\n",
            "The function must take a  kt.HyperParameters object as an argument, which it can\n",
            "use  to  define  hyperparameters  (integers,  floats,  strings,  etc.)  along  with  their  range\n",
            "of  possible  values,  and  these  hyperparameters  may  be  used  to  build  and  compile\n",
            "the model. For example, the following function builds and compiles an MLP to clas‐\n",
            "sify  Fashion  MNIST  images,  using  hyperparameters  such  as  the  number  of  hidden\n",
            "layers  (n_hidden),  the  number  of  neurons  per  layer  (n_neurons),  the  learning  rate\n",
            "(learning_rate), and the type of optimizer to use (optimizer):\n",
            "\n",
            "344 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fimport keras_tuner as kt\n",
            "\n",
            "def build_model(hp):\n",
            "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
            "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
            "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
            "                             sampling=\"log\")\n",
            "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
            "    if optimizer == \"sgd\":\n",
            "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
            "    else:\n",
            "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
            "\n",
            "    model = tf.keras.Sequential()\n",
            "    model.add(tf.keras.layers.Flatten())\n",
            "    for _ in range(n_hidden):\n",
            "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
            "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
            "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
            "                  metrics=[\"accuracy\"])\n",
            "    return model\n",
            "\n",
            "The  first  part  of  the  function  defines  the  hyperparameters.  For  example,\n",
            "hp.Int(\"n_hidden\",  min_value=0,  max_value=8,  default=2)  checks  whether  a\n",
            "hyperparameter named \"n_hidden\" is already present in the HyperParameters object\n",
            "hp, and if so it returns its value. If not, then it registers a new integer hyperparameter\n",
            "named  \"n_hidden\",  whose  possible  values  range  from  0  to  8  (inclusive),  and  it\n",
            "returns  the  default  value,  which  is  2  in  this  case  (when  default  is  not  set,  then\n",
            "min_value  is  returned).  The  \"n_neurons\"  hyperparameter  is  registered  in  a  similar\n",
            "way. The \"learning_rate\" hyperparameter is registered as a float ranging from 10–4\n",
            "to 10–2, and since sampling=\"log\", learning rates of all scales will be sampled equally.\n",
            "Lastly,  the  optimizer  hyperparameter  is  registered  with  two  possible  values:  \"sgd\"\n",
            "or \"adam\" (the default value is the first one, which is \"sgd\" in this case). Depending\n",
            "on the value of optimizer, we create an SGD optimizer or an Adam optimizer with the\n",
            "given learning rate.\n",
            "\n",
            "The  second  part  of  the  function  just  builds  the  model  using  the  hyperparameter\n",
            "values. It creates a Sequential model starting with a Flatten layer, followed by the\n",
            "requested number of hidden layers (as determined by the n_hidden hyperparameter)\n",
            "using  the  ReLU  activation  function,  and  an  output  layer  with  10  neurons  (one  per\n",
            "class) using the softmax activation function. Lastly, the function compiles the model\n",
            "and returns it.\n",
            "\n",
            "Now  if  you  want  to  do  a  basic  random  search,  you  can  create  a  kt.RandomSearch\n",
            "tuner,  passing  the  build_model  function  to  the  constructor,  and  call  the  tuner’s\n",
            "search() method:\n",
            "\n",
            "Fine-Tuning Neural Network Hyperparameters \n",
            "\n",
            "| \n",
            "\n",
            "345\n",
            "\n",
            "\frandom_search_tuner = kt.RandomSearch(\n",
            "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
            "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
            "random_search_tuner.search(X_train, y_train, epochs=10,\n",
            "                           validation_data=(X_valid, y_valid))\n",
            "\n",
            "tuner \n",
            "\n",
            "The  RandomSearch \n",
            "first  calls  build_model()  once  with  an  empty\n",
            "Hyperparameters  object,  just  to  gather  all  the  hyperparameter  specifications.  Then,\n",
            "in this example, it runs 5 trials; for each trial it builds a model using hyperparame‐\n",
            "ters  sampled  randomly  within  their  respective  ranges,  then  it  trains  that  model  for\n",
            "10  epochs  and  saves  it  to  a  subdirectory  of  the  my_fashion_mnist/my_rnd_search\n",
            "directory. Since overwrite=True, the my_rnd_search directory is deleted before train‐\n",
            "ing  starts.  If  you  run  this  code  a  second  time  but  with  overwrite=False  and  max_\n",
            "trials=10, the tuner will continue tuning where it left off, running 5 more trials: this\n",
            "means you don’t have to run all the trials in one shot. Lastly, since objective is set to\n",
            "\"val_accuracy\", the tuner prefers models with a higher validation accuracy, so once\n",
            "the tuner has finished searching, you can get the best models like this:\n",
            "\n",
            "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
            "best_model = top3_models[0]\n",
            "\n",
            "You  can  also  call  get_best_hyperparameters()  to  get  the  kt.HyperParameters  of\n",
            "the best models:\n",
            "\n",
            ">>> top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
            ">>> top3_params[0].values  # best hyperparameter values\n",
            "{'n_hidden': 5,\n",
            " 'n_neurons': 70,\n",
            " 'learning_rate': 0.00041268008323824807,\n",
            " 'optimizer': 'adam'}\n",
            "\n",
            "Each tuner is guided by a so-called oracle: before each trial, the tuner asks the oracle\n",
            "to tell it what the next trial should be. The RandomSearch tuner uses a RandomSearch\n",
            "Oracle, which is pretty basic: it just picks the next trial randomly, as we saw earlier.\n",
            "Since the oracle keeps track of all the trials, you can ask it to give you the best one,\n",
            "and you can display a summary of that trial:\n",
            "\n",
            ">>> best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
            ">>> best_trial.summary()\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_hidden: 5\n",
            "n_neurons: 70\n",
            "learning_rate: 0.00041268008323824807\n",
            "optimizer: adam\n",
            "Score: 0.8736000061035156\n",
            "\n",
            "346 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fThis shows the best hyperparameters (like earlier), as well as the validation accuracy.\n",
            "You can also access all the metrics directly:\n",
            "\n",
            ">>> best_trial.metrics.get_last_value(\"val_accuracy\")\n",
            "0.8736000061035156\n",
            "\n",
            "If you are happy with the best model’s performance, you may continue training it for\n",
            "a few epochs on the full training set (X_train_full and y_train_full), then evaluate\n",
            "it on the test set, and deploy it to production (see Chapter 19):\n",
            "\n",
            "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
            "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
            "\n",
            "In  some  cases,  you  may  want  to  fine-tune  data  preprocessing  hyperparameters,  or\n",
            "model.fit()  arguments,  such  as  the  batch  size.  For  this,  you  must  use  a  slightly\n",
            "different technique: instead of writing a build_model() function, you must subclass\n",
            "the kt.HyperModel class and define two methods, build() and fit(). The build()\n",
            "method does the exact same thing as the build_model() function. The fit() method\n",
            "takes  a  HyperParameters  object  and  a  compiled  model  as  an  argument,  as  well  as\n",
            "all  the  model.fit()  arguments,  and  fits  the  model  and  returns  the  History  object.\n",
            "Crucially,  the  fit()  method  may  use  hyperparameters  to  decide  how  to  preprocess\n",
            "the data, tweak the batch size, and more. For example, the following class builds the\n",
            "same  model  as  before,  with  the  same  hyperparameters,  but  it  also  uses  a  Boolean\n",
            "\"normalize\"  hyperparameter  to  control  whether  or  not  to  standardize  the  training\n",
            "data before fitting the model:\n",
            "\n",
            "class MyClassificationHyperModel(kt.HyperModel):\n",
            "    def build(self, hp):\n",
            "        return build_model(hp)\n",
            "\n",
            "    def fit(self, hp, model, X, y, **kwargs):\n",
            "        if hp.Boolean(\"normalize\"):\n",
            "            norm_layer = tf.keras.layers.Normalization()\n",
            "            X = norm_layer(X)\n",
            "        return model.fit(X, y, **kwargs)\n",
            "\n",
            "You  can  then  pass  an  instance  of  this  class  to  the  tuner  of  your  choice,  instead  of\n",
            "passing  the  build_model  function.  For  example,  let’s  build  a  kt.Hyperband  tuner\n",
            "based on a MyClassificationHyperModel instance:\n",
            "\n",
            "hyperband_tuner = kt.Hyperband(\n",
            "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
            "    max_epochs=10, factor=3, hyperband_iterations=2,\n",
            "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")\n",
            "\n",
            "This tuner is similar to the HalvingRandomSearchCV class we discussed in Chapter 2:\n",
            "it  starts  by  training  many  different  models  for  few  epochs,  then  it  eliminates  the\n",
            "worst models and keeps only the top  1 / factor models (i.e., the top third in this\n",
            "\n",
            "Fine-Tuning Neural Network Hyperparameters \n",
            "\n",
            "| \n",
            "\n",
            "347\n",
            "\n",
            "\fcase),  repeating  this  selection  process  until  a  single  model  is  left.19  The  max_epochs\n",
            "argument  controls  the  max  number  of  epochs  that  the  best  model  will  be  trained\n",
            "for.  The  whole  process  is  repeated  twice  in  this  case  (hyperband_iterations=2).\n",
            "The total number of training epochs across all models for each hyperband iteration\n",
            "is  about  max_epochs  *  (log(max_epochs)  /  log(factor))  **  2,  so  it’s  about  44\n",
            "epochs in this example. The other arguments are the same as for kt.RandomSearch.\n",
            "\n",
            "Let’s  run  the  Hyperband  tuner  now.  We’ll  use  the  TensorBoard  callback,  this  time\n",
            "pointing  to  the  root  log  directory  (the  tuner  will  take  care  of  using  a  different\n",
            "subdirectory for each trial), as well as an EarlyStopping callback:\n",
            "\n",
            "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
            "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
            "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\n",
            "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
            "                       validation_data=(X_valid, y_valid),\n",
            "                       callbacks=[early_stopping_cb, tensorboard_cb])\n",
            "\n",
            "Now  if  you  open  TensorBoard,  pointing  --logdir  to  the  my_fashion_mnist/hyper‐\n",
            "band/tensorboard  directory,  you  will  see  all  the  trial  results  as  they  unfold.  Make\n",
            "sure  to  visit  the  HPARAMS  tab:  it  contains  a  summary  of  all  the  hyperparameter\n",
            "combinations  that  were  tried,  along  with  the  corresponding  metrics.  Notice  that\n",
            "there  are  three  tabs  inside  the  HPARAMS  tab:  a  table  view,  a  parallel  coordinates\n",
            "view,  and  a  scatterplot  matrix  view.  In  the  lower  part  of  the  left  panel,  uncheck\n",
            "all  metrics  except  for  validation.epoch_accuracy:  this  will  make  the  graphs\n",
            "clearer.  In  the  parallel  coordinates  view,  try  selecting  a  range  of  high  values  in\n",
            "the  validation.epoch_accuracy  column:  this  will  filter  only  the  hyperparameter\n",
            "combinations  that  reached  a  good  performance.  Click  one  of  the  hyperparameter\n",
            "combinations,  and  the  corresponding  learning  curves  will  appear  at  the  bottom  of\n",
            "the page. Take some time to go through each tab; this will help you understand the\n",
            "effect of each hyperparameter on performance, as well as the interactions between the\n",
            "hyperparameters.\n",
            "\n",
            "Hyperband  is  smarter  than  pure  random  search  in  the  way  it  allocates  resources,\n",
            "but  at  its  core  it  still  explores  the  hyperparameter  space  randomly;  it’s  fast,  but\n",
            "coarse. However, Keras Tuner also includes a  kt.BayesianOptimization tuner: this\n",
            "algorithm  gradually  learns  which  regions  of  the  hyperparameter  space  are  most\n",
            "promising by fitting a probabilistic model called a Gaussian process. This allows it to\n",
            "gradually zoom in on the best hyperparameters. The downside is that the algorithm\n",
            "has  its  own  hyperparameters:  alpha  represents  the  level  of  noise  you  expect  in  the\n",
            "performance measures across trials (it defaults to 10–4), and beta specifies how much\n",
            "\n",
            "19 Hyperband is actually a bit more sophisticated than successive halving; see the paper by Lisha Li et al.,\n",
            "\n",
            "“Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization”, Journal of Machine Learning\n",
            "Research 18 (April 2018): 1–52.\n",
            "\n",
            "348 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fyou  want  the  algorithm  to  explore,  instead  of  simply  exploiting  the  known  good\n",
            "regions of hyperparameter space (it defaults to 2.6). Other than that, this tuner can be\n",
            "used just like the previous ones:\n",
            "\n",
            "bayesian_opt_tuner = kt.BayesianOptimization(\n",
            "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
            "    max_trials=10, alpha=1e-4, beta=2.6,\n",
            "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n",
            "bayesian_opt_tuner.search([...])\n",
            "\n",
            "Hyperparameter tuning is still an active area of research, and many other approaches\n",
            "are being explored. For example, check out DeepMind’s excellent 2017 paper,20 where\n",
            "the authors used an evolutionary algorithm to jointly optimize a population of mod‐\n",
            "els  and  their  hyperparameters.  Google  has  also  used  an  evolutionary  approach,  not\n",
            "just to search for hyperparameters but also to explore all sorts of model architectures:\n",
            "it  powers  their  AutoML  service  on  Google  Vertex  AI  (see  Chapter  19).  The  term\n",
            "AutoML  refers  to  any  system  that  takes  care  of  a  large  part  of  the  ML  workflow.\n",
            "Evolutionary algorithms have even been used successfully to train individual neural\n",
            "networks,  replacing  the  ubiquitous  gradient  descent!  For  an  example,  see  the  2017\n",
            "post by Uber where the authors introduce their Deep Neuroevolution technique.\n",
            "\n",
            "But despite all this exciting progress and all these tools and services, it still helps to\n",
            "have an idea of what values are reasonable for each hyperparameter so that you can\n",
            "build a quick prototype and restrict the search space. The following sections provide\n",
            "guidelines for choosing the number of hidden layers and neurons in an MLP and for\n",
            "selecting good values for some of the main hyperparameters.\n",
            "\n",
            "Number of Hidden Layers\n",
            "For  many  problems,  you  can  begin  with  a  single  hidden  layer  and  get  reasonable\n",
            "results.  An  MLP  with  just  one  hidden  layer  can  theoretically  model  even  the  most\n",
            "complex functions, provided it has enough neurons. But for complex problems, deep\n",
            "networks have a much higher parameter efficiency than shallow ones: they can model\n",
            "complex  functions  using  exponentially  fewer  neurons  than  shallow  nets,  allowing\n",
            "them to reach much better performance with the same amount of training data.\n",
            "\n",
            "To understand why, suppose you are asked to draw a forest using some drawing soft‐\n",
            "ware, but you are forbidden to copy and paste anything. It would take an enormous\n",
            "amount  of  time:  you  would  have  to  draw  each  tree  individually,  branch  by  branch,\n",
            "leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch,\n",
            "then copy and paste that branch to create a tree, and finally copy and paste this tree to\n",
            "make a forest, you would be finished in no time. Real-world data is often structured\n",
            "\n",
            "20 Max Jaderberg et al., “Population Based Training of Neural Networks”, arXiv preprint arXiv:1711.09846\n",
            "\n",
            "(2017).\n",
            "\n",
            "Fine-Tuning Neural Network Hyperparameters \n",
            "\n",
            "| \n",
            "\n",
            "349\n",
            "\n",
            "\fin such a hierarchical way, and deep neural networks automatically take advantage of\n",
            "this fact: lower hidden layers model low-level structures (e.g., line segments of various\n",
            "shapes  and  orientations),  intermediate  hidden  layers  combine  these  low-level  struc‐\n",
            "tures  to  model  intermediate-level  structures  (e.g.,  squares,  circles),  and  the  highest\n",
            "hidden  layers  and  the  output  layer  combine  these  intermediate  structures  to  model\n",
            "high-level structures (e.g., faces).\n",
            "\n",
            "Not  only  does  this  hierarchical  architecture  help  DNNs  converge  faster  to  a  good\n",
            "solution, but it also improves their ability to generalize to new datasets. For example,\n",
            "if you have already trained a model to recognize faces in pictures and you now want\n",
            "to train a new neural network to recognize hairstyles, you can kickstart the training\n",
            "by reusing the lower layers of the first network. Instead of randomly initializing the\n",
            "weights and biases of the first few layers of the new neural network, you can initialize\n",
            "them to the values of the weights and biases of the lower layers of the first network.\n",
            "This way the network will not have to learn from scratch all the low-level structures\n",
            "that occur in most pictures; it will only have to learn the higher-level structures (e.g.,\n",
            "hairstyles). This is called transfer learning.\n",
            "\n",
            "In  summary,  for  many  problems  you  can  start  with  just  one  or  two  hidden  layers\n",
            "and the neural network will work just fine. For instance, you can easily reach above\n",
            "97% accuracy on the MNIST dataset using just one hidden layer with a few hundred\n",
            "neurons,  and  above  98%  accuracy  using  two  hidden  layers  with  the  same  total\n",
            "number of neurons, in roughly the same amount of training time. For more complex\n",
            "problems,  you  can  ramp  up  the  number  of  hidden  layers  until  you  start  overfitting\n",
            "the  training  set.  Very  complex  tasks,  such  as  large  image  classification  or  speech\n",
            "recognition, typically require networks with dozens of layers (or even hundreds, but\n",
            "not fully connected ones, as you will see in Chapter 14), and they need a huge amount\n",
            "of training data. You will rarely have to train such networks from scratch: it is much\n",
            "more common to reuse parts of a pretrained state-of-the-art network that performs\n",
            "a similar task. Training will then be a lot faster and require much less data (we will\n",
            "discuss this in Chapter 11).\n",
            "\n",
            "Number of Neurons per Hidden Layer\n",
            "The number of neurons in the input and output layers is determined by the type of\n",
            "input and output your task requires. For example, the MNIST task requires 28 × 28 =\n",
            "784 inputs and 10 output neurons.\n",
            "\n",
            "As  for  the  hidden  layers,  it  used  to  be  common  to  size  them  to  form  a  pyramid,\n",
            "with fewer and fewer neurons at each layer—the rationale being that many low-level\n",
            "features can coalesce into far fewer high-level features. A typical neural network for\n",
            "MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200,\n",
            "and  the  third  with  100.  However,  this  practice  has  been  largely  abandoned  because\n",
            "it  seems  that  using  the  same  number  of  neurons  in  all  hidden  layers  performs  just\n",
            "\n",
            "350 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fas well in most cases, or even better; plus, there is only one hyperparameter to tune,\n",
            "instead of one per layer. That said, depending on the dataset, it can sometimes help to\n",
            "make the first hidden layer bigger than the others.\n",
            "\n",
            "Just like the number of layers, you can try increasing the number of neurons gradu‐\n",
            "ally  until  the  network  starts  overfitting.  Alternatively,  you  can  try  building  a  model\n",
            "with slightly more layers and neurons than you actually need, then use early stopping\n",
            "and other regularization techniques to prevent it from overfitting too much. Vincent\n",
            "Vanhoucke,  a  scientist  at  Google,  has  dubbed  this  the  “stretch  pants”  approach:\n",
            "instead of wasting time looking for pants that perfectly match your size, just use large\n",
            "stretch  pants  that  will  shrink  down  to  the  right  size.  With  this  approach,  you  avoid\n",
            "bottleneck layers that could ruin your model. Indeed, if a layer has too few neurons,\n",
            "it will not have enough representational power to preserve all the useful information\n",
            "from the inputs (e.g., a layer with two neurons can only output 2D data, so if it gets\n",
            "3D data as input, some information will be lost). No matter how big and powerful the\n",
            "rest of the network is, that information will never be recovered.\n",
            "\n",
            "In general you will get more bang for your buck by increasing the\n",
            "number of layers instead of the number of neurons per layer.\n",
            "\n",
            "Learning Rate, Batch Size, and Other Hyperparameters\n",
            "The number of hidden layers and neurons are not the only hyperparameters you can\n",
            "tweak in an MLP. Here are some of the most important ones, as well as tips on how to\n",
            "set them:\n",
            "\n",
            "Learning rate\n",
            "\n",
            "The learning rate is arguably the most important hyperparameter. In general, the\n",
            "optimal learning rate is about half of the maximum learning rate (i.e., the learn‐\n",
            "ing  rate  above  which  the  training  algorithm  diverges,  as  we  saw  in  Chapter  4).\n",
            "One way to find a good learning rate is to train the model for a few hundred iter‐\n",
            "ations, starting with a very low learning rate (e.g., 10–5) and gradually increasing\n",
            "it up to a very large value (e.g., 10). This is done by multiplying the learning rate\n",
            "by a constant factor at each iteration (e.g., by (10 / 10-5)1 / 500 to go from 10–5 to\n",
            "10 in 500 iterations). If you plot the loss as a function of the learning rate (using\n",
            "a  log  scale  for  the  learning  rate),  you  should  see  it  dropping  at  first.  But  after\n",
            "a  while,  the  learning  rate  will  be  too  large,  so  the  loss  will  shoot  back  up:  the\n",
            "optimal  learning  rate  will  be  a  bit  lower  than  the  point  at  which  the  loss  starts\n",
            "to  climb  (typically  about  10  times  lower  than  the  turning  point).  You  can  then\n",
            "reinitialize  your  model  and  train  it  normally  using  this  good  learning  rate.  We\n",
            "will look at more learning rate optimization techniques in Chapter 11.\n",
            "\n",
            "Fine-Tuning Neural Network Hyperparameters \n",
            "\n",
            "| \n",
            "\n",
            "351\n",
            "\n",
            "\fOptimizer\n",
            "\n",
            "Choosing  a  better  optimizer  than  plain  old  mini-batch  gradient  descent  (and\n",
            "tuning  its  hyperparameters)  is  also  quite  important.  We  will  examine  several\n",
            "advanced optimizers in Chapter 11.\n",
            "\n",
            "Batch size\n",
            "\n",
            "The  batch  size  can  have  a  significant  impact  on  your  model’s  performance  and\n",
            "training time. The main benefit of using large batch sizes is that hardware accel‐\n",
            "erators like GPUs can process them efficiently (see Chapter 19), so the training\n",
            "algorithm  will  see  more  instances  per  second.  Therefore,  many  researchers  and\n",
            "practitioners recommend using the largest batch size that can fit in GPU RAM.\n",
            "There’s  a  catch,  though:  in  practice,  large  batch  sizes  often  lead  to  training\n",
            "instabilities, especially at the beginning of training, and the resulting model may\n",
            "not generalize as well as a model trained with a small batch size. In April 2018,\n",
            "Yann LeCun even tweeted “Friends don’t let friends use mini-batches larger than\n",
            "32”, citing a 2018 paper21 by Dominic Masters and Carlo Luschi which concluded\n",
            "that  using  small  batches  (from  2  to  32)  was  preferable  because  small  batches\n",
            "led to better models in less training time. Other research points in the opposite\n",
            "direction, however. For example, in 2017, papers by Elad Hoffer et al.22 and Priya\n",
            "Goyal  et  al.23  showed  that  it  was  possible  to  use  very  large  batch  sizes  (up  to\n",
            "8,192) along with various techniques such as warming up the learning rate (i.e.,\n",
            "starting  training  with  a  small  learning  rate,  then  ramping  it  up,  as  discussed  in\n",
            "Chapter 11) and to obtain very short training times, without any generalization\n",
            "gap.  So,  one  strategy  is  to  try  to  using  a  large  batch  size,  with  learning  rate\n",
            "warmup,  and  if  training  is  unstable  or  the  final  performance  is  disappointing,\n",
            "then try using a small batch size instead.\n",
            "\n",
            "Activation function\n",
            "\n",
            "We  discussed  how  to  choose  the  activation  function  earlier  in  this  chapter:  in\n",
            "general, the ReLU activation function will be a good default for all hidden layers,\n",
            "but for the output layer it really depends on your task.\n",
            "\n",
            "Number of iterations\n",
            "\n",
            "In  most  cases,  the  number  of  training  iterations  does  not  actually  need  to  be\n",
            "tweaked: just use early stopping instead.\n",
            "\n",
            "21 Dominic Masters and Carlo Luschi, “Revisiting Small Batch Training for Deep Neural Networks”, arXiv\n",
            "\n",
            "preprint arXiv:1804.07612 (2018).\n",
            "\n",
            "22 Elad Hoffer et al., “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training\n",
            "\n",
            "of Neural Networks”, Proceedings of the 31st International Conference on Neural Information Processing Systems\n",
            "(2017): 1729–1739.\n",
            "\n",
            "23 Priya Goyal et al., “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv preprint\n",
            "\n",
            "arXiv:1706.02677 (2017).\n",
            "\n",
            "352 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fThe optimal learning rate depends on the other hyperparameters—\n",
            "especially  the  batch  size—so  if  you  modify  any  hyperparameter,\n",
            "make sure to update the learning rate as well.\n",
            "\n",
            "For more best practices regarding tuning neural network hyperparameters, check out\n",
            "the excellent 2018 paper24 by Leslie Smith.\n",
            "\n",
            "This  concludes  our  introduction  to  artificial  neural  networks  and  their  implemen‐\n",
            "tation  with  Keras.  In  the  next  few  chapters,  we  will  discuss  techniques  to  train\n",
            "very  deep  nets.  We  will  also  explore  how  to  customize  models  using  TensorFlow’s\n",
            "lower-level  API  and  how  to  load  and  preprocess  data  efficiently  using  the  tf.data\n",
            "API. And we will dive into other popular neural network architectures: convolutional\n",
            "neural  networks  for  image  processing,  recurrent  neural  networks  and  transformers\n",
            "for sequential data and text, autoencoders for representation learning, and generative\n",
            "adversarial networks to model and generate data.25\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. The  TensorFlow  playground  is  a  handy  neural  network  simulator  built  by  the\n",
            "TensorFlow team. In this exercise, you will train several binary classifiers in just\n",
            "a few clicks, and tweak the model’s architecture and its hyperparameters to gain\n",
            "some  intuition  on  how  neural  networks  work  and  what  their  hyperparameters\n",
            "do. Take some time to explore the following:\n",
            "\n",
            "a. The patterns learned by a neural net. Try training the default neural network\n",
            "a.\n",
            "by  clicking  the  Run  button  (top  left).  Notice  how  it  quickly  finds  a  good\n",
            "solution for the classification task. The neurons in the first hidden layer have\n",
            "learned  simple  patterns,  while  the  neurons  in  the  second  hidden  layer  have\n",
            "learned  to  combine  the  simple  patterns  of  the  first  hidden  layer  into  more\n",
            "complex patterns. In general, the more layers there are, the more complex the\n",
            "patterns can be.\n",
            "\n",
            "b. Activation functions. Try replacing the tanh activation function with a ReLU\n",
            "b.\n",
            "activation function, and train the network again. Notice that it finds a solution\n",
            "even faster, but this time the boundaries are linear. This is due to the shape of\n",
            "the ReLU function.\n",
            "\n",
            "c. The  risk  of  local  minima.  Modify  the  network  architecture  to  have  just  one\n",
            "c.\n",
            "hidden layer with three neurons. Train it multiple times (to reset the network\n",
            "\n",
            "24 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch\n",
            "\n",
            "Size, Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).\n",
            "\n",
            "25 A few extra ANN architectures are presented in the online notebook at https://homl.info/extra-anns.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "353\n",
            "\n",
            "\fweights, click the Reset button next to the Play button). Notice that the train‐\n",
            "ing time varies a lot, and sometimes it even gets stuck in a local minimum.\n",
            "\n",
            "d. What  happens  when  neural  nets  are  too  small.  Remove  one  neuron  to  keep\n",
            "d.\n",
            "just  two.  Notice  that  the  neural  network  is  now  incapable  of  finding  a  good\n",
            "solution,  even  if  you  try  multiple  times.  The  model  has  too  few  parameters\n",
            "and systematically underfits the training set.\n",
            "\n",
            "e.\n",
            "e. What happens when neural nets are large enough. Set the number of neurons\n",
            "to eight, and train the network several times. Notice that it is now consistently\n",
            "fast  and  never  gets  stuck.  This  highlights  an  important  finding  in  neural\n",
            "network  theory:  large  neural  networks  rarely  get  stuck  in  local  minima,  and\n",
            "even when they do these local optima are often almost as good as the global\n",
            "optimum. However, they can still get stuck on long plateaus for a long time.\n",
            "\n",
            "f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the\n",
            "f.\n",
            "bottom-right dataset under “DATA”), and change the network architecture to\n",
            "have  four  hidden  layers  with  eight  neurons  each.  Notice  that  training  takes\n",
            "much  longer  and  often  gets  stuck  on  plateaus  for  long  periods  of  time.  Also\n",
            "notice  that  the  neurons  in  the  highest  layers  (on  the  right)  tend  to  evolve\n",
            "faster than the neurons in the lowest layers (on the left). This problem, called\n",
            "the vanishing gradients problem, can be alleviated with better weight initializa‐\n",
            "tion and other techniques, better optimizers (such as AdaGrad or Adam), or\n",
            "batch normalization (discussed in Chapter 11).\n",
            "\n",
            "g. Go  further.  Take  an  hour  or  so  to  play  around  with  other  parameters  and\n",
            "g.\n",
            "get a feel for what they do, to build an intuitive understanding about neural\n",
            "networks.\n",
            "\n",
            "2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\n",
            "2.\n",
            "that  computes  A  ⊕  B  (where  ⊕  represents  the  XOR  operation).  Hint:  A  ⊕  B  =\n",
            "(A ∧ ¬ B) ∨ (¬ A ∧ B).\n",
            "\n",
            "3. Why is it generally preferable to use a logistic regression classifier rather than a\n",
            "3.\n",
            "classic  perceptron  (i.e.,  a  single  layer  of  threshold  logic  units  trained  using  the\n",
            "perceptron  training  algorithm)?  How  can  you  tweak  a  perceptron  to  make  it\n",
            "equivalent to a logistic regression classifier?\n",
            "\n",
            "4.\n",
            "4. Why  was  the  sigmoid  activation  function  a  key  ingredient  in  training  the  first\n",
            "\n",
            "MLPs?\n",
            "\n",
            "5. Name three popular activation functions. Can you draw them?\n",
            "5.\n",
            "\n",
            "6. Suppose  you  have  an  MLP  composed  of  one  input  layer  with  10  passthrough\n",
            "6.\n",
            "neurons,  followed  by  one  hidden  layer  with  50  artificial  neurons,  and  finally\n",
            "one  output  layer  with  3  artificial  neurons.  All  artificial  neurons  use  the  ReLU\n",
            "activation function.\n",
            "\n",
            "a.\n",
            "a. What is the shape of the input matrix X?\n",
            "\n",
            "354 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
            "\n",
            "\fb. What are the shapes of the hidden layer’s weight matrix Wh and bias vector bh?\n",
            "b.\n",
            "c.\n",
            "c. What are the shapes of the output layer’s weight matrix Wo and bias vector bo?\n",
            "d.\n",
            "d. What is the shape of the network’s output matrix Y?\n",
            "\n",
            "e.\n",
            "e. Write the equation that computes the network’s output matrix Y as a function\n",
            "\n",
            "of X, Wh, bh, Wo, and bo.\n",
            "\n",
            "7. How many neurons do you need in the output layer if you want to classify email\n",
            "7.\n",
            "into spam or ham? What activation function should you use in the output layer?\n",
            "If  instead  you  want  to  tackle  MNIST,  how  many  neurons  do  you  need  in  the\n",
            "output  layer,  and  which  activation  function  should  you  use?  What  about  for\n",
            "getting your network to predict housing prices, as in Chapter 2?\n",
            "\n",
            "8.\n",
            "8. What is backpropagation and how does it work? What is the difference between\n",
            "\n",
            "backpropagation and reverse-mode autodiff?\n",
            "\n",
            "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP\n",
            "9.\n",
            "overfits the training data, how could you tweak these hyperparameters to try to\n",
            "solve the problem?\n",
            "\n",
            "10. Train  a  deep  MLP  on  the  MNIST  dataset  (you  can  load  it  using  tf.keras.\n",
            "10.\n",
            "datasets.mnist.load_data()).  See  if  you  can  get  over  98%  accuracy  by  man‐\n",
            "ually tuning the hyperparameters. Try searching for the optimal learning rate by\n",
            "using  the  approach  presented  in  this  chapter  (i.e.,  by  growing  the  learning  rate\n",
            "exponentially, plotting the loss, and finding the point where the loss shoots up).\n",
            "Next,  try  tuning  the  hyperparameters  using  Keras  Tuner  with  all  the  bells  and\n",
            "whistles—save  checkpoints,  use  early  stopping,  and  plot  learning  curves  using\n",
            "TensorBoard.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "355\n",
            "\n",
            "\f\fCHAPTER 11\n",
            "Training Deep Neural Networks\n",
            "\n",
            "In Chapter 10 you built, trained, and fine-tuned your first artificial neural networks.\n",
            "But they were shallow nets, with just a few hidden layers. What if you need to tackle\n",
            "a complex problem, such as detecting hundreds of types of objects in high-resolution\n",
            "images? You may need to train a much deeper ANN, perhaps with 10 layers or many\n",
            "more,  each  containing  hundreds  of  neurons,  linked  by  hundreds  of  thousands  of\n",
            "connections. Training a deep neural network isn’t a walk in the park. Here are some\n",
            "of the problems you could run into:\n",
            "\n",
            "•\n",
            "• You may be faced with the problem of gradients growing ever smaller or larger,\n",
            "when flowing backward through the DNN during training. Both of these prob‐\n",
            "lems make lower layers very hard to train.\n",
            "\n",
            "•\n",
            "• You might not have enough training data for such a large network, or it might be\n",
            "\n",
            "too costly to label.\n",
            "\n",
            "• Training may be extremely slow.\n",
            "•\n",
            "\n",
            "• A model with millions of parameters would severely risk overfitting the training\n",
            "•\n",
            "set, especially if there are not enough training instances or if they are too noisy.\n",
            "\n",
            "In  this  chapter  we  will  go  through  each  of  these  problems  and  present  techniques\n",
            "to  solve  them.  We  will  start  by  exploring  the  vanishing  and  exploding  gradients\n",
            "problems  and  some  of  their  most  popular  solutions.  Next,  we  will  look  at  transfer\n",
            "learning and unsupervised pretraining, which can help you tackle complex tasks even\n",
            "when  you  have  little  labeled  data.  Then  we  will  discuss  various  optimizers  that  can\n",
            "speed  up  training  large  models  tremendously.  Finally,  we  will  cover  a  few  popular\n",
            "regularization techniques for large neural networks.\n",
            "\n",
            "With these tools, you will be able to train very deep nets. Welcome to deep learning!\n",
            "\n",
            "357\n",
            "\n",
            "\fThe Vanishing/Exploding Gradients Problems\n",
            "As  discussed  in  Chapter  10,  the  backpropagation  algorithm’s  second  phase  works\n",
            "by  going  from  the  output  layer  to  the  input  layer,  propagating  the  error  gradient\n",
            "along  the  way.  Once  the  algorithm  has  computed  the  gradient  of  the  cost  function\n",
            "with regard to each parameter in the network, it uses these gradients to update each\n",
            "parameter with a gradient descent step.\n",
            "\n",
            "Unfortunately,  gradients  often  get  smaller  and  smaller  as  the  algorithm  progresses\n",
            "down  to  the  lower  layers.  As  a  result,  the  gradient  descent  update  leaves  the  lower\n",
            "layers’  connection  weights  virtually  unchanged,  and  training  never  converges  to  a\n",
            "good  solution.  This  is  called  the  vanishing  gradients  problem.  In  some  cases,  the\n",
            "opposite  can  happen:  the  gradients  can  grow  bigger  and  bigger  until  layers  get\n",
            "insanely  large  weight  updates  and  the  algorithm  diverges.  This  is  the  exploding\n",
            "gradients  problem,  which  surfaces  most  often  in  recurrent  neural  networks  (see\n",
            "Chapter  15).  More  generally,  deep  neural  networks  suffer  from  unstable  gradients;\n",
            "different layers may learn at widely different speeds.\n",
            "\n",
            "This unfortunate behavior was empirically observed long ago, and it was one of the\n",
            "reasons  deep  neural  networks  were  mostly  abandoned  in  the  early  2000s.  It  wasn’t\n",
            "clear  what  caused  the  gradients  to  be  so  unstable  when  training  a  DNN,  but  some\n",
            "light  was  shed  in  a  2010  paper  by  Xavier  Glorot  and  Yoshua  Bengio.1  The  authors\n",
            "found  a  few  suspects,  including  the  combination  of  the  popular  sigmoid  (logistic)\n",
            "activation function and the weight initialization technique that was most popular at\n",
            "the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1).\n",
            "In short, they showed that with this activation function and this initialization scheme,\n",
            "the  variance  of  the  outputs  of  each  layer  is  much  greater  than  the  variance  of  its\n",
            "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
            "until  the  activation  function  saturates  at  the  top  layers.  This  saturation  is  actually\n",
            "made  worse  by  the  fact  that  the  sigmoid  function  has  a  mean  of  0.5,  not  0  (the\n",
            "hyperbolic  tangent  function  has  a  mean  of  0  and  behaves  slightly  better  than  the\n",
            "sigmoid function in deep networks).\n",
            "\n",
            "Looking at the sigmoid activation function (see Figure 11-1), you can see that when\n",
            "inputs  become  large  (negative  or  positive),  the  function  saturates  at  0  or  1,  with  a\n",
            "derivative  extremely  close  to  0  (i.e.,  the  curve  is  flat  at  both  extremes).  Thus,  when\n",
            "backpropagation kicks in it has virtually no gradient to propagate back through the\n",
            "network,  and  what  little  gradient  exists  keeps  getting  diluted  as  backpropagation\n",
            "progresses down through the top layers, so there is really nothing left for the lower\n",
            "layers.\n",
            "\n",
            "1 Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural\n",
            "\n",
            "Networks”, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 249–\n",
            "256.\n",
            "\n",
            "358 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fFigure 11-1. Sigmoid activation function saturation\n",
            "\n",
            "Glorot and He Initialization\n",
            "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable\n",
            "gradients problem. They point out that we need the signal to flow properly in both\n",
            "directions:  in  the  forward  direction  when  making  predictions,  and  in  the  reverse\n",
            "direction when backpropagating gradients. We don’t want the signal to die out, nor\n",
            "do  we  want  it  to  explode  and  saturate.  For  the  signal  to  flow  properly,  the  authors\n",
            "argue  that  we  need  the  variance  of  the  outputs  of  each  layer  to  be  equal  to  the\n",
            "variance of its inputs,2 and we need the gradients to have equal variance before and\n",
            "after  flowing  through  a  layer  in  the  reverse  direction  (please  check  out  the  paper  if\n",
            "you are interested in the mathematical details). It is actually not possible to guarantee\n",
            "both unless the layer has an equal number of inputs and outputs (these numbers are\n",
            "called  the  fan-in  and  fan-out  of  the  layer),  but  Glorot  and  Bengio  proposed  a  good\n",
            "compromise that has proven to work very well in practice: the connection weights of\n",
            "each layer must be initialized randomly as described in Equation 11-1, where fanavg =\n",
            "(fanin + fanout) / 2. This initialization strategy is called Xavier initialization or Glorot\n",
            "initialization, after the paper’s first author.\n",
            "\n",
            "2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice,\n",
            "but if you set it too close to the max, your voice will be saturated and people won’t understand what you are\n",
            "saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to\n",
            "come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same\n",
            "amplitude as it came in.\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "359\n",
            "\n",
            "\fEquation 11-1. Glorot initialization (when using the sigmoid activation function)\n",
            "\n",
            "Normal distribution with mean 0 and variance σ2 =\n",
            "\n",
            "1\n",
            "fanavg\n",
            "\n",
            "Or a uniform distribution between −r and  + r, with r =\n",
            "\n",
            "3\n",
            "fanavg\n",
            "\n",
            "If you replace fanavg with fanin in Equation 11-1, you get an initialization strategy that\n",
            "Yann LeCun proposed in the 1990s. He called it LeCun initialization. Genevieve Orr\n",
            "and Klaus-Robert Müller even recommended it in their 1998 book Neural Networks:\n",
            "Tricks of the Trade (Springer). LeCun initialization is equivalent to Glorot initializa‐\n",
            "tion when fanin = fanout. It took over a decade for researchers to realize how important\n",
            "this trick is. Using Glorot initialization can speed up training considerably, and it is\n",
            "one of the practices that led to the success of deep learning.\n",
            "\n",
            "Some  papers3  have  provided  similar  strategies  for  different  activation  functions.\n",
            "These  strategies  differ  only  by  the  scale  of  the  variance  and  whether  they  use  fanavg\n",
            "or  fanin,  as  shown  in  Table  11-1  (for  the  uniform  distribution,  just  use  r = 3σ2).\n",
            "The initialization strategy proposed for the ReLU activation function and its variants\n",
            "is  called  He  initialization  or  Kaiming  initialization,  after  the  paper’s  first  author.  For\n",
            "SELU, use Yann LeCun’s initialization method, preferably with a normal distribution.\n",
            "We will cover all these activation functions shortly.\n",
            "\n",
            "Table 11-1. Initialization parameters for each type of activation function\n",
            "\n",
            "Initialization Activation functions\n",
            "Glorot\n",
            "\n",
            "σ² (Normal)\n",
            "1 / fanavg\n",
            "None, tanh, sigmoid, softmax\n",
            "ReLU, Leaky ReLU, ELU, GELU, Swish, Mish 2 / fanin\n",
            "1 / fanin\n",
            "SELU\n",
            "\n",
            "He\n",
            "\n",
            "LeCun\n",
            "\n",
            "By  default,  Keras  uses  Glorot  initialization  with  a  uniform  distribution.  When  you\n",
            "create  a  layer,  you  can  switch  to  He  initialization  by  setting  kernel_initializer=\n",
            "\"he_uniform\" or kernel_initializer=\"he_normal\" like this:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "dense = tf.keras.layers.Dense(50, activation=\"relu\",\n",
            "                              kernel_initializer=\"he_normal\")\n",
            "\n",
            "Alternatively, you can obtain any of the initializations listed in Table 11-1 and more\n",
            "using  the  VarianceScaling  initializer.  For  example,  if  you  want  He  initialization\n",
            "\n",
            "3 E.g., Kaiming He et al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet\n",
            "\n",
            "Classification,” Proceedings of the 2015 IEEE International Conference on Computer Vision (2015): 1026–1034.\n",
            "\n",
            "360 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fwith  a  uniform  distribution  and  based  on  fanavg  (rather  than  fanin),  you  can  use  the\n",
            "following code:\n",
            "\n",
            "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n",
            "                                                    distribution=\"uniform\")\n",
            "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\",\n",
            "                              kernel_initializer=he_avg_init)\n",
            "\n",
            "Better Activation Functions\n",
            "One  of  the  insights  in  the  2010  paper  by  Glorot  and  Bengio  was  that  the  problems\n",
            "with unstable gradients were in part due to a poor choice of activation function. Until\n",
            "then  most  people  had  assumed  that  if  Mother  Nature  had  chosen  to  use  roughly\n",
            "sigmoid activation functions in biological neurons, they must be an excellent choice.\n",
            "But  it  turns  out  that  other  activation  functions  behave  much  better  in  deep  neural\n",
            "networks—in  particular,  the  ReLU  activation  function,  mostly  because  it  does  not\n",
            "saturate for positive values, and also because it is very fast to compute.\n",
            "\n",
            "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem\n",
            "known as the dying ReLUs: during training, some neurons effectively “die”, meaning\n",
            "they  stop  outputting  anything  other  than  0.  In  some  cases,  you  may  find  that  half\n",
            "of  your  network’s  neurons  are  dead,  especially  if  you  used  a  large  learning  rate.  A\n",
            "neuron dies when its weights get tweaked in such a way that the input of the ReLU\n",
            "function (i.e., the weighted sum of the neuron’s inputs plus its bias term) is negative\n",
            "for all instances in the training set. When this happens, it just keeps outputting zeros,\n",
            "and  gradient  descent  does  not  affect  it  anymore  because  the  gradient  of  the  ReLU\n",
            "function is zero when its input is negative.4\n",
            "\n",
            "To solve this problem, you may want to use a variant of the ReLU function, such as\n",
            "the leaky ReLU.\n",
            "\n",
            "Leaky ReLU\n",
            "\n",
            "The  leaky  ReLU  activation  function  is  defined  as  LeakyReLUα(z)  =  max(αz,  z)  (see\n",
            "Figure  11-2).  The  hyperparameter  α  defines  how  much  the  function  “leaks”:  it  is\n",
            "the  slope  of  the  function  for  z  <  0.  Having  a  slope  for  z  <  0  ensures  that  leaky\n",
            "ReLUs never die; they can go into a long coma, but they have a chance to eventually\n",
            "wake  up.  A  2015  paper  by  Bing  Xu  et  al.5  compared  several  variants  of  the  ReLU\n",
            "activation  function,  and  one  of  its  conclusions  was  that  the  leaky  variants  always\n",
            "\n",
            "4 A dead neuron may come back to life if its inputs evolve over time and eventually return within a range where\n",
            "the ReLU activation function gets a positive input again. For example, this may happen if gradient descent\n",
            "tweaks the neurons in the layers below the dead neuron.\n",
            "\n",
            "5 Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network,” arXiv preprint\n",
            "\n",
            "arXiv:1505.00853 (2015).\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "361\n",
            "\n",
            "\foutperformed  the  strict  ReLU  activation  function.  In  fact,  setting  α  =  0.2  (a  huge\n",
            "leak) seemed to result in better performance than α = 0.01 (a small leak). The paper\n",
            "also  evaluated  the  randomized  leaky  ReLU  (RReLU),  where  α  is  picked  randomly  in\n",
            "a given range during training and is fixed to an average value during testing. RReLU\n",
            "also  performed  fairly  well  and  seemed  to  act  as  a  regularizer,  reducing  the  risk  of\n",
            "overfitting  the  training  set.  Finally,  the  paper  evaluated  the  parametric  leaky  ReLU\n",
            "(PReLU),  where  α  is  authorized  to  be  learned  during  training:  instead  of  being  a\n",
            "hyperparameter,  it  becomes  a  parameter  that  can  be  modified  by  backpropagation\n",
            "like any other parameter. PReLU was reported to strongly outperform ReLU on large\n",
            "image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n",
            "\n",
            "Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values\n",
            "\n",
            "Keras  includes  the  classes  LeakyReLU  and  PReLU  in  the  tf.keras.layers  package.\n",
            "Just  like  for  other  ReLU  variants,  you  should  use  He  initialization  with  these.  For\n",
            "example:\n",
            "\n",
            "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3\n",
            "dense = tf.keras.layers.Dense(50, activation=leaky_relu,\n",
            "                              kernel_initializer=\"he_normal\")\n",
            "\n",
            "If you prefer, you can also use LeakyReLU as a separate layer in your model; it makes\n",
            "no difference for training and predictions:\n",
            "\n",
            "model = tf.keras.models.Sequential([\n",
            "    [...]  # more layers\n",
            "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
            "    tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer\n",
            "    [...]  # more layers\n",
            "])\n",
            "\n",
            "For PReLU, replace LeakyReLU with PReLU. There is currently no official implementa‐\n",
            "tion of RReLU in Keras, but you can fairly easily implement your own (to learn how\n",
            "to do that, see the exercises at the end of Chapter 12).\n",
            "\n",
            "362 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fReLU,  leaky  ReLU,  and  PReLU  all  suffer  from  the  fact  that  they  are  not  smooth\n",
            "functions:  their  derivatives  abruptly  change  (at  z  =  0).  As  we  saw  in  Chapter  4\n",
            "when we discussed lasso, this sort of discontinuity can make gradient descent bounce\n",
            "around  the  optimum,  and  slow  down  convergence.  So  now  we  will  look  at  some\n",
            "smooth variants of the ReLU activation function, starting with ELU and SELU.\n",
            "\n",
            "ELU and SELU\n",
            "\n",
            "In  2015,  a  paper  by  Djork-Arné  Clevert  et  al.6  proposed  a  new  activation  function,\n",
            "called  the  exponential  linear  unit  (ELU),  that  outperformed  all  the  ReLU  variants\n",
            "in  the  authors’  experiments:  training  time  was  reduced,  and  the  neural  network\n",
            "performed  better  on  the  test  set.  Equation  11-2  shows  this  activation  function’s\n",
            "definition.\n",
            "\n",
            "Equation 11-2. ELU activation function\n",
            "\n",
            "ELUα z =\n",
            "\n",
            "α exp z − 1 if z < 0\n",
            "if z ≥ 0\n",
            "z\n",
            "\n",
            "The  ELU  activation  function  looks  a  lot  like  the  ReLU  function  (see  Figure  11-3),\n",
            "with a few major differences:\n",
            "\n",
            "• It  takes  on  negative  values  when  z  <  0,  which  allows  the  unit  to  have  an\n",
            "•\n",
            "average  output  closer  to  0  and  helps  alleviate  the  vanishing  gradients  problem.\n",
            "The  hyperparameter  α  defines  the  opposite  of  the  value  that  the  ELU  function\n",
            "approaches when z is a large negative number. It is usually set to 1, but you can\n",
            "tweak it like any other hyperparameter.\n",
            "\n",
            "•\n",
            "• It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
            "\n",
            "• If α is equal to 1 then the function is smooth everywhere, including around z =\n",
            "•\n",
            "0, which helps speed up gradient descent since it does not bounce as much to the\n",
            "left and right of z = 0.\n",
            "\n",
            "Using  ELU  with  Keras  is  as  easy  as  setting  activation=\"elu\",  and  like  with  other\n",
            "ReLU  variants,  you  should  use  He  initialization.  The  main  drawback  of  the  ELU\n",
            "activation  function  is  that  it  is  slower  to  compute  than  the  ReLU  function  and  its\n",
            "variants  (due  to  the  use  of  the  exponential  function).  Its  faster  convergence  rate\n",
            "during training may compensate for that slow computation, but still, at test time an\n",
            "ELU network will be a bit slower than a ReLU network.\n",
            "\n",
            "6 Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),”\n",
            "\n",
            "Proceedings of the International Conference on Learning Representations, arXiv preprint (2015).\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "363\n",
            "\n",
            "\fFigure 11-3. ELU and SELU activation functions\n",
            "\n",
            "Not long after, a 2017 paper by Günter Klambauer et al.7 introduced the scaled ELU\n",
            "(SELU)  activation  function:  as  its  name  suggests,  it  is  a  scaled  variant  of  the  ELU\n",
            "activation  function  (about  1.05  times  ELU,  using  α  ≈  1.67).  The  authors  showed\n",
            "that  if  you  build  a  neural  network  composed  exclusively  of  a  stack  of  dense  layers\n",
            "(i.e.,  an  MLP),  and  if  all  hidden  layers  use  the  SELU  activation  function,  then  the\n",
            "network will self-normalize: the output of each layer will tend to preserve a mean of 0\n",
            "and a standard deviation of 1 during training, which solves the vanishing/exploding\n",
            "gradients problem. As a result, the SELU activation function may outperform other\n",
            "activation  functions  for  MLPs,  especially  deep  ones.  To  use  it  with  Keras,  just  set\n",
            "activation=\"selu\".  There  are,  however,  a  few  conditions  for  self-normalization  to\n",
            "happen (see the paper for the mathematical justification):\n",
            "\n",
            "•\n",
            "• The input features must be standardized: mean 0 and standard deviation 1.\n",
            "\n",
            "•\n",
            "• Every  hidden  layer’s  weights  must  be  initialized  using  LeCun  normal  initializa‐\n",
            "\n",
            "tion. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
            "\n",
            "• The  self-normalizing  property  is  only  guaranteed  with  plain  MLPs.  If  you  try\n",
            "•\n",
            "to use SELU in other architectures, like recurrent networks (see Chapter 15) or\n",
            "networks with skip connections (i.e., connections that skip layers, such as in Wide\n",
            "& Deep nets), it will probably not outperform ELU.\n",
            "\n",
            "•\n",
            "• You cannot use regularization techniques like ℓ1 or ℓ2 regularization, max-norm,\n",
            "\n",
            "batch-norm, or regular dropout (these are discussed later in this chapter).\n",
            "\n",
            "These  are  significant  constraints,  so  despite  its  promises,  SELU  did  not  gain  a  lot\n",
            "of  traction.  Moreover,  three  more  activation  functions  seem  to  outperform  it  quite\n",
            "consistently on most tasks: GELU, Swish, and Mish.\n",
            "\n",
            "7 Günter Klambauer et al., “Self-Normalizing Neural Networks”, Proceedings of the 31st International Conference\n",
            "\n",
            "on Neural Information Processing Systems (2017): 972–981.\n",
            "\n",
            "364 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fGELU, Swish, and Mish\n",
            "\n",
            "GELU was introduced in a 2016 paper by Dan Hendrycks and Kevin Gimpel.8 Once\n",
            "again,  you  can  think  of  it  as  a  smooth  variant  of  the  ReLU  activation  function.  Its\n",
            "definition  is  given  in  Equation  11-3,  where  Φ  is  the  standard  Gaussian  cumulative\n",
            "distribution  function  (CDF):  Φ(z)  corresponds  to  the  probability  that  a  value  sam‐\n",
            "pled randomly from a normal distribution of mean 0 and variance 1 is lower than z.\n",
            "\n",
            "Equation 11-3. GELU activation function\n",
            "\n",
            "GELU z = z Φ z\n",
            "\n",
            "As you can see in Figure 11-4, GELU resembles ReLU: it approaches 0 when its input\n",
            "z is very negative, and it approaches z when z is very positive. However, whereas all\n",
            "the  activation  functions  we’ve  discussed  so  far  were  both  convex  and  monotonic,9\n",
            "the GELU activation function is neither: from left to right, it starts by going straight,\n",
            "then it wiggles down, reaches a low point around –0.17 (near z ≈ –0.75), and finally\n",
            "bounces  up  and  ends  up  going  straight  toward  the  top  right.  This  fairly  complex\n",
            "shape and the fact that it has a curvature at every point may explain why it works so\n",
            "well, especially for complex tasks: gradient descent may find it easier to fit complex\n",
            "patterns.  In  practice,  it  often  outperforms  every  other  activation  function  discussed\n",
            "so far. However, it is a bit more computationally intensive, and the performance boost\n",
            "it provides is not always sufficient to justify the extra cost. That said, it is possible to\n",
            "show that it is approximately equal to zσ(1.702 z), where σ is the sigmoid function:\n",
            "using  this  approximation  also  works  very  well,  and  it  has  the  advantage  of  being\n",
            "much faster to compute.\n",
            "\n",
            "Figure 11-4. GELU, Swish, parametrized Swish, and Mish activation functions\n",
            "\n",
            "8 Dan Hendrycks and Kevin Gimpel, “Gaussian Error Linear Units (GELUs)”, arXiv preprint arXiv:1606.08415\n",
            "\n",
            "(2016).\n",
            "\n",
            "9 A function is convex if the line segment between any two points on the curve never lies below the curve. A\n",
            "\n",
            "monotonic function only increases, or only decreases.\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "365\n",
            "\n",
            "\fThe GELU paper also introduced the sigmoid linear unit (SiLU) activation function,\n",
            "which is equal to zσ(z), but it was outperformed by GELU in the authors’ tests. Inter‐\n",
            "estingly, a 2017 paper by Prajit Ramachandran et al.10 rediscovered the SiLU function\n",
            "by automatically searching for good activation functions. The authors named it Swish,\n",
            "and  the  name  caught  on.  In  their  paper,  Swish  outperformed  every  other  function,\n",
            "including  GELU.  Ramachandran  et  al.  later  generalized  Swish  by  adding  an  extra\n",
            "hyperparameter β to scale the sigmoid function’s input. The generalized Swish func‐\n",
            "tion is Swishβ(z) = zσ(βz), so GELU is approximately equal to the generalized Swish\n",
            "function using β = 1.702. You can tune β like any other hyperparameter. Alternatively,\n",
            "it’s  also  possible  to  make  β  trainable  and  let  gradient  descent  optimize  it:  much\n",
            "like  PReLU,  this  can  make  your  model  more  powerful,  but  it  also  runs  the  risk  of\n",
            "overfitting the data.\n",
            "\n",
            "Another  quite  similar  activation  function  is  Mish,  which  was  introduced  in  a  2019\n",
            "paper  by  Diganta  Misra.11  It  is  defined  as  mish(z)  =  ztanh(softplus(z)),  where  soft‐\n",
            "plus(z) = log(1 + exp(z)). Just like GELU and Swish, it is a smooth, nonconvex, and\n",
            "nonmonotonic  variant  of  ReLU,  and  once  again  the  author  ran  many  experiments\n",
            "and found that Mish generally outperformed other activation functions—even Swish\n",
            "and GELU, by a tiny margin. Figure 11-4 shows GELU, Swish (both with the default β\n",
            "= 1 and with β = 0.6), and lastly Mish. As you can see, Mish overlaps almost perfectly\n",
            "with Swish when z is negative, and almost perfectly with GELU when z is positive.\n",
            "\n",
            "So, which activation function should you use for the hidden layers\n",
            "of  your  deep  neural  networks?  ReLU  remains  a  good  default  for\n",
            "simple tasks: it’s often just as good as the more sophisticated activa‐\n",
            "tion  functions,  plus  it’s  very  fast  to  compute,  and  many  libraries\n",
            "and  hardware  accelerators  provide  ReLU-specific  optimizations.\n",
            "However,  Swish  is  probably  a  better  default  for  more  complex\n",
            "tasks, and you can even try parametrized Swish with a learnable β\n",
            "parameter for the most complex tasks. Mish may give you slightly\n",
            "better  results,  but  it  requires  a  bit  more  compute.  If  you  care  a\n",
            "lot  about  runtime  latency,  then  you  may  prefer  leaky  ReLU,  or\n",
            "parametrized leaky ReLU for more complex tasks. For deep MLPs,\n",
            "give  SELU  a  try,  but  make  sure  to  respect  the  constraints  listed\n",
            "earlier. If you have spare time and computing power, you can use\n",
            "cross-validation to evaluate other activation functions as well.\n",
            "\n",
            "10 Prajit Ramachandran et al., “Searching for Activation Functions”, arXiv preprint arXiv:1710.05941 (2017).\n",
            "\n",
            "11 Diganta Misra, “Mish: A Self Regularized Non-Monotonic Activation Function”, arXiv preprint\n",
            "\n",
            "arXiv:1908.08681 (2019).\n",
            "\n",
            "366 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fKeras  supports  GELU  and  Swish  out  of  the  box;  just  use  activation=\"gelu\"  or\n",
            "activation=\"swish\".  However,  it  does  not  support  Mish  or  the  generalized  Swish\n",
            "activation  function  yet  (but  see  Chapter  12  to  see  how  to  implement  your  own\n",
            "activation functions and layers).\n",
            "\n",
            "That’s  all  for  activation  functions!  Now,  let’s  look  at  a  completely  different  way  to\n",
            "solve the unstable gradients problem: batch normalization.\n",
            "\n",
            "Batch Normalization\n",
            "Although  using  He  initialization  along  with  ReLU  (or  any  of  its  variants)  can  sig‐\n",
            "nificantly  reduce  the  danger  of  the  vanishing/exploding  gradients  problems  at  the\n",
            "beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
            "\n",
            "In  a  2015  paper,12  Sergey  Ioffe  and  Christian  Szegedy  proposed  a  technique  called\n",
            "batch  normalization  (BN)  that  addresses  these  problems.  The  technique  consists  of\n",
            "adding an operation in the model just before or after the activation function of each\n",
            "hidden  layer.  This  operation  simply  zero-centers  and  normalizes  each  input,  then\n",
            "scales and shifts the result using two new parameter vectors per layer: one for scaling,\n",
            "the other for shifting. In other words, the operation lets the model learn the optimal\n",
            "scale  and  mean  of  each  of  the  layer’s  inputs.  In  many  cases,  if  you  add  a  BN  layer\n",
            "as  the  very  first  layer  of  your  neural  network,  you  do  not  need  to  standardize  your\n",
            "training set. That is, there’s no need for StandardScaler or Normalization; the BN\n",
            "layer will do it for you (well, approximately, since it only looks at one batch at a time,\n",
            "and it can also rescale and shift each input feature).\n",
            "\n",
            "In  order  to  zero-center  and  normalize  the  inputs,  the  algorithm  needs  to  estimate\n",
            "each  input’s  mean  and  standard  deviation.  It  does  so  by  evaluating  the  mean  and\n",
            "standard deviation of the input over the current mini-batch (hence the name “batch\n",
            "normalization”). The whole operation is summarized step by step in Equation 11-4.\n",
            "\n",
            "12 Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing\n",
            "Internal Covariate Shift”, Proceedings of the 32nd International Conference on Machine Learning (2015): 448–\n",
            "456.\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "367\n",
            "\n",
            "\fEquation 11-4. Batch normalization algorithm\n",
            "\n",
            "1 .\n",
            "\n",
            "μB =\n",
            "\n",
            "x i\n",
            "\n",
            "1\n",
            "mB\n",
            "\n",
            "mB\n",
            "∑\n",
            "i = 1\n",
            "mB\n",
            "1\n",
            "∑\n",
            "mB\n",
            "i = 1\n",
            "x i − μB\n",
            "2 + ε\n",
            "σB\n",
            "\n",
            "x i − μB\n",
            "\n",
            "2\n",
            "\n",
            "σB\n",
            "\n",
            "2 =\n",
            "\n",
            "x i\n",
            "\n",
            "=\n",
            "\n",
            "z i = γ ⊗ x i\n",
            "\n",
            "+ β\n",
            "\n",
            "2 .\n",
            "\n",
            "3 .\n",
            "\n",
            "4 .\n",
            "\n",
            "In this algorithm:\n",
            "\n",
            "•\n",
            "• μB  is  the  vector  of  input  means,  evaluated  over  the  whole  mini-batch  B  (it\n",
            "\n",
            "contains one mean per input).\n",
            "\n",
            "• mB is the number of instances in the mini-batch.\n",
            "•\n",
            "•\n",
            "• σB  is  the  vector  of  input  standard  deviations,  also  evaluated  over  the  whole\n",
            "\n",
            "mini-batch (it contains one standard deviation per input).\n",
            "\n",
            "• x(i) is the vector of zero-centered and normalized inputs for instance i.\n",
            "•\n",
            "\n",
            "•\n",
            "• ε  is  a  tiny  number  that  avoids  division  by  zero  and  ensures  the  gradients  don’t\n",
            "\n",
            "grow too large (typically 10–5). This is called a smoothing term.\n",
            "\n",
            "•\n",
            "• γ is the output scale parameter vector for the layer (it contains one scale parame‐\n",
            "\n",
            "ter per input).\n",
            "\n",
            "• ⊗ represents element-wise multiplication (each input is multiplied by its corre‐\n",
            "•\n",
            "\n",
            "sponding output scale parameter).\n",
            "\n",
            "• β is the output shift (offset) parameter vector for the layer (it contains one offset\n",
            "•\n",
            "parameter per input). Each input is offset by its corresponding shift parameter.\n",
            "• z(i)  is  the  output  of  the  BN  operation.  It  is  a  rescaled  and  shifted  version  of  the\n",
            "•\n",
            "\n",
            "inputs.\n",
            "\n",
            "So during training, BN standardizes its inputs, then rescales and offsets them. Good!\n",
            "What  about  at  test  time?  Well,  it’s  not  that  simple.  Indeed,  we  may  need  to  make\n",
            "predictions for individual instances rather than for batches of instances: in this case,\n",
            "we will have no way to compute each input’s mean and standard deviation. Moreover,\n",
            "even  if  we  do  have  a  batch  of  instances,  it  may  be  too  small,  or  the  instances  may\n",
            "not be independent and identically distributed, so computing statistics over the batch\n",
            "instances would be unreliable. One solution could be to wait until the end of training,\n",
            "then run the whole training set through the neural network and compute the mean\n",
            "and  standard  deviation  of  each  input  of  the  BN  layer.  These  “final”  input  means\n",
            "\n",
            "368 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fand  standard  deviations  could  then  be  used  instead  of  the  batch  input  means  and\n",
            "standard  deviations  when  making  predictions.  However,  most  implementations  of\n",
            "batch normalization estimate these final statistics during training by using a moving\n",
            "average of the layer’s input means and standard deviations. This is what Keras does\n",
            "automatically when you use the BatchNormalization layer. To sum up, four parame‐\n",
            "ter  vectors  are  learned  in  each  batch-normalized  layer:  γ  (the  output  scale  vector)\n",
            "and  β  (the  output  offset  vector)  are  learned  through  regular  backpropagation,  and\n",
            "μ  (the  final  input  mean  vector)  and  σ  (the  final  input  standard  deviation  vector)\n",
            "are estimated using an exponential moving average. Note that μ and σ are estimated\n",
            "during  training,  but  they  are  used  only  after  training  (to  replace  the  batch  input\n",
            "means and standard deviations in Equation 11-4).\n",
            "\n",
            "Ioffe and Szegedy demonstrated that batch normalization considerably improved all\n",
            "the deep neural networks they experimented with, leading to a huge improvement in\n",
            "the ImageNet classification task (ImageNet is a large database of images classified into\n",
            "many  classes,  commonly  used  to  evaluate  computer  vision  systems).  The  vanishing\n",
            "gradients problem was strongly reduced, to the point that they could use saturating\n",
            "activation functions such as the tanh and even the sigmoid activation function. The\n",
            "networks were also much less sensitive to the weight initialization. The authors were\n",
            "able to use much larger learning rates, significantly speeding up the learning process.\n",
            "Specifically, they note that:\n",
            "\n",
            "Applied to a state-of-the-art image classification model, batch normalization achieves\n",
            "the  same  accuracy  with  14  times  fewer  training  steps,  and  beats  the  original  model\n",
            "by  a  significant  margin.  […]  Using  an  ensemble  of  batch-normalized  networks,  we\n",
            "improve  upon  the  best  published  result  on  ImageNet  classification:  reaching  4.9%\n",
            "top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.\n",
            "\n",
            "Finally,  like  a  gift  that  keeps  on  giving,  batch  normalization  acts  like  a  regularizer,\n",
            "reducing  the  need  for  other  regularization  techniques  (such  as  dropout,  described\n",
            "later in this chapter).\n",
            "\n",
            "Batch normalization does, however, add some complexity to the model (although it\n",
            "can remove the need for normalizing the input data, as discussed earlier). Moreover,\n",
            "there  is  a  runtime  penalty:  the  neural  network  makes  slower  predictions  due  to  the\n",
            "extra computations required at each layer. Fortunately, it’s often possible to fuse the\n",
            "BN layer with the previous layer after training, thereby avoiding the runtime penalty.\n",
            "This  is  done  by  updating  the  previous  layer’s  weights  and  biases  so  that  it  directly\n",
            "produces  outputs  of  the  appropriate  scale  and  offset.  For  example,  if  the  previous\n",
            "layer  computes  XW  +  b,  then  the  BN  layer  will  compute  γ  ⊗  (XW  +  b  –  μ)  /  σ  +\n",
            "β  (ignoring  the  smoothing  term  ε  in  the  denominator).  If  we  define  W′  =  γ⊗W  /\n",
            "σ  and  b′  =  γ  ⊗  (b  –  μ)  /  σ  +  β,  the  equation  simplifies  to  XW′  +  b′.  So,  if  we\n",
            "replace  the  previous  layer’s  weights  and  biases  (W  and  b)  with  the  updated  weights\n",
            "and biases (W′ and b′), we can get rid of the BN layer (TFLite’s converter does this\n",
            "automatically; see Chapter 19).\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "369\n",
            "\n",
            "\fYou may find that training is rather slow, because each epoch takes\n",
            "much more time when you use batch normalization. This is usually\n",
            "counterbalanced  by  the  fact  that  convergence  is  much  faster  with\n",
            "BN, so it will take fewer epochs to reach the same performance. All\n",
            "in all, wall time will usually be shorter (this is the time measured by\n",
            "the clock on your wall).\n",
            "\n",
            "Implementing batch normalization with Keras\n",
            "\n",
            "As with most things with Keras, implementing batch normalization is straightforward\n",
            "and intuitive. Just add a BatchNormalization layer before or after each hidden layer’s\n",
            "activation function. You may also add a BN layer as the first layer in your model, but\n",
            "a plain Normalization layer generally performs just as well in this location (its only\n",
            "drawback  is  that  you  must  first  call  its  adapt()  method).  For  example,  this  model\n",
            "applies BN after every hidden layer and as the first layer in the model (after flattening\n",
            "the input images):\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Dense(300, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "That’s  all!  In  this  tiny  example  with  just  two  hidden  layers  batch  normalization  is\n",
            "unlikely  to  have  a  large  impact,  but  for  deeper  networks  it  can  make  a  tremendous\n",
            "difference.\n",
            "\n",
            "Let’s display the model summary:\n",
            "\n",
            ">>> model.summary()\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #\n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0\n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136\n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500\n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200\n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100\n",
            "_________________________________________________________________\n",
            "\n",
            "370 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fbatch_normalization_2 (Batch (None, 100)               400\n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010\n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n",
            "\n",
            "As  you  can  see,  each  BN  layer  adds  four  parameters  per  input:  γ,  β,  μ,  and  σ  (for\n",
            "example,  the  first  BN  layer  adds  3,136  parameters,  which  is  4  ×  784).  The  last  two\n",
            "parameters, μ and σ, are the moving averages; they are not affected by backpropaga‐\n",
            "tion,  so  Keras  calls  them  “non-trainable”13  (if  you  count  the  total  number  of  BN\n",
            "parameters,  3,136  +  1,200  +  400,  and  divide  by  2,  you  get  2,368,  which  is  the  total\n",
            "number of non-trainable parameters in this model).\n",
            "\n",
            "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropaga‐\n",
            "tion), and two are not:\n",
            "\n",
            ">>> [(var.name, var.trainable) for var in model.layers[1].variables]\n",
            "[('batch_normalization/gamma:0', True),\n",
            " ('batch_normalization/beta:0', True),\n",
            " ('batch_normalization/moving_mean:0', False),\n",
            " ('batch_normalization/moving_variance:0', False)]\n",
            "\n",
            "The  authors  of  the  BN  paper  argued  in  favor  of  adding  the  BN  layers  before  the\n",
            "activation  functions,  rather  than  after  (as  we  just  did).  There  is  some  debate  about\n",
            "this,  as  which  is  preferable  seems  to  depend  on  the  task—you  can  experiment  with\n",
            "this too to see which option works best on your dataset. To add the BN layers before\n",
            "the  activation  function,  you  must  remove  the  activation  functions  from  the  hidden\n",
            "layers  and  add  them  as  separate  layers  after  the  BN  layers.  Moreover,  since  a  batch\n",
            "normalization layer includes one offset parameter per input, you can remove the bias\n",
            "term  from  the  previous  layer  by  passing  use_bias=False  when  creating  it.  Lastly,\n",
            "you  can  usually  drop  the  first  BN  layer  to  avoid  sandwiching  the  first  hidden  layer\n",
            "between two BN layers. The updated code looks like this:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
            "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Activation(\"relu\"),\n",
            "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Activation(\"relu\"),\n",
            "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "13 However, they are estimated during training based on the training data, so arguably they are trainable. In\n",
            "\n",
            "Keras, “non-trainable” really means “untouched by backpropagation”.\n",
            "\n",
            "The Vanishing/Exploding Gradients Problems \n",
            "\n",
            "| \n",
            "\n",
            "371\n",
            "\n",
            "\fThe BatchNormalization class has quite a few hyperparameters you can tweak. The\n",
            "defaults  will  usually  be  fine,  but  you  may  occasionally  need  to  tweak  the  momentum.\n",
            "This  hyperparameter  is  used  by  the  BatchNormalization  layer  when  it  updates  the\n",
            "exponential moving averages; given a new value v (i.e., a new vector of input means\n",
            "or  standard  deviations  computed  over  the  current  batch),  the  layer  updates  the\n",
            "running average v using the following equation:\n",
            "\n",
            "v\n",
            "\n",
            "v × momentum + v × 1 − momentum\n",
            "\n",
            "A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999. You\n",
            "want more 9s for larger datasets and for smaller mini-batches.\n",
            "\n",
            "Another  important  hyperparameter  is  axis:  it  determines  which  axis  should  be\n",
            "normalized.  It  defaults  to  –1,  meaning  that  by  default  it  will  normalize  the  last  axis\n",
            "(using the means and standard deviations computed across the other axes). When the\n",
            "input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each\n",
            "input feature will be normalized based on the mean and standard deviation computed\n",
            "across all the instances in the batch. For example, the first BN layer in the previous\n",
            "code  example  will  independently  normalize  (and  rescale  and  shift)  each  of  the  784\n",
            "input features. If we move the first BN layer before the Flatten layer, then the input\n",
            "batches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will\n",
            "compute  28  means  and  28  standard  deviations  (1  per  column  of  pixels,  computed\n",
            "across  all  instances  in  the  batch  and  across  all  rows  in  the  column),  and  it  will\n",
            "normalize all pixels in a given column using the same mean and standard deviation.\n",
            "There will also be just 28 scale parameters and 28 shift parameters. If instead you still\n",
            "want to treat each of the 784 pixels independently, then you should set axis=[1, 2].\n",
            "\n",
            "Batch  normalization  has  become  one  of  the  most-used  layers  in  deep  neural  net‐\n",
            "works,  especially  deep  convolutional  neural  networks  discussed  in  (Chapter  14),  to\n",
            "the point that it is often omitted in the architecture diagrams: it is assumed that BN\n",
            "is  added  after  every  layer.  Now  let’s  look  at  one  last  technique  to  stabilize  gradients\n",
            "during training: gradient clipping.\n",
            "\n",
            "Gradient Clipping\n",
            "Another technique to mitigate the exploding gradients problem is to clip the gradi‐\n",
            "ents during backpropagation so that they never exceed some threshold. This is called\n",
            "gradient  clipping.14  This  technique  is  generally  used  in  recurrent  neural  networks,\n",
            "where using batch normalization is tricky (as you will see in Chapter 15).\n",
            "\n",
            "14 Razvan Pascanu et al., “On the Difficulty of Training Recurrent Neural Networks”, Proceedings of the 30th\n",
            "\n",
            "International Conference on Machine Learning (2013): 1310–1318.\n",
            "\n",
            "372 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fIn Keras, implementing gradient clipping is just a matter of setting the clipvalue or\n",
            "clipnorm argument when creating an optimizer, like this:\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
            "model.compile([...], optimizer=optimizer)\n",
            "\n",
            "This  optimizer  will  clip  every  component  of  the  gradient  vector  to  a  value  between\n",
            "–1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each\n",
            "and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold\n",
            "is  a  hyperparameter  you  can  tune.  Note  that  it  may  change  the  orientation  of  the\n",
            "gradient  vector.  For  instance,  if  the  original  gradient  vector  is  [0.9,  100.0],  it  points\n",
            "mostly in the direction of the second axis; but once you clip it by value, you get [0.9,\n",
            "1.0],  which  points  roughly  at  the  diagonal  between  the  two  axes.  In  practice,  this\n",
            "approach  works  well.  If  you  want  to  ensure  that  gradient  clipping  does  not  change\n",
            "the  direction  of  the  gradient  vector,  you  should  clip  by  norm  by  setting  clipnorm\n",
            "instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\n",
            "the  threshold  you  picked.  For  example,  if  you  set  clipnorm=1.0,  then  the  vector\n",
            "[0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but\n",
            "almost  eliminating  the  first  component.  If  you  observe  that  the  gradients  explode\n",
            "during training (you can track the size of the gradients using TensorBoard), you may\n",
            "want to try clipping by value or clipping by norm, with different thresholds, and see\n",
            "which option performs best on the validation set.\n",
            "\n",
            "Reusing Pretrained Layers\n",
            "It  is  generally  not  a  good  idea  to  train  a  very  large  DNN  from  scratch  without  first\n",
            "trying to find an existing neural network that accomplishes a similar task to the one\n",
            "you are trying to tackle (I will discuss how to find them in Chapter 14). If you find\n",
            "such as neural network, then you can generally reuse most of its layers, except for the\n",
            "top ones. This technique is called transfer learning. It will not only speed up training\n",
            "considerably, but also requires significantly less training data.\n",
            "\n",
            "Suppose  you  have  access  to  a  DNN  that  was  trained  to  classify  pictures  into  100\n",
            "different categories, including animals, plants, vehicles, and everyday objects, and you\n",
            "now want to train a DNN to classify specific types of vehicles. These tasks are very\n",
            "similar, even partly overlapping, so you should try to reuse parts of the first network\n",
            "(see Figure 11-5).\n",
            "\n",
            "If  the  input  pictures  for  your  new  task  don’t  have  the  same  size\n",
            "as the ones used in the original task, you will usually have to add\n",
            "a  preprocessing  step  to  resize  them  to  the  size  expected  by  the\n",
            "original  model.  More  generally,  transfer  learning  will  work  best\n",
            "when the inputs have similar low-level features.\n",
            "\n",
            "Reusing Pretrained Layers \n",
            "\n",
            "| \n",
            "\n",
            "373\n",
            "\n",
            "\fFigure 11-5. Reusing pretrained layers\n",
            "\n",
            "The output layer of the original model should usually be replaced because it is most\n",
            "likely not useful at all for the new task, and probably will not have the right number\n",
            "of outputs.\n",
            "\n",
            "Similarly, the upper hidden layers of the original model are less likely to be as useful\n",
            "as the lower layers, since the high-level features that are most useful for the new task\n",
            "may differ significantly from the ones that were most useful for the original task. You\n",
            "want to find the right number of layers to reuse.\n",
            "\n",
            "The  more  similar  the  tasks  are,  the  more  layers  you  will  want  to\n",
            "reuse (starting with the lower layers). For very similar tasks, try to\n",
            "keep all the hidden layers and just replace the output layer.\n",
            "\n",
            "Try freezing all the reused layers first (i.e., make their weights non-trainable so that\n",
            "gradient  descent  won’t  modify  them  and  they  will  remain  fixed),  then  train  your\n",
            "model  and  see  how  it  performs.  Then  try  unfreezing  one  or  two  of  the  top  hidden\n",
            "layers to let backpropagation tweak them and see if performance improves. The more\n",
            "training data you have, the more layers you can unfreeze. It is also useful to reduce\n",
            "the  learning  rate  when  you  unfreeze  reused  layers:  this  will  avoid  wrecking  their\n",
            "fine-tuned weights.\n",
            "\n",
            "If  you  still  cannot  get  good  performance,  and  you  have  little  training  data,  try\n",
            "dropping the top hidden layer(s) and freezing all the remaining hidden layers again.\n",
            "\n",
            "374 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fYou can iterate until you find the right number of layers to reuse. If you have plenty of\n",
            "training data, you may try replacing the top hidden layers instead of dropping them,\n",
            "and even adding more hidden layers.\n",
            "\n",
            "Transfer Learning with Keras\n",
            "Let’s  look  at  an  example.  Suppose  the  Fashion  MNIST  dataset  only  contained  eight\n",
            "classes—for  example,  all  the  classes  except  for  sandal  and  shirt.  Someone  built  and\n",
            "trained  a  Keras  model  on  that  set  and  got  reasonably  good  performance  (>90%\n",
            "accuracy). Let’s call this model A. You now want to tackle a different task: you have\n",
            "images  of  T-shirts  and  pullovers,  and  you  want  to  train  a  binary  classifier:  positive\n",
            "for T-shirts (and tops), negative for sandals. Your dataset is quite small; you only have\n",
            "200 labeled images. When you train a new model for this task (let’s call it model B)\n",
            "with the same architecture as model A, you get 91.85% test accuracy. While drinking\n",
            "your morning coffee, you realize that your task is quite similar to task A, so perhaps\n",
            "transfer learning can help? Let’s find out!\n",
            "\n",
            "First, you need to load model A and create a new model based on that model’s layers.\n",
            "You decide to reuse all the layers except for the output layer:\n",
            "\n",
            "[...]  # Assuming model A was already trained and saved to \"my_model_A\"\n",
            "model_A = tf.keras.models.load_model(\"my_model_A\")\n",
            "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
            "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
            "\n",
            "Note  that  model_A  and  model_B_on_A  now  share  some  layers.  When  you  train\n",
            "model_B_on_A,  it  will  also  affect  model_A.  If  you  want  to  avoid  that,  you  need  to\n",
            "clone model_A before you reuse its layers. To do this, you clone model A’s architecture\n",
            "with clone_model(), then copy its weights:\n",
            "\n",
            "model_A_clone = tf.keras.models.clone_model(model_A)\n",
            "model_A_clone.set_weights(model_A.get_weights())\n",
            "\n",
            "tf.keras.models.clone_model() only clones the architecture, not\n",
            "the weights. If you don’t copy them manually using set_weights(),\n",
            "they  will  be  initialized  randomly  when  the  cloned  model  is  first\n",
            "used.\n",
            "\n",
            "Now  you  could  train  model_B_on_A  for  task  B,  but  since  the  new  output  layer  was\n",
            "initialized randomly it will make large errors (at least during the first few epochs), so\n",
            "there will be large error gradients that may wreck the reused weights. To avoid this,\n",
            "one approach is to freeze the reused layers during the first few epochs, giving the new\n",
            "layer  some  time  to  learn  reasonable  weights.  To  do  this,  set  every  layer’s  trainable\n",
            "attribute to False and compile the model:\n",
            "\n",
            "Reusing Pretrained Layers \n",
            "\n",
            "| \n",
            "\n",
            "375\n",
            "\n",
            "\ffor layer in model_B_on_A.layers[:-1]:\n",
            "    layer.trainable = False\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
            "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
            "                     metrics=[\"accuracy\"])\n",
            "\n",
            "You must always compile your model after you freeze or unfreeze\n",
            "layers.\n",
            "\n",
            "Now you can train the model for a few epochs, then unfreeze the reused layers (which\n",
            "requires  compiling  the  model  again)  and  continue  training  to  fine-tune  the  reused\n",
            "layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\n",
            "the learning rate, once again to avoid damaging the reused weights.\n",
            "\n",
            "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
            "                           validation_data=(X_valid_B, y_valid_B))\n",
            "\n",
            "for layer in model_B_on_A.layers[:-1]:\n",
            "    layer.trainable = True\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
            "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
            "                     metrics=[\"accuracy\"])\n",
            "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
            "                           validation_data=(X_valid_B, y_valid_B))\n",
            "\n",
            "So, what’s the final verdict? Well, this model’s test accuracy is 93.85%, up exactly two\n",
            "percentage points from 91.85%! This means that transfer learning reduced the error\n",
            "rate by almost 25%:\n",
            "\n",
            ">>> model_B_on_A.evaluate(X_test_B, y_test_B)\n",
            "[0.2546142041683197, 0.9384999871253967]\n",
            "\n",
            "Are you convinced? You shouldn’t be: I cheated! I tried many configurations until I\n",
            "found one that demonstrated a strong improvement. If you try to change the classes\n",
            "or  the  random  seed,  you  will  see  that  the  improvement  generally  drops,  or  even\n",
            "vanishes or reverses. What I did is called “torturing the data until it confesses”. When\n",
            "a  paper  just  looks  too  positive,  you  should  be  suspicious:  perhaps  the  flashy  new\n",
            "technique  does  not  actually  help  much  (in  fact,  it  may  even  degrade  performance),\n",
            "but the authors tried many variants and reported only the best results (which may be\n",
            "due to sheer luck), without mentioning how many failures they encountered on the\n",
            "way. Most of the time, this is not malicious at all, but it is part of the reason so many\n",
            "results in science can never be reproduced.\n",
            "\n",
            "376 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fWhy  did  I  cheat?  It  turns  out  that  transfer  learning  does  not  work  very  well  with\n",
            "small  dense  networks,  presumably  because  small  networks  learn  few  patterns,  and\n",
            "dense networks learn very specific patterns, which are unlikely to be useful in other\n",
            "tasks. Transfer learning works best with deep convolutional neural networks, which\n",
            "tend  to  learn  feature  detectors  that  are  much  more  general  (especially  in  the  lower\n",
            "layers). We will revisit transfer learning in Chapter 14, using the techniques we just\n",
            "discussed (and this time there will be no cheating, I promise!).\n",
            "\n",
            "Unsupervised Pretraining\n",
            "Suppose  you  want  to  tackle  a  complex  task  for  which  you  don’t  have  much  labeled\n",
            "training  data,  but  unfortunately  you  cannot  find  a  model  trained  on  a  similar  task.\n",
            "Don’t  lose  hope!  First,  you  should  try  to  gather  more  labeled  training  data,  but  if\n",
            "you can’t, you may still be able to perform unsupervised pretraining (see Figure 11-6).\n",
            "Indeed, it is often cheap to gather unlabeled training examples, but expensive to label\n",
            "them. If you can gather plenty of unlabeled training data, you can try to use it to train\n",
            "an unsupervised model, such as an autoencoder or a generative adversarial network\n",
            "(GAN;  see  Chapter  17).  Then  you  can  reuse  the  lower  layers  of  the  autoencoder  or\n",
            "the  lower  layers  of  the  GAN’s  discriminator,  add  the  output  layer  for  your  task  on\n",
            "top, and fine-tune the final network using supervised learning (i.e., with the labeled\n",
            "training examples).\n",
            "\n",
            "It  is  this  technique  that  Geoffrey  Hinton  and  his  team  used  in  2006,  and  which  led\n",
            "to the revival of neural networks and the success of deep learning. Until 2010, unsu‐\n",
            "pervised  pretraining—typically  with  restricted  Boltzmann  machines  (RBMs;  see  the\n",
            "notebook at https://homl.info/extra-anns)—was the norm for deep nets, and only after\n",
            "the  vanishing  gradients  problem  was  alleviated  did  it  become  much  more  common\n",
            "to  train  DNNs  purely  using  supervised  learning.  Unsupervised  pretraining  (today\n",
            "typically using autoencoders or GANs rather than RBMs) is still a good option when\n",
            "you have a complex task to solve, no similar model you can reuse, and little labeled\n",
            "training data but plenty of unlabeled training data.\n",
            "\n",
            "Note  that  in  the  early  days  of  deep  learning  it  was  difficult  to  train  deep  models,\n",
            "so people would use a technique called greedy layer-wise pretraining (depicted in Fig‐\n",
            "ure 11-6). They would first train an unsupervised model with a single layer, typically\n",
            "an  RBM,  then  they  would  freeze  that  layer  and  add  another  one  on  top  of  it,  then\n",
            "train  the  model  again  (effectively  just  training  the  new  layer),  then  freeze  the  new\n",
            "layer and add another layer on top of it, train the model again, and so on. Nowadays,\n",
            "things  are  much  simpler:  people  generally  train  the  full  unsupervised  model  in  one\n",
            "shot and use autoencoders or GANs rather than RBMs.\n",
            "\n",
            "Reusing Pretrained Layers \n",
            "\n",
            "| \n",
            "\n",
            "377\n",
            "\n",
            "\fFigure 11-6. In unsupervised training, a model is trained on all data, including the unla‐\n",
            "beled data, using an unsupervised learning technique, then it is fine-tuned for the final\n",
            "task on just the labeled data using a supervised learning technique; the unsupervised\n",
            "part may train one layer at a time as shown here, or it may train the full model directly\n",
            "\n",
            "Pretraining on an Auxiliary Task\n",
            "If  you  do  not  have  much  labeled  training  data,  one  last  option  is  to  train  a  first\n",
            "neural  network  on  an  auxiliary  task  for  which  you  can  easily  obtain  or  generate\n",
            "labeled training data, then reuse the lower layers of that network for your actual task.\n",
            "The first neural network’s lower layers will learn feature detectors that will likely be\n",
            "reusable by the second neural network.\n",
            "\n",
            "For  example,  if  you  want  to  build  a  system  to  recognize  faces,  you  may  only  have\n",
            "a  few  pictures  of  each  individual—clearly  not  enough  to  train  a  good  classifier.\n",
            "Gathering  hundreds  of  pictures  of  each  person  would  not  be  practical.  You  could,\n",
            "however, gather a lot of pictures of random people on the web and train a first neural\n",
            "network to detect whether or not two different pictures feature the same person. Such\n",
            "a  network  would  learn  good  feature  detectors  for  faces,  so  reusing  its  lower  layers\n",
            "would allow you to train a good face classifier that uses little training data.\n",
            "\n",
            "For  natural  language  processing  (NLP)  applications,  you  can  download  a  corpus\n",
            "of  millions  of  text  documents  and  automatically  generate  labeled  data  from  it.  For\n",
            "example,  you  could  randomly  mask  out  some  words  and  train  a  model  to  predict\n",
            "what  the  missing  words  are  (e.g.,  it  should  predict  that  the  missing  word  in  the\n",
            "\n",
            "378 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fsentence  “What  ___  you  saying?”  is  probably  “are”  or  “were”).  If  you  can  train  a\n",
            "model to reach good performance on this task, then it will already know quite a lot\n",
            "about language, and you can certainly reuse it for your actual task and fine-tune it on\n",
            "your labeled data (we will discuss more pretraining tasks in Chapter 15).\n",
            "\n",
            "Self-supervised  learning  is  when  you  automatically  generate  the\n",
            "labels from the data itself, as in the text-masking example, then you\n",
            "train  a  model  on  the  resulting  “labeled”  dataset  using  supervised\n",
            "learning techniques.\n",
            "\n",
            "Faster Optimizers\n",
            "Training  a  very  large  deep  neural  network  can  be  painfully  slow.  So  far  we  have\n",
            "seen  four  ways  to  speed  up  training  (and  reach  a  better  solution):  applying  a  good\n",
            "initialization  strategy  for  the  connection  weights,  using  a  good  activation  function,\n",
            "using batch normalization, and reusing parts of a pretrained network (possibly built\n",
            "for  an  auxiliary  task  or  using  unsupervised  learning).  Another  huge  speed  boost\n",
            "comes from using a faster optimizer than the regular gradient descent optimizer. In\n",
            "this  section  we  will  present  the  most  popular  optimization  algorithms:  momentum,\n",
            "Nesterov accelerated gradient, AdaGrad, RMSProp, and finally Adam and its variants.\n",
            "\n",
            "Momentum\n",
            "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\n",
            "out slowly, but it will quickly pick up momentum until it eventually reaches terminal\n",
            "velocity  (if  there  is  some  friction  or  air  resistance).  This  is  the  core  idea  behind\n",
            "momentum  optimization,  proposed  by  Boris  Polyak  in  1964.15  In  contrast,  regular\n",
            "gradient descent will take small steps when the slope is gentle and big steps when the\n",
            "slope is steep, but it will never pick up speed. As a result, regular gradient descent is\n",
            "generally much slower to reach the minimum than momentum optimization.\n",
            "\n",
            "Recall that gradient descent updates the weights θ by directly subtracting the gradient\n",
            "of  the  cost  function  J(θ)  with  regard  to  the  weights  (∇θJ(θ))  multiplied  by  the\n",
            "learning  rate  η.  The  equation  is  θ  ←  θ  –  η∇θJ(θ).  It  does  not  care  about  what  the\n",
            "earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
            "\n",
            "Momentum  optimization  cares  a  great  deal  about  what  previous  gradients  were:  at\n",
            "each  iteration,  it  subtracts  the  local  gradient  from  the  momentum  vector  m  (multi‐\n",
            "plied  by  the  learning  rate  η),  and  it  updates  the  weights  by  adding  this  momentum\n",
            "vector (see Equation 11-5). In other words, the gradient is used as an acceleration, not\n",
            "\n",
            "15 Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods”, USSR Computational\n",
            "\n",
            "Mathematics and Mathematical Physics 4, no. 5 (1964): 1–17.\n",
            "\n",
            "Faster Optimizers \n",
            "\n",
            "| \n",
            "\n",
            "379\n",
            "\n",
            "\fas a speed. To simulate some sort of friction mechanism and prevent the momentum\n",
            "from growing too large, the algorithm introduces a new hyperparameter β, called the\n",
            "momentum, which must be set between 0 (high friction) and 1 (no friction). A typical\n",
            "momentum value is 0.9.\n",
            "\n",
            "Equation 11-5. Momentum algorithm\n",
            "\n",
            "1 . m βm − η∇θJ θ\n",
            "θ + m\n",
            "2 .\n",
            "\n",
            "θ\n",
            "\n",
            "You  can  verify  that  if  the  gradient  remains  constant,  the  terminal  velocity  (i.e.,  the\n",
            "maximum  size  of  the  weight  updates)  is  equal  to  that  gradient  multiplied  by  the\n",
            "learning rate η multiplied by 1 / (1 – β) (ignoring the sign). For example, if β = 0.9,\n",
            "then the terminal velocity is equal to 10 times the gradient times the learning rate, so\n",
            "momentum  optimization  ends  up  going  10  times  faster  than  gradient  descent!  This\n",
            "allows  momentum  optimization  to  escape  from  plateaus  much  faster  than  gradient\n",
            "descent.  We  saw  in  Chapter  4  that  when  the  inputs  have  very  different  scales,  the\n",
            "cost  function  will  look  like  an  elongated  bowl  (see  Figure  4-7).  Gradient  descent\n",
            "goes  down  the  steep  slope  quite  fast,  but  then  it  takes  a  very  long  time  to  go  down\n",
            "the valley. In contrast, momentum optimization will roll down the valley faster and\n",
            "faster until it reaches the bottom (the optimum). In deep neural networks that don’t\n",
            "use batch normalization, the upper layers will often end up having inputs with very\n",
            "different scales, so using momentum optimization helps a lot. It can also help roll past\n",
            "local optima.\n",
            "\n",
            "Due  to  the  momentum,  the  optimizer  may  overshoot  a  bit,  then\n",
            "come  back,  overshoot  again,  and  oscillate  like  this  many  times\n",
            "before  stabilizing  at  the  minimum.  This  is  one  of  the  reasons  it’s\n",
            "good  to  have  a  bit  of  friction  in  the  system:  it  gets  rid  of  these\n",
            "oscillations and thus speeds up convergence.\n",
            "\n",
            "Implementing  momentum  optimization  in  Keras  is  a  no-brainer:  just  use  the  SGD\n",
            "optimizer and set its momentum hyperparameter, then lie back and profit!\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
            "\n",
            "The one drawback of momentum optimization is that it adds yet another hyperpara‐\n",
            "meter  to  tune.  However,  the  momentum  value  of  0.9  usually  works  well  in  practice\n",
            "and almost always goes faster than regular gradient descent.\n",
            "\n",
            "380 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fNesterov Accelerated Gradient\n",
            "One small variant to momentum optimization, proposed by Yurii Nesterov in 1983,16\n",
            "is  almost  always  faster  than  regular  momentum  optimization.  The  Nesterov  acceler‐\n",
            "ated gradient (NAG) method, also known as Nesterov momentum optimization, meas‐\n",
            "ures the gradient of the cost function not at the local position θ but slightly ahead in\n",
            "the direction of the momentum, at θ + βm (see Equation 11-6).\n",
            "\n",
            "Equation 11-6. Nesterov accelerated gradient algorithm\n",
            "\n",
            "1 . m βm − η∇θJ θ + βm\n",
            "2 .\n",
            "\n",
            "θ + m\n",
            "\n",
            "θ\n",
            "\n",
            "This small tweak works because in general the momentum vector will be pointing in\n",
            "the right direction (i.e., toward the optimum), so it will be slightly more accurate to\n",
            "use  the  gradient  measured  a  bit  farther  in  that  direction  rather  than  the  gradient  at\n",
            "the original position, as you can see in Figure 11-7 (where ∇1 represents the gradient\n",
            "of the cost function measured at the starting point θ, and ∇2 represents the gradient\n",
            "at the point located at θ + βm).\n",
            "\n",
            "Figure 11-7. Regular versus Nesterov momentum optimization: the former applies the\n",
            "gradients computed before the momentum step, while the latter applies the gradients\n",
            "computed after\n",
            "\n",
            "As  you  can  see,  the  Nesterov  update  ends  up  closer  to  the  optimum.  After  a  while,\n",
            "these  small  improvements  add  up  and  NAG  ends  up  being  significantly  faster  than\n",
            "regular momentum optimization. Moreover, note that when the momentum pushes\n",
            "\n",
            "16 Yurii Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence\n",
            "\n",
            "O(1/k2),” Doklady AN USSR 269 (1983): 543–547.\n",
            "\n",
            "Faster Optimizers \n",
            "\n",
            "| \n",
            "\n",
            "381\n",
            "\n",
            "\fthe  weights  across  a  valley,  ∇1  continues  to  push  farther  across  the  valley,  while  ∇2\n",
            "pushes back toward the bottom of the valley. This helps reduce oscillations and thus\n",
            "NAG converges faster.\n",
            "\n",
            "To use NAG, simply set nesterov=True when creating the SGD optimizer:\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
            "                                    nesterov=True)\n",
            "\n",
            "AdaGrad\n",
            "Consider the elongated bowl problem again: gradient descent starts by quickly going\n",
            "down  the  steepest  slope,  which  does  not  point  straight  toward  the  global  optimum,\n",
            "then  it  very  slowly  goes  down  to  the  bottom  of  the  valley.  It  would  be  nice  if  the\n",
            "algorithm  could  correct  its  direction  earlier  to  point  a  bit  more  toward  the  global\n",
            "optimum.  The  AdaGrad  algorithm17  achieves  this  correction  by  scaling  down  the\n",
            "gradient vector along the steepest dimensions (see Equation 11-7).\n",
            "\n",
            "Equation 11-7. AdaGrad algorithm\n",
            "\n",
            "1 .\n",
            "\n",
            "2 .\n",
            "\n",
            "s\n",
            "\n",
            "θ\n",
            "\n",
            "s + ∇θJ θ ⊗ ∇θJ θ\n",
            "θ − η ∇θJ θ ⊘ s + ε\n",
            "\n",
            "The  first  step  accumulates  the  square  of  the  gradients  into  the  vector  s  (recall  that\n",
            "the  ⊗  symbol  represents  the  element-wise  multiplication).  This  vectorized  form  is\n",
            "equivalent  to  computing  si  ←  si  +  (∂  J(θ)  /  ∂  θi)2  for  each  element  si  of  the  vector\n",
            "s;  in  other  words,  each  si  accumulates  the  squares  of  the  partial  derivative  of  the\n",
            "cost  function  with  regard  to  parameter  θi.  If  the  cost  function  is  steep  along  the  ith\n",
            "dimension, then si will get larger and larger at each iteration.\n",
            "\n",
            "The second step is almost identical to gradient descent, but with one big difference:\n",
            "s + ε  (the  ⊘  symbol  represents\n",
            "the  gradient  vector  is  scaled  down  by  a  factor  of \n",
            "the  element-wise  division,  and  ε  is  a  smoothing  term  to  avoid  division  by  zero,\n",
            "typically set to 10–10). This vectorized form is equivalent to simultaneously computing\n",
            "θi\n",
            "\n",
            "θi − η ∂J θ / ∂θi/ si + ε for all parameters θi.\n",
            "\n",
            "In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐\n",
            "sions  than  for  dimensions  with  gentler  slopes.  This  is  called  an  adaptive  learning\n",
            "rate.  It  helps  point  the  resulting  updates  more  directly  toward  the  global  optimum\n",
            "(see  Figure  11-8).  One  additional  benefit  is  that  it  requires  much  less  tuning  of  the\n",
            "learning rate hyperparameter η.\n",
            "\n",
            "17 John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”, Journal\n",
            "\n",
            "of Machine Learning Research 12 (2011): 2121–2159.\n",
            "\n",
            "382 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fFigure 11-8. AdaGrad versus gradient descent: the former can correct its direction\n",
            "earlier to point to the optimum\n",
            "\n",
            "AdaGrad frequently performs well for simple quadratic problems, but it often stops\n",
            "too early when training neural networks: the learning rate gets scaled down so much\n",
            "that  the  algorithm  ends  up  stopping  entirely  before  reaching  the  global  optimum.\n",
            "So  even  though  Keras  has  an  Adagrad  optimizer,  you  should  not  use  it  to  train\n",
            "deep neural networks (it may be efficient for simpler tasks such as linear regression,\n",
            "though).  Still,  understanding  AdaGrad  is  helpful  to  comprehend  the  other  adaptive\n",
            "learning rate optimizers.\n",
            "\n",
            "RMSProp\n",
            "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never con‐\n",
            "verging to the global optimum. The RMSProp algorithm18 fixes this by accumulating\n",
            "only  the  gradients  from  the  most  recent  iterations,  as  opposed  to  all  the  gradients\n",
            "since the beginning of training. It does so by using exponential decay in the first step\n",
            "(see Equation 11-8).\n",
            "\n",
            "Equation 11-8. RMSProp algorithm\n",
            "\n",
            "1 .\n",
            "\n",
            "2 .\n",
            "\n",
            "s\n",
            "\n",
            "θ\n",
            "\n",
            "ρs + 1 − ρ ∇θJ θ ⊗ ∇θJ θ\n",
            "θ − η ∇θJ θ ⊘ s + ε\n",
            "\n",
            "18 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey\n",
            "Hinton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58).\n",
            "Amusingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in\n",
            "lecture 6e” in their papers.\n",
            "\n",
            "Faster Optimizers \n",
            "\n",
            "| \n",
            "\n",
            "383\n",
            "\n",
            "\fThe decay rate ρ is typically set to 0.9.19 Yes, it is once again a new hyperparameter,\n",
            "but this default value often works well, so you may not need to tune it at all.\n",
            "\n",
            "As you might expect, Keras has an RMSprop optimizer:\n",
            "\n",
            "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
            "\n",
            "Except on very simple problems, this optimizer almost always performs much better\n",
            "than AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\n",
            "ers until Adam optimization came around.\n",
            "\n",
            "Adam\n",
            "Adam,20 which stands for adaptive moment estimation, combines the ideas of momen‐\n",
            "tum optimization and RMSProp: just like momentum optimization, it keeps track of\n",
            "an exponentially decaying average of past gradients; and just like RMSProp, it keeps\n",
            "track  of  an  exponentially  decaying  average  of  past  squared  gradients  (see  Equation\n",
            "11-9). These are estimations of the mean and (uncentered) variance of the gradients.\n",
            "The mean is often called the first moment while the variance is often called the second\n",
            "moment, hence the name of the algorithm.\n",
            "\n",
            "Equation 11-9. Adam algorithm\n",
            "\n",
            "1 . m β1m − 1 − β1 ∇θJ θ\n",
            "2 .\n",
            "\n",
            "β2s + 1 − β2 ∇θJ θ ⊗ ∇θJ θ\n",
            "\n",
            "s\n",
            "\n",
            "3 . m\n",
            "\n",
            "4 .\n",
            "\n",
            "5 .\n",
            "\n",
            "s\n",
            "\n",
            "θ\n",
            "\n",
            "t\n",
            "\n",
            "m\n",
            "1 − β1\n",
            "s\n",
            "1 − β2\n",
            "θ + η m ⊘ s + ε\n",
            "\n",
            "t\n",
            "\n",
            "In this equation, t represents the iteration number (starting at 1).\n",
            "\n",
            "If  you  just  look  at  steps  1,  2,  and  5,  you  will  notice  Adam’s  close  similarity  to\n",
            "both  momentum  optimization  and  RMSProp:  β1  corresponds  to  β  in  momentum\n",
            "optimization, and β2 corresponds to ρ in RMSProp. The only difference is that step\n",
            "1 computes an exponentially decaying average rather than an exponentially decaying\n",
            "sum,  but  these  are  actually  equivalent  except  for  a  constant  factor  (the  decaying\n",
            "average  is  just  1  –  β1  times  the  decaying  sum).  Steps  3  and  4  are  somewhat  of  a\n",
            "\n",
            "19 ρ is the Greek letter rho.\n",
            "\n",
            "20 Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization”, arXiv preprint\n",
            "\n",
            "arXiv:1412.6980 (2014).\n",
            "\n",
            "384 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\ftechnical detail: since m and s are initialized at 0, they will be biased toward 0 at the\n",
            "beginning of training, so these two steps will help boost m and s at the beginning of\n",
            "training.\n",
            "\n",
            "The  momentum  decay  hyperparameter  β1  is  typically  initialized  to  0.9,  while  the\n",
            "scaling decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing\n",
            "term ε is usually initialized to a tiny number such as 10–7. These are the default values\n",
            "for the Adam class. Here is how to create an Adam optimizer using Keras:\n",
            "\n",
            "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,\n",
            "                                     beta_2=0.999)\n",
            "\n",
            "Since  Adam  is  an  adaptive  learning  rate  algorithm,  like  AdaGrad  and  RMSProp,  it\n",
            "requires  less  tuning  of  the  learning  rate  hyperparameter  η.  You  can  often  use  the\n",
            "default value η = 0.001, making Adam even easier to use than gradient descent.\n",
            "\n",
            "If you are starting to feel overwhelmed by all these different techni‐\n",
            "ques and are wondering how to choose the right ones for your task,\n",
            "don’t  worry:  some  practical  guidelines  are  provided  at  the  end  of\n",
            "this chapter.\n",
            "\n",
            "Finally, three variants of Adam are worth mentioning: AdaMax, Nadam, and AdamW.\n",
            "\n",
            "AdaMax\n",
            "The  Adam  paper  also  introduced  AdaMax.  Notice  that  in  step  2  of  Equation  11-9,\n",
            "Adam accumulates the squares of the gradients in s (with a greater weight for more\n",
            "recent  gradients).  In  step  5,  if  we  ignore  ε  and  steps  3  and  4  (which  are  technical\n",
            "details anyway), Adam scales down the parameter updates by the square root of s. In\n",
            "short, Adam scales down the parameter updates by the ℓ2 norm of the time-decayed\n",
            "gradients (recall that the ℓ2 norm is the square root of the sum of squares).\n",
            "\n",
            "AdaMax  replaces  the  ℓ2  norm  with  the  ℓ∞  norm  (a  fancy  way  of  saying  the  max).\n",
            "Specifically, it replaces step 2 in Equation 11-9 with s max β2s, abs( ∇θJ θ\n",
            ", it\n",
            "drops step 4, and in step 5 it scales down the gradient updates by a factor of s, which\n",
            "is the max of the absolute value of the time-decayed gradients.\n",
            "\n",
            "In practice, this can make AdaMax more stable than Adam, but it really depends on\n",
            "the dataset, and in general Adam performs better. So, this is just one more optimizer\n",
            "you can try if you experience problems with Adam on some task.\n",
            "\n",
            "Faster Optimizers \n",
            "\n",
            "| \n",
            "\n",
            "385\n",
            "\n",
            "\fNadam\n",
            "Nadam  optimization  is  Adam  optimization  plus  the  Nesterov  trick,  so  it  will  often\n",
            "converge  slightly  faster  than  Adam.  In  his  report  introducing  this  technique,21  the\n",
            "researcher Timothy Dozat compares many different optimizers on various tasks and\n",
            "finds  that  Nadam  generally  outperforms  Adam  but  is  sometimes  outperformed  by\n",
            "RMSProp.\n",
            "\n",
            "AdamW\n",
            "AdamW22  is  a  variant  of  Adam  that  integrates  a  regularization  technique  called\n",
            "weight  decay.  Weight  decay  reduces  the  size  of  the  model’s  weights  at  each  training\n",
            "iteration  by  multiplying  them  by  a  decay  factor  such  as  0.99.  This  may  remind  you\n",
            "of  ℓ2  regularization  (introduced  in  Chapter  4),  which  also  aims  to  keep  the  weights\n",
            "small, and indeed it can be shown mathematically that ℓ2 regularization is equivalent\n",
            "to  weight  decay  when  using  SGD.  However,  when  using  Adam  or  its  variants,  ℓ2\n",
            "regularization  and  weight  decay  are  not  equivalent:  in  practice,  combining  Adam\n",
            "with  ℓ2  regularization  results  in  models  that  often  don’t  generalize  as  well  as  those\n",
            "produced by SGD. AdamW fixes this issue by properly combining Adam with weight\n",
            "decay.\n",
            "\n",
            "Adaptive optimization methods (including RMSProp, Adam, Ada‐\n",
            "Max, Nadam, and AdamW optimization) are often great, converg‐\n",
            "ing  fast  to  a  good  solution.  However,  a  2017  paper23  by  Ashia  C.\n",
            "Wilson et al. showed that they can lead to solutions that generalize\n",
            "poorly  on  some  datasets.  So  when  you  are  disappointed  by  your\n",
            "model’s performance, try using NAG instead: your dataset may just\n",
            "be allergic to adaptive gradients. Also check out the latest research,\n",
            "because it’s moving fast.\n",
            "\n",
            "in  Keras,  replace  tf.keras.optimiz\n",
            "To  use  Nadam,  AdaMax,  or  AdamW \n",
            "ers.Adam  with  tf.keras.optimizers.Nadam,  tf.keras.optimizers.Adamax,  or\n",
            "tf.keras.optimizers.experimental.AdamW.  For  AdamW,  you  probably  want  to\n",
            "tune the weight_decay hyperparameter.\n",
            "\n",
            "All  the  optimization  techniques  discussed  so  far  only  rely  on  the  first-order  partial\n",
            "derivatives (Jacobians). The optimization literature also contains amazing algorithms\n",
            "\n",
            "21 Timothy Dozat, “Incorporating Nesterov Momentum into Adam” (2016).\n",
            "\n",
            "22 Ilya Loshchilov, and Frank Hutter, “Decoupled Weight Decay Regularization”, arXiv preprint\n",
            "\n",
            "arXiv:1711.05101 (2017).\n",
            "\n",
            "23 Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning”, Advances in\n",
            "\n",
            "Neural Information Processing Systems 30 (2017): 4148–4158.\n",
            "\n",
            "386 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fbased  on  the  second-order  partial  derivatives  (the  Hessians,  which  are  the  partial\n",
            "derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\n",
            "to  deep  neural  networks  because  there  are  n2  Hessians  per  output  (where  n  is  the\n",
            "number of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐\n",
            "cally  have  tens  of  thousands  of  parameters  or  more,  the  second-order  optimization\n",
            "algorithms  often  don’t  even  fit  in  memory,  and  even  when  they  do,  computing  the\n",
            "Hessians is just too slow.\n",
            "\n",
            "Training Sparse Models\n",
            "All  the  optimization  algorithms  we  just  discussed  produce  dense  models,  meaning\n",
            "that most parameters will be nonzero. If you need a blazingly fast model at runtime,\n",
            "or  if  you  need  it  to  take  up  less  memory,  you  may  prefer  to  end  up  with  a  sparse\n",
            "model instead.\n",
            "\n",
            "One way to achieve this is to train the model as usual, then get rid of the tiny weights\n",
            "(set them to zero). However, this will typically not lead to a very sparse model, and it\n",
            "may degrade the model’s performance.\n",
            "\n",
            "A  better  option  is  to  apply  strong  ℓ1  regularization  during  training  (you’ll  see  how\n",
            "later in this chapter), as it pushes the optimizer to zero out as many weights as it can\n",
            "(as discussed in “Lasso Regression” on page 158).\n",
            "\n",
            "If  these  techniques  remain  insufficient,  check  out  the  TensorFlow  Model  Optimiza‐\n",
            "tion Toolkit (TF-MOT), which provides a pruning API capable of iteratively remov‐\n",
            "ing connections during training based on their magnitude.\n",
            "\n",
            "Table 11-2 compares all the optimizers we’ve discussed so far (* is bad, ** is average,\n",
            "and *** is good).\n",
            "\n",
            "Table 11-2. Optimizer comparison\n",
            "\n",
            "Class\n",
            "\n",
            "SGD\n",
            "\n",
            "Convergence speed Convergence quality\n",
            "*\n",
            "\n",
            "***\n",
            "\n",
            "SGD(momentum=...)\n",
            "\n",
            "**\n",
            "\n",
            "SGD(momentum=..., nesterov=True) **\n",
            "\n",
            "Adagrad\n",
            "\n",
            "RMSprop\n",
            "\n",
            "Adam\n",
            "\n",
            "AdaMax\n",
            "\n",
            "Nadam\n",
            "\n",
            "AdamW\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "***\n",
            "\n",
            "* (stops too early)\n",
            "\n",
            "** or ***\n",
            "\n",
            "** or ***\n",
            "\n",
            "** or ***\n",
            "\n",
            "** or ***\n",
            "\n",
            "** or ***\n",
            "\n",
            "Faster Optimizers \n",
            "\n",
            "| \n",
            "\n",
            "387\n",
            "\n",
            "\fLearning Rate Scheduling\n",
            "Finding a good learning rate is very important. If you set it much too high, training\n",
            "may diverge (as discussed in “Gradient Descent” on page 138). If you set it too low,\n",
            "training  will  eventually  converge  to  the  optimum,  but  it  will  take  a  very  long  time.\n",
            "If  you  set  it  slightly  too  high,  it  will  make  progress  very  quickly  at  first,  but  it  will\n",
            "end  up  dancing  around  the  optimum  and  never  really  settling  down.  If  you  have  a\n",
            "limited computing budget, you may have to interrupt training before it has converged\n",
            "properly, yielding a suboptimal solution (see Figure 11-9).\n",
            "\n",
            "Figure 11-9. Learning curves for various learning rates η\n",
            "\n",
            "As discussed in Chapter 10, you can find a good learning rate by training the model\n",
            "for  a  few  hundred  iterations,  exponentially  increasing  the  learning  rate  from  a  very\n",
            "small value to a very large value, and then looking at the learning curve and picking\n",
            "a learning rate slightly lower than the one at which the learning curve starts shooting\n",
            "back up. You can then reinitialize your model and train it with that learning rate.\n",
            "\n",
            "But you can do better than a constant learning rate: if you start with a large learning\n",
            "rate  and  then  reduce  it  once  training  stops  making  fast  progress,  you  can  reach  a\n",
            "good  solution  faster  than  with  the  optimal  constant  learning  rate.  There  are  many\n",
            "different strategies to reduce the learning rate during training. It can also be beneficial\n",
            "to start with a low learning rate, increase it, then drop it again. These strategies are\n",
            "called  learning  schedules  (I  briefly  introduced  this  concept  in  Chapter  4).  These  are\n",
            "the most commonly used learning schedules:\n",
            "\n",
            "388 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fPower scheduling\n",
            "\n",
            "Set the learning rate to a function of the iteration number t: η(t) = η0 / (1 + t/s)c.\n",
            "The  initial  learning  rate  η0,  the  power  c  (typically  set  to  1),  and  the  steps  s  are\n",
            "hyperparameters. The learning rate drops at each step. After s steps, the learning\n",
            "rate is down to η0 / 2. After s more steps it is down to η0 / 3, then it goes down\n",
            "to η0 / 4, then η0 / 5, and so on. As you can see, this schedule first drops quickly,\n",
            "then more and more slowly. Of course, power scheduling requires tuning η0 and s\n",
            "(and possibly c).\n",
            "\n",
            "Exponential scheduling\n",
            "\n",
            "Set the learning rate to η(t) = η0 0.1t/s. The learning rate will gradually drop by a\n",
            "factor of 10 every s steps. While power scheduling reduces the learning rate more\n",
            "and more slowly, exponential scheduling keeps slashing it by a factor of 10 every\n",
            "s steps.\n",
            "\n",
            "Piecewise constant scheduling\n",
            "\n",
            "Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),\n",
            "then  a  smaller  learning  rate  for  another  number  of  epochs  (e.g.,  η1  =  0.001  for\n",
            "50  epochs),  and  so  on.  Although  this  solution  can  work  very  well,  it  requires\n",
            "fiddling around to figure out the right sequence of learning rates and how long to\n",
            "use each of them.\n",
            "\n",
            "Performance scheduling\n",
            "\n",
            "Measure  the  validation  error  every  N  steps  (just  like  for  early  stopping),  and\n",
            "reduce the learning rate by a factor of λ when the error stops dropping.\n",
            "\n",
            "1cycle scheduling\n",
            "\n",
            "1cycle  was  introduced  in  a  2018  paper  by  Leslie  Smith.24  Contrary  to  the  other\n",
            "approaches,  it  starts  by  increasing  the  initial  learning  rate  η0,  growing  linearly\n",
            "up  to  η1  halfway  through  training.  Then  it  decreases  the  learning  rate  linearly\n",
            "down to η0 again during the second half of training, finishing the last few epochs\n",
            "by  dropping  the  rate  down  by  several  orders  of  magnitude  (still  linearly).  The\n",
            "maximum learning rate η1 is chosen using the same approach we used to find the\n",
            "optimal  learning  rate,  and  the  initial  learning  rate  η0  is  usually  10  times  lower.\n",
            "When using a momentum, we start with a high momentum first (e.g., 0.95), then\n",
            "drop it down to a lower momentum during the first half of training (e.g., down to\n",
            "0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) dur‐\n",
            "ing the second half of training, finishing the last few epochs with that maximum\n",
            "value.  Smith  did  many  experiments  showing  that  this  approach  was  often  able\n",
            "to  speed  up  training  considerably  and  reach  better  performance.  For  example,\n",
            "\n",
            "24 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch\n",
            "\n",
            "Size, Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).\n",
            "\n",
            "Learning Rate Scheduling \n",
            "\n",
            "| \n",
            "\n",
            "389\n",
            "\n",
            "\fon the popular CIFAR10 image dataset, this approach reached 91.9% validation\n",
            "accuracy in just 100 epochs, compared to 90.3% accuracy in 800 epochs through\n",
            "a standard approach (with the same neural network architecture). This feat was\n",
            "dubbed super-convergence.\n",
            "\n",
            "A  2013  paper  by  Andrew  Senior  et  al.25  compared  the  performance  of  some  of\n",
            "the  most  popular  learning  schedules  when  using  momentum  optimization  to  train\n",
            "deep  neural  networks  for  speech  recognition.  The  authors  concluded  that,  in  this\n",
            "setting,  both  performance  scheduling  and  exponential  scheduling  performed  well.\n",
            "They  favored  exponential  scheduling  because  it  was  easy  to  tune  and  it  converged\n",
            "slightly  faster  to  the  optimal  solution.  They  also  mentioned  that  it  was  easier  to\n",
            "implement  than  performance  scheduling,  but  in  Keras  both  options  are  easy.  That\n",
            "said, the 1cycle approach seems to perform even better.\n",
            "\n",
            "Implementing  power  scheduling  in  Keras  is  the  easiest  option—just  set  the  decay\n",
            "hyperparameter when creating an optimizer:\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n",
            "\n",
            "The decay is the inverse of s (the number of steps it takes to divide the learning rate\n",
            "by one more unit), and Keras assumes that c is equal to 1.\n",
            "\n",
            "Exponential scheduling and piecewise scheduling are quite simple too. You first need\n",
            "to  define  a  function  that  takes  the  current  epoch  and  returns  the  learning  rate.  For\n",
            "example, let’s implement exponential scheduling:\n",
            "\n",
            "def exponential_decay_fn(epoch):\n",
            "    return 0.01 * 0.1 ** (epoch / 20)\n",
            "\n",
            "If  you  do  not  want  to  hardcode  η0  and  s,  you  can  create  a  function  that  returns  a\n",
            "configured function:\n",
            "\n",
            "def exponential_decay(lr0, s):\n",
            "    def exponential_decay_fn(epoch):\n",
            "        return lr0 * 0.1 ** (epoch / s)\n",
            "    return exponential_decay_fn\n",
            "\n",
            "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
            "\n",
            "Next, create a LearningRateScheduler callback, giving it the schedule function, and\n",
            "pass this callback to the fit() method:\n",
            "\n",
            "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
            "history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])\n",
            "\n",
            "25 Andrew Senior et al., “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recogni‐\n",
            "tion”, Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013):\n",
            "6724–6728.\n",
            "\n",
            "390 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fThe LearningRateScheduler will update the optimizer’s learning_rate attribute at\n",
            "the  beginning  of  each  epoch.  Updating  the  learning  rate  once  per  epoch  is  usually\n",
            "enough,  but  if  you  want  it  to  be  updated  more  often,  for  example  at  every  step,\n",
            "you  can  always  write  your  own  callback  (see  the  “Exponential  Scheduling”  section\n",
            "of this chapter’s notebook for an example). Updating the learning rate at every step\n",
            "may help if there are many steps per epoch. Alternatively, you can use the tf.keras.\n",
            "optimizers.schedules approach, described shortly.\n",
            "\n",
            "After training, history.history[\"lr\"] gives you access to the list\n",
            "of learning rates used during training.\n",
            "\n",
            "The schedule function can optionally take the current learning rate as a second argu‐\n",
            "ment. For example, the following schedule function multiplies the previous learning\n",
            "rate  by  0.11/20,  which  results  in  the  same  exponential  decay  (except  the  decay  now\n",
            "starts at the beginning of epoch 0 instead of 1):\n",
            "\n",
            "def exponential_decay_fn(epoch, lr):\n",
            "    return lr * 0.1 ** (1 / 20)\n",
            "\n",
            "This  implementation  relies  on  the  optimizer’s  initial  learning  rate  (contrary  to  the\n",
            "previous implementation), so make sure to set it appropriately.\n",
            "\n",
            "When you save a model, the optimizer and its learning rate get saved along with it.\n",
            "This means that with this new schedule function, you could just load a trained model\n",
            "and continue training where it left off, no problem. Things are not so simple if your\n",
            "schedule function uses the epoch argument, however: the epoch does not get saved,\n",
            "and it gets reset to 0 every time you call the fit() method. If you were to continue\n",
            "training a model where it left off, this could lead to a very large learning rate, which\n",
            "would likely damage your model’s weights. One solution is to manually set the fit()\n",
            "method’s initial_epoch argument so the epoch starts at the right value.\n",
            "\n",
            "For piecewise constant scheduling, you can use a schedule function like the following\n",
            "one  (as  earlier,  you  can  define  a  more  general  function  if  you  want;  see  the  “Piece‐\n",
            "wise  Constant  Scheduling”  section  of  the  notebook  for  an  example),  then  create  a\n",
            "LearningRateScheduler callback with this function and pass it to the fit() method,\n",
            "just like for exponential scheduling:\n",
            "\n",
            "def piecewise_constant_fn(epoch):\n",
            "    if epoch < 5:\n",
            "        return 0.01\n",
            "    elif epoch < 15:\n",
            "        return 0.005\n",
            "    else:\n",
            "        return 0.001\n",
            "\n",
            "Learning Rate Scheduling \n",
            "\n",
            "| \n",
            "\n",
            "391\n",
            "\n",
            "\fFor  performance  scheduling,  use  the  ReduceLROnPlateau  callback.  For  example,  if\n",
            "you pass the following callback to the fit() method, it will multiply the learning rate\n",
            "by 0.5 whenever the best validation loss does not improve for five consecutive epochs\n",
            "(other options are available; please check the documentation for more details):\n",
            "\n",
            "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
            "history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])\n",
            "\n",
            "Lastly,  Keras  offers  an  alternative  way  to  implement  learning  rate  scheduling:  you\n",
            "can define a scheduled learning rate using one of the classes available in tf.keras.\n",
            "optimizers.schedules,  then  pass  it  to  any  optimizer.  This  approach  updates  the\n",
            "learning  rate  at  each  step  rather  than  at  each  epoch.  For  example,  here  is  how  to\n",
            "implement the same exponential schedule as the exponential_decay_fn() function\n",
            "we defined earlier:\n",
            "\n",
            "import math\n",
            "\n",
            "batch_size = 32\n",
            "n_epochs = 25\n",
            "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
            "scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
            "    initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1)\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)\n",
            "\n",
            "This  is  nice  and  simple,  plus  when  you  save  the  model,  the  learning  rate  and  its\n",
            "schedule (including its state) get saved as well.\n",
            "\n",
            "As  for  1cycle,  Keras  does  not  support  it,  but  it’s  possible  to  implement  it  in\n",
            "less  than  30  lines  of  code  by  creating  a  custom  callback  that  modifies  the\n",
            "learning  rate  at  each  iteration.  To  update  the  optimizer’s  learning  rate  from\n",
            "within  the  callback’s  on_batch_begin()  method,  you  need  to  call  tf.keras.\n",
            "backend.set_value(self.model.optimizer.learning_rate,  new_learning_rate).\n",
            "See the “1Cycle Scheduling” section of the notebook for an example.\n",
            "\n",
            "To sum up, exponential decay, performance scheduling, and 1cycle can considerably\n",
            "speed up convergence, so give them a try!\n",
            "\n",
            "Avoiding Overfitting Through Regularization\n",
            "\n",
            "With  four  parameters  I  can  fit  an  elephant  and  with  five  I  can  make  him  wiggle  his\n",
            "trunk.\n",
            "\n",
            "—John von Neumann, cited by Enrico Fermi in Nature 427\n",
            "\n",
            "With thousands of parameters, you can fit the whole zoo. Deep neural networks typi‐\n",
            "cally have tens of thousands of parameters, sometimes even millions. This gives them\n",
            "an incredible amount of freedom and means they can fit a huge variety of complex\n",
            "\n",
            "392 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fdatasets.  But  this  great  flexibility  also  makes  the  network  prone  to  overfitting  the\n",
            "training set. Regularization is often needed to prevent this.\n",
            "\n",
            "We  already  implemented  one  of  the  best  regularization  techniques  in  Chapter  10:\n",
            "early stopping. Moreover, even though batch normalization was designed to solve the\n",
            "unstable gradients problems, it also acts like a pretty good regularizer. In this section\n",
            "we will examine other popular regularization techniques for neural networks: ℓ1 and\n",
            "ℓ2 regularization, dropout, and max-norm regularization.\n",
            "\n",
            "ℓ1 and ℓ2 Regularization\n",
            "Just like you did in Chapter 4 for simple linear models, you can use ℓ2 regularization\n",
            "to  constrain  a  neural  network’s  connection  weights,  and/or  ℓ1  regularization  if  you\n",
            "want a sparse model (with many weights equal to 0). Here is how to apply ℓ2 regulari‐\n",
            "zation to a Keras layer’s connection weights, using a regularization factor of 0.01:\n",
            "\n",
            "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
            "                              kernel_initializer=\"he_normal\",\n",
            "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
            "\n",
            "The l2() function returns a regularizer that will be called at each step during training\n",
            "to compute the regularization loss. This is then added to the final loss. As you might\n",
            "expect, you can just use tf.keras.regularizers.l1() if you want ℓ1 regularization;\n",
            "if  you  want  both  ℓ1  and  ℓ2  regularization,  use  tf.keras.regularizers.l1_l2()\n",
            "(specifying both regularization factors).\n",
            "\n",
            "Since  you  will  typically  want  to  apply  the  same  regularizer  to  all  layers  in  your\n",
            "network,  as  well  as  using  the  same  activation  function  and  the  same  initialization\n",
            "strategy  in  all  hidden  layers,  you  may  find  yourself  repeating  the  same  arguments.\n",
            "This makes the code ugly and error-prone. To avoid this, you can try refactoring your\n",
            "code to use loops. Another option is to use Python’s functools.partial() function,\n",
            "which  lets  you  create  a  thin  wrapper  for  any  callable,  with  some  default  argument\n",
            "values:\n",
            "\n",
            "from functools import partial\n",
            "\n",
            "RegularizedDense = partial(tf.keras.layers.Dense,\n",
            "                           activation=\"relu\",\n",
            "                           kernel_initializer=\"he_normal\",\n",
            "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
            "    RegularizedDense(100),\n",
            "    RegularizedDense(100),\n",
            "    RegularizedDense(10, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "Avoiding Overfitting Through Regularization \n",
            "\n",
            "| \n",
            "\n",
            "393\n",
            "\n",
            "\fAs  we  saw  earlier,  ℓ2  regularization  is  fine  when  using  SGD,\n",
            "momentum optimization, and Nesterov momentum optimization,\n",
            "but  not  with  Adam  and  its  variants.  If  you  want  to  use  Adam\n",
            "with  weight  decay,  then  do  not  use  ℓ2  regularization:  use  AdamW\n",
            "instead.\n",
            "\n",
            "Dropout\n",
            "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  net‐\n",
            "works.  It  was  proposed  in  a  paper26  by  Geoffrey  Hinton  et  al.  in  2012  and  further\n",
            "detailed  in  a  2014  paper27  by  Nitish  Srivastava  et  al.,  and  it  has  proven  to  be  highly\n",
            "successful:  many  state-of-the-art  neural  networks  use  dropout,  as  it  gives  them  a\n",
            "1%–2% accuracy boost. This may not sound like a lot, but when a model already has\n",
            "95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost\n",
            "40% (going from 5% error to roughly 3%).\n",
            "\n",
            "It  is  a  fairly  simple  algorithm:  at  every  training  step,  every  neuron  (including  the\n",
            "input neurons, but always excluding the output neurons) has a probability p of being\n",
            "temporarily  “dropped  out”,  meaning  it  will  be  entirely  ignored  during  this  training\n",
            "step, but it may be active during the next step (see Figure 11-10). The hyperparameter\n",
            "p  is  called  the  dropout  rate,  and  it  is  typically  set  between  10%  and  50%:  closer  to\n",
            "20%–30% in recurrent neural nets (see Chapter 15), and closer to 40%–50% in convo‐\n",
            "lutional neural networks (see Chapter 14). After training, neurons don’t get dropped\n",
            "anymore. And that’s all (except for a technical detail we will discuss momentarily).\n",
            "\n",
            "It’s surprising at first that this destructive technique works at all. Would a company\n",
            "perform  better  if  its  employees  were  told  to  toss  a  coin  every  morning  to  decide\n",
            "whether  or  not  to  go  to  work?  Well,  who  knows;  perhaps  it  would!  The  company\n",
            "would  be  forced  to  adapt  its  organization;  it  could  not  rely  on  any  single  person\n",
            "to  work  the  coffee  machine  or  perform  any  other  critical  tasks,  so  this  expertise\n",
            "would  have  to  be  spread  across  several  people.  Employees  would  have  to  learn  to\n",
            "cooperate  with  many  of  their  coworkers,  not  just  a  handful  of  them.  The  company\n",
            "would become much more resilient. If one person quit, it wouldn’t make much of a\n",
            "difference.  It’s  unclear  whether  this  idea  would  actually  work  for  companies,  but  it\n",
            "certainly  does  for  neural  networks.  Neurons  trained  with  dropout  cannot  co-adapt\n",
            "with  their  neighboring  neurons;  they  have  to  be  as  useful  as  possible  on  their  own.\n",
            "They also cannot rely excessively on just a few input neurons; they must pay attention\n",
            "to each of their input neurons. They end up being less sensitive to slight changes in\n",
            "the inputs. In the end, you get a more robust network that generalizes better.\n",
            "\n",
            "26 Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”,\n",
            "\n",
            "arXiv preprint arXiv:1207.0580 (2012).\n",
            "\n",
            "27 Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting”, Journal of\n",
            "\n",
            "Machine Learning Research 15 (2014): 1929–1958.\n",
            "\n",
            "394 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fFigure 11-10. With dropout regularization, at each training iteration a random subset\n",
            "of all neurons in one or more layers—except the output layer—are “dropped out”; these\n",
            "neurons output 0 at this iteration (represented by the dashed arrows)\n",
            "\n",
            "Another  way  to  understand  the  power  of  dropout  is  to  realize  that  a  unique  neural\n",
            "network  is  generated  at  each  training  step.  Since  each  neuron  can  be  either  present\n",
            "or absent, there are a total of 2N possible networks (where N is the total number of\n",
            "droppable  neurons).  This  is  such  a  huge  number  that  it  is  virtually  impossible  for\n",
            "the  same  neural  network  to  be  sampled  twice.  Once  you  have  run  10,000  training\n",
            "steps,  you  have  essentially  trained  10,000  different  neural  networks,  each  with  just\n",
            "one training instance. These neural networks are obviously not independent because\n",
            "they share many of their weights, but they are nevertheless all different. The resulting\n",
            "neural  network  can  be  seen  as  an  averaging  ensemble  of  all  these  smaller  neural\n",
            "networks.\n",
            "\n",
            "Avoiding Overfitting Through Regularization \n",
            "\n",
            "| \n",
            "\n",
            "395\n",
            "\n",
            "\fIn practice, you can usually apply dropout only to the neurons in\n",
            "the top one to three layers (excluding the output layer).\n",
            "\n",
            "There is one small but important technical detail. Suppose p = 75%: on average only\n",
            "25%  of  all  neurons  are  active  at  each  step  during  training.  This  means  that  after\n",
            "training,  a  neuron  would  be  connected  to  four  times  as  many  input  neurons  as  it\n",
            "would  be  during  training.  To  compensate  for  this  fact,  we  need  to  multiply  each\n",
            "neuron’s  input  connection  weights  by  four  during  training.  If  we  don’t,  the  neural\n",
            "network will not perform well as it will see different data during and after training.\n",
            "More  generally,  we  need  to  divide  the  connection  weights  by  the  keep  probability\n",
            "(1 – p) during training.\n",
            "\n",
            "To implement dropout using Keras, you can use the tf.keras.layers.Dropout layer.\n",
            "During training, it randomly drops some inputs (setting them to 0) and divides the\n",
            "remaining inputs by the keep probability. After training, it does nothing at all; it just\n",
            "passes the inputs to the next layer. The following code applies dropout regularization\n",
            "before every dense layer, using a dropout rate of 0.2:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
            "    tf.keras.layers.Dropout(rate=0.2),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.Dropout(rate=0.2),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.Dropout(rate=0.2),\n",
            "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
            "])\n",
            "[...]  # compile and train the model\n",
            "\n",
            "Since dropout is only active during training, comparing the train‐\n",
            "ing  loss  and  the  validation  loss  can  be  misleading.  In  particular,\n",
            "a  model  may  be  overfitting  the  training  set  and  yet  have  similar\n",
            "training and validation losses. So, make sure to evaluate the train‐\n",
            "ing loss without dropout (e.g., after training).\n",
            "\n",
            "If  you  observe  that  the  model  is  overfitting,  you  can  increase  the  dropout  rate.\n",
            "Conversely,  you  should  try  decreasing  the  dropout  rate  if  the  model  underfits  the\n",
            "training set. It can also help to increase the dropout rate for large layers, and reduce\n",
            "it for small ones. Moreover, many state-of-the-art architectures only use dropout after\n",
            "the last hidden layer, so you may want to try this if full dropout is too strong.\n",
            "\n",
            "396 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fDropout  does  tend  to  significantly  slow  down  convergence,  but  it  often  results  in  a\n",
            "better  model  when  tuned  properly.  So,  it  is  generally  well  worth  the  extra  time  and\n",
            "effort, especially for large models.\n",
            "\n",
            "If you want to regularize a self-normalizing network based on the\n",
            "SELU  activation  function  (as  discussed  earlier),  you  should  use\n",
            "alpha dropout: this is a variant of dropout that preserves the mean\n",
            "and standard deviation of its inputs. It was introduced in the same\n",
            "paper as SELU, as regular dropout would break self-normalization.\n",
            "\n",
            "Monte Carlo (MC) Dropout\n",
            "In  2016,  a  paper28  by  Yarin  Gal  and  Zoubin  Ghahramani  added  a  few  more  good\n",
            "reasons to use dropout:\n",
            "\n",
            "• First,  the  paper  established  a  profound  connection  between  dropout  networks\n",
            "•\n",
            "(i.e.,  neural  networks  containing  Dropout  layers)  and  approximate  Bayesian\n",
            "inference,29 giving dropout a solid mathematical justification.\n",
            "\n",
            "• Second, the authors introduced a powerful technique called MC dropout, which\n",
            "•\n",
            "can  boost  the  performance  of  any  trained  dropout  model  without  having  to\n",
            "retrain it or even modify it at all. It also provides a much better measure of the\n",
            "model’s uncertainty, and it can be implemented in just a few lines of code.\n",
            "\n",
            "If  this  all  sounds  like  some  “one  weird  trick”  clickbait,  then  take  a  look  at  the\n",
            "following  code.  It  is  the  full  implementation  of  MC  dropout,  boosting  the  dropout\n",
            "model we trained earlier without retraining it:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "y_probas = np.stack([model(X_test, training=True)\n",
            "                     for sample in range(100)])\n",
            "y_proba = y_probas.mean(axis=0)\n",
            "\n",
            "Note that model(X) is similar to model.predict(X) except it returns a tensor rather\n",
            "than a NumPy array, and it supports the training argument. In this code example,\n",
            "setting  training=True  ensures  that  the  Dropout  layer  remains  active,  so  all  predic‐\n",
            "tions  will  be  a  bit  different.  We  just  make  100  predictions  over  the  test  set,  and  we\n",
            "compute their average. More specifically, each call to the model returns a matrix with\n",
            "one  row  per  instance  and  one  column  per  class.  Because  there  are  10,000  instances\n",
            "\n",
            "28 Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty\n",
            "in Deep Learning”, Proceedings of the 33rd International Conference on Machine Learning (2016): 1050–1059.\n",
            "\n",
            "29 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\n",
            "\n",
            "inference in a specific type of probabilistic model called a deep Gaussian process.\n",
            "\n",
            "Avoiding Overfitting Through Regularization \n",
            "\n",
            "| \n",
            "\n",
            "397\n",
            "\n",
            "\fin the test set and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such\n",
            "matrices, so y_probas is a 3D array of shape [100, 10000, 10]. Once we average over\n",
            "the first dimension (axis=0) we get  y_proba, an array of shape [10000, 10], like we\n",
            "would  get  with  a  single  prediction.  That’s  all!  Averaging  over  multiple  predictions\n",
            "with  dropout  turned  on  gives  us  a  Monte  Carlo  estimate  that  is  generally  more\n",
            "reliable than the result of a single prediction with dropout turned off. For example,\n",
            "let’s look at the model’s prediction for the first instance in the Fashion MNIST test set,\n",
            "with dropout turned off:\n",
            "\n",
            ">>> model.predict(X_test[:1]).round(3)\n",
            "array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.024, 0.   , 0.132, 0.   ,\n",
            "        0.844]], dtype=float32)\n",
            "\n",
            "The model is fairly confident (84.4%) that this image belongs to class 9 (ankle boot).\n",
            "Compare this with the MC dropout prediction:\n",
            "\n",
            ">>> y_proba[0].round(3)\n",
            "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.067, 0.   , 0.209, 0.001,\n",
            "       0.723], dtype=float32)\n",
            "\n",
            "The  model  still  seems  to  prefer  class  9,  but  its  confidence  dropped  down  to  72.3%,\n",
            "and the estimated probabilities for classes 5 (sandal) and 7 (sneaker) have increased,\n",
            "which makes sense given they’re also footwear.\n",
            "\n",
            "MC dropout tends to improve the reliability of the model’s probability estimates. This\n",
            "means  that  it’s  less  likely  to  be  confident  but  wrong,  which  can  be  dangerous:  just\n",
            "imagine  a  self-driving  car  confidently  ignoring  a  stop  sign.  It’s  also  useful  to  know\n",
            "exactly  which  other  classes  are  most  likely.  Additionally,  you  can  take  a  look  at  the\n",
            "standard deviation of the probability estimates:\n",
            "\n",
            ">>> y_std = y_probas.std(axis=0)\n",
            ">>> y_std[0].round(3)\n",
            "array([0.   , 0.   , 0.   , 0.001, 0.   , 0.096, 0.   , 0.162, 0.001,\n",
            "       0.183], dtype=float32)\n",
            "\n",
            "Apparently there’s quite a lot of variance in the probability estimates for class 9: the\n",
            "standard deviation is 0.183, which should be compared to the estimated probability\n",
            "of 0.723: if you were building a risk-sensitive system (e.g., a medical or financial sys‐\n",
            "tem),  you  would  probably  treat  such  an  uncertain  prediction  with  extreme  caution.\n",
            "You  would  definitely  not  treat  it  like  an  84.4%  confident  prediction.  The  model’s\n",
            "accuracy also got a (very) small boost from 87.0% to 87.2%:\n",
            "\n",
            ">>> y_pred = y_proba.argmax(axis=1)\n",
            ">>> accuracy = (y_pred == y_test).sum() / len(y_test)\n",
            ">>> accuracy\n",
            "0.8717\n",
            "\n",
            "398 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fThe number of Monte Carlo samples you use (100 in this example)\n",
            "is  a  hyperparameter  you  can  tweak.  The  higher  it  is,  the  more\n",
            "accurate  the  predictions  and  their  uncertainty  estimates  will  be.\n",
            "However,  if  you  double  it,  inference  time  will  also  be  doubled.\n",
            "Moreover,  above  a  certain  number  of  samples,  you  will  notice\n",
            "little improvement. Your job is to find the right trade-off between\n",
            "latency and accuracy, depending on your application.\n",
            "\n",
            "If your model contains other layers that behave in a special way during training (such\n",
            "as BatchNormalization layers), then you should not force training mode like we just\n",
            "did.  Instead,  you  should  replace  the  Dropout  layers  with  the  following  MCDropout\n",
            "class:30\n",
            "\n",
            "class MCDropout(tf.keras.layers.Dropout):\n",
            "    def call(self, inputs, training=False):\n",
            "        return super().call(inputs, training=True)\n",
            "\n",
            "Here, we just subclass the Dropout layer and override the call() method to force its\n",
            "training argument to True (see Chapter 12). Similarly, you could define an MCAlpha\n",
            "Dropout class by subclassing AlphaDropout instead. If you are creating a model from\n",
            "scratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a\n",
            "model that was already trained using Dropout, you need to create a new model that’s\n",
            "identical to the existing model except with Dropout instead of MCDropout, then copy\n",
            "the existing model’s weights to your new model.\n",
            "\n",
            "In short, MC dropout is a great technique that boosts dropout models and provides\n",
            "better  uncertainty  estimates.  And  of  course,  since  it  is  just  regular  dropout  during\n",
            "training, it also acts like a regularizer.\n",
            "\n",
            "Max-Norm Regularization\n",
            "Another  popular  regularization  technique  for  neural  networks  is  called  max-norm\n",
            "regularization: for each neuron, it constrains the weights w of the incoming connec‐\n",
            "tions such that ∥ w ∥2 ≤ r, where r is the max-norm hyperparameter and ∥ · ∥2 is the ℓ2\n",
            "norm.\n",
            "\n",
            "Max-norm regularization does not add a regularization loss term to the overall loss\n",
            "function. Instead, it is typically implemented by computing ∥ w ∥2 after each training\n",
            "step and rescaling w if needed (w ← w r / ∥ w ∥2).\n",
            "\n",
            "30 This MCDropout class will work with all Keras APIs, including the sequential API. If you only care about the\n",
            "\n",
            "functional API or the subclassing API, you do not have to create an MCDropout class; you can create a regular\n",
            "Dropout layer and call it with training=True.\n",
            "\n",
            "Avoiding Overfitting Through Regularization \n",
            "\n",
            "| \n",
            "\n",
            "399\n",
            "\n",
            "\fReducing  r  increases  the  amount  of  regularization  and  helps  reduce  overfitting.\n",
            "Max-norm  regularization  can  also  help  alleviate  the  unstable  gradients  problems  (if\n",
            "you are not using batch normalization).\n",
            "\n",
            "To  implement  max-norm  regularization  in  Keras,  set  the  kernel_constraint  argu‐\n",
            "ment of each hidden layer to a max_norm() constraint with the appropriate max value,\n",
            "like this:\n",
            "\n",
            "dense = tf.keras.layers.Dense(\n",
            "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
            "    kernel_constraint=tf.keras.constraints.max_norm(1.))\n",
            "\n",
            "After each training iteration, the model’s fit() method will call the object returned\n",
            "by  max_norm(), passing it the layer’s weights and getting rescaled weights in return,\n",
            "which  then  replace  the  layer’s  weights.  As  you’ll  see  in  Chapter  12,  you  can\n",
            "define  your  own  custom  constraint  function  if  necessary  and  use  it  as  the  kernel_\n",
            "constraint.  You  can  also  constrain  the  bias  terms  by  setting  the  bias_constraint\n",
            "argument.\n",
            "\n",
            "The  max_norm()  function  has  an  axis  argument  that  defaults  to  0.  A  Dense  layer\n",
            "usually has weights of shape [number of inputs, number of neurons], so using axis=0\n",
            "means  that  the  max-norm  constraint  will  apply  independently  to  each  neuron’s\n",
            "weight  vector.  If  you  want  to  use  max-norm  with  convolutional  layers  (see  Chap‐\n",
            "ter  14),  make  sure  to  set  the  max_norm()  constraint’s  axis  argument  appropriately\n",
            "(usually axis=[0, 1, 2]).\n",
            "\n",
            "Summary and Practical Guidelines\n",
            "In  this  chapter  we  have  covered  a  wide  range  of  techniques,  and  you  may  be  won‐\n",
            "dering  which  ones  you  should  use.  This  depends  on  the  task,  and  there  is  no  clear\n",
            "consensus  yet,  but  I  have  found  the  configuration  in  Table  11-3  to  work  fine  in\n",
            "most cases, without requiring much hyperparameter tuning. That said, please do not\n",
            "consider these defaults as hard rules!\n",
            "\n",
            "Table 11-3. Default DNN configuration\n",
            "\n",
            "Hyperparameter\n",
            "Kernel initializer\n",
            "\n",
            "Default value\n",
            "He initialization\n",
            "\n",
            "Activation function\n",
            "\n",
            "ReLU if shallow; Swish if deep\n",
            "\n",
            "Normalization\n",
            "\n",
            "Regularization\n",
            "\n",
            "Optimizer\n",
            "\n",
            "None if shallow; batch norm if deep\n",
            "\n",
            "Early stopping; weight decay if needed\n",
            "\n",
            "Nesterov accelerated gradients or AdamW\n",
            "\n",
            "Learning rate schedule\n",
            "\n",
            "Performance scheduling or 1cycle\n",
            "\n",
            "400 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fIf the network is a simple stack of dense layers, then it can self-normalize, and you\n",
            "should use the configuration in Table 11-4 instead.\n",
            "\n",
            "Table 11-4. DNN configuration for a self-normalizing net\n",
            "\n",
            "Hyperparameter\n",
            "Kernel initializer\n",
            "\n",
            "Default value\n",
            "LeCun initialization\n",
            "\n",
            "Activation function\n",
            "\n",
            "SELU\n",
            "\n",
            "Normalization\n",
            "\n",
            "Regularization\n",
            "\n",
            "Optimizer\n",
            "\n",
            "None (self-normalization)\n",
            "\n",
            "Alpha dropout if needed\n",
            "\n",
            "Nesterov accelerated gradients\n",
            "\n",
            "Learning rate schedule\n",
            "\n",
            "Performance scheduling or 1cycle\n",
            "\n",
            "Don’t forget to normalize the input features! You should also try to reuse parts of a\n",
            "pretrained neural network if you can find one that solves a similar problem, or use\n",
            "unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an\n",
            "auxiliary task if you have a lot of labeled data for a similar task.\n",
            "\n",
            "While the previous guidelines should cover most cases, here are some exceptions:\n",
            "\n",
            "•\n",
            "• If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\n",
            "the tiny weights after training). If you need an even sparser model, you can use\n",
            "the TensorFlow Model Optimization Toolkit. This will break self-normalization,\n",
            "so you should use the default configuration in this case.\n",
            "\n",
            "• If  you  need  a  low-latency  model  (one  that  performs  lightning-fast  predictions),\n",
            "•\n",
            "you may need to use fewer layers, use a fast activation function such as ReLU or\n",
            "leaky ReLU, and fold the batch normalization layers into the previous layers after\n",
            "training. Having a sparse model will also help. Finally, you may want to reduce\n",
            "the float precision from 32 bits to 16 or even 8 bits (see “Deploying a Model to a\n",
            "Mobile or Embedded Device” on page 741). Again, check out TF-MOT.\n",
            "\n",
            "• If  you  are  building  a  risk-sensitive  application,  or  inference  latency  is  not  very\n",
            "•\n",
            "important  in  your  application,  you  can  use  MC  dropout  to  boost  performance\n",
            "and get more reliable probability estimates, along with uncertainty estimates.\n",
            "\n",
            "With  these  guidelines,  you  are  now  ready  to  train  very  deep  nets!  I  hope  you  are\n",
            "now convinced that you can go quite a long way using just the convenient Keras API.\n",
            "There  may  come  a  time,  however,  when  you  need  to  have  even  more  control;  for\n",
            "example, to write a custom loss function or to tweak the training algorithm. For such\n",
            "cases  you  will  need  to  use  TensorFlow’s  lower-level  API,  as  you  will  see  in  the  next\n",
            "chapter.\n",
            "\n",
            "Summary and Practical Guidelines \n",
            "\n",
            "| \n",
            "\n",
            "401\n",
            "\n",
            "\fExercises\n",
            "\n",
            "1.\n",
            "1. What is the problem that Glorot initialization and He initialization aim to fix?\n",
            "\n",
            "2.\n",
            "2. Is  it  OK  to  initialize  all  the  weights  to  the  same  value  as  long  as  that  value  is\n",
            "\n",
            "selected randomly using He initialization?\n",
            "\n",
            "3. Is it OK to initialize the bias terms to 0?\n",
            "3.\n",
            "\n",
            "4.\n",
            "4. In  which  cases  would  you  want  to  use  each  of  the  activation  functions  we\n",
            "\n",
            "discussed in this chapter?\n",
            "\n",
            "5. What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g.,\n",
            "5.\n",
            "\n",
            "0.99999) when using an SGD optimizer?\n",
            "\n",
            "6.\n",
            "6. Name three ways you can produce a sparse model.\n",
            "\n",
            "7.\n",
            "7. Does  dropout  slow  down  training?  Does  it  slow  down  inference  (i.e.,  making\n",
            "\n",
            "predictions on new instances)? What about MC dropout?\n",
            "\n",
            "8.\n",
            "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
            "\n",
            "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
            "a.\n",
            "it’s  the  point  of  this  exercise).  Use  He  initialization  and  the  Swish  activation\n",
            "function.\n",
            "\n",
            "b. Using  Nadam  optimization  and  early  stopping,  train  the  network  on  the\n",
            "b.\n",
            "CIFAR10  dataset.  You  can  load  it  with  tf.keras.datasets.cifar10.load_\n",
            "data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000\n",
            "for  training,  10,000  for  testing)  with  10  classes,  so  you’ll  need  a  softmax\n",
            "output layer with 10 neurons. Remember to search for the right learning rate\n",
            "each time you change the model’s architecture or hyperparameters.\n",
            "\n",
            "c.\n",
            "c. Now  try  adding  batch  normalization  and  compare  the  learning  curves:  is  it\n",
            "converging  faster  than  before?  Does  it  produce  a  better  model?  How  does  it\n",
            "affect training speed?\n",
            "\n",
            "d. Try replacing batch normalization with SELU, and make the necessary adjust‐\n",
            "d.\n",
            "ments  to  ensure  the  network  self-normalizes  (i.e.,  standardize  the  input  fea‐\n",
            "tures,  use  LeCun  normal  initialization,  make  sure  the  DNN  contains  only  a\n",
            "sequence of dense layers, etc.).\n",
            "\n",
            "e.\n",
            "e. Try regularizing the model with alpha dropout. Then, without retraining your\n",
            "\n",
            "model, see if you can achieve better accuracy using MC dropout.\n",
            "\n",
            "f. Retrain  your  model  using  1cycle  scheduling  and  see  if  it  improves  training\n",
            "f.\n",
            "\n",
            "speed and model accuracy.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "402 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 11: Training Deep Neural Networks\n",
            "\n",
            "\fCHAPTER 12\n",
            "Custom Models and Training\n",
            "with TensorFlow\n",
            "\n",
            "Up until now, we’ve used only TensorFlow’s high-level API, Keras, but it already got\n",
            "us pretty far: we built various neural network architectures, including regression and\n",
            "classification  nets,  Wide  &  Deep  nets,  and  self-normalizing  nets,  using  all  sorts  of\n",
            "techniques,  such  as  batch  normalization,  dropout,  and  learning  rate  schedules.  In\n",
            "fact,  95%  of  the  use  cases  you  will  encounter  will  not  require  anything  other  than\n",
            "Keras (and tf.data; see Chapter 13). But now it’s time to dive deeper into TensorFlow\n",
            "and take a look at its lower-level Python API. This will be useful when you need extra\n",
            "control  to  write  custom  loss  functions,  custom  metrics,  layers,  models,  initializers,\n",
            "regularizers,  weight  constraints,  and  more.  You  may  even  need  to  fully  control  the\n",
            "training loop itself; for example, to apply special transformations or constraints to the\n",
            "gradients (beyond just clipping them) or to use multiple optimizers for different parts\n",
            "of the network. We will cover all these cases in this chapter, and we will also look at\n",
            "how you can boost your custom models and training algorithms using TensorFlow’s\n",
            "automatic graph generation feature. But first, let’s take a quick tour of TensorFlow.\n",
            "\n",
            "A Quick Tour of TensorFlow\n",
            "As you know, TensorFlow is a powerful library for numerical computation, particu‐\n",
            "larly well suited and fine-tuned for large-scale machine learning (but you can use it\n",
            "for anything else that requires heavy computations). It was developed by the Google\n",
            "Brain  team  and  it  powers  many  of  Google’s  large-scale  services,  such  as  Google\n",
            "Cloud Speech, Google Photos, and Google Search. It was open sourced in November\n",
            "\n",
            "403\n",
            "\n",
            "\f2015,  and  it  is  now  the  most  widely  used  deep  learning  library  in  the  industry:1\n",
            "countless  projects  use  TensorFlow  for  all  sorts  of  machine  learning  tasks,  such  as\n",
            "image  classification,  natural  language  processing,  recommender  systems,  and  time\n",
            "series forecasting.\n",
            "\n",
            "So what does TensorFlow offer? Here’s a summary:\n",
            "\n",
            "• Its core is very similar to NumPy, but with GPU support.\n",
            "•\n",
            "\n",
            "•\n",
            "• It supports distributed computing (across multiple devices and servers).\n",
            "\n",
            "• It includes a kind of just-in-time (JIT) compiler that allows it to optimize com‐\n",
            "•\n",
            "putations  for  speed  and  memory  usage.  It  works  by  extracting  the  computation\n",
            "graph from a Python function, optimizing it (e.g., by pruning unused nodes), and\n",
            "running  it  efficiently  (e.g.,  by  automatically  running  independent  operations  in\n",
            "parallel).\n",
            "\n",
            "• Computation  graphs  can  be  exported  to  a  portable  format,  so  you  can  train  a\n",
            "•\n",
            "TensorFlow model in one environment (e.g., using Python on Linux) and run it\n",
            "in another (e.g., using Java on an Android device).\n",
            "\n",
            "• It implements reverse-mode autodiff (see Chapter 10 and Appendix B) and pro‐\n",
            "•\n",
            "vides some excellent optimizers, such as RMSProp and Nadam (see Chapter 11),\n",
            "so you can easily minimize all sorts of loss functions.\n",
            "\n",
            "TensorFlow  offers  many  more  features  built  on  top  of  these  core  features:  the  most\n",
            "important  is  of  course  Keras,2  but  it  also  has  data  loading  and  preprocessing  ops\n",
            "(tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops (tf.signal),\n",
            "and more (see Figure 12-1 for an overview of TensorFlow’s Python API).\n",
            "\n",
            "We  will  cover  many  of  the  packages  and  functions  of  the  Tensor‐\n",
            "Flow API, but it’s impossible to cover them all, so you should really\n",
            "take some time to browse through the API; you will find that it is\n",
            "quite rich and well documented.\n",
            "\n",
            "At  the  lowest  level,  each  TensorFlow  operation  (op  for  short)  is  implemented  using\n",
            "highly  efficient  C++  code.3  Many  operations  have  multiple  implementations  called\n",
            "kernels:  each  kernel  is  dedicated  to  a  specific  device  type,  such  as  CPUs,  GPUs,  or\n",
            "even TPUs (tensor processing units). As you may know, GPUs can dramatically speed\n",
            "up  computations  by  splitting  them  into  many  smaller  chunks  and  running  them  in\n",
            "\n",
            "1 However, Facebook’s PyTorch library is currently more popular in academia: more papers cite PyTorch than\n",
            "\n",
            "TensorFlow or Keras. Moreover, Google’s JAX library is gaining momentum, especially in academia.\n",
            "\n",
            "2 TensorFlow includes another deep learning API called the estimators API, but it is now deprecated.\n",
            "\n",
            "3 If you ever need to (but you probably won’t), you can write your own operations using the C++ API.\n",
            "\n",
            "404 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fparallel across many GPU threads. TPUs are even faster: they are custom ASIC chips\n",
            "built specifically for deep learning operations4 (we will discuss how to use TensorFlow\n",
            "with GPUs or TPUs in Chapter 19).\n",
            "\n",
            "Figure 12-1. TensorFlow’s Python API\n",
            "\n",
            "TensorFlow’s  architecture  is  shown  in  Figure  12-2.  Most  of  the  time  your  code  will\n",
            "use the high-level APIs (especially Keras and tf.data), but when you need more flexi‐\n",
            "bility you will use the lower-level Python API, handling tensors directly. In any case,\n",
            "TensorFlow’s  execution  engine  will  take  care  of  running  the  operations  efficiently,\n",
            "even across multiple devices and machines if you tell it to.\n",
            "\n",
            "TensorFlow  runs  not  only  on  Windows,  Linux,  and  macOS,  but  also  on  mobile\n",
            "devices  (using  TensorFlow  Lite),  including  both  iOS  and  Android  (see  Chapter  19).\n",
            "Note  that  APIs  for  other  languages  are  also  available,  if  you  do  not  want  to  use  the\n",
            "Python  API:  there  are  C++,  Java,  and  Swift  APIs.  There  is  even  a  JavaScript  imple‐\n",
            "mentation called TensorFlow.js that makes it possible to run your models directly in\n",
            "your browser.\n",
            "\n",
            "4 To learn more about TPUs and how they work, check out https://homl.info/tpus.\n",
            "\n",
            "A Quick Tour of TensorFlow \n",
            "\n",
            "| \n",
            "\n",
            "405\n",
            "\n",
            "\fFigure 12-2. TensorFlow’s architecture\n",
            "\n",
            "There’s  more  to  TensorFlow  than  the  library.  TensorFlow  is  at  the  center  of  an\n",
            "extensive  ecosystem  of  libraries.  First,  there’s  TensorBoard  for  visualization  (see\n",
            "Chapter  10).  Next,  there’s  TensorFlow  Extended  (TFX),  which  is  a  set  of  libraries\n",
            "built by Google to productionize TensorFlow projects: it includes tools for data vali‐\n",
            "dation, preprocessing, model analysis, and serving (with TF Serving; see Chapter 19).\n",
            "Google’s  TensorFlow  Hub  provides  a  way  to  easily  download  and  reuse  pretrained\n",
            "neural networks. You can also get many neural network architectures, some of them\n",
            "pretrained, in TensorFlow’s model garden. Check out the TensorFlow Resources and\n",
            "https://github.com/jtoy/awesome-tensorflow for more TensorFlow-based projects. You\n",
            "will  find  hundreds  of  TensorFlow  projects  on  GitHub,  so  it  is  often  easy  to  find\n",
            "existing code for whatever you are trying to do.\n",
            "\n",
            "More and more ML papers are released along with their implemen‐\n",
            "tations,  and  sometimes  even  with  pretrained  models.  Check  out\n",
            "https://paperswithcode.com to easily find them.\n",
            "\n",
            "Last  but  not  least,  TensorFlow  has  a  dedicated  team  of  passionate  and  helpful\n",
            "developers, as well as a large community contributing to improving it. To ask techni‐\n",
            "cal  questions,  you  should  use  https://stackoverflow.com  and  tag  your  question  with\n",
            "tensorflow  and  python.  You  can  file  bugs  and  feature  requests  through  GitHub.  For\n",
            "general discussions, join the TensorFlow Forum.\n",
            "\n",
            "OK, it’s time to start coding!\n",
            "\n",
            "406 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fUsing TensorFlow like NumPy\n",
            "TensorFlow’s API revolves around tensors, which flow from operation to operation—\n",
            "hence  the  name  TensorFlow.  A  tensor  is  very  similar  to  a  NumPy  ndarray:  it  is\n",
            "usually a multidimensional array, but it can also hold a scalar (a simple value, such as\n",
            "42). These tensors will be important when we create custom cost functions, custom\n",
            "metrics, custom layers, and more, so let’s see how to create and manipulate them.\n",
            "\n",
            "Tensors and Operations\n",
            "You can create a tensor with tf.constant(). For example, here is a tensor represent‐\n",
            "ing a matrix with two rows and three columns of floats:\n",
            "\n",
            ">>> import tensorflow as tf\n",
            ">>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # matrix\n",
            ">>> t\n",
            "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)>\n",
            "\n",
            "Just like an ndarray, a tf.Tensor has a shape and a data type (dtype):\n",
            "\n",
            ">>> t.shape\n",
            "TensorShape([2, 3])\n",
            ">>> t.dtype\n",
            "tf.float32\n",
            "\n",
            "Indexing works much like in NumPy:\n",
            "\n",
            ">>> t[:, 1:]\n",
            "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
            "array([[2., 3.],\n",
            "       [5., 6.]], dtype=float32)>\n",
            ">>> t[..., 1, tf.newaxis]\n",
            "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [5.]], dtype=float32)>\n",
            "\n",
            "Most importantly, all sorts of tensor operations are available:\n",
            "\n",
            ">>> t + 10\n",
            "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "array([[11., 12., 13.],\n",
            "       [14., 15., 16.]], dtype=float32)>\n",
            ">>> tf.square(t)\n",
            "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "array([[ 1.,  4.,  9.],\n",
            "       [16., 25., 36.]], dtype=float32)>\n",
            ">>> t @ tf.transpose(t)\n",
            "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
            "array([[14., 32.],\n",
            "       [32., 77.]], dtype=float32)>\n",
            "\n",
            "Using TensorFlow like NumPy \n",
            "\n",
            "| \n",
            "\n",
            "407\n",
            "\n",
            "\fNote that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls\n",
            "the magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators,\n",
            "like - and *, are also supported. The @ operator was added in Python 3.5, for matrix\n",
            "multiplication: it is equivalent to calling the tf.matmul() function.\n",
            "\n",
            "Many  functions  and  classes  have  aliases.  For  example,  tf.add()\n",
            "and tf.math.add() are the same function. This allows TensorFlow\n",
            "to  have  concise  names  for  the  most  common  operations5  while\n",
            "preserving well-organized packages.\n",
            "\n",
            "A tensor can also hold a scalar value. In this case, the shape is empty:\n",
            "\n",
            ">>> tf.constant(42)\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=42>\n",
            "\n",
            "located \n",
            "\n",
            "its  own \n",
            "\n",
            "low-level  API, \n",
            "\n",
            "in\n",
            "The  Keras  API  has \n",
            "tf.keras.backend. This package is usually imported as K, for con‐\n",
            "ciseness.  It  used  to  include  functions  like  K.square(),  K.exp(),\n",
            "and  K.sqrt(),  which  you  may  run  across  in  existing  code:  this\n",
            "was  useful  to  write  portable  code  back  when  Keras  supported\n",
            "multiple  backends,  but  now  that  Keras  is  TensorFlow-only,  you\n",
            "should  call  TensorFlow’s  low-level  API  directly  (e.g.,  tf.square()\n",
            "instead of K.square()). Technically K.square() and its friends are\n",
            "still there for backward compatibility, but the documentation of the\n",
            "tf.keras.backend package only lists a handful of utility functions,\n",
            "such as clear_session() (mentioned in Chapter 10).\n",
            "\n",
            "You  will  find  all  the  basic  math  operations  you  need  (tf.add(),  tf.multiply(),\n",
            "tf.square(),  tf.exp(),  tf.sqrt(),  etc.)  and  most  operations  that  you  can  find\n",
            "in  NumPy  (e.g.,  tf.reshape(),  tf.squeeze(),  tf.tile()).  Some  functions  have  a\n",
            "different  name  than  in  NumPy;  for  instance,  tf.reduce_mean(),  tf.reduce_sum(),\n",
            "tf.reduce_max(),  and  tf.math.log()  are  the  equivalent  of  np.mean(),  np.sum(),\n",
            "np.max(), and np.log(). When the name differs, there is usually a good reason for\n",
            "it.  For  example,  in  TensorFlow  you  must  write  tf.transpose(t);  you  cannot  just\n",
            "write t.T like in NumPy. The reason is that the tf.transpose() function does not do\n",
            "exactly the same thing as NumPy’s T attribute: in TensorFlow, a new tensor is created\n",
            "with  its  own  copy  of  the  transposed  data,  while  in  NumPy,  t.T  is  just  a  transposed\n",
            "view on the same data. Similarly, the tf.reduce_sum() operation is named this way\n",
            "because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that does\n",
            "\n",
            "5 A notable exception is tf.math.log(), which is commonly used but doesn’t have a tf.log() alias, as it might\n",
            "\n",
            "be confused with logging.\n",
            "\n",
            "408 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fnot guarantee the order in which the elements are added: because 32-bit floats have\n",
            "limited  precision,  the  result  may  change  ever  so  slightly  every  time  you  call  this\n",
            "operation. The same is true of tf.reduce_mean() (but of course tf.reduce_max() is\n",
            "deterministic).\n",
            "\n",
            "Tensors and NumPy\n",
            "Tensors  play  nice  with  NumPy:  you  can  create  a  tensor  from  a  NumPy  array,  and\n",
            "vice versa. You can even apply TensorFlow operations to NumPy arrays and NumPy\n",
            "operations to tensors:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> a = np.array([2., 4., 5.])\n",
            ">>> tf.constant(a)\n",
            "<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\n",
            ">>> t.numpy()  # or np.array(t)\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)\n",
            ">>> tf.square(a)\n",
            "<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\n",
            ">>> np.square(t)\n",
            "array([[ 1.,  4.,  9.],\n",
            "       [16., 25., 36.]], dtype=float32)\n",
            "\n",
            "Notice that NumPy uses 64-bit precision by default, while Tensor‐\n",
            "Flow uses 32-bit. This is because 32-bit precision is generally more\n",
            "than  enough  for  neural  networks,  plus  it  runs  faster  and  uses  less\n",
            "RAM. So when you create a tensor from a NumPy array, make sure\n",
            "to set dtype=tf.float32.\n",
            "\n",
            "Type Conversions\n",
            "Type  conversions  can  significantly  hurt  performance,  and  they  can  easily  go  unno‐\n",
            "ticed when they are done automatically. To avoid this, TensorFlow does not perform\n",
            "any type conversions automatically: it just raises an exception if you try to execute an\n",
            "operation  on  tensors  with  incompatible  types.  For  example,  you  cannot  add  a  float\n",
            "tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\n",
            "\n",
            ">>> tf.constant(2.) + tf.constant(40)\n",
            "[...] InvalidArgumentError: [...] expected to be a float tensor [...]\n",
            ">>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
            "[...] InvalidArgumentError: [...] expected to be a float tensor [...]\n",
            "\n",
            "This may be a bit annoying at first, but remember that it’s for a good cause! And of\n",
            "course you can use tf.cast() when you really need to convert types:\n",
            "\n",
            ">>> t2 = tf.constant(40., dtype=tf.float64)\n",
            ">>> tf.constant(2.0) + tf.cast(t2, tf.float32)\n",
            "<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\n",
            "\n",
            "Using TensorFlow like NumPy \n",
            "\n",
            "| \n",
            "\n",
            "409\n",
            "\n",
            "\fVariables\n",
            "The tf.Tensor values we’ve seen so far are immutable: we cannot modify them. This\n",
            "means that we cannot use regular tensors to implement weights in a neural network,\n",
            "since they need to be tweaked by backpropagation. Plus, other parameters may also\n",
            "need to change over time (e.g., a momentum optimizer keeps track of past gradients).\n",
            "What we need is a tf.Variable:\n",
            "\n",
            ">>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
            ">>> v\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)>\n",
            "\n",
            "A  tf.Variable  acts  much  like  a  tf.Tensor:  you  can  perform  the  same  operations\n",
            "with  it,  it  plays  nicely  with  NumPy  as  well,  and  it  is  just  as  picky  with  types.  But\n",
            "it  can  also  be  modified  in  place  using  the  assign()  method  (or  assign_add()  or\n",
            "assign_sub(),  which  increment  or  decrement  the  variable  by  the  given  value).  You\n",
            "can  also  modify  individual  cells  (or  slices),  by  using  the  cell’s  (or  slice’s)  assign()\n",
            "method or by using the scatter_update() or scatter_nd_update() methods:\n",
            "\n",
            "v.assign(2 * v)           # v now equals [[2., 4., 6.], [8., 10., 12.]]\n",
            "v[0, 1].assign(42)        # v now equals [[2., 42., 6.], [8., 10., 12.]]\n",
            "v[:, 2].assign([0., 1.])  # v now equals [[2., 42., 0.], [8., 10., 1.]]\n",
            "v.scatter_nd_update(      # v now equals [[100., 42., 0.], [8., 10., 200.]]\n",
            "    indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
            "\n",
            "Direct assignment will not work:\n",
            "\n",
            ">>> v[1] = [7., 8., 9.]\n",
            "[...] TypeError: 'ResourceVariable' object does not support item assignment\n",
            "\n",
            "In practice you will rarely have to create variables manually; Keras\n",
            "provides an add_weight() method that will take care of it for you,\n",
            "as you’ll see. Moreover, model parameters will generally be updated\n",
            "directly  by  the  optimizers,  so  you  will  rarely  need  to  update  vari‐\n",
            "ables manually.\n",
            "\n",
            "Other Data Structures\n",
            "TensorFlow  supports  several  other  data  structures,  including  the  following  (see  the\n",
            "“Other  Data  Structures”  section  in  this  chapter’s  notebook  or  Appendix  C  for  more\n",
            "details):\n",
            "\n",
            "410 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fSparse tensors (tf.SparseTensor)\n",
            "\n",
            "Efficiently  represent  tensors  containing  mostly  zeros.  The  tf.sparse  package\n",
            "contains operations for sparse tensors.\n",
            "\n",
            "Tensor arrays (tf.TensorArray)\n",
            "\n",
            "Are  lists  of  tensors.  They  have  a  fixed  length  by  default  but  can  optionally  be\n",
            "made  extensible.  All  tensors  they  contain  must  have  the  same  shape  and  data\n",
            "type.\n",
            "\n",
            "Ragged tensors (tf.RaggedTensor)\n",
            "\n",
            "Represent  lists  of  tensors,  all  of  the  same  rank  and  data  type,  but  with  varying\n",
            "sizes.  The  dimensions  along  which  the  tensor  sizes  vary  are  called  the  ragged\n",
            "dimensions. The tf.ragged package contains operations for ragged tensors.\n",
            "\n",
            "String tensors\n",
            "\n",
            "Are regular tensors of type tf.string. These represent byte strings, not Unicode\n",
            "strings,  so  if  you  create  a  string  tensor  using  a  Unicode  string  (e.g.,  a  regular\n",
            "Python  3  string  like  \"café\"),  then  it  will  get  encoded  to  UTF-8  automatically\n",
            "(e.g.,  b\"caf\\xc3\\xa9\").  Alternatively,  you  can  represent  Unicode  strings  using\n",
            "tensors of type tf.int32, where each item represents a Unicode code point (e.g.,\n",
            "[99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte\n",
            "strings  and  Unicode  strings  (and  to  convert  one  into  the  other).  It’s  important\n",
            "to  note  that  a  tf.string  is  atomic,  meaning  that  its  length  does  not  appear  in\n",
            "the tensor’s shape. Once you convert it to a Unicode tensor (i.e., a tensor of type\n",
            "tf.int32 holding Unicode code points), the length appears in the shape.\n",
            "\n",
            "Sets\n",
            "\n",
            "tensors).  For  example,\n",
            "Are  represented  as  regular \n",
            "tf.constant([[1, 2], [3, 4]]) represents the two sets {1, 2} and {3, 4}. More\n",
            "generally,  each  set  is  represented  by  a  vector  in  the  tensor’s  last  axis.  You  can\n",
            "manipulate sets using operations from the tf.sets package.\n",
            "\n",
            "tensors  (or  sparse \n",
            "\n",
            "Queues\n",
            "\n",
            "Store  tensors  across  multiple  steps.  TensorFlow  offers  various  kinds  of  queues:\n",
            "basic  first-in,  first-out  (FIFO)  queues  (FIFOQueue),  plus  queues  that  can  priori‐\n",
            "tize some items (PriorityQueue), shuffle their items (RandomShuffleQueue), and\n",
            "batch items of different shapes by padding (PaddingFIFOQueue). These classes are\n",
            "all in the tf.queue package.\n",
            "\n",
            "With tensors, operations, variables, and various data structures at your disposal, you\n",
            "are now ready to customize your models and training algorithms!\n",
            "\n",
            "Using TensorFlow like NumPy \n",
            "\n",
            "| \n",
            "\n",
            "411\n",
            "\n",
            "\fCustomizing Models and Training Algorithms\n",
            "You’ll start by creating a custom loss function, which is a straightforward and com‐\n",
            "mon use case.\n",
            "\n",
            "Custom Loss Functions\n",
            "Suppose  you  want  to  train  a  regression  model,  but  your  training  set  is  a  bit  noisy.\n",
            "Of  course,  you  start  by  trying  to  clean  up  your  dataset  by  removing  or  fixing  the\n",
            "outliers,  but  that  turns  out  to  be  insufficient;  the  dataset  is  still  noisy.  Which  loss\n",
            "function  should  you  use?  The  mean  squared  error  might  penalize  large  errors  too\n",
            "much  and  cause  your  model  to  be  imprecise.  The  mean  absolute  error  would  not\n",
            "penalize  outliers  as  much,  but  training  might  take  a  while  to  converge,  and  the\n",
            "trained  model  might  not  be  very  precise.  This  is  probably  a  good  time  to  use  the\n",
            "Huber loss (introduced in Chapter 10) instead of the good old MSE. The Huber loss is\n",
            "available in Keras (just use an instance of the tf.keras.losses.Huber class), but let’s\n",
            "pretend it’s not there. To implement it, just create a function that takes the labels and\n",
            "the model’s predictions as arguments, and uses TensorFlow operations to compute a\n",
            "tensor containing all the losses (one per sample):\n",
            "\n",
            "def huber_fn(y_true, y_pred):\n",
            "    error = y_true - y_pred\n",
            "    is_small_error = tf.abs(error) < 1\n",
            "    squared_loss = tf.square(error) / 2\n",
            "    linear_loss  = tf.abs(error) - 0.5\n",
            "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
            "\n",
            "For  better  performance,  you  should  use  a  vectorized  implementa‐\n",
            "tion,  as  in  this  example.  Moreover,  if  you  want  to  benefit  from\n",
            "TensorFlow’s  graph  optimization  features,  you  should  use  only\n",
            "TensorFlow operations.\n",
            "\n",
            "It is also possible to return the mean loss instead of the individual sample losses, but\n",
            "this  is  not  recommended  as  it  makes  it  impossible  to  use  class  weights  or  sample\n",
            "weights when you need them (see Chapter 10).\n",
            "\n",
            "Now you can use this Huber loss function when you compile the Keras model, then\n",
            "train your model as usual:\n",
            "\n",
            "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
            "model.fit(X_train, y_train, [...])\n",
            "\n",
            "And that’s it! For each batch during training, Keras will call the huber_fn() function\n",
            "to compute the loss, then it will use reverse-mode autodiff to compute the gradients\n",
            "of  the  loss  with  regard  to  all  the  model  parameters,  and  finally  it  will  perform  a\n",
            "gradient  descent  step  (in  this  example  using  a  Nadam  optimizer).  Moreover,  it  will\n",
            "\n",
            "412 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fkeep  track  of  the  total  loss  since  the  beginning  of  the  epoch,  and  it  will  display  the\n",
            "mean loss.\n",
            "\n",
            "But what happens to this custom loss when you save the model?\n",
            "\n",
            "Saving and Loading Models That Contain Custom Components\n",
            "Saving  a  model  containing  a  custom  loss  function  works  fine,  but  when  you  load\n",
            "it,  you’ll  need  to  provide  a  dictionary  that  maps  the  function  name  to  the  actual\n",
            "function.  More  generally,  when  you  load  a  model  containing  custom  objects,  you\n",
            "need to map the names to the objects:\n",
            "\n",
            "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss\",\n",
            "                                   custom_objects={\"huber_fn\": huber_fn})\n",
            "\n",
            "If  you  decorate  the  huber_fn()  function  with  @keras.utils.\n",
            "register_keras_serializable(), it will automatically be available\n",
            "to  the  load_model()  function:  there’s  no  need  to  include  it  in  the\n",
            "custom_objects dictionary.\n",
            "\n",
            "With the current implementation, any error between –1 and 1 is considered “small”.\n",
            "But what if you want a different threshold? One solution is to create a function that\n",
            "creates a configured loss function:\n",
            "\n",
            "def create_huber(threshold=1.0):\n",
            "    def huber_fn(y_true, y_pred):\n",
            "        error = y_true - y_pred\n",
            "        is_small_error = tf.abs(error) < threshold\n",
            "        squared_loss = tf.square(error) / 2\n",
            "        linear_loss  = threshold * tf.abs(error) - threshold ** 2 / 2\n",
            "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
            "    return huber_fn\n",
            "\n",
            "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
            "\n",
            "Unfortunately, when you save the model, the threshold will not be saved. This means\n",
            "that you will have to specify the threshold value when loading the model (note that\n",
            "the name to use is \"huber_fn\", which is the name of the function you gave Keras, not\n",
            "the name of the function that created it):\n",
            "\n",
            "model = tf.keras.models.load_model(\n",
            "    \"my_model_with_a_custom_loss_threshold_2\",\n",
            "    custom_objects={\"huber_fn\": create_huber(2.0)}\n",
            ")\n",
            "\n",
            "You can solve this by creating a subclass of the tf.keras.losses.Loss class, and then\n",
            "implementing its get_config() method:\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "413\n",
            "\n",
            "\fclass HuberLoss(tf.keras.losses.Loss):\n",
            "    def __init__(self, threshold=1.0, **kwargs):\n",
            "        self.threshold = threshold\n",
            "        super().__init__(**kwargs)\n",
            "\n",
            "    def call(self, y_true, y_pred):\n",
            "        error = y_true - y_pred\n",
            "        is_small_error = tf.abs(error) < self.threshold\n",
            "        squared_loss = tf.square(error) / 2\n",
            "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
            "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
            "\n",
            "    def get_config(self):\n",
            "        base_config = super().get_config()\n",
            "        return {**base_config, \"threshold\": self.threshold}\n",
            "\n",
            "Let’s walk through this code:\n",
            "\n",
            "• The  constructor  accepts  **kwargs  and  passes  them  to  the  parent  constructor,\n",
            "•\n",
            "which handles standard hyperparameters: the name of the loss and the reduction\n",
            "algorithm  to  use  to  aggregate  the  individual  instance  losses.  By  default  this  is\n",
            "\"AUTO\", which is equivalent to \"SUM_OVER_BATCH_SIZE\": the loss will be the sum\n",
            "of the instance losses, weighted by the sample weights, if any, and divided by the\n",
            "batch size (not by the sum of weights, so this is not the weighted mean).6 Other\n",
            "possible values are \"SUM\" and \"NONE\".\n",
            "\n",
            "• The  call()  method  takes  the  labels  and  predictions,  computes  all  the  instance\n",
            "•\n",
            "\n",
            "losses, and returns them.\n",
            "\n",
            "• The  get_config()  method  returns  a  dictionary  mapping  each  hyperparameter\n",
            "•\n",
            "name to its value. It first calls the parent class’s get_config() method, then adds\n",
            "the new hyperparameters to this dictionary.7\n",
            "\n",
            "You can then use any instance of this class when you compile the model:\n",
            "\n",
            "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
            "\n",
            "When you save the model, the threshold will be saved along with it; and when you\n",
            "load the model, you just need to map the class name to the class itself:\n",
            "\n",
            "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n",
            "                                   custom_objects={\"HuberLoss\": HuberLoss})\n",
            "\n",
            "6 It would not be a good idea to use a weighted mean: if you did, then two instances with the same weight but in\n",
            "\n",
            "different batches would have a different impact on training, depending on the total weight of each batch.\n",
            "7 The {**x, [...]} syntax was added in Python 3.5, to merge all the key/value pairs from dictionary x into\n",
            "another dictionary. Since Python 3.9, you can use the nicer x | y syntax instead (where x and y are two\n",
            "dictionaries).\n",
            "\n",
            "414 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fWhen  you  save  a  model,  Keras  calls  the  loss  instance’s  get_config()  method  and\n",
            "saves  the  config  in  the  SavedModel  format.  When  you  load  the  model,  it  calls  the\n",
            "from_config() class method on the HuberLoss class: this method is implemented by\n",
            "the  base  class  (Loss)  and  creates  an  instance  of  the  class,  passing  **config  to  the\n",
            "constructor.\n",
            "\n",
            "That’s it for losses! As you’ll see now, custom activation functions, initializers, regu‐\n",
            "larizers, and constraints are not much different.\n",
            "\n",
            "Custom Activation Functions, Initializers, Regularizers,\n",
            "and Constraints\n",
            "Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\n",
            "rics,  activation  functions,  layers,  and  even  full  models,  can  be  customized  in  much\n",
            "the  same  way.  Most  of  the  time,  you  will  just  need  to  write  a  simple  function  with\n",
            "the appropriate inputs and outputs. Here are examples of a custom activation func‐\n",
            "tion  (equivalent  to  tf.keras.activations.softplus()  or  tf.nn.softplus()),  a\n",
            "custom Glorot initializer (equivalent to tf.keras.initializers.glorot_normal()),\n",
            "a  custom  ℓ1  regularizer  (equivalent  to  tf.keras.regularizers.l1(0.01)),  and  a\n",
            "custom  constraint  that  ensures  weights  are  all  positive  (equivalent  to  tf.keras.\n",
            "constraints.nonneg() or tf.nn.relu()):\n",
            "\n",
            "def my_softplus(z):\n",
            "    return tf.math.log(1.0 + tf.exp(z))\n",
            "\n",
            "def my_glorot_initializer(shape, dtype=tf.float32):\n",
            "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
            "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
            "\n",
            "def my_l1_regularizer(weights):\n",
            "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
            "\n",
            "def my_positive_weights(weights):  # return value is just tf.nn.relu(weights)\n",
            "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
            "\n",
            "As you can see, the arguments depend on the type of custom function. These custom\n",
            "functions can then be used normally, as shown here:\n",
            "\n",
            "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
            "                              kernel_initializer=my_glorot_initializer,\n",
            "                              kernel_regularizer=my_l1_regularizer,\n",
            "                              kernel_constraint=my_positive_weights)\n",
            "\n",
            "The activation function will be applied to the output of this Dense layer, and its result\n",
            "will  be  passed  on  to  the  next  layer.  The  layer’s  weights  will  be  initialized  using  the\n",
            "value  returned  by  the  initializer.  At  each  training  step  the  weights  will  be  passed  to\n",
            "the regularization function to compute the regularization loss, which will be added to\n",
            "the  main  loss  to  get  the  final  loss  used  for  training.  Finally,  the  constraint  function\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "415\n",
            "\n",
            "\fwill be called after each training step, and the layer’s weights will be replaced by the\n",
            "constrained weights.\n",
            "\n",
            "If  a  function  has  hyperparameters  that  need  to  be  saved  along  with  the  model,\n",
            "then  you  will  want  to  subclass  the  appropriate  class,  such  as  tf.keras.regular\n",
            "izers.Regularizer, tf.keras.constraints.Constraint, tf.keras.initializers.\n",
            "Initializer,  or  tf.keras.layers.Layer  (for  any  layer,  including  activation  func‐\n",
            "tions). Much as you did for the custom loss, here is a simple class for ℓ1 regularization\n",
            "that  saves  its  factor  hyperparameter  (this  time  you  do  not  need  to  call  the  parent\n",
            "constructor or the get_config() method, as they are not defined by the parent class):\n",
            "\n",
            "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
            "    def __init__(self, factor):\n",
            "        self.factor = factor\n",
            "\n",
            "    def __call__(self, weights):\n",
            "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
            "\n",
            "    def get_config(self):\n",
            "        return {\"factor\": self.factor}\n",
            "\n",
            "Note that you must implement the call() method for losses, layers (including activa‐\n",
            "tion functions), and models, or the __call__() method for regularizers, initializers,\n",
            "and constraints. For metrics, things are a bit different, as you will see now.\n",
            "\n",
            "Custom Metrics\n",
            "Losses  and  metrics  are  conceptually  not  the  same  thing:  losses  (e.g.,  cross  entropy)\n",
            "are  used  by  gradient  descent  to  train  a  model,  so  they  must  be  differentiable  (at\n",
            "least at the points where they are evaluated), and their gradients should not be zero\n",
            "everywhere. Plus, it’s OK if they are not easily interpretable by humans. In contrast,\n",
            "metrics  (e.g.,  accuracy)  are  used  to  evaluate  a  model:  they  must  be  more  easily\n",
            "interpretable, and they can be nondifferentiable or have zero gradients everywhere.\n",
            "\n",
            "That  said,  in  most  cases,  defining  a  custom  metric  function  is  exactly  the  same  as\n",
            "defining a custom loss function. In fact, we could even use the Huber loss function we\n",
            "created earlier as a metric;8 it would work just fine (and persistence would also work\n",
            "the same way, in this case only saving the name of the function, \"huber_fn\", not the\n",
            "threshold):\n",
            "\n",
            "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
            "\n",
            "8 However, the Huber loss is seldom used as a metric—the MAE or MSE is generally preferred.\n",
            "\n",
            "416 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fFor each batch during training, Keras will compute this metric and keep track of its\n",
            "mean  since  the  beginning  of  the  epoch.  Most  of  the  time,  this  is  exactly  what  you\n",
            "want.  But  not  always!  Consider  a  binary  classifier’s  precision,  for  example.  As  you\n",
            "saw in Chapter 3, precision is the number of true positives divided by the number of\n",
            "positive  predictions  (including  both  true  positives  and  false  positives).  Suppose  the\n",
            "model  made  five  positive  predictions  in  the  first  batch,  four  of  which  were  correct:\n",
            "that’s 80% precision. Then suppose the model made three positive predictions in the\n",
            "second  batch,  but  they  were  all  incorrect:  that’s  0%  precision  for  the  second  batch.\n",
            "If  you  just  compute  the  mean  of  these  two  precisions,  you  get  40%.  But  wait  a\n",
            "second—that’s  not  the  model’s  precision  over  these  two  batches!  Indeed,  there  were\n",
            "a total of four true positives (4 + 0) out of eight positive predictions (5 + 3), so the\n",
            "overall precision is 50%, not 40%. What we need is an object that can keep track of\n",
            "the number of true positives and the number of false positives and that can compute\n",
            "the  precision  based  on  these  numbers  when  requested.  This  is  precisely  what  the\n",
            "tf.keras.metrics.Precision class does:\n",
            "\n",
            ">>> precision = tf.keras.metrics.Precision()\n",
            ">>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>\n",
            ">>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n",
            "\n",
            "In  this  example,  we  created  a  Precision  object,  then  we  used  it  like  a  function,\n",
            "passing it the labels and predictions for the first batch, then for the second batch (you\n",
            "can optionally pass sample weights as well, if you want). We used the same number\n",
            "of true and false positives as in the example we just discussed. After the first batch, it\n",
            "returns a precision of 80%; then after the second batch, it returns 50% (which is the\n",
            "overall  precision  so  far,  not  the  second  batch’s  precision).  This  is  called  a  streaming\n",
            "metric (or stateful metric), as it is gradually updated, batch after batch.\n",
            "\n",
            "At  any  point,  we  can  call  the  result()  method  to  get  the  current  value  of  the\n",
            "metric.  We  can  also  look  at  its  variables  (tracking  the  number  of  true  and  false\n",
            "positives) by using the variables attribute, and we can reset these variables using the\n",
            "reset_states() method:\n",
            "\n",
            ">>> precision.result()\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n",
            ">>> precision.variables\n",
            "[<tf.Variable 'true_positives:0' [...], numpy=array([4.], dtype=float32)>,\n",
            " <tf.Variable 'false_positives:0' [...], numpy=array([4.], dtype=float32)>]\n",
            ">>> precision.reset_states()  # both variables get reset to 0.0\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "417\n",
            "\n",
            "\fIf  you  need  to  define  your  own  custom  streaming  metric,  create  a  subclass  of  the\n",
            "tf.keras.metrics.Metric class. Here is a basic example that keeps track of the total\n",
            "Huber  loss  and  the  number  of  instances  seen  so  far.  When  asked  for  the  result,  it\n",
            "returns the ratio, which is just the mean Huber loss:\n",
            "\n",
            "class HuberMetric(tf.keras.metrics.Metric):\n",
            "    def __init__(self, threshold=1.0, **kwargs):\n",
            "        super().__init__(**kwargs)  # handles base args (e.g., dtype)\n",
            "        self.threshold = threshold\n",
            "        self.huber_fn = create_huber(threshold)\n",
            "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
            "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
            "\n",
            "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
            "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
            "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
            "\n",
            "    def result(self):\n",
            "        return self.total / self.count\n",
            "\n",
            "    def get_config(self):\n",
            "        base_config = super().get_config()\n",
            "        return {**base_config, \"threshold\": self.threshold}\n",
            "\n",
            "Let’s walk through this code:9\n",
            "\n",
            "• The constructor uses the add_weight() method to create the variables needed to\n",
            "•\n",
            "keep track of the metric’s state over multiple batches—in this case, the sum of all\n",
            "Huber losses (total) and the number of instances seen so far (count). You could\n",
            "just create variables manually if you preferred. Keras tracks any tf.Variable that\n",
            "is set as an attribute (and more generally, any “trackable” object, such as layers or\n",
            "models).\n",
            "\n",
            "• The update_state() method is called when you use an instance of this class as\n",
            "•\n",
            "a function (as we did with the Precision object). It updates the variables, given\n",
            "the labels and predictions for one batch (and sample weights, but in this case we\n",
            "ignore them).\n",
            "\n",
            "• The  result()  method  computes  and  returns  the  final  result,  in  this  case  the\n",
            "•\n",
            "mean  Huber  metric  over  all  instances.  When  you  use  the  metric  as  a  function,\n",
            "the update_state() method gets called first, then the result() method is called,\n",
            "and its output is returned.\n",
            "\n",
            "9 This class is for illustration purposes only. A simpler and better implementation would just subclass the\n",
            "\n",
            "tf.keras.metrics.Mean class; see the “Streaming Metrics” section of this chapter’s notebook for an example.\n",
            "\n",
            "418 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\f• We  also  implement  the  get_config()  method  to  ensure  the  threshold  gets\n",
            "•\n",
            "\n",
            "saved along with the model.\n",
            "\n",
            "• The default implementation of the reset_states() method resets all variables to\n",
            "•\n",
            "\n",
            "0.0 (but you can override it if needed).\n",
            "\n",
            "Keras will take care of variable persistence seamlessly; no action is\n",
            "required.\n",
            "\n",
            "When  you  define  a  metric  using  a  simple  function,  Keras  automatically  calls  it  for\n",
            "each  batch,  and  it  keeps  track  of  the  mean  during  each  epoch,  just  like  we  did\n",
            "manually. So the only benefit of our HuberMetric class is that the threshold will be\n",
            "saved.  But  of  course,  some  metrics,  like  precision,  cannot  simply  be  averaged  over\n",
            "batches: in those cases, there’s no other option than to implement a streaming metric.\n",
            "\n",
            "Now  that  you’ve  built  a  streaming  metric,  building  a  custom  layer  will  seem  like  a\n",
            "walk in the park!\n",
            "\n",
            "Custom Layers\n",
            "You may occasionally want to build an architecture that contains an exotic layer for\n",
            "which  TensorFlow  does  not  provide  a  default  implementation.  Or  you  may  simply\n",
            "want  to  build  a  very  repetitive  architecture,  in  which  a  particular  block  of  layers  is\n",
            "repeated many times, and it would be convenient to treat each block as a single layer.\n",
            "For such cases, you’ll want to build a custom layer.\n",
            "\n",
            "There  are  some  layers  that  have  no  weights,  such  as  tf.keras.layers.Flatten  or\n",
            "tf.keras.layers.ReLU. If you want to create a custom layer without any weights, the\n",
            "simplest option is to write a function and wrap it in a tf.keras.layers.Lambda layer.\n",
            "For example, the following layer will apply the exponential function to its inputs:\n",
            "\n",
            "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n",
            "\n",
            "This custom layer can then be used like any other layer, using the sequential API, the\n",
            "functional API, or the subclassing API. You can also use it as an activation function,\n",
            "or  you  could  use  activation=tf.exp.  The  exponential  layer  is  sometimes  used  in\n",
            "the output layer of a regression model when the values to predict have very different\n",
            "scales (e.g., 0.001, 10., 1,000.). In fact, the exponential function is one of the standard\n",
            "activation functions in Keras, so you can just use activation=\"exponential\".\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "419\n",
            "\n",
            "\fAs  you  might  guess,  to  build  a  custom  stateful  layer  (i.e.,  a  layer  with  weights),\n",
            "you need to create a subclass of the tf.keras.layers.Layer class. For example, the\n",
            "following class implements a simplified version of the Dense layer:\n",
            "\n",
            "class MyDense(tf.keras.layers.Layer):\n",
            "    def __init__(self, units, activation=None, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "        self.activation = tf.keras.activations.get(activation)\n",
            "\n",
            "    def build(self, batch_input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
            "            initializer=\"glorot_normal\")\n",
            "        self.bias = self.add_weight(\n",
            "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
            "\n",
            "    def call(self, X):\n",
            "        return self.activation(X @ self.kernel + self.bias)\n",
            "\n",
            "    def get_config(self):\n",
            "        base_config = super().get_config()\n",
            "        return {**base_config, \"units\": self.units,\n",
            "                \"activation\": tf.keras.activations.serialize(self.activation)}\n",
            "\n",
            "Let’s walk through this code:\n",
            "\n",
            "• The  constructor  takes  all  the  hyperparameters  as  arguments  (in  this  example,\n",
            "•\n",
            "units  and  activation),  and  importantly  it  also  takes  a  **kwargs  argument.  It\n",
            "calls  the  parent  constructor,  passing  it  the  kwargs:  this  takes  care  of  standard\n",
            "arguments such as input_shape, trainable, and name. Then it saves the hyper‐\n",
            "parameters as attributes, converting the activation argument to the appropriate\n",
            "activation function using the tf.keras.activations.get() function (it accepts\n",
            "functions, standard strings like \"relu\" or \"swish\", or simply None).\n",
            "\n",
            "• The  build()  method’s  role  is  to  create  the  layer’s  variables  by  calling  the\n",
            "•\n",
            "add_weight()  method  for  each  weight.  The  build()  method  is  called  the  first\n",
            "time  the  layer  is  used.  At  that  point,  Keras  will  know  the  shape  of  this  layer’s\n",
            "inputs,  and  it  will  pass  it  to  the  build()  method,10  which  is  often  necessary\n",
            "to  create  some  of  the  weights.  For  example,  we  need  to  know  the  number  of\n",
            "neurons  in  the  previous  layer  in  order  to  create  the  connection  weights  matrix\n",
            "(i.e.,  the  \"kernel\"):  this  corresponds  to  the  size  of  the  last  dimension  of  the\n",
            "inputs.  At  the  end  of  the  build()  method  (and  only  at  the  end),  you  must  call\n",
            "\n",
            "10 The Keras API calls this argument input_shape, but since it also includes the batch dimension, I prefer to call\n",
            "\n",
            "it batch_input_shape.\n",
            "\n",
            "420 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fthe  parent’s  build()  method:  this  tells  Keras  that  the  layer  is  built  (it  just  sets\n",
            "self.built = True).\n",
            "\n",
            "• The  call()  method  performs  the  desired  operations.  In  this  case,  we  compute\n",
            "•\n",
            "the matrix multiplication of the inputs  X and the layer’s kernel, we add the bias\n",
            "vector,  and  we  apply  the  activation  function  to  the  result,  and  this  gives  us  the\n",
            "output of the layer.\n",
            "\n",
            "• The  get_config()  method  is  just  like  in  the  previous  custom  classes.  Note\n",
            "•\n",
            "that  we  save  the  activation  function’s  full  configuration  by  calling  tf.keras.\n",
            "activations.serialize().\n",
            "\n",
            "You can now use a MyDense layer just like any other layer!\n",
            "\n",
            "Keras automatically infers the output shape, except when the layer\n",
            "is  dynamic  (as  you  will  see  shortly).  In  this  (rare)  case,  you  need\n",
            "to  implement  the  compute_output_shape()  method,  which  must\n",
            "return a TensorShape object.\n",
            "\n",
            "To create a layer with multiple inputs (e.g., Concatenate), the argument to the call()\n",
            "method  should  be  a  tuple  containing  all  the  inputs.  To  create  a  layer  with  multiple\n",
            "outputs,  the  call()  method  should  return  the  list  of  outputs.  For  example,  the\n",
            "following toy layer takes two inputs and returns three outputs:\n",
            "\n",
            "class MyMultiLayer(tf.keras.layers.Layer):\n",
            "    def call(self, X):\n",
            "        X1, X2 = X\n",
            "        return X1 + X2, X1 * X2, X1 / X2\n",
            "\n",
            "This  layer  may  now  be  used  like  any  other  layer,  but  of  course  only  using  the\n",
            "functional  and  subclassing  APIs,  not  the  sequential  API  (which  only  accepts  layers\n",
            "with one input and one output).\n",
            "\n",
            "If  your  layer  needs  to  have  a  different  behavior  during  training  and  during  test‐\n",
            "ing  (e.g.,  if  it  uses  Dropout  or  BatchNormalization  layers),  then  you  must  add  a\n",
            "training argument to the call() method and use this argument to decide what to\n",
            "do.  For  example,  let’s  create  a  layer  that  adds  Gaussian  noise  during  training  (for\n",
            "regularization) but does nothing during testing (Keras has a layer that does the same\n",
            "thing, tf.keras.layers.GaussianNoise):\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "421\n",
            "\n",
            "\fclass MyGaussianNoise(tf.keras.layers.Layer):\n",
            "    def __init__(self, stddev, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.stddev = stddev\n",
            "\n",
            "    def call(self, X, training=False):\n",
            "        if training:\n",
            "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
            "            return X + noise\n",
            "        else:\n",
            "            return X\n",
            "\n",
            "With  that,  you  can  now  build  any  custom  layer  you  need!  Now  let’s  look  at  how  to\n",
            "create custom models.\n",
            "\n",
            "Custom Models\n",
            "We  already  looked  at  creating  custom  model  classes  in  Chapter  10,  when  we  dis‐\n",
            "cussed the subclassing API.11 It’s straightforward: subclass the tf.keras.Model class,\n",
            "create layers and variables in the constructor, and implement the call() method to\n",
            "do whatever you want the model to do. For example, suppose we want to build the\n",
            "model represented in Figure 12-3.\n",
            "\n",
            "Figure 12-3. Custom model example: an arbitrary model with a custom ResidualBlock\n",
            "layer containing a skip connection\n",
            "\n",
            "The inputs go through a first dense layer, then through a residual block composed of\n",
            "two dense layers and an addition operation (as you will see in Chapter 14, a residual\n",
            "\n",
            "11 The name “subclassing API” in Keras usually refers only to the creation of custom models by subclassing,\n",
            "\n",
            "although many other things can be created by subclassing, as you’ve seen in this chapter.\n",
            "\n",
            "422 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fblock adds its inputs to its outputs), then through this same residual block three more\n",
            "times, then through a second residual block, and the final result goes through a dense\n",
            "output layer. Don’t worry if this model does not make much sense; it’s just an example\n",
            "to illustrate the fact that you can easily build any kind of model you want, even one\n",
            "that contains loops and skip connections. To implement this model, it is best to first\n",
            "create a ResidualBlock layer, since we are going to create a couple of identical blocks\n",
            "(and we might want to reuse it in another model):\n",
            "\n",
            "class ResidualBlock(tf.keras.layers.Layer):\n",
            "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n",
            "                                             kernel_initializer=\"he_normal\")\n",
            "                       for _ in range(n_layers)]\n",
            "\n",
            "    def call(self, inputs):\n",
            "        Z = inputs\n",
            "        for layer in self.hidden:\n",
            "            Z = layer(Z)\n",
            "        return inputs + Z\n",
            "\n",
            "This layer is a bit special since it contains other layers. This is handled transparently\n",
            "by Keras: it automatically detects that the hidden attribute contains trackable objects\n",
            "(layers  in  this  case),  so  their  variables  are  automatically  added  to  this  layer’s  list  of\n",
            "variables. The rest of this class is self-explanatory. Next, let’s use the subclassing API\n",
            "to define the model itself:\n",
            "\n",
            "class ResidualRegressor(tf.keras.Model):\n",
            "    def __init__(self, output_dim, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n",
            "                                             kernel_initializer=\"he_normal\")\n",
            "        self.block1 = ResidualBlock(2, 30)\n",
            "        self.block2 = ResidualBlock(2, 30)\n",
            "        self.out = tf.keras.layers.Dense(output_dim)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        Z = self.hidden1(inputs)\n",
            "        for _ in range(1 + 3):\n",
            "            Z = self.block1(Z)\n",
            "        Z = self.block2(Z)\n",
            "        return self.out(Z)\n",
            "\n",
            "We  create  the  layers  in  the  constructor  and  use  them  in  the  call()  method.  This\n",
            "model can then be used like any other model (compile it, fit it, evaluate it, and use it\n",
            "to make predictions). If you also want to be able to save the model using the save()\n",
            "method  and  load  it  using  the  tf.keras.models.load_model()  function,  you  must\n",
            "implement the get_config() method (as we did earlier) in both the ResidualBlock\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "423\n",
            "\n",
            "\fclass  and  the  ResidualRegressor  class.  Alternatively,  you  can  save  and  load  the\n",
            "weights using the save_weights() and load_weights() methods.\n",
            "\n",
            "The Model class is a subclass of the Layer class, so models can be defined and used\n",
            "exactly  like  layers.  But  a  model  has  some  extra  functionalities,  including  of  course\n",
            "its compile(), fit(), evaluate(), and predict() methods (and a few variants), plus\n",
            "the  get_layer()  method  (which  can  return  any  of  the  model’s  layers  by  name  or\n",
            "by index) and the save() method (and support for tf.keras.models.load_model()\n",
            "and tf.keras.models.clone_model()).\n",
            "\n",
            "If  models  provide  more  functionality  than  layers,  why  not  just\n",
            "define  every  layer  as  a  model?  Well,  technically  you  could,  but  it\n",
            "is  usually  cleaner  to  distinguish  the  internal  components  of  your\n",
            "model  (i.e.,  layers  or  reusable  blocks  of  layers)  from  the  model\n",
            "itself (i.e., the object you will train). The former should subclass the\n",
            "Layer class, while the latter should subclass the Model class.\n",
            "\n",
            "With that, you can naturally and concisely build almost any model that you find in\n",
            "a paper, using the sequential API, the functional API, the subclassing API, or even a\n",
            "mix of these. “Almost” any model? Yes, there are still a few things that we need to look\n",
            "at: first, how to define losses or metrics based on model internals, and second, how to\n",
            "build a custom training loop.\n",
            "\n",
            "Losses and Metrics Based on Model Internals\n",
            "The custom losses and metrics we defined earlier were all based on the labels and the\n",
            "predictions  (and  optionally  sample  weights).  There  will  be  times  when  you  want  to\n",
            "define losses based on other parts of your model, such as the weights or activations of\n",
            "its hidden layers. This may be useful for regularization purposes or to monitor some\n",
            "internal aspect of your model.\n",
            "\n",
            "To  define  a  custom  loss  based  on  model  internals,  compute  it  based  on  any  part  of\n",
            "the  model  you  want,  then  pass  the  result  to  the  add_loss()  method.  For  example,\n",
            "let’s build a custom regression MLP model composed of a stack of five hidden layers\n",
            "plus an output layer. This custom model will also have an auxiliary output on top of\n",
            "the  upper  hidden  layer.  The  loss  associated  with  this  auxiliary  output  will  be  called\n",
            "the reconstruction loss (see Chapter 17): it is the mean squared difference between the\n",
            "reconstruction  and  the  inputs.  By  adding  this  reconstruction  loss  to  the  main  loss,\n",
            "we  will  encourage  the  model  to  preserve  as  much  information  as  possible  through\n",
            "the hidden layers—even information that is not directly useful for the regression task\n",
            "itself. In practice, this loss sometimes improves generalization (it is a regularization\n",
            "loss).  It  is  also  possible  to  add  a  custom  metric  using  the  model’s  add_metric()\n",
            "\n",
            "424 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fmethod.  Here  is  the  code  for  this  custom  model  with  a  custom  reconstruction  loss\n",
            "and a corresponding metric:\n",
            "\n",
            "class ReconstructingRegressor(tf.keras.Model):\n",
            "    def __init__(self, output_dim, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n",
            "                                             kernel_initializer=\"he_normal\")\n",
            "                       for _ in range(5)]\n",
            "        self.out = tf.keras.layers.Dense(output_dim)\n",
            "        self.reconstruction_mean = tf.keras.metrics.Mean(\n",
            "            name=\"reconstruction_error\")\n",
            "\n",
            "    def build(self, batch_input_shape):\n",
            "        n_inputs = batch_input_shape[-1]\n",
            "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        Z = inputs\n",
            "        for layer in self.hidden:\n",
            "            Z = layer(Z)\n",
            "        reconstruction = self.reconstruct(Z)\n",
            "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
            "        self.add_loss(0.05 * recon_loss)\n",
            "        if training:\n",
            "            result = self.reconstruction_mean(recon_loss)\n",
            "            self.add_metric(result)\n",
            "        return self.out(Z)\n",
            "\n",
            "Let’s go through this code:\n",
            "\n",
            "• The  constructor  creates  the  DNN  with  five  dense  hidden  layers  and  one  dense\n",
            "•\n",
            "output layer. We also create a Mean streaming metric to keep track of the recon‐\n",
            "struction error during training.\n",
            "\n",
            "• The build() method creates an extra dense layer that will be used to reconstruct\n",
            "•\n",
            "the  inputs  of  the  model.  It  must  be  created  here  because  its  number  of  units\n",
            "must be equal to the number of inputs, and this number is unknown before the\n",
            "build() method is called.12\n",
            "\n",
            "• The  call()  method  processes  the  inputs  through  all  five  hidden  layers,  then\n",
            "•\n",
            "passes  the  result  through  the  reconstruction  layer,  which  produces  the  recon‐\n",
            "struction.\n",
            "\n",
            "• Then  the  call()  method  computes  the  reconstruction  loss  (the  mean  squared\n",
            "•\n",
            "difference between the reconstruction and the inputs), and adds it to the model’s\n",
            "\n",
            "12 Due to TensorFlow issue #46858, the call to super().build() may fail in this case, unless the issue was fixed\n",
            "\n",
            "by the time you read this. If not, you need to replace this line with self.built = True.\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "425\n",
            "\n",
            "\flist  of  losses  using  the  add_loss()  method.13  Notice  that  we  scale  down  the\n",
            "reconstruction  loss  by  multiplying  it  by  0.05  (this  is  a  hyperparameter  you  can\n",
            "tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
            "• Next, during training only, the call() method updates the reconstruction metric\n",
            "•\n",
            "and adds it to the model so it can be displayed. This code example can actually be\n",
            "simplified by calling self.add_metric(recon_loss) instead: Keras will automat‐\n",
            "ically track the mean for you.\n",
            "\n",
            "• Finally, the call() method passes the output of the hidden layers to the output\n",
            "•\n",
            "\n",
            "layer and returns its output.\n",
            "\n",
            "Both the total loss and the reconstruction loss will go down during training:\n",
            "\n",
            "Epoch 1/5\n",
            "363/363 [========] - 1s 820us/step - loss: 0.7640 - reconstruction_error: 1.2728\n",
            "Epoch 2/5\n",
            "363/363 [========] - 0s 809us/step - loss: 0.4584 - reconstruction_error: 0.6340\n",
            "[...]\n",
            "\n",
            "In  most  cases,  everything  we  have  discussed  so  far  will  be  sufficient  to  implement\n",
            "whatever  model  you  want  to  build,  even  with  complex  architectures,  losses,  and\n",
            "metrics.  However,  for  some  architectures,  such  as  GANs  (see  Chapter  17),  you  will\n",
            "have to customize the training loop itself. Before we get there, we must look at how to\n",
            "compute gradients automatically in TensorFlow.\n",
            "\n",
            "Computing Gradients Using Autodiff\n",
            "To  understand  how  to  use  autodiff  (see  Chapter  10  and  Appendix  B)  to  compute\n",
            "gradients automatically, let’s consider a simple toy function:\n",
            "\n",
            "def f(w1, w2):\n",
            "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
            "\n",
            "If  you  know  calculus,  you  can  analytically  find  that  the  partial  derivative  of  this\n",
            "function  with  regard  to  w1  is  6  *  w1  +  2  *  w2.  You  can  also  find  that  its  partial\n",
            "derivative with regard to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3),\n",
            "these partial derivatives are equal to 36 and 10, respectively, so the gradient vector at\n",
            "this point is (36, 10). But if this were a neural network, the function would be much\n",
            "more complex, typically with tens of thousands of parameters, and finding the partial\n",
            "derivatives  analytically  by  hand  would  be  a  virtually  impossible  task.  One  solution\n",
            "could be to compute an approximation of each partial derivative by measuring how\n",
            "much the function’s output changes when you tweak the corresponding parameter by\n",
            "a tiny amount:\n",
            "\n",
            "13 You can also call add_loss() on any layer inside the model, as the model recursively gathers losses from all of\n",
            "\n",
            "its layers.\n",
            "\n",
            "426 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\f>>> w1, w2 = 5, 3\n",
            ">>> eps = 1e-6\n",
            ">>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n",
            "36.000003007075065\n",
            ">>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n",
            "10.000000003174137\n",
            "\n",
            "Looks about right! This works rather well and is easy to implement, but it is just an\n",
            "approximation, and importantly you need to call f() at least once per parameter (not\n",
            "twice, since we could compute f(w1, w2) just once). Having to call f() at least once\n",
            "per parameter makes this approach intractable for large neural networks. So instead,\n",
            "we should use reverse-mode autodiff. TensorFlow makes this pretty simple:\n",
            "\n",
            "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
            "with tf.GradientTape() as tape:\n",
            "    z = f(w1, w2)\n",
            "\n",
            "gradients = tape.gradient(z, [w1, w2])\n",
            "\n",
            "We  first  define  two  variables  w1  and  w2,  then  we  create  a  tf.GradientTape  context\n",
            "that will automatically record every operation that involves a variable, and finally we\n",
            "ask  this  tape  to  compute  the  gradients  of  the  result  z  with  regard  to  both  variables\n",
            "[w1, w2]. Let’s take a look at the gradients that TensorFlow computed:\n",
            "\n",
            ">>> gradients\n",
            "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
            " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n",
            "\n",
            "Perfect! Not only is the result accurate (the precision is only limited by the floating-\n",
            "point  errors),  but  the  gradient()  method  only  goes  through  the  recorded  compu‐\n",
            "tations  once  (in  reverse  order),  no  matter  how  many  variables  there  are,  so  it  is\n",
            "incredibly efficient. It’s like magic!\n",
            "\n",
            "In  order  to  save  memory,  only  put  the  strict  minimum  inside  the\n",
            "tf.GradientTape() block. Alternatively, pause recording by creat‐\n",
            "ing a with tape.stop_recording() block inside the tf.Gradient\n",
            "Tape() block.\n",
            "\n",
            "The tape is automatically erased immediately after you call its gradient() method, so\n",
            "you will get an exception if you try to call gradient() twice:\n",
            "\n",
            "with tf.GradientTape() as tape:\n",
            "    z = f(w1, w2)\n",
            "\n",
            "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
            "dz_dw2 = tape.gradient(z, w2)  # raises a RuntimeError!\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "427\n",
            "\n",
            "\fIf  you  need  to  call  gradient()  more  than  once,  you  must  make  the  tape  persistent\n",
            "and delete it each time you are done with it to free resources:14\n",
            "\n",
            "with tf.GradientTape(persistent=True) as tape:\n",
            "    z = f(w1, w2)\n",
            "\n",
            "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
            "dz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!\n",
            "del tape\n",
            "\n",
            "By  default,  the  tape  will  only  track  operations  involving  variables,  so  if  you  try  to\n",
            "compute  the  gradient  of  z  with  regard  to  anything  other  than  a  variable,  the  result\n",
            "will be None:\n",
            "\n",
            "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
            "with tf.GradientTape() as tape:\n",
            "    z = f(c1, c2)\n",
            "\n",
            "gradients = tape.gradient(z, [c1, c2])  # returns [None, None]\n",
            "\n",
            "However,  you  can  force  the  tape  to  watch  any  tensors  you  like,  to  record  every\n",
            "operation  that  involves  them.  You  can  then  compute  gradients  with  regard  to  these\n",
            "tensors, as if they were variables:\n",
            "\n",
            "with tf.GradientTape() as tape:\n",
            "    tape.watch(c1)\n",
            "    tape.watch(c2)\n",
            "    z = f(c1, c2)\n",
            "\n",
            "gradients = tape.gradient(z, [c1, c2])  # returns [tensor 36., tensor 10.]\n",
            "\n",
            "This can be useful in some cases, like if you want to implement a regularization loss\n",
            "that  penalizes  activations  that  vary  a  lot  when  the  inputs  vary  little:  the  loss  will  be\n",
            "based on the gradient of the activations with regard to the inputs. Since the inputs are\n",
            "not variables, you’ll need to tell the tape to watch them.\n",
            "\n",
            "Most  of  the  time  a  gradient  tape  is  used  to  compute  the  gradients  of  a  single  value\n",
            "(usually the loss) with regard to a set of values (usually the model parameters). This\n",
            "is  where  reverse-mode  autodiff  shines,  as  it  just  needs  to  do  one  forward  pass  and\n",
            "one reverse pass to get all the gradients at once. If you try to compute the gradients\n",
            "of  a  vector,  for  example  a  vector  containing  multiple  losses,  then  TensorFlow  will\n",
            "compute the gradients of the vector’s sum. So if you ever need to get the individual\n",
            "gradients  (e.g.,  the  gradients  of  each  loss  with  regard  to  the  model  parameters),\n",
            "you  must  call  the  tape’s  jacobian()  method:  it  will  perform  reverse-mode  autodiff\n",
            "once  for  each  loss  in  the  vector  (all  in  parallel  by  default).  It  is  even  possible  to\n",
            "\n",
            "14 If the tape goes out of scope, for example when the function that used it returns, Python’s garbage collector\n",
            "\n",
            "will delete it for you.\n",
            "\n",
            "428 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fcompute  second-order  partial  derivatives  (the  Hessians,  i.e.,  the  partial  derivatives\n",
            "of  the  partial  derivatives),  but  this  is  rarely  needed  in  practice  (see  the  “Computing\n",
            "Gradients Using Autodiff ” section of this chapter’s notebook for an example).\n",
            "\n",
            "In some cases you may want to stop gradients from backpropagating through some\n",
            "part of your neural network. To do this, you must use the tf.stop_gradient() func‐\n",
            "tion.  The  function  returns  its  inputs  during  the  forward  pass  (like  tf.identity()),\n",
            "but it does not let gradients through during backpropagation (it acts like a constant):\n",
            "\n",
            "def f(w1, w2):\n",
            "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
            "\n",
            "with tf.GradientTape() as tape:\n",
            "    z = f(w1, w2)  # the forward pass is not affected by stop_gradient()\n",
            "\n",
            "gradients = tape.gradient(z, [w1, w2])  # returns [tensor 30., None]\n",
            "\n",
            "Finally, you may occasionally run into some numerical issues when computing gradi‐\n",
            "ents.  For  example,  if  you  compute  the  gradients  of  the  square  root  function  at  x  =\n",
            "10–50, the result will be infinite. In reality, the slope at that point is not infinite, but it’s\n",
            "more than 32-bit floats can handle:\n",
            "\n",
            ">>> x = tf.Variable(1e-50)\n",
            ">>> with tf.GradientTape() as tape:\n",
            "...     z = tf.sqrt(x)\n",
            "...\n",
            ">>> tape.gradient(z, [x])\n",
            "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]\n",
            "\n",
            "To  solve  this,  it’s  often  a  good  idea  to  add  a  tiny  value  to  x  (such  as  10–6)  when\n",
            "computing its square root.\n",
            "\n",
            "The exponential function is also a frequent source of headaches, as it grows extremely\n",
            "fast. For example, the way my_softplus() was defined earlier is not numerically sta‐\n",
            "ble. If you compute my_softplus(100.0), you will get infinity rather than the correct\n",
            "result  (about  100).  But  it’s  possible  to  rewrite  the  function  to  make  it  numerically\n",
            "stable:  the  softplus  function  is  defined  as  log(1  +  exp(z)),  which  is  also  equal  to\n",
            "log(1 + exp(–|z|)) + max(z, 0) (see the notebook for the mathematical proof) and the\n",
            "advantage of this second form is that the exponential term cannot explode. So, here’s\n",
            "a better implementation of the my_softplus() function:\n",
            "\n",
            "def my_softplus(z):\n",
            "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
            "\n",
            "In some rare cases, a numerically stable function may still have numerically unstable\n",
            "gradients.  In  such  cases,  you  will  have  to  tell  TensorFlow  which  equation  to  use\n",
            "for  the  gradients,  rather  than  letting  it  use  autodiff.  For  this,  you  must  use  the\n",
            "@tf.custom_gradient  decorator  when  defining  the  function,  and  return  both  the\n",
            "function’s usual result and a function that computes the gradients. For example, let’s\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "429\n",
            "\n",
            "\fupdate  the  my_softplus()  function  to  also  return  a  numerically  stable  gradients\n",
            "function:\n",
            "\n",
            "@tf.custom_gradient\n",
            "def my_softplus(z):\n",
            "    def my_softplus_gradients(grads):  # grads = backprop'ed from upper layers\n",
            "        return grads * (1 - 1 / (1 + tf.exp(z)))  # stable grads of softplus\n",
            "\n",
            "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
            "    return result, my_softplus_gradients\n",
            "\n",
            "If  you  know  differential  calculus  (see  the  tutorial  notebook  on  this  topic),  you  can\n",
            "find that the derivative of log(1 + exp(z)) is exp(z) / (1 + exp(z)). But this form is not\n",
            "stable: for large values of z, it ends up computing infinity divided by infinity, which\n",
            "returns  NaN.  However,  with  a  bit  of  algebraic  manipulation,  you  can  show  that  it’s\n",
            "also  equal  to  1  –  1  /  (1  +  exp(z)),  which  is  stable.  The  my_softplus_gradients()\n",
            "function  uses  this  equation  to  compute  the  gradients.  Note  that  this  function  will\n",
            "receive as input the gradients that were backpropagated so far, down to the my_soft\n",
            "plus()  function,  and  according  to  the  chain  rule  we  must  multiply  them  with  this\n",
            "function’s gradients.\n",
            "\n",
            "Now  when  we  compute  the  gradients  of  the  my_softplus()  function,  we  get  the\n",
            "proper result, even for large input values.\n",
            "\n",
            "Congratulations!  You  can  now  compute  the  gradients  of  any  function  (provided  it\n",
            "is differentiable at the point where you compute it), even blocking backpropagation\n",
            "when needed, and write your own gradient functions! This is probably more flexibil‐\n",
            "ity than you will ever need, even if you build your own custom training loops. You’ll\n",
            "see how to do that next.\n",
            "\n",
            "Custom Training Loops\n",
            "In some cases, the fit() method may not be flexible enough for what you need to do.\n",
            "For example, the Wide & Deep paper we discussed in Chapter 10 uses two different\n",
            "optimizers:  one  for  the  wide  path  and  the  other  for  the  deep  path.  Since  the  fit()\n",
            "method only uses one optimizer (the one that we specify when compiling the model),\n",
            "implementing this paper requires writing your own custom loop.\n",
            "\n",
            "You may also like to write custom training loops simply to feel more confident that\n",
            "they  do  precisely  what  you  intend  them  to  do  (perhaps  you  are  unsure  about  some\n",
            "details of the fit() method). It can sometimes feel safer to make everything explicit.\n",
            "However, remember that writing a custom training loop will make your code longer,\n",
            "more error-prone, and harder to maintain.\n",
            "\n",
            "430 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fUnless  you’re  learning  or  you  really  need  the  extra  flexibility,  you\n",
            "should  prefer  using  the  fit()  method  rather  than  implementing\n",
            "your own training loop, especially if you work in a team.\n",
            "\n",
            "First, let’s build a simple model. There’s no need to compile it, since we will handle the\n",
            "training loop manually:\n",
            "\n",
            "l2_reg = tf.keras.regularizers.l2(0.05)\n",
            "model = tf.keras.models.Sequential([\n",
            "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
            "                          kernel_regularizer=l2_reg),\n",
            "    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
            "])\n",
            "\n",
            "Next, let’s create a tiny function that will randomly sample a batch of instances from\n",
            "the training set (in Chapter 13 we will discuss the tf.data API, which offers a much\n",
            "better alternative):\n",
            "\n",
            "def random_batch(X, y, batch_size=32):\n",
            "    idx = np.random.randint(len(X), size=batch_size)\n",
            "    return X[idx], y[idx]\n",
            "\n",
            "Let’s also define a function that will display the training status, including the number\n",
            "of steps, the total number of steps, the mean loss since the start of the epoch (we will\n",
            "use the Mean metric to compute it), and other metrics:\n",
            "\n",
            "def print_status_bar(step, total, loss, metrics=None):\n",
            "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
            "                          for m in [loss] + (metrics or [])])\n",
            "    end = \"\" if step < total else \"\\n\"\n",
            "    print(f\"\\r{step}/{total} - \" + metrics, end=end)\n",
            "\n",
            "This  code  is  self-explanatory,  unless  you  are  unfamiliar  with  Python  string  format‐\n",
            "ting: {m.result():.4f} will format the metric’s result as a float with four digits after\n",
            "the decimal point, and using \\r (carriage return) along with end=\"\" ensures that the\n",
            "status bar always gets printed on the same line.\n",
            "\n",
            "With that, let’s get down to business! First, we need to define some hyperparameters\n",
            "and  choose  the  optimizer,  the  loss  function,  and  the  metrics  (just  the  MAE  in  this\n",
            "example):\n",
            "\n",
            "n_epochs = 5\n",
            "batch_size = 32\n",
            "n_steps = len(X_train) // batch_size\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
            "loss_fn = tf.keras.losses.mean_squared_error\n",
            "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
            "metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
            "\n",
            "And now we are ready to build the custom loop!\n",
            "\n",
            "Customizing Models and Training Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "431\n",
            "\n",
            "\ffor epoch in range(1, n_epochs + 1):\n",
            "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
            "    for step in range(1, n_steps + 1):\n",
            "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = model(X_batch, training=True)\n",
            "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
            "            loss = tf.add_n([main_loss] + model.losses)\n",
            "\n",
            "        gradients = tape.gradient(loss, model.trainable_variables)\n",
            "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
            "        mean_loss(loss)\n",
            "        for metric in metrics:\n",
            "            metric(y_batch, y_pred)\n",
            "\n",
            "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
            "\n",
            "    for metric in [mean_loss] + metrics:\n",
            "        metric.reset_states()\n",
            "\n",
            "There’s a lot going on in this code, so let’s walk through it:\n",
            "\n",
            "•\n",
            "• We create two nested loops: one for the epochs, the other for the batches within\n",
            "\n",
            "an epoch.\n",
            "\n",
            "• Then we sample a random batch from the training set.\n",
            "•\n",
            "• Inside the tf.GradientTape() block, we make a prediction for one batch, using\n",
            "•\n",
            "the  model  as  a  function,  and  we  compute  the  loss:  it  is  equal  to  the  main  loss\n",
            "plus  the  other  losses  (in  this  model,  there  is  one  regularization  loss  per  layer).\n",
            "Since  the  mean_squared_error()  function  returns  one  loss  per  instance,  we\n",
            "compute  the  mean  over  the  batch  using  tf.reduce_mean()  (if  you  wanted  to\n",
            "apply  different  weights  to  each  instance,  this  is  where  you  would  do  it).  The\n",
            "regularization losses are already reduced to a single scalar each, so we just need\n",
            "to sum them (using tf.add_n(), which sums multiple tensors of the same shape\n",
            "and data type).\n",
            "\n",
            "• Next,  we  ask  the  tape  to  compute  the  gradients  of  the  loss  with  regard  to  each\n",
            "•\n",
            "trainable  variable—not  all  variables!—and  we  apply  them  to  the  optimizer  to\n",
            "perform a gradient descent step.\n",
            "\n",
            "• Then we update the mean loss and the metrics (over the current epoch), and we\n",
            "•\n",
            "\n",
            "display the status bar.\n",
            "\n",
            "•\n",
            "• At the end of each epoch, we reset the states of the mean loss and the metrics.\n",
            "\n",
            "If you want to apply gradient clipping (see Chapter 11), set the optimizer’s clipnorm\n",
            "or clipvalue hyperparameter. If you want to apply any other transformation to the\n",
            "gradients,  simply  do  so  before  calling  the  apply_gradients()  method.  And  if  you\n",
            "want  to  add  weight  constraints  to  your  model  (e.g.,  by  setting  kernel_constraint\n",
            "\n",
            "432 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\for  bias_constraint when creating a layer), you should update the training loop to\n",
            "apply these constraints just after apply_gradients(), like so:\n",
            "\n",
            "for variable in model.variables:\n",
            "    if variable.constraint is not None:\n",
            "        variable.assign(variable.constraint(variable))\n",
            "\n",
            "Don’t  forget  to  set  training=True  when  calling  the  model  in\n",
            "the  training  loop,  especially  if  your  model  behaves  differently\n",
            "during  training  and  testing  (e.g.,  if  it  uses  BatchNormalization\n",
            "or  Dropout).  If  it’s  a  custom  model,  make  sure  to  propagate  the\n",
            "training argument to the layers that your model calls.\n",
            "\n",
            "As you can see, there are quite a lot of things you need to get right, and it’s easy to\n",
            "make a mistake. But on the bright side, you get full control.\n",
            "\n",
            "Now that you know how to customize any part of your models15 and training algo‐\n",
            "rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\n",
            "can speed up your custom code considerably, and it will also make it portable to any\n",
            "platform supported by TensorFlow (see Chapter 19).\n",
            "\n",
            "TensorFlow Functions and Graphs\n",
            "Back in TensorFlow 1, graphs were unavoidable (as were the complexities that came\n",
            "with them) because they were a central part of TensorFlow’s API. Since TensorFlow 2\n",
            "(released in 2019), graphs are still there, but not as central, and they’re much (much!)\n",
            "simpler  to  use.  To  show  just  how  simple,  let’s  start  with  a  trivial  function  that\n",
            "computes the cube of its input:\n",
            "\n",
            "def cube(x):\n",
            "    return x ** 3\n",
            "\n",
            "We can obviously call this function with a Python value, such as an int or a float, or\n",
            "we can call it with a tensor:\n",
            "\n",
            ">>> cube(2)\n",
            "8\n",
            ">>> cube(tf.constant(2.0))\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n",
            "\n",
            "Now,  let’s  use  tf.function()  to  convert  this  Python  function  to  a  TensorFlow\n",
            "function:\n",
            "\n",
            "15 With the exception of optimizers, as very few people ever customize these; see the “Custom Optimizers”\n",
            "\n",
            "section in the notebook for an example.\n",
            "\n",
            "TensorFlow Functions and Graphs \n",
            "\n",
            "| \n",
            "\n",
            "433\n",
            "\n",
            "\f>>> tf_cube = tf.function(cube)\n",
            ">>> tf_cube\n",
            "<tensorflow.python.eager.def_function.Function at 0x7fbfe0c54d50>\n",
            "\n",
            "This  TF  function  can  then  be  used  exactly  like  the  original  Python  function,  and  it\n",
            "will return the same result (but always as tensors):\n",
            "\n",
            ">>> tf_cube(2)\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=8>\n",
            ">>> tf_cube(tf.constant(2.0))\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n",
            "\n",
            "Under the hood, tf.function() analyzed the computations performed by the cube()\n",
            "function  and  generated  an  equivalent  computation  graph!  As  you  can  see,  it  was\n",
            "rather painless (we will look at how this works shortly). Alternatively, we could have\n",
            "used tf.function as a decorator; this is actually more common:\n",
            "\n",
            "@tf.function\n",
            "def tf_cube(x):\n",
            "    return x ** 3\n",
            "\n",
            "The original Python function is still available via the TF function’s python_function\n",
            "attribute, in case you ever need it:\n",
            "\n",
            ">>> tf_cube.python_function(2)\n",
            "8\n",
            "\n",
            "TensorFlow  optimizes  the  computation  graph,  pruning  unused  nodes,  simplifying\n",
            "expressions  (e.g.,  1  +  2  would  get  replaced  with  3),  and  more.  Once  the  optimized\n",
            "graph  is  ready,  the  TF  function  efficiently  executes  the  operations  in  the  graph,  in\n",
            "the  appropriate  order  (and  in  parallel  when  it  can).  As  a  result,  a  TF  function  will\n",
            "usually  run  much  faster  than  the  original  Python  function,  especially  if  it  performs\n",
            "complex computations.16 Most of the time you will not really need to know more than\n",
            "that: when you want to boost a Python function, just transform it into a TF function.\n",
            "That’s all!\n",
            "\n",
            "Moreover,  if  you  set  jit_compile=True  when  calling  tf.function(),  then  Tensor‐\n",
            "Flow will use accelerated linear algebra (XLA) to compile dedicated kernels for your\n",
            "graph,  often  fusing  multiple  operations.  For  example,  if  your  TF  function  calls\n",
            "tf.reduce_sum(a  *  b  +  c),  then  without  XLA  the  function  would  first  need  to\n",
            "compute  a  *  b  and  store  the  result  in  a  temporary  variable,  then  add  c  to  that\n",
            "variable, and lastly call tf.reduce_sum() on the result. With XLA, the whole compu‐\n",
            "tation gets compiled into a single kernel, which will compute tf.reduce_sum(a * b\n",
            "+ c) in one shot, without using any large temporary variable. Not only will this be\n",
            "much faster, it will also use dramatically less RAM.\n",
            "\n",
            "16 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\n",
            "\n",
            "tf_cube() actually runs much slower than cube().\n",
            "\n",
            "434 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fWhen you write a custom loss function, a custom metric, a custom layer, or any other\n",
            "custom  function  and  you  use  it  in  a  Keras  model  (as  we’ve  done  throughout  this\n",
            "chapter),  Keras  automatically  converts  your  function  into  a  TF  function—no  need\n",
            "to  use  tf.function().  So  most  of  the  time,  the  magic  is  100%  transparent.  And  if\n",
            "you want Keras to use XLA, you just need to set jit_compile=True when calling the\n",
            "compile() method. Easy!\n",
            "\n",
            "You  can  tell  Keras  not  to  convert  your  Python  functions  to  TF\n",
            "functions  by  setting  dynamic=True  when  creating  a  custom  layer\n",
            "or  a  custom  model.  Alternatively,  you  can  set  run_eagerly=True\n",
            "when calling the model’s compile() method.\n",
            "\n",
            "By  default,  a  TF  function  generates  a  new  graph  for  every  unique  set  of  input\n",
            "shapes  and  data  types  and  caches  it  for  subsequent  calls.  For  example,  if  you  call\n",
            "tf_cube(tf.constant(10)), a graph will be generated for int32 tensors of shape [].\n",
            "Then  if  you  call  tf_cube(tf.constant(20)),  the  same  graph  will  be  reused.  But\n",
            "if  you  then  call  tf_cube(tf.constant([10,  20])),  a  new  graph  will  be  generated\n",
            "for int32 tensors of shape [2]. This is how TF functions handle polymorphism (i.e.,\n",
            "varying argument types and shapes). However, this is only true for tensor arguments:\n",
            "if you pass numerical Python values to a TF function, a new graph will be generated\n",
            "for  every  distinct  value:  for  example,  calling  tf_cube(10)  and  tf_cube(20)  will\n",
            "generate two graphs.\n",
            "\n",
            "If  you  call  a  TF  function  many  times  with  different  numerical\n",
            "Python values, then many graphs will be generated, slowing down\n",
            "your program and using up a lot of RAM (you must delete the TF\n",
            "function to release it). Python values should be reserved for argu‐\n",
            "ments  that  will  have  few  unique  values,  such  as  hyperparameters\n",
            "like  the  number  of  neurons  per  layer.  This  allows  TensorFlow  to\n",
            "better optimize each variant of your model.\n",
            "\n",
            "AutoGraph and Tracing\n",
            "So how does TensorFlow generate graphs? It starts by analyzing the Python function’s\n",
            "source code to capture all the control flow statements, such as for loops, while loops,\n",
            "and  if  statements,  as  well  as  break,  continue,  and  return  statements.  This  first\n",
            "step  is  called  AutoGraph.  The  reason  TensorFlow  has  to  analyze  the  source  code  is\n",
            "that  Python  does  not  provide  any  other  way  to  capture  control  flow  statements:  it\n",
            "offers magic methods like __add__() and __mul__() to capture operators like + and\n",
            "*,  but  there  are  no  __while__()  or  __if__()  magic  methods.  After  analyzing  the\n",
            "function’s code, AutoGraph outputs an upgraded version of that function in which all\n",
            "the  control  flow  statements  are  replaced  by  the  appropriate  TensorFlow  operations,\n",
            "\n",
            "TensorFlow Functions and Graphs \n",
            "\n",
            "| \n",
            "\n",
            "435\n",
            "\n",
            "\fsuch  as  tf.while_loop()  for  loops  and  tf.cond()  for  if  statements.  For  example,\n",
            "in  Figure  12-4,  AutoGraph  analyzes  the  source  code  of  the  sum_squares()  Python\n",
            "function, and it generates the tf__sum_squares() function. In this function, the for\n",
            "loop is replaced by the definition of the loop_body() function (containing the body\n",
            "of the original for loop), followed by a call to the for_stmt() function. This call will\n",
            "build the appropriate tf.while_loop() operation in the computation graph.\n",
            "\n",
            "Figure 12-4. How TensorFlow generates graphs using AutoGraph and tracing\n",
            "\n",
            "Next, TensorFlow calls this “upgraded” function, but instead of passing the argument,\n",
            "it  passes  a  symbolic  tensor—a  tensor  without  any  actual  value,  only  a  name,  a  data\n",
            "type,  and  a  shape.  For  example,  if  you  call  sum_squares(tf.constant(10)),  then\n",
            "the tf__sum_squares() function will be called with a symbolic tensor of type int32\n",
            "and  shape  [].  The  function  will  run  in  graph  mode,  meaning  that  each  TensorFlow\n",
            "operation will add a node in the graph to represent itself and its output tensor(s) (as\n",
            "opposed to the regular mode, called eager execution, or eager mode). In graph mode,\n",
            "TF operations do not perform any computations. Graph mode was the default mode\n",
            "in TensorFlow 1. In Figure 12-4, you can see the tf__sum_squares() function being\n",
            "called with a symbolic tensor as its argument (in this case, an int32 tensor of shape [])\n",
            "and the final graph being generated during tracing. The nodes represent operations,\n",
            "\n",
            "436 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\fand  the  arrows  represent  tensors  (both  the  generated  function  and  the  graph  are\n",
            "simplified).\n",
            "\n",
            "In  order  to  view  the  generated  function’s  source  code,  you  can\n",
            "call tf.autograph.to_code(sum_squares.python_function). The\n",
            "code  is  not  meant  to  be  pretty,  but  it  can  sometimes  help  for\n",
            "debugging.\n",
            "\n",
            "TF Function Rules\n",
            "Most of the time, converting a Python function that performs TensorFlow operations\n",
            "into a TF function is trivial: decorate it with @tf.function or let Keras take care of it\n",
            "for you. However, there are a few rules to respect:\n",
            "\n",
            "• If  you  call  any  external  library,  including  NumPy  or  even  the  standard  library,\n",
            "•\n",
            "this  call  will  run  only  during  tracing;  it  will  not  be  part  of  the  graph.  Indeed,  a\n",
            "TensorFlow graph can only include TensorFlow constructs (tensors, operations,\n",
            "variables, datasets, and so on). So, make sure you use tf.reduce_sum() instead\n",
            "of  np.sum(),  tf.sort()  instead  of  the  built-in  sorted()  function,  and  so  on\n",
            "(unless  you  really  want  the  code  to  run  only  during  tracing).  This  has  a  few\n",
            "additional implications:\n",
            "— If  you  define  a  TF  function  f(x)  that  just  returns  np.random.rand(),  a\n",
            "—\n",
            "random  number  will  only  be  generated  when  the  function  is  traced,  so\n",
            "f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random\n",
            "number,  but  f(tf.constant([2.,  3.]))  will  return  a  different  one.  If  you\n",
            "replace np.random.rand() with tf.random.uniform([]), then a new random\n",
            "number will be generated upon every call, since the operation will be part of\n",
            "the graph.\n",
            "\n",
            "— If  your  non-TensorFlow  code  has  side  effects  (such  as  logging  something  or\n",
            "—\n",
            "updating a Python counter), then you should not expect those side effects to\n",
            "occur  every  time  you  call  the  TF  function,  as  they  will  only  occur  when  the\n",
            "function is traced.\n",
            "\n",
            "— You  can  wrap  arbitrary  Python  code  in  a  tf.py_function()  operation,  but\n",
            "—\n",
            "doing  so  will  hinder  performance,  as  TensorFlow  will  not  be  able  to  do  any\n",
            "graph  optimization  on  this  code.  It  will  also  reduce  portability,  as  the  graph\n",
            "will  only  run  on  platforms  where  Python  is  available  (and  where  the  right\n",
            "libraries are installed).\n",
            "\n",
            "• You  can  call  other  Python  functions  or  TF  functions,  but  they  should  follow\n",
            "•\n",
            "the  same  rules,  as  TensorFlow  will  capture  their  operations  in  the  computa‐\n",
            "tion  graph.  Note  that  these  other  functions  do  not  need  to  be  decorated  with\n",
            "@tf.function.\n",
            "\n",
            "TensorFlow Functions and Graphs \n",
            "\n",
            "| \n",
            "\n",
            "437\n",
            "\n",
            "\f• If  the  function  creates  a  TensorFlow  variable  (or  any  other  stateful  TensorFlow\n",
            "•\n",
            "object, such as a dataset or a queue), it must do so upon the very first call, and\n",
            "only  then,  or  else  you  will  get  an  exception.  It  is  usually  preferable  to  create\n",
            "variables  outside  of  the  TF  function  (e.g.,  in  the  build()  method  of  a  custom\n",
            "layer).  If  you  want  to  assign  a  new  value  to  the  variable,  make  sure  you  call  its\n",
            "assign() method instead of using the = operator.\n",
            "\n",
            "• The  source  code  of  your  Python  function  should  be  available  to  TensorFlow.  If\n",
            "•\n",
            "the  source  code  is  unavailable  (for  example,  if  you  define  your  function  in  the\n",
            "Python shell, which does not give access to the source code, or if you deploy only\n",
            "the compiled *.pyc Python files to production), then the graph generation process\n",
            "will fail or have limited functionality.\n",
            "\n",
            "• TensorFlow  will  only  capture  for  loops  that  iterate  over  a  tensor  or  a\n",
            "•\n",
            "tf.data.Dataset  (see  Chapter  13).  Therefore,  make  sure  you  use  for  i  in\n",
            "tf.range(x)  rather  than  for  i  in  range(x),  or  else  the  loop  will  not  be\n",
            "captured in the graph. Instead, it will run during tracing. (This may be what you\n",
            "want if the for loop is meant to build the graph; for example, to create each layer\n",
            "in a neural network.)\n",
            "\n",
            "•\n",
            "• As always, for performance reasons, you should prefer a vectorized implementa‐\n",
            "\n",
            "tion whenever you can, rather than using loops.\n",
            "\n",
            "It’s time to sum up! In this chapter we started with a brief overview of TensorFlow,\n",
            "then  we  looked  at  TensorFlow’s  low-level  API,  including  tensors,  operations,  vari‐\n",
            "ables, and special data structures. We then used these tools to customize almost every\n",
            "component  in  the  Keras  API.  Finally,  we  looked  at  how  TF  functions  can  boost\n",
            "performance, how graphs are generated using AutoGraph and tracing, and what rules\n",
            "to  follow  when  you  write  TF  functions  (if  you  would  like  to  open  the  black  box\n",
            "a  bit  further  and  explore  the  generated  graphs,  you  will  find  technical  details  in\n",
            "Appendix D).\n",
            "\n",
            "In the next chapter, we will look at how to efficiently load and preprocess data with\n",
            "TensorFlow.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. How  would  you  describe  TensorFlow  in  a  short  sentence?  What  are  its  main\n",
            "\n",
            "features? Can you name other popular deep learning libraries?\n",
            "\n",
            "2. Is TensorFlow a drop-in replacement for NumPy? What are the main differences\n",
            "2.\n",
            "\n",
            "between the two?\n",
            "\n",
            "3. Do  you  get  the  same  result  with  tf.range(10)  and  tf.constant(np.\n",
            "3.\n",
            "\n",
            "arange(10))?\n",
            "\n",
            "438 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 12: Custom Models and Training with TensorFlow\n",
            "\n",
            "\f4.\n",
            "4. Can you name six other data structures available in TensorFlow, beyond regular\n",
            "\n",
            "tensors?\n",
            "\n",
            "5.\n",
            "5. You can define a custom loss function by writing a function or by subclassing the\n",
            "\n",
            "tf.keras.losses.Loss class. When would you use each option?\n",
            "\n",
            "6.\n",
            "6. Similarly,  you  can  define  a  custom  metric  in  a  function  or  as  a  subclass  of\n",
            "\n",
            "tf.keras.metrics.Metric. When would you use each option?\n",
            "\n",
            "7.\n",
            "7. When should you create a custom layer versus a custom model?\n",
            "\n",
            "8.\n",
            "8. What are some use cases that require writing your own custom training loop?\n",
            "\n",
            "9.\n",
            "9. Can  custom  Keras  components  contain  arbitrary  Python  code,  or  must  they  be\n",
            "\n",
            "convertible to TF functions?\n",
            "\n",
            "10.\n",
            "10. What are the main rules to respect if you want a function to be convertible to a\n",
            "\n",
            "TF function?\n",
            "\n",
            "11.\n",
            "11. When would you need to create a dynamic Keras model? How do you do that?\n",
            "\n",
            "Why not make all your models dynamic?\n",
            "\n",
            "12.\n",
            "12. Implement  a  custom  layer  that  performs  layer  normalization  (we  will  use  this\n",
            "\n",
            "type of layer in Chapter 15):\n",
            "a. The  build()  method  should  define  two  trainable  weights  α  and  β,  both  of\n",
            "a.\n",
            "shape  input_shape[-1:]  and  data  type  tf.float32.  α  should  be  initialized\n",
            "with 1s, and β with 0s.\n",
            "\n",
            "b. The  call()  method  should  compute  the  mean  μ  and  standard  deviation  σ\n",
            "b.\n",
            "of  each  instance’s  features.  For  this,  you  can  use  tf.nn.moments(inputs,\n",
            "axes=-1,  keepdims=True),  which  returns  the  mean  μ  and  the  variance  σ2\n",
            "of all instances (compute the square root of the variance to get the standard\n",
            "deviation). Then the function should compute and return α ⊗ (X – μ)/(σ + ε)\n",
            "+ β, where ⊗ represents itemwise multiplication (*) and ε is a smoothing term\n",
            "(a small constant to avoid division by zero, e.g., 0.001).\n",
            "\n",
            "c.\n",
            "c. Ensure  that  your  custom  layer  produces  the  same  (or  very  nearly  the  same)\n",
            "\n",
            "output as the tf.keras.layers.LayerNormalization layer.\n",
            "\n",
            "13. Train a model using a custom training loop to tackle the Fashion MNIST dataset\n",
            "13.\n",
            "\n",
            "(see Chapter 10):\n",
            "\n",
            "a.\n",
            "a. Display the epoch, iteration, mean training loss, and mean accuracy over each\n",
            "epoch (updated at each iteration), as well as the validation loss and accuracy at\n",
            "the end of each epoch.\n",
            "\n",
            "b.\n",
            "b. Try  using  a  different  optimizer  with  a  different  learning  rate  for  the  upper\n",
            "\n",
            "layers and the lower layers.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "439\n",
            "\n",
            "\f\fCHAPTER 13\n",
            "Loading and Preprocessing\n",
            "Data with TensorFlow\n",
            "\n",
            "In  Chapter  2,  you  saw  that  loading  and  preprocessing  data  is  an  important  part  of\n",
            "any  machine  learning  project.  You  used  Pandas  to  load  and  explore  the  (modified)\n",
            "California housing dataset—which was stored in a CSV file—and you applied Scikit-\n",
            "Learn’s  transformers  for  preprocessing.  These  tools  are  quite  convenient,  and  you\n",
            "will probably be using them often, especially when exploring and experimenting with\n",
            "data.\n",
            "\n",
            "However, when training TensorFlow models on large datasets, you may prefer to use\n",
            "TensorFlow’s own data loading and preprocessing API, called tf.data. It is capable of\n",
            "loading  and  preprocessing  data  extremely  efficiently,  reading  from  multiple  files  in\n",
            "parallel using multithreading and queuing, shuffling and batching samples, and more.\n",
            "Plus, it can do all of this on the fly—it loads and preprocesses the next batch of data\n",
            "across multiple CPU cores, while your GPUs or TPUs are busy training the current\n",
            "batch of data.\n",
            "\n",
            "The  tf.data  API  lets  you  handle  datasets  that  don’t  fit  in  memory,  and  it  allows  you\n",
            "to  make  full  use  of  your  hardware  resources,  thereby  speeding  up  training.  Off  the\n",
            "shelf,  the  tf.data  API  can  read  from  text  files  (such  as  CSV  files),  binary  files  with\n",
            "fixed-size  records,  and  binary  files  that  use  TensorFlow’s  TFRecord  format,  which\n",
            "supports records of varying sizes.\n",
            "\n",
            "TFRecord is a flexible and efficient binary format usually containing protocol buffers\n",
            "(an  open  source  binary  format).  The  tf.data  API  also  has  support  for  reading  from\n",
            "SQL databases. Moreover, many open source extensions are available to read from all\n",
            "sorts of data sources, such as Google’s BigQuery service (see https://tensorflow.org/io).\n",
            "\n",
            "441\n",
            "\n",
            "\fKeras  also  comes  with  powerful  yet  easy-to-use  preprocessing  layers  that  can  be\n",
            "embedded  in  your  models:  this  way,  when  you  deploy  a  model  to  production,  it\n",
            "will  be  able  to  ingest  raw  data  directly,  without  you  having  to  add  any  additional\n",
            "preprocessing code. This eliminates the risk of mismatch between the preprocessing\n",
            "code  used  during  training  and  the  preprocessing  code  used  in  production,  which\n",
            "would  likely  cause  training/serving  skew.  And  if  you  deploy  your  model  in  multiple\n",
            "apps coded in different programming languages, you won’t have to reimplement the\n",
            "same preprocessing code multiple times, which also reduces the risk of mismatch.\n",
            "\n",
            "As you will see, both APIs can be used jointly—for example, to benefit from the effi‐\n",
            "cient data loading offered by tf.data and the convenience of the Keras preprocessing\n",
            "layers.\n",
            "\n",
            "In this chapter, we will first cover the tf.data API and the TFRecord format. Then we\n",
            "will explore the Keras preprocessing layers and how to use them with the tf.data API.\n",
            "Lastly, we will take a quick look at a few related libraries that you may find useful for\n",
            "loading  and  preprocessing  data,  such  as  TensorFlow  Datasets  and  TensorFlow  Hub.\n",
            "So, let’s get started!\n",
            "\n",
            "The tf.data API\n",
            "The  whole  tf.data  API  revolves  around  the  concept  of  a  tf.data.Dataset:  this\n",
            "represents a sequence of data items. Usually you will use datasets that gradually read\n",
            "data from disk, but for simplicity let’s create a dataset from a simple data tensor using\n",
            "tf.data.Dataset.from_tensor_slices():\n",
            "\n",
            ">>> import tensorflow as tf\n",
            ">>> X = tf.range(10)  # any data tensor\n",
            ">>> dataset = tf.data.Dataset.from_tensor_slices(X)\n",
            ">>> dataset\n",
            "<TensorSliceDataset shapes: (), types: tf.int32>\n",
            "\n",
            "The  from_tensor_slices()  function  takes  a  tensor  and  creates  a  tf.data.Dataset\n",
            "whose  elements  are  all  the  slices  of  X  along  the  first  dimension,  so  this  dataset\n",
            "contains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\n",
            "dataset  if  we  had  used  tf.data.Dataset.range(10)  (except  the  elements  would  be\n",
            "64-bit integers instead of 32-bit integers).\n",
            "\n",
            "You can simply iterate over a dataset’s items like this:\n",
            "\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "[...]\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n",
            "\n",
            "442 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fThe tf.data API is a streaming API: you can very efficiently iterate\n",
            "through a dataset’s items, but the API is not designed for indexing\n",
            "or slicing.\n",
            "\n",
            "A dataset may also contain tuples of tensors, or dictionaries of name/tensor pairs, or\n",
            "even nested tuples and dictionaries of tensors. When slicing a tuple, a dictionary, or\n",
            "a nested structure, the dataset will only slice the tensors it contains, while preserving\n",
            "the tuple/dictionary structure. For example:\n",
            "\n",
            ">>> X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n",
            ">>> dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "{'a': (<tf.Tensor: [...]=1>, <tf.Tensor: [...]=4>), 'b': <tf.Tensor: [...]=7>}\n",
            "{'a': (<tf.Tensor: [...]=2>, <tf.Tensor: [...]=5>), 'b': <tf.Tensor: [...]=8>}\n",
            "{'a': (<tf.Tensor: [...]=3>, <tf.Tensor: [...]=6>), 'b': <tf.Tensor: [...]=9>}\n",
            "\n",
            "Chaining Transformations\n",
            "Once  you  have  a  dataset,  you  can  apply  all  sorts  of  transformations  to  it  by  calling\n",
            "its  transformation  methods.  Each  method  returns  a  new  dataset,  so  you  can  chain\n",
            "transformations like this (this chain is illustrated in Figure 13-1):\n",
            "\n",
            ">>> dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
            ">>> dataset = dataset.repeat(3).batch(7)\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
            "\n",
            "In  this  example,  we  first  call  the  repeat()  method  on  the  original  dataset,  and  it\n",
            "returns  a  new  dataset  that  repeats  the  items  of  the  original  dataset  three  times.  Of\n",
            "course, this will not copy all the data in memory three times! If you call this method\n",
            "with no arguments, the new dataset will repeat the source dataset forever, so the code\n",
            "that iterates over the dataset will have to decide when to stop.\n",
            "\n",
            "Then we call the batch() method on this new dataset, and again this creates a new\n",
            "dataset.  This  one  will  group  the  items  of  the  previous  dataset  in  batches  of  seven\n",
            "items.\n",
            "\n",
            "The tf.data API \n",
            "\n",
            "| \n",
            "\n",
            "443\n",
            "\n",
            "\fFigure 13-1. Chaining dataset transformations\n",
            "\n",
            "Finally,  we  iterate  over  the  items  of  this  final  dataset.  The  batch()  method  had  to\n",
            "output  a  final  batch  of  size  two  instead  of  seven,  but  you  can  call  batch()  with\n",
            "drop_remainder=True  if  you  want  it  to  drop  this  final  batch,  such  that  all  batches\n",
            "have the exact same size.\n",
            "\n",
            "The  dataset  methods  do  not  modify  datasets—they  create  new\n",
            "ones. So make sure to keep a reference to these new datasets (e.g.,\n",
            "with dataset = ...), or else nothing will happen.\n",
            "\n",
            "You  can  also  transform  the  items  by  calling  the  map()  method.  For  example,  this\n",
            "creates a new dataset with all batches multiplied by two:\n",
            "\n",
            ">>> dataset = dataset.map(lambda x: x * 2)  # x is a batch\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "[...]\n",
            "\n",
            "This  map()  method  is  the  one  you  will  call  to  apply  any  preprocessing  to  your\n",
            "data.  Sometimes  this  will  include  computations  that  can  be  quite  intensive,  such  as\n",
            "reshaping or rotating an image, so you will usually want to spawn multiple threads to\n",
            "speed  things  up.  This  can  be  done  by  setting  the  num_parallel_calls  argument  to\n",
            "the  number  of  threads  to  run,  or  to  tf.data.AUTOTUNE.  Note  that  the  function  you\n",
            "pass to the map() method must be convertible to a TF function (see Chapter 12).\n",
            "\n",
            "It is also possible to simply filter the dataset using the filter() method. For example,\n",
            "this code creates a dataset that only contains the batchs whose sum is greater than 50:\n",
            "\n",
            "444 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\f>>> dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "\n",
            "You will often want to look at just a few items from a dataset. You can use the take()\n",
            "method for that:\n",
            "\n",
            ">>> for item in dataset.take(2):\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "\n",
            "Shuffling the Data\n",
            "As we discussed in Chapter 4, gradient descent works best when the instances in the\n",
            "training set are independent and identically distributed (IID). A simple way to ensure\n",
            "this  is  to  shuffle  the  instances,  using  the  shuffle()  method.  It  will  create  a  new\n",
            "dataset that will start by filling up a buffer with the first items of the source dataset.\n",
            "Then, whenever it is asked for an item, it will pull one out randomly from the buffer\n",
            "and replace it with a fresh one from the source dataset, until it has iterated entirely\n",
            "through the source dataset. At this point it will continue to pull out items randomly\n",
            "from the buffer until it is empty. You must specify the buffer size, and it is important\n",
            "to make it large enough, or else shuffling will not be very effective.1 Just don’t exceed\n",
            "the amount of RAM you have, though even if you have plenty of it, there’s no need\n",
            "to go beyond the dataset’s size. You can provide a random seed if you want the same\n",
            "random  order  every  time  you  run  your  program.  For  example,  the  following  code\n",
            "creates and displays a dataset containing the integers 0 to 9, repeated twice, shuffled\n",
            "using a buffer of size 4 and a random seed of 42, and batched with a batch size of 7:\n",
            "\n",
            ">>> dataset = tf.data.Dataset.range(10).repeat(2)\n",
            ">>> dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
            ">>> for item in dataset:\n",
            "...     print(item)\n",
            "...\n",
            "tf.Tensor([3 0 1 6 2 5 7], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 4 1 9 4 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 5 0 8 9 6], shape=(6,), dtype=int64)\n",
            "\n",
            "1 Imagine a sorted deck of cards on your left: suppose you just take the top three cards and shuffle them, then\n",
            "pick one randomly and put it to your right, keeping the other two in your hands. Take another card on your\n",
            "left, shuffle the three cards in your hands and pick one of them randomly, and put it on your right. When you\n",
            "are done going through all the cards like this, you will have a deck of cards on your right: do you think it will\n",
            "be perfectly shuffled?\n",
            "\n",
            "The tf.data API \n",
            "\n",
            "| \n",
            "\n",
            "445\n",
            "\n",
            "\fIf you call repeat() on a shuffled dataset, by default it will generate\n",
            "a new order at every iteration. This is generally a good idea, but if\n",
            "you prefer to reuse the same order at each iteration (e.g., for tests\n",
            "or  debugging),  you  can  set  reshuffle_each_iteration=False\n",
            "when calling shuffle().\n",
            "\n",
            "For a large dataset that does not fit in memory, this simple shuffling-buffer approach\n",
            "may  not  be  sufficient,  since  the  buffer  will  be  small  compared  to  the  dataset.  One\n",
            "solution is to shuffle the source data itself (for example, on Linux you can shuffle text\n",
            "files  using  the  shuf  command).  This  will  definitely  improve  shuffling  a  lot!  Even  if\n",
            "the source data is shuffled, you will usually want to shuffle it some more, or else the\n",
            "same order will be repeated at each epoch, and the model may end up being biased\n",
            "(e.g., due to some spurious patterns present by chance in the source data’s order). To\n",
            "shuffle the instances some more, a common approach is to split the source data into\n",
            "multiple files, then read them in a random order during training. However, instances\n",
            "located in the same file will still end up close to each other. To avoid this you can pick\n",
            "multiple  files  randomly  and  read  them  simultaneously,  interleaving  their  records.\n",
            "Then on top of that you can add a shuffling buffer using the  shuffle() method. If\n",
            "this sounds like a lot of work, don’t worry: the tf.data API makes all this possible in\n",
            "just a few lines of code. Let’s go over how you can do this.\n",
            "\n",
            "Interleaving Lines from Multiple Files\n",
            "First, suppose you’ve loaded the California housing dataset, shuffled it (unless it was\n",
            "already shuffled), and split it into a training set, a validation set, and a test set. Then\n",
            "you split each set into many CSV files that each look like this (each row contains eight\n",
            "input features plus the target median house value):\n",
            "\n",
            "MedInc,HouseAge,AveRooms,AveBedrms,Popul…,AveOccup,Lat…,Long…,MedianHouseValue\n",
            "3.5214,15.0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.687\n",
            "3.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621\n",
            "[...]\n",
            "\n",
            "Let’s  also  suppose  train_filepaths  contains  the  list  of  training  filepaths  (and  you\n",
            "also have valid_filepaths and test_filepaths):\n",
            "\n",
            ">>> train_filepaths\n",
            "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]\n",
            "\n",
            "Alternatively,  you  could  use  file  patterns;  for  example,  train_filepaths  =\n",
            "\"datasets/housing/my_train_*.csv\".  Now  let’s  create  a  dataset  containing  only\n",
            "these filepaths:\n",
            "\n",
            "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
            "\n",
            "446 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fBy default, the list_files() function returns a dataset that shuffles the filepaths. In\n",
            "general this is a good thing, but you can set shuffle=False if you do not want that\n",
            "for some reason.\n",
            "\n",
            "Next,  you  can  call  the  interleave()  method  to  read  from  five  files  at  a  time  and\n",
            "interleave their lines. You can also skip the first line of each file—which is the header\n",
            "row—using the skip() method):\n",
            "\n",
            "n_readers = 5\n",
            "dataset = filepath_dataset.interleave(\n",
            "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
            "    cycle_length=n_readers)\n",
            "\n",
            "The interleave() method will create a dataset that will pull five filepaths from the\n",
            "filepath_dataset, and for each one it will call the function you gave it (a lambda in\n",
            "this example) to create a new dataset (in this case a TextLineDataset). To be clear, at\n",
            "this stage there will be seven datasets in all: the filepath dataset, the interleave dataset,\n",
            "and the five TextLineDatasets created internally by the interleave dataset. When you\n",
            "iterate over the interleave dataset, it will cycle through these five TextLineDatasets,\n",
            "reading one line at a time from each until all datasets are out of items. Then it will\n",
            "fetch  the  next  five  filepaths  from  the  filepath_dataset  and  interleave  them  the\n",
            "same way, and so on until it runs out of filepaths. For interleaving to work best, it is\n",
            "preferable  to  have  files  of  identical  length;  otherwise  the  end  of  the  longest  file  will\n",
            "not be interleaved.\n",
            "\n",
            "By  default,  interleave()  does  not  use  parallelism;  it  just  reads  one  line  at  a  time\n",
            "from each file, sequentially. If you want it to actually read files in parallel, you can set\n",
            "the interleave() method’s num_parallel_calls argument to the number of threads\n",
            "you  want  (recall  that  the  map()  method  also  has  this  argument).  You  can  even  set\n",
            "it  to  tf.data.AUTOTUNE  to  make  TensorFlow  choose  the  right  number  of  threads\n",
            "dynamically based on the available CPU. Let’s look at what the dataset contains now:\n",
            "\n",
            ">>> for line in dataset.take(5):\n",
            "...     print(line)\n",
            "...\n",
            "tf.Tensor(b'4.5909,16.0,[...],33.63,-117.71,2.418', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.4792,24.0,[...],34.18,-118.38,2.0', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.2708,45.0,[...],37.48,-122.19,2.67', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.1856,41.0,[...],32.76,-117.12,1.205', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.1812,52.0,[...],33.73,-118.31,3.215', shape=(), dtype=string)\n",
            "\n",
            "These are the first rows (ignoring the header row) of five CSV files, chosen randomly.\n",
            "Looks good!\n",
            "\n",
            "The tf.data API \n",
            "\n",
            "| \n",
            "\n",
            "447\n",
            "\n",
            "\fIt’s possible to pass a list of filepaths to the TextLineDataset con‐\n",
            "structor:  it  will  go  through  each  file  in  order,  line  by  line.  If  you\n",
            "also  set  the  num_parallel_reads  argument  to  a  number  greater\n",
            "than one, then the dataset will read that number of files in parallel\n",
            "and interleave their lines (without having to call the interleave()\n",
            "method).  However,  it  will  not  shuffle  the  files,  nor  will  it  skip  the\n",
            "header lines.\n",
            "\n",
            "Preprocessing the Data\n",
            "Now that we have a housing dataset that returns each instance as a tensor containing\n",
            "a byte string, we need to do a bit of preprocessing, including parsing the strings and\n",
            "scaling  the  data.  Let’s  implement  a  couple  custom  functions  that  will  perform  this\n",
            "preprocessing:\n",
            "\n",
            "X_mean, X_std = [...]  # mean and scale of each feature in the training set\n",
            "n_inputs = 8\n",
            "\n",
            "def parse_csv_line(line):\n",
            "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
            "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
            "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
            "\n",
            "def preprocess(line):\n",
            "    x, y = parse_csv_line(line)\n",
            "    return (x - X_mean) / X_std, y\n",
            "\n",
            "Let’s walk through this code:\n",
            "\n",
            "• First, the code assumes that we have precomputed the mean and standard devia‐\n",
            "•\n",
            "tion of each feature in the training set. X_mean and X_std are just 1D tensors (or\n",
            "NumPy  arrays)  containing  eight  floats,  one  per  input  feature.  This  can  be  done\n",
            "using  a  Scikit-Learn  StandardScaler  on  a  large  enough  random  sample  of  the\n",
            "dataset. Later in this chapter, we will use a Keras preprocessing layer instead.\n",
            "\n",
            "• The parse_csv_line() function takes one CSV line and parses it. To help with\n",
            "•\n",
            "that,  it  uses  the  tf.io.decode_csv()  function,  which  takes  two  arguments:  the\n",
            "first is the line to parse, and the second is an array containing the default value\n",
            "for each column in the CSV file. This array (defs) tells TensorFlow not only the\n",
            "default value for each column, but also the number of columns and their types.\n",
            "In  this  example,  we  tell  it  that  all  feature  columns  are  floats  and  that  missing\n",
            "values should default to zero, but we provide an empty array of type tf.float32\n",
            "as  the  default  value  for  the  last  column  (the  target):  the  array  tells  TensorFlow\n",
            "that this column contains floats, but that there is no default value, so it will raise\n",
            "an exception if it encounters a missing value.\n",
            "\n",
            "448 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\f• The  tf.io.decode_csv()  function  returns  a  list  of  scalar  tensors  (one  per  col‐\n",
            "•\n",
            "umn),  but  we  need  to  return  a  1D  tensor  array.  So  we  call  tf.stack()  on  all\n",
            "tensors  except  for  the  last  one  (the  target):  this  will  stack  these  tensors  into  a\n",
            "1D  array.  We  then  do  the  same  for  the  target  value:  this  makes  it  a  1D  tensor\n",
            "array  with  a  single  value,  rather  than  a  scalar  tensor.  The  tf.io.decode_csv()\n",
            "function is done, so it returns the input features and the target.\n",
            "\n",
            "• Finally, the custom preprocess() function just calls the parse_csv_line() func‐\n",
            "•\n",
            "tion, scales the input features by subtracting the feature means and then dividing\n",
            "by  the  feature  standard  deviations,  and  returns  a  tuple  containing  the  scaled\n",
            "features and the target.\n",
            "\n",
            "Let’s test this preprocessing function:\n",
            "\n",
            ">>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
            "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
            " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
            "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
            " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\n",
            "\n",
            "Looks good! The preprocess() function can convert an instance from a byte string\n",
            "to  a  nice  scaled  tensor,  with  its  corresponding  label.  We  can  now  use  the  dataset’s\n",
            "map() method to apply the preprocess() function to each sample in the dataset.\n",
            "\n",
            "Putting Everything Together\n",
            "To make the code more reusable, let’s put together everything we have discussed so\n",
            "far into another helper function; it will create and return a dataset that will efficiently\n",
            "load  California  housing  data  from  multiple  CSV  files,  preprocess  it,  shuffle  it,  and\n",
            "batch it (see Figure 13-2):\n",
            "\n",
            "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
            "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
            "                       batch_size=32):\n",
            "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
            "    dataset = dataset.interleave(\n",
            "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
            "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
            "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
            "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
            "    return dataset.batch(batch_size).prefetch(1)\n",
            "\n",
            "Note that we use the prefetch() method on the very last line. This is important for\n",
            "performance, as you will see now.\n",
            "\n",
            "The tf.data API \n",
            "\n",
            "| \n",
            "\n",
            "449\n",
            "\n",
            "\fFigure 13-2. Loading and preprocessing data from multiple CSV files\n",
            "\n",
            "Prefetching\n",
            "By  calling  prefetch(1)  at  the  end  of  the  custom  csv_reader_dataset()  function,\n",
            "we  are  creating  a  dataset  that  will  do  its  best  to  always  be  one  batch  ahead.2  In\n",
            "other words, while our training algorithm is working on one batch, the dataset will\n",
            "already be working in parallel on getting the next batch ready (e.g., reading the data\n",
            "from  disk  and  preprocessing  it).  This  can  improve  performance  dramatically,  as  is\n",
            "illustrated in Figure 13-3.\n",
            "\n",
            "If  we  also  ensure  that  loading  and  preprocessing  are  multithreaded  (by  setting\n",
            "num_parallel_calls when calling interleave() and map()), we can exploit multiple\n",
            "CPU cores and hopefully make preparing one batch of data shorter than running a\n",
            "training step on the GPU: this way the GPU will be almost 100% utilized (except for\n",
            "the data transfer time from the CPU to the GPU3), and training will run much faster.\n",
            "\n",
            "2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more.\n",
            "\n",
            "Alternatively, you can let TensorFlow decide automatically by passing tf.data.AUTOTUNE to prefetch().\n",
            "\n",
            "3 But check out the experimental tf.data.experimental.prefetch_to_device() function, which can prefetch\n",
            "\n",
            "data directly to the GPU. Any TensorFlow function or class with experimental in its name may change\n",
            "without warning in future versions. If an experimental function fails, try removing the word experimental:\n",
            "it may have been moved to the core API. If not, then please check the notebook, as I will ensure it contains\n",
            "up-to-date code.\n",
            "\n",
            "450 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fFigure 13-3. With prefetching, the CPU and the GPU work in parallel: as the GPU works\n",
            "on one batch, the CPU works on the next\n",
            "\n",
            "If  you  plan  to  purchase  a  GPU  card,  its  processing  power  and  its\n",
            "memory  size  are  of  course  very  important  (in  particular,  a  large\n",
            "amount of RAM is crucial for large computer vision or natural lan‐\n",
            "guage processing models). Just as important for good performance\n",
            "is the GPU’s memory bandwidth; this is the number of gigabytes of\n",
            "data it can get into or out of its RAM per second.\n",
            "\n",
            "If the dataset is small enough to fit in memory, you can significantly speed up train‐\n",
            "ing by using the dataset’s cache() method to cache its content to RAM. You should\n",
            "generally  do  this  after  loading  and  preprocessing  the  data,  but  before  shuffling,\n",
            "repeating,  batching,  and  prefetching.  This  way,  each  instance  will  only  be  read  and\n",
            "preprocessed  once  (instead  of  once  per  epoch),  but  the  data  will  still  be  shuffled\n",
            "differently at each epoch, and the next batch will still be prepared in advance.\n",
            "\n",
            "You have now learned how to build efficient input pipelines to load and preprocess\n",
            "data  from  multiple  text  files.  We  have  discussed  the  most  common  dataset  meth‐\n",
            "ods,  but  there  are  a  few  more  you  may  want  to  look  at,  such  as  concatenate(),\n",
            "zip(),  window(),  reduce(),  shard(),  flat_map(),  apply(),  unbatch(),  and  pad\n",
            "ded_batch().  There  are  also  a  few  more  class  methods,  such  as  from_generator()\n",
            "\n",
            "The tf.data API \n",
            "\n",
            "| \n",
            "\n",
            "451\n",
            "\n",
            "\fand  from_tensors(),  which  create  a  new  dataset  from  a  Python  generator  or  a  list\n",
            "of  tensors,  respectively.  Please  check  the  API  documentation  for  more  details.  Also\n",
            "note  that  there  are  experimental  features  available  in  tf.data.experimental,  many\n",
            "of  which  will  likely  make  it  to  the  core  API  in  future  releases  (e.g.,  check  out  the\n",
            "CsvDataset  class,  as  well  as  the  make_csv_dataset()  method,  which  takes  care  of\n",
            "inferring the type of each column).\n",
            "\n",
            "Using the Dataset with Keras\n",
            "Now  we  can  use  the  custom  csv_reader_dataset()  function  we  wrote  earlier  to\n",
            "create  a  dataset  for  the  training  set,  and  for  the  validation  set  and  the  test  set.  The\n",
            "training set will be shuffled at each epoch (note that the validation set and the test set\n",
            "will also be shuffled, even though we don’t really need that):\n",
            "\n",
            "train_set = csv_reader_dataset(train_filepaths)\n",
            "valid_set = csv_reader_dataset(valid_filepaths)\n",
            "test_set = csv_reader_dataset(test_filepaths)\n",
            "\n",
            "Now you can simply build and train a Keras model using these datasets. When you\n",
            "call  the  model’s  fit()  method,  you  pass  train_set  instead  of  X_train,  y_train,\n",
            "and  pass  validation_data=valid_set  instead  of  validation_data=(X_valid,\n",
            "y_valid).  The  fit()  method  will  take  care  of  repeating  the  training  dataset  once\n",
            "per epoch, using a different random order at each epoch:\n",
            "\n",
            "model = tf.keras.Sequential([...])\n",
            "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
            "model.fit(train_set, validation_data=valid_set, epochs=5)\n",
            "\n",
            "Similarly, you can pass a dataset to the evaluate() and predict() methods:\n",
            "\n",
            "test_mse = model.evaluate(test_set)\n",
            "new_set = test_set.take(3)  # pretend we have 3 new samples\n",
            "y_pred = model.predict(new_set)  # or you could just pass a NumPy array\n",
            "\n",
            "Unlike the other sets, the new_set will usually not contain labels. If it does, as is the\n",
            "case here, Keras will ignore them. Note that in all these cases, you can still use NumPy\n",
            "arrays instead of datasets if you prefer (but of course they need to have been loaded\n",
            "and preprocessed first).\n",
            "\n",
            "If you want to build your own custom training loop (as discussed in Chapter 12), you\n",
            "can just iterate over the training set, very naturally:\n",
            "\n",
            "n_epochs = 5\n",
            "for epoch in range(n_epochs):\n",
            "    for X_batch, y_batch in train_set:\n",
            "        [...]  # perform one gradient descent step\n",
            "\n",
            "In  fact,  it  is  even  possible  to  create  a  TF  function  (see  Chapter  12)  that  trains  the\n",
            "model for a whole epoch. This can really speed up training:\n",
            "\n",
            "452 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\f@tf.function\n",
            "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
            "    for X_batch, y_batch in train_set:\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = model(X_batch)\n",
            "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
            "            loss = tf.add_n([main_loss] + model.losses)\n",
            "        gradients = tape.gradient(loss, model.trainable_variables)\n",
            "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
            "loss_fn = tf.keras.losses.mean_squared_error\n",
            "for epoch in range(n_epochs):\n",
            "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
            "    train_one_epoch(model, optimizer, loss_fn, train_set)\n",
            "\n",
            "In  Keras,  the  steps_per_execution  argument  of  the  compile()  method  lets  you\n",
            "define the number of batches that the fit() method will process during each call to\n",
            "the  tf.function  it  uses  for  training.  The  default  is  just  1,  so  if  you  set  it  to  50  you\n",
            "will  often  see  a  significant  performance  improvement.  However,  the  on_batch_*()\n",
            "methods of Keras callbacks will only be called every 50 batches.\n",
            "\n",
            "Congratulations,  you  now  know  how  to  build  powerful  input  pipelines  using  the\n",
            "tf.data API! However, so far we’ve been using CSV files, which are common, simple,\n",
            "and  convenient  but  not  really  efficient,  and  do  not  support  large  or  complex  data\n",
            "structures  (such  as  images  or  audio)  very  well.  So,  let’s  see  how  to  use  TFRecords\n",
            "instead.\n",
            "\n",
            "If you are happy with CSV files (or whatever other format you are\n",
            "using), you do not have to use TFRecords. As the saying goes, if it\n",
            "ain’t broke, don’t fix it! TFRecords are useful when the bottleneck\n",
            "during training is loading and parsing the data.\n",
            "\n",
            "The TFRecord Format\n",
            "The TFRecord format is TensorFlow’s preferred format for storing large amounts of\n",
            "data  and  reading  it  efficiently.  It  is  a  very  simple  binary  format  that  just  contains  a\n",
            "sequence of binary records of varying sizes (each record is comprised of a length, a\n",
            "CRC checksum to check that the length was not corrupted, then the actual data, and\n",
            "finally a CRC checksum for the data). You can easily create a TFRecord file using the\n",
            "tf.io.TFRecordWriter class:\n",
            "\n",
            "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
            "    f.write(b\"This is the first record\")\n",
            "    f.write(b\"And this is the second record\")\n",
            "\n",
            "The TFRecord Format \n",
            "\n",
            "| \n",
            "\n",
            "453\n",
            "\n",
            "\fAnd  you  can  then  use  a  tf.data.TFRecordDataset  to  read  one  or  more  TFRecord\n",
            "files:\n",
            "\n",
            "filepaths = [\"my_data.tfrecord\"]\n",
            "dataset = tf.data.TFRecordDataset(filepaths)\n",
            "for item in dataset:\n",
            "    print(item)\n",
            "\n",
            "This will output:\n",
            "\n",
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n",
            "\n",
            "By default, a  TFRecordDataset will read files one by one, but you\n",
            "can  make  it  read  multiple  files  in  parallel  and  interleave  their\n",
            "records  by  passing  the  constructor  a  list  of  filepaths  and  setting\n",
            "num_parallel_reads  to  a  number  greater  than  one.  Alternatively,\n",
            "you  could  obtain  the  same  result  by  using  list_files()  and\n",
            "interleave() as we did earlier to read multiple CSV files.\n",
            "\n",
            "Compressed TFRecord Files\n",
            "It can sometimes be useful to compress your TFRecord files, especially if they need to\n",
            "be loaded via a network connection. You can create a compressed TFRecord file by\n",
            "setting the options argument:\n",
            "\n",
            "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
            "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
            "    f.write(b\"Compress, compress, compress!\")\n",
            "\n",
            "When reading a compressed TFRecord file, you need to specify the compression type:\n",
            "\n",
            "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
            "                                  compression_type=\"GZIP\")\n",
            "\n",
            "A Brief Introduction to Protocol Buffers\n",
            "Even  though  each  record  can  use  any  binary  format  you  want,  TFRecord  files  usu‐\n",
            "ally  contain  serialized  protocol  buffers  (also  called  protobufs).  This  is  a  portable,\n",
            "extensible,  and  efficient  binary  format  developed  at  Google  back  in  2001  and  made\n",
            "open source in 2008; protobufs are now widely used, in particular in gRPC, Google’s\n",
            "remote  procedure  call  system.  They  are  defined  using  a  simple  language  that  looks\n",
            "like this:\n",
            "\n",
            "syntax = \"proto3\";\n",
            "message Person {\n",
            "    string name = 1;\n",
            "    int32 id = 2;\n",
            "    repeated string email = 3;\n",
            "}\n",
            "\n",
            "454 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fThis  protobuf  definition  says  we  are  using  version  3  of  the  protobuf  format,  and  it\n",
            "specifies that each Person object4 may (optionally) have a name of type string, an id\n",
            "of type int32, and zero or more email fields, each of type string. The numbers 1, 2,\n",
            "and 3 are the field identifiers: they will be used in each record’s binary representation.\n",
            "Once you have a definition in a .proto file, you can compile it. This requires protoc,\n",
            "the protobuf compiler, to generate access classes in Python (or some other language).\n",
            "Note that the protobuf definitions you will generally use in TensorFlow have already\n",
            "been  compiled  for  you,  and  their  Python  classes  are  part  of  the  TensorFlow  library,\n",
            "so  you  will  not  need  to  use  protoc.  All  you  need  to  know  is  how  to  use  protobuf\n",
            "access  classes  in  Python.  To  illustrate  the  basics,  let’s  look  at  a  simple  example  that\n",
            "uses the access classes generated for the Person protobuf (the code is explained in the\n",
            "comments):\n",
            "\n",
            ">>> from person_pb2 import Person  # import the generated access class\n",
            ">>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
            ">>> print(person)  # display the Person\n",
            "name: \"Al\"\n",
            "id: 123\n",
            "email: \"a@b.com\"\n",
            ">>> person.name  # read a field\n",
            "'Al'\n",
            ">>> person.name = \"Alice\"  # modify a field\n",
            ">>> person.email[0]  # repeated fields can be accessed like arrays\n",
            "'a@b.com'\n",
            ">>> person.email.append(\"c@d.com\")  # add an email address\n",
            ">>> serialized = person.SerializeToString()  # serialize person to a byte string\n",
            ">>> serialized\n",
            "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
            ">>> person2 = Person()  # create a new Person\n",
            ">>> person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)\n",
            "27\n",
            ">>> person == person2  # now they are equal\n",
            "True\n",
            "\n",
            "In short, we import the Person class generated by protoc, we create an instance and\n",
            "play  with  it,  visualizing  it  and  reading  and  writing  some  fields,  then  we  serialize  it\n",
            "using  the  SerializeToString()  method.  This  is  the  binary  data  that  is  ready  to  be\n",
            "saved or transmitted over the network. When reading or receiving this binary data,\n",
            "we can parse it using the ParseFromString() method, and we get a copy of the object\n",
            "that was serialized.5\n",
            "\n",
            "You could save the serialized  Person object to a TFRecord file, then load and parse\n",
            "it:  everything  would  work  fine.  However,  ParseFromString()  is  not  a  TensorFlow\n",
            "\n",
            "4 Since protobuf objects are meant to be serialized and transmitted, they are called messages.\n",
            "\n",
            "5 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\n",
            "\n",
            "about protobufs, please visit https://homl.info/protobuf.\n",
            "\n",
            "The TFRecord Format \n",
            "\n",
            "| \n",
            "\n",
            "455\n",
            "\n",
            "\foperation,  so  you  couldn’t  use  it  in  a  preprocessing  function  in  a  tf.data  pipeline\n",
            "(except by wrapping it in a tf.py_function() operation, which would make the code\n",
            "slower  and  less  portable,  as  you  saw  in  Chapter  12).  However,  you  could  use  the\n",
            "tf.io.decode_proto() function, which can parse any protobuf you want, provided\n",
            "you give it the protobuf definition (see the notebook for an example). That said, in\n",
            "practice  you  will  generally  want  to  use  instead  the  predefined  protobufs  for  which\n",
            "TensorFlow  provides  dedicated  parsing  operations.  Let’s  look  at  these  predefined\n",
            "protobufs now.\n",
            "\n",
            "TensorFlow Protobufs\n",
            "The main protobuf typically used in a TFRecord file is the Example protobuf, which\n",
            "represents one instance in a dataset. It contains a list of named features, where each\n",
            "feature can either be a list of byte strings, a list of floats, or a list of integers. Here is\n",
            "the protobuf definition (from TensorFlow’s source code):\n",
            "\n",
            "syntax = \"proto3\";\n",
            "message BytesList { repeated bytes value = 1; }\n",
            "message FloatList { repeated float value = 1 [packed = true]; }\n",
            "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
            "message Feature {\n",
            "    oneof kind {\n",
            "        BytesList bytes_list = 1;\n",
            "        FloatList float_list = 2;\n",
            "        Int64List int64_list = 3;\n",
            "    }\n",
            "};\n",
            "message Features { map<string, Feature> feature = 1; };\n",
            "message Example { Features features = 1; };\n",
            "\n",
            "The  definitions  of  BytesList,  FloatList,  and  Int64List  are  straightforward\n",
            "enough.  Note  that  [packed  =  true]  is  used  for  repeated  numerical  fields,  for  a\n",
            "more efficient encoding. A Feature contains either a BytesList, a FloatList, or an\n",
            "Int64List. A  Features (with an  s) contains a dictionary that maps a feature name\n",
            "to the corresponding feature value. And finally, an Example contains only a Features\n",
            "object.\n",
            "\n",
            "Why  was  Example  even  defined,  since  it  contains  no  more  than\n",
            "a  Features  object?  Well,  TensorFlow’s  developers  may  one  day\n",
            "decide to add more fields to it. As long as the new Example defini‐\n",
            "tion  still  contains  the  features  field,  with  the  same  ID,  it  will  be\n",
            "backward compatible. This extensibility is one of the great features\n",
            "of protobufs.\n",
            "\n",
            "456 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fHere is how you could create a  tf.train.Example representing the same person as\n",
            "earlier:\n",
            "\n",
            "from tensorflow.train import BytesList, FloatList, Int64List\n",
            "from tensorflow.train import Feature, Features, Example\n",
            "\n",
            "person_example = Example(\n",
            "    features=Features(\n",
            "        feature={\n",
            "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
            "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
            "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
            "                                                          b\"c@d.com\"]))\n",
            "        }))\n",
            "\n",
            "The  code  is  a  bit  verbose  and  repetitive,  but  you  could  easily  wrap  it  inside  a  small\n",
            "helper function. Now that we have an Example protobuf, we can serialize it by calling\n",
            "its  SerializeToString()  method,  then  write  the  resulting  data  to  a  TFRecord  file.\n",
            "Let’s write it five times to pretend we have several contacts:\n",
            "\n",
            "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
            "    for _ in range(5):\n",
            "        f.write(person_example.SerializeToString())\n",
            "\n",
            "Normally  you  would  write  much  more  than  five  Examples!  Typically,  you  would\n",
            "create a conversion script that reads from your current format (say, CSV files), creates\n",
            "an  Example  protobuf  for  each  instance,  serializes  them,  and  saves  them  to  several\n",
            "TFRecord files, ideally shuffling them in the process. This requires a bit of work, so\n",
            "once  again  make  sure  it  is  really  necessary  (perhaps  your  pipeline  works  fine  with\n",
            "CSV files).\n",
            "\n",
            "Now that we have a nice TFRecord file containing several serialized Examples, let’s try\n",
            "to load it.\n",
            "\n",
            "Loading and Parsing Examples\n",
            "To  load  the  serialized  Example  protobufs,  we  will  use  a  tf.data.TFRecordDataset\n",
            "once again, and we will parse each Example using tf.io.parse_single_example(). It\n",
            "requires at least two arguments: a string scalar tensor containing the serialized data,\n",
            "and  a  description  of  each  feature.  The  description  is  a  dictionary  that  maps  each\n",
            "feature  name  to  either  a  tf.io.FixedLenFeature  descriptor  indicating  the  feature’s\n",
            "shape, type, and default value, or a tf.io.VarLenFeature descriptor indicating only\n",
            "the type if the length of the feature’s list may vary (such as for the \"emails\" feature).\n",
            "\n",
            "The following code defines a description dictionary, then creates a TFRecordDataset\n",
            "and  applies  a  custom  preprocessing  function  to  it  to  parse  each  serialized  Example\n",
            "protobuf that this dataset contains:\n",
            "\n",
            "The TFRecord Format \n",
            "\n",
            "| \n",
            "\n",
            "457\n",
            "\n",
            "\ffeature_description = {\n",
            "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
            "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
            "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
            "}\n",
            "\n",
            "def parse(serialized_example):\n",
            "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
            "\n",
            "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
            "for parsed_example in dataset:\n",
            "    print(parsed_example)\n",
            "\n",
            "The  fixed-length  features  are  parsed  as  regular  tensors,  but  the  variable-length  fea‐\n",
            "tures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor\n",
            "using tf.sparse.to_dense(), but in this case it is simpler to just access its values:\n",
            "\n",
            ">>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n",
            "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
            ">>> parsed_example[\"emails\"].values\n",
            "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
            "\n",
            "Instead of parsing examples one by one using tf.io.parse_single_example(), you\n",
            "may want to parse them batch by batch using tf.io.parse_example():\n",
            "\n",
            "def parse(serialized_examples):\n",
            "    return tf.io.parse_example(serialized_examples, feature_description)\n",
            "\n",
            "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
            "for parsed_examples in dataset:\n",
            "    print(parsed_examples)  # two examples at a time\n",
            "\n",
            "Lastly,  a  BytesList  can  contain  any  binary  data  you  want,  including  any  serialized\n",
            "object.  For  example,  you  can  use  tf.io.encode_jpeg()  to  encode  an  image  using\n",
            "the  JPEG  format  and  put  this  binary  data  in  a  BytesList.  Later,  when  your  code\n",
            "reads  the  TFRecord,  it  will  start  by  parsing  the  Example,  then  it  will  need  to  call\n",
            "tf.io.decode_jpeg()  to  parse  the  data  and  get  the  original  image  (or  you  can  use\n",
            "tf.io.decode_image(),  which  can  decode  any  BMP,  GIF,  JPEG,  or  PNG  image).\n",
            "You  can  also  store  any  tensor  you  want  in  a  BytesList  by  serializing  the  tensor\n",
            "using  tf.io.serialize_tensor()  then  putting  the  resulting  byte  string  in  a  Bytes\n",
            "List  feature.  Later,  when  you  parse  the  TFRecord,  you  can  parse  this  data  using\n",
            "tf.io.parse_tensor().  See  this  chapter’s  notebook  at  https://homl.info/colab3  for\n",
            "examples of storing images and tensors in a TFRecord file.\n",
            "\n",
            "As you can see, the Example protobuf is quite flexible, so it will probably be sufficient\n",
            "for  most  use  cases.  However,  it  may  be  a  bit  cumbersome  to  use  when  you  are\n",
            "dealing with lists of lists. For example, suppose you want to classify text documents.\n",
            "Each document may be represented as a list of sentences, where each sentence is rep‐\n",
            "resented as a list of words. And perhaps each document also has a list of comments,\n",
            "\n",
            "458 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fwhere each comment is represented as a list of words. There may be some contextual\n",
            "data  too,  such  as  the  document’s  author,  title,  and  publication  date.  TensorFlow’s\n",
            "SequenceExample protobuf is designed for such use cases.\n",
            "\n",
            "Handling Lists of Lists Using the SequenceExample Protobuf\n",
            "Here is the definition of the SequenceExample protobuf:\n",
            "\n",
            "message FeatureList { repeated Feature feature = 1; };\n",
            "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
            "message SequenceExample {\n",
            "    Features context = 1;\n",
            "    FeatureLists feature_lists = 2;\n",
            "};\n",
            "\n",
            "A  SequenceExample  contains  a  Features  object  for  the  contextual  data  and  a\n",
            "FeatureLists object that contains one or more named  FeatureList objects (e.g., a\n",
            "FeatureList named \"content\" and another named \"comments\"). Each FeatureList\n",
            "contains  a  list  of  Feature  objects,  each  of  which  may  be  a  list  of  byte  strings,  a  list\n",
            "of 64-bit integers, or a list of floats (in this example, each Feature would represent a\n",
            "sentence or a comment, perhaps in the form of a list of word identifiers). Building a\n",
            "SequenceExample, serializing it, and parsing it is similar to building, serializing, and\n",
            "parsing  an  Example,  but  you  must  use  tf.io.parse_single_sequence_example()\n",
            "to parse a single  SequenceExample or  tf.io.parse_sequence_example() to parse a\n",
            "batch. Both functions return a tuple containing the context features (as a dictionary)\n",
            "and  the  feature  lists  (also  as  a  dictionary).  If  the  feature  lists  contain  sequences  of\n",
            "varying sizes (as in the preceding example), you may want to convert them to ragged\n",
            "tensors using tf.RaggedTensor.from_sparse() (see the notebook for the full code):\n",
            "\n",
            "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
            "    serialized_sequence_example, context_feature_descriptions,\n",
            "    sequence_feature_descriptions)\n",
            "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
            "\n",
            "Now  that  you  know  how  to  efficiently  store,  load,  parse,  and  preprocess  the  data\n",
            "using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention to the\n",
            "Keras preprocessing layers.\n",
            "\n",
            "Keras Preprocessing Layers\n",
            "Preparing your data for a neural network typically requires normalizing the numeri‐\n",
            "cal features, encoding the categorical features and text, cropping and resizing images,\n",
            "and more. There are several options for this:\n",
            "\n",
            "• The preprocessing can be done ahead of time when preparing your training data\n",
            "•\n",
            "files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. You will\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "459\n",
            "\n",
            "\fneed to apply the exact same preprocessing steps in production, to ensure your\n",
            "production model receives preprocessed inputs similar to the ones it was trained\n",
            "on.\n",
            "\n",
            "•\n",
            "• Alternatively,  you  can  preprocess  your  data  on  the  fly  while  loading  it  with\n",
            "tf.data, by applying a preprocessing function to every element of a dataset using\n",
            "that dataset’s map() method, as we did earlier in this chapter. Again, you will need\n",
            "to apply the same preprocessing steps in production.\n",
            "\n",
            "• One last approach is to include preprocessing layers directly inside your model\n",
            "•\n",
            "so  it  can  preprocess  all  the  input  data  on  the  fly  during  training,  then  use  the\n",
            "same preprocessing layers in production. The rest of this chapter will look at this\n",
            "last approach.\n",
            "\n",
            "Keras  offers  many  preprocessing  layers  that  you  can  include  in  your  models:  they\n",
            "can  be  applied  to  numerical  features,  categorical  features,  images,  and  text.  We’ll\n",
            "go  over  the  numerical  and  categorical  features  in  the  next  sections,  as  well  as  basic\n",
            "text  preprocessing,  and  we  will  cover  image  preprocessing  in  Chapter  14  and  more\n",
            "advanced text preprocessing in Chapter 16.\n",
            "\n",
            "The Normalization Layer\n",
            "As  we  saw  in  Chapter  10,  Keras  provides  a  Normalization  layer  that  we  can  use  to\n",
            "standardize the input features. We can either specify the mean and variance of each\n",
            "feature when creating the layer or—more simply—pass the training set to the layer’s\n",
            "adapt() method before fitting the model, so the layer can measure the feature means\n",
            "and variances on its own before training:\n",
            "\n",
            "norm_layer = tf.keras.layers.Normalization()\n",
            "model = tf.keras.models.Sequential([\n",
            "    norm_layer,\n",
            "    tf.keras.layers.Dense(1)\n",
            "])\n",
            "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
            "norm_layer.adapt(X_train)  # computes the mean and variance of every feature\n",
            "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)\n",
            "\n",
            "The  data  sample  passed  to  the  adapt()  method  must  be  large\n",
            "enough to be representative of your dataset, but it does not have to\n",
            "be the full training set: for the Normalization layer, a few hundred\n",
            "instances randomly sampled from the training set will generally be\n",
            "sufficient to get a good estimate of the feature means and variances.\n",
            "\n",
            "Since we included the Normalization layer inside the model, we can now deploy this\n",
            "model to production without having to worry about normalization again: the model\n",
            "will  just  handle  it  (see  Figure  13-4).  Fantastic!  This  approach  completely  eliminates\n",
            "\n",
            "460 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fthe  risk  of  preprocessing  mismatch,  which  happens  when  people  try  to  maintain\n",
            "different preprocessing code for training and production but update one and forget\n",
            "to update the other. The production model then ends up receiving data preprocessed\n",
            "in  a  way  it  doesn’t  expect.  If  they’re  lucky,  they  get  a  clear  bug.  If  not,  the  model’s\n",
            "accuracy just silently degrades.\n",
            "\n",
            "Figure 13-4. Including preprocessing layers inside a model\n",
            "\n",
            "Including  the  preprocessing  layer  directly  in  the  model  is  nice  and  straightforward,\n",
            "but  it  will  slow  down  training  (only  very  slightly  in  the  case  of  the  Normalization\n",
            "layer): indeed, since preprocessing is performed on the fly during training, it happens\n",
            "once  per  epoch.  We  can  do  better  by  normalizing  the  whole  training  set  just  once\n",
            "before  training.  To  do  this,  we  can  use  the  Normalization  layer  in  a  standalone\n",
            "fashion (much like a Scikit-Learn StandardScaler):\n",
            "\n",
            "norm_layer = tf.keras.layers.Normalization()\n",
            "norm_layer.adapt(X_train)\n",
            "X_train_scaled = norm_layer(X_train)\n",
            "X_valid_scaled = norm_layer(X_valid)\n",
            "\n",
            "Now  we  can  train  a  model  on  the  scaled  data,  this  time  without  a  Normalization\n",
            "layer:\n",
            "\n",
            "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
            "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
            "model.fit(X_train_scaled, y_train, epochs=5,\n",
            "          validation_data=(X_valid_scaled, y_valid))\n",
            "\n",
            "Good!  This  should  speed  up  training  a  bit.  But  now  the  model  won’t  preprocess\n",
            "its  inputs  when  we  deploy  it  to  production.  To  fix  this,  we  just  need  to  create  a\n",
            "new model that wraps both the adapted Normalization layer and the model we just\n",
            "trained.  We  can  then  deploy  this  final  model  to  production,  and  it  will  take  care  of\n",
            "both preprocessing its inputs and making predictions (see Figure 13-5):\n",
            "\n",
            "final_model = tf.keras.Sequential([norm_layer, model])\n",
            "X_new = X_test[:3]  # pretend we have a few new instances (unscaled)\n",
            "y_pred = final_model(X_new)  # preprocesses the data and makes predictions\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "461\n",
            "\n",
            "\fFigure 13-5. Preprocessing the data just once before training using preprocessing layers,\n",
            "then deploying these layers inside the final model\n",
            "\n",
            "Now we have the best of both worlds: training is fast because we only preprocess the\n",
            "data once before training begins, and the final model can preprocess its inputs on the\n",
            "fly without any risk of preprocessing mismatch.\n",
            "\n",
            "Moreover,  the  Keras  preprocessing  layers  play  nicely  with  the  tf.data  API.  For\n",
            "example,  it’s  possible  to  pass  a  tf.data.Dataset  to  a  preprocessing  layer’s  adapt()\n",
            "method. It’s also possible to apply a Keras preprocessing layer to a tf.data.Dataset\n",
            "using the dataset’s map() method. For example, here’s how you could apply an adap‐\n",
            "ted Normalization layer to the input features of each batch in a dataset:\n",
            "\n",
            "dataset = dataset.map(lambda X, y: (norm_layer(X), y))\n",
            "\n",
            "Lastly,  if  you  ever  need  more  features  than  the  Keras  preprocessing  layers  provide,\n",
            "you can always write your own Keras layer, just like we discussed in Chapter 12. For\n",
            "example, if the Normalization layer didn’t exist, you could get a similar result using\n",
            "the following custom layer:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "class MyNormalization(tf.keras.layers.Layer):\n",
            "    def adapt(self, X):\n",
            "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
            "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        eps = tf.keras.backend.epsilon()  # a small smoothing term\n",
            "        return (inputs - self.mean_) / (self.std_ + eps)\n",
            "\n",
            "Next,  let’s  look  at  another  Keras  preprocessing  layer  for  numerical  features:  the\n",
            "Discretization layer.\n",
            "\n",
            "462 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fThe Discretization Layer\n",
            "The Discretization layer’s goal is to transform a numerical feature into a categorical\n",
            "feature by mapping value ranges (called bins) to categories. This is sometimes useful\n",
            "for  features  with  multimodal  distributions,  or  with  features  that  have  a  highly  non-\n",
            "linear relationship with the target. For example, the following code maps a numerical\n",
            "age feature to three categories, less than 18, 18 to 50 (not included), and 50 or over:\n",
            "\n",
            ">>> age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
            ">>> discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
            ">>> age_categories = discretize_layer(age)\n",
            ">>> age_categories\n",
            "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[0],[2],[2],[1],[1],[0]])>\n",
            "\n",
            "In  this  example,  we  provided  the  desired  bin  boundaries.  If  you  prefer,  you  can\n",
            "instead provide the number of bins you want, then call the layer’s adapt() method to\n",
            "let it find the appropriate bin boundaries based on the value percentiles. For example,\n",
            "if we set num_bins=3, then the bin boundaries will be located at the values just below\n",
            "the 33rd and 66th percentiles (in this example, at the values 10 and 37):\n",
            "\n",
            ">>> discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
            ">>> discretize_layer.adapt(age)\n",
            ">>> age_categories = discretize_layer(age)\n",
            ">>> age_categories\n",
            "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[1],[2],[2],[1],[2],[0]])>\n",
            "\n",
            "Category identifiers such as these should generally not be passed directly to a neural\n",
            "network,  as  their  values  cannot  be  meaningfully  compared.  Instead,  they  should  be\n",
            "encoded, for example using one-hot encoding. Let’s look at how to do this now.\n",
            "\n",
            "The CategoryEncoding Layer\n",
            "When  there  are  only  a  few  categories  (e.g.,  less  than  a  dozen  or  two),  then  one-hot\n",
            "encoding  is  often  a  good  option  (as  discussed  in  Chapter  2).  To  do  this,  Keras\n",
            "provides  the  CategoryEncoding  layer.  For  example,  let’s  one-hot  encode  the  age_\n",
            "categories feature we just created:\n",
            "\n",
            ">>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
            ">>> onehot_layer(age_categories)\n",
            "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
            "array([[0., 1., 0.],\n",
            "       [0., 0., 1.],\n",
            "       [0., 0., 1.],\n",
            "       [0., 1., 0.],\n",
            "       [0., 0., 1.],\n",
            "       [1., 0., 0.]], dtype=float32)>\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "463\n",
            "\n",
            "\fIf you try to encode more than one categorical feature at a time (which only makes\n",
            "sense  if  they  all  use  the  same  categories),  the  CategoryEncoding  class  will  perform\n",
            "multi-hot  encoding  by  default:  the  output  tensor  will  contain  a  1  for  each  category\n",
            "present in any input feature. For example:\n",
            "\n",
            ">>> two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
            ">>> onehot_layer(two_age_categories)\n",
            "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
            "array([[1., 1., 0.],\n",
            "       [0., 0., 1.],\n",
            "       [1., 0., 1.]], dtype=float32)>\n",
            "\n",
            "If  you  believe  it’s  useful  to  know  how  many  times  each  category  occurred,  you  can\n",
            "set output_mode=\"count\" when creating the CategoryEncoding layer, in which case\n",
            "the  output  tensor  will  contain  the  number  of  occurrences  of  each  category.  In  the\n",
            "preceding example, the output would be the same except for the second row, which\n",
            "would become [0., 0., 2.].\n",
            "\n",
            "Note that both multi-hot encoding and count encoding lose information, since it’s not\n",
            "possible  to  know  which  feature  each  active  category  came  from.  For  example,  both\n",
            "[0,  1]  and  [1,  0]  are  encoded  as  [1.,  1.,  0.].  If  you  want  to  avoid  this,  then\n",
            "you need to one-hot encode each feature separately and concatenate the outputs. This\n",
            "way, [0, 1] would get encoded as [1., 0., 0., 0., 1., 0.] and [1, 0] would get\n",
            "encoded as [0., 1., 0., 1., 0., 0.]. You can get the same result by tweaking the\n",
            "category identifiers so they don’t overlap. For example:\n",
            "\n",
            ">>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
            ">>> onehot_layer(two_age_categories + [0, 3])  # adds 3 to the second feature\n",
            "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
            "array([[0., 1., 0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0., 0., 1.],\n",
            "       [0., 0., 1., 1., 0., 0.]], dtype=float32)>\n",
            "\n",
            "In  this  output,  the  first  three  columns  correspond  to  the  first  feature,  and  the  last\n",
            "three  correspond  to  the  second  feature.  This  allows  the  model  to  distinguish  the\n",
            "two  features.  However,  it  also  increases  the  number  of  features  fed  to  the  model,\n",
            "and thereby requires more model parameters. It’s hard to know in advance whether\n",
            "a  single  multi-hot  encoding  or  a  per-feature  one-hot  encoding  will  work  best:  it\n",
            "depends on the task, and you may need to test both options.\n",
            "\n",
            "Now you can encode categorical integer features using one-hot or multi-hot encod‐\n",
            "ing. But what about categorical text features? For this, you can use the StringLookup\n",
            "layer.\n",
            "\n",
            "464 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fThe StringLookup Layer\n",
            "Let’s use the Keras StringLookup layer to one-hot encode a cities feature:\n",
            "\n",
            ">>> cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
            ">>> str_lookup_layer = tf.keras.layers.StringLookup()\n",
            ">>> str_lookup_layer.adapt(cities)\n",
            ">>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n",
            "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[1], [3], [3], [0]])>\n",
            "\n",
            "We first create a StringLookup layer, then we adapt it to the data: it finds that there\n",
            "are  three  distinct  categories.  Then  we  use  the  layer  to  encode  a  few  cities.  They  are\n",
            "encoded as integers by default. Unknown categories get mapped to 0, as is the case for\n",
            "“Montreal” in this example. The known categories are numbered starting at 1, from\n",
            "the most frequent category to the least frequent.\n",
            "\n",
            "Conveniently,  if  you  set  output_mode=\"one_hot\"  when  creating  the  StringLookup\n",
            "layer, it will output a one-hot vector for each category, instead of an integer:\n",
            "\n",
            ">>> str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
            ">>> str_lookup_layer.adapt(cities)\n",
            ">>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n",
            "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
            "array([[0., 1., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0.]], dtype=float32)>\n",
            "\n",
            "Keras also includes an IntegerLookup layer that acts much like the\n",
            "StringLookup layer but takes integers as input, rather than strings.\n",
            "\n",
            "If  the  training  set  is  very  large,  it  may  be  convenient  to  adapt  the  layer  to  just  a\n",
            "random subset of the training set. In this case, the layer’s adapt() method may miss\n",
            "some  of  the  rarer  categories.  By  default,  it  would  then  map  them  all  to  category  0,\n",
            "making them indistinguishable by the model. To reduce this risk (while still adapting\n",
            "the  layer  only  on  a  subset  of  the  training  set),  you  can  set  num_oov_indices  to  an\n",
            "integer  greater  than  1.  This  is  the  number  of  out-of-vocabulary  (OOV)  buckets  to\n",
            "use:  each  unknown  category  will  get  mapped  pseudorandomly  to  one  of  the  OOV\n",
            "buckets, using a hash function modulo the number of OOV buckets. This will allow\n",
            "the model to distinguish at least some of the rare categories. For example:\n",
            "\n",
            ">>> str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
            ">>> str_lookup_layer.adapt(cities)\n",
            ">>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])\n",
            "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[5], [7], [4], [3], [4]])>\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "465\n",
            "\n",
            "\fSince there are five OOV buckets, the first known category’s ID is now 5 (\"Paris\").\n",
            "But  \"Foo\",  \"Bar\",  and  \"Baz\"  are  unknown,  so  they  each  get  mapped  to  one  of  the\n",
            "OOV  buckets.  \"Bar\"  gets  its  own  dedicated  bucket  (with  ID  3),  but  sadly  \"Foo\"\n",
            "and  \"Baz\"  happen  to  be  mapped  to  the  same  bucket  (with  ID  4),  so  they  remain\n",
            "indistinguishable  by  the  model.  This  is  called  a  hashing  collision.  The  only  way  to\n",
            "reduce the risk of collision is to increase the number of OOV buckets. However, this\n",
            "will also increase the total number of categories, which will require more RAM and\n",
            "extra  model  parameters  once  the  categories  are  one-hot  encoded.  So,  don’t  increase\n",
            "that number too much.\n",
            "\n",
            "This  idea  of  mapping  categories  pseudorandomly  to  buckets  is  called  the  hashing\n",
            "trick. Keras provides a dedicated layer which does just that: the Hashing layer.\n",
            "\n",
            "The Hashing Layer\n",
            "For each category, the Keras Hashing layer computes a hash, modulo the number of\n",
            "buckets  (or  “bins”).  The  mapping  is  entirely  pseudorandom,  but  stable  across  runs\n",
            "and platforms (i.e., the same category will always be mapped to the same integer, as\n",
            "long as the number of bins is unchanged). For example, let’s use the Hashing layer to\n",
            "encode a few cities:\n",
            "\n",
            ">>> hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
            ">>> hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])\n",
            "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[0], [1], [9], [1]])>\n",
            "\n",
            "The  benefit  of  this  layer  is  that  it  does  not  need  to  be  adapted  at  all,  which  may\n",
            "sometimes  be  useful,  especially  in  an  out-of-core  setting  (when  the  dataset  is  too\n",
            "large to fit in memory). However, we once again get a hashing collision: “Tokyo” and\n",
            "“Montreal” are mapped to the same ID, making them indistinguishable by the model.\n",
            "So, it’s usually preferable to stick to the StringLookup layer.\n",
            "\n",
            "Let’s now look at another way to encode categories: trainable embeddings.\n",
            "\n",
            "Encoding Categorical Features Using Embeddings\n",
            "An  embedding  is  a  dense  representation  of  some  higher-dimensional  data,  such  as\n",
            "a  category,  or  a  word  in  a  vocabulary.  If  there  are  50,000  possible  categories,  then\n",
            "one-hot encoding would produce a 50,000-dimensional sparse vector (i.e., containing\n",
            "mostly  zeros).  In  contrast,  an  embedding  would  be  a  comparatively  small  dense\n",
            "vector; for example, with just 100 dimensions.\n",
            "\n",
            "In  deep  learning,  embeddings  are  usually  initialized  randomly,  and  they  are  then\n",
            "trained by gradient descent, along with the other model parameters. For example, the\n",
            "\"NEAR BAY\" category in the California housing dataset could be represented initially\n",
            "by a random vector such as [0.131, 0.890], while the \"NEAR OCEAN\" category might\n",
            "be represented by another random vector such as [0.631, 0.791]. In this example,\n",
            "\n",
            "466 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fwe use 2D embeddings, but the number of dimensions is a hyperparameter you can\n",
            "tweak.\n",
            "\n",
            "Since  these  embeddings  are  trainable,  they  will  gradually  improve  during  training;\n",
            "and  as  they  represent  fairly  similar  categories  in  this  case,  gradient  descent  will\n",
            "certainly  end  up  pushing  them  closer  together,  while  it  will  tend  to  move  them\n",
            "away  from  the  \"INLAND\"  category’s  embedding  (see  Figure  13-6).  Indeed,  the  better\n",
            "the  representation,  the  easier  it  will  be  for  the  neural  network  to  make  accurate\n",
            "predictions, so training tends to make embeddings useful representations of the cate‐\n",
            "gories. This is called representation learning (you will see other types of representation\n",
            "learning in Chapter 17).\n",
            "\n",
            "Figure 13-6. Embeddings will gradually improve during training\n",
            "\n",
            "Word Embeddings\n",
            "Not  only  will  embeddings  generally  be  useful  representations  for  the  task  at  hand,\n",
            "but  quite  often  these  same  embeddings  can  be  reused  successfully  for  other  tasks.\n",
            "The most common example of this is word embeddings (i.e., embeddings of individual\n",
            "words): when you are working on a natural language processing task, you are often\n",
            "better off reusing pretrained word embeddings than training your own.\n",
            "\n",
            "The  idea  of  using  vectors  to  represent  words  dates  back  to  the  1960s,  and  many\n",
            "sophisticated  techniques  have  been  used  to  generate  useful  vectors,  including  using\n",
            "neural networks. But things really took off in 2013, when Tomáš Mikolov and other\n",
            "Google researchers published a paper6 describing an efficient technique to learn word\n",
            "embeddings  using  neural  networks,  significantly  outperforming  previous  attempts.\n",
            "\n",
            "6 Tomáš Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality”,\n",
            "\n",
            "Proceedings of the 26th International Conference on Neural Information Processing Systems 2 (2013): 3111–\n",
            "3119.\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "467\n",
            "\n",
            "\fThis  allowed  them  to  learn  embeddings  on  a  very  large  corpus  of  text:  they  trained\n",
            "a neural network to predict the words near any given word and obtained astounding\n",
            "word embeddings. For example, synonyms had very close embeddings, and semanti‐\n",
            "cally related words such as France, Spain, and Italy ended up clustered together.\n",
            "\n",
            "It’s  not  just  about  proximity,  though:  word  embeddings  were  also  organized  along\n",
            "meaningful axes in the embedding space. Here is a famous example: if you compute\n",
            "King  –  Man  +  Woman  (adding  and  subtracting  the  embedding  vectors  of  these\n",
            "words),  then  the  result  will  be  very  close  to  the  embedding  of  the  word  Queen  (see\n",
            "Figure  13-7).  In  other  words,  the  word  embeddings  encode  the  concept  of  gender!\n",
            "Similarly,  you  can  compute  Madrid  –  Spain  +  France,  and  the  result  is  close  to\n",
            "Paris,  which  seems  to  show  that  the  notion  of  capital  city  was  also  encoded  in  the\n",
            "embeddings.\n",
            "\n",
            "Figure 13-7. Word embeddings of similar words tend to be close, and some axes seem to\n",
            "encode meaningful concepts\n",
            "\n",
            "Unfortunately,  word  embeddings  sometimes  capture  our  worst  biases.  For  example,\n",
            "although  they  correctly  learn  that  Man  is  to  King  as  Woman  is  to  Queen,  they  also\n",
            "seem  to  learn  that  Man  is  to  Doctor  as  Woman  is  to  Nurse:  quite  a  sexist  bias!  To\n",
            "be  fair,  this  particular  example  is  probably  exaggerated,  as  was  pointed  out  in  a\n",
            "2019 paper7 by Malvina Nissim et al. Nevertheless, ensuring fairness in deep learning\n",
            "algorithms is an important and active research topic.\n",
            "\n",
            "Keras  provides  an  Embedding  layer,  which  wraps  an  embedding  matrix:  this  matrix\n",
            "has one row per category and one column per embedding dimension. By default, it\n",
            "is  initialized  randomly.  To  convert  a  category  ID  to  an  embedding,  the  Embedding\n",
            "layer  just  looks  up  and  returns  the  row  that  corresponds  to  that  category.  That’s  all\n",
            "\n",
            "7 Malvina Nissim et al., “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv\n",
            "\n",
            "preprint arXiv:1905.09866 (2019).\n",
            "\n",
            "468 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fthere  is  to  it!  For  example,  let’s  initialize  an  Embedding  layer  with  five  rows  and  2D\n",
            "embeddings, and use it to encode some categories:\n",
            "\n",
            ">>> tf.random.set_seed(42)\n",
            ">>> embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
            ">>> embedding_layer(np.array([2, 4, 2]))\n",
            "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
            "array([[-0.04663396,  0.01846724],\n",
            "       [-0.02736737, -0.02768031],\n",
            "       [-0.04663396,  0.01846724]], dtype=float32)>\n",
            "\n",
            "As  you  can  see,  category  2  gets  encoded  (twice)  as  the  2D  vector  [-0.04663396,\n",
            "0.01846724], while category 4 gets encoded as [-0.02736737, -0.02768031]. Since\n",
            "the layer is not trained yet, these encodings are just random.\n",
            "\n",
            "An  Embedding  layer  is  initialized  randomly,  so  it  does  not  make\n",
            "sense  to  use  it  outside  of  a  model  as  a  standalone  preprocessing\n",
            "layer unless you initialize it with pretrained weights.\n",
            "\n",
            "If  you  want  to  embed  a  categorical  text  attribute,  you  can  simply  chain  a  String\n",
            "Lookup layer and an Embedding layer, like this:\n",
            "\n",
            ">>> tf.random.set_seed(42)\n",
            ">>> ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
            ">>> str_lookup_layer = tf.keras.layers.StringLookup()\n",
            ">>> str_lookup_layer.adapt(ocean_prox)\n",
            ">>> lookup_and_embed = tf.keras.Sequential([\n",
            "...     str_lookup_layer,\n",
            "...     tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n",
            "...                               output_dim=2)\n",
            "... ])\n",
            "...\n",
            ">>> lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))\n",
            "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
            "array([[-0.01896119,  0.02223358],\n",
            "       [ 0.02401174,  0.03724445],\n",
            "       [-0.01896119,  0.02223358]], dtype=float32)>\n",
            "\n",
            "Note  that  the  number  of  rows  in  the  embedding  matrix  needs  to  be  equal  to  the\n",
            "vocabulary size: that’s the total number of categories, including the known categories\n",
            "plus the OOV buckets (just one by default). The vocabulary_size() method of the\n",
            "StringLookup class conveniently returns this number.\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "469\n",
            "\n",
            "\fIn  this  example  we  used  2D  embeddings,  but  as  a  rule  of  thumb\n",
            "embeddings typically have 10 to 300 dimensions, depending on the\n",
            "task, the vocabulary size, and the size of your training set. You will\n",
            "have to tune this hyperparameter.\n",
            "\n",
            "Putting  everything  together,  we  can  now  create  a  Keras  model  that  can  process  a\n",
            "categorical text feature along with regular numerical features and learn an embedding\n",
            "for each category (as well as for each OOV bucket):\n",
            "\n",
            "X_train_num, X_train_cat, y_train = [...]  # load the training set\n",
            "X_valid_num, X_valid_cat, y_valid = [...]  # and the validation set\n",
            "\n",
            "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
            "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
            "cat_embeddings = lookup_and_embed(cat_input)\n",
            "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
            "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
            "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
            "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
            "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n",
            "                    validation_data=((X_valid_num, X_valid_cat), y_valid))\n",
            "\n",
            "This  model  takes  two  inputs:  num_input,  which  contains  eight  numerical  features\n",
            "per  instance,  plus  cat_input,  which  contains  a  single  categorical  text  input  per\n",
            "instance. The model uses the lookup_and_embed model we created earlier to encode\n",
            "each  ocean-proximity  category  as  the  corresponding  trainable  embedding.  Next,  it\n",
            "concatenates  the  numerical  inputs  and  the  embeddings  using  the  concatenate()\n",
            "function  to  produce  the  complete  encoded  inputs,  which  are  ready  to  be  fed  to  a\n",
            "neural  network.  We  could  add  any  kind  of  neural  network  at  this  point,  but  for\n",
            "simplicity we just add a single dense output layer, and then we create the Keras Model\n",
            "with the inputs and output we’ve just defined. Next we compile the model and train\n",
            "it, passing both the numerical and categorical inputs.\n",
            "\n",
            "As  you  saw  in  Chapter  10,  since  the  Input  layers  are  named  \"num\"  and  \"cat\",  we\n",
            "could  also  have  passed  the  training  data  to  the  fit()  method  using  a  dictionary\n",
            "instead  of  a  tuple:  {\"num\":  X_train_num,  \"cat\":  X_train_cat}.  Alternatively,\n",
            "we  could  have  passed  a  tf.data.Dataset  containing  batches,  each  represented  as\n",
            "((X_batch_num,  X_batch_cat),  y_batch)  or  as  ({\"num\":  X_batch_num,  \"cat\":\n",
            "X_batch_cat}, y_batch). And of course the same goes for the validation data.\n",
            "\n",
            "470 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fOne-hot  encoding  followed  by  a  Dense  layer  (with  no  activation\n",
            "function  and  no  biases)  is  equivalent  to  an  Embedding  layer.\n",
            "However,  the  Embedding  layer  uses  way  fewer  computations  as  it\n",
            "avoids  many  multiplications  by  zero—the  performance  difference\n",
            "becomes  clear  when  the  size  of  the  embedding  matrix  grows.\n",
            "The  Dense  layer’s  weight  matrix  plays  the  role  of  the  embedding\n",
            "matrix. For example, using one-hot vectors of size 20 and a Dense\n",
            "layer with 10 units is equivalent to using an Embedding layer with\n",
            "input_dim=20 and output_dim=10. As a result, it would be wasteful\n",
            "to  use  more  embedding  dimensions  than  the  number  of  units  in\n",
            "the layer that follows the Embedding layer.\n",
            "\n",
            "OK, now that you have learned how to encode categorical features, it’s time to turn\n",
            "our attention to text preprocessing.\n",
            "\n",
            "Text Preprocessing\n",
            "Keras provides a TextVectorization layer for basic text preprocessing. Much like the\n",
            "StringLookup layer, you must either pass it a vocabulary upon creation, or let it learn\n",
            "the vocabulary from some training data using the  adapt() method. Let’s look at an\n",
            "example:\n",
            "\n",
            ">>> train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
            ">>> text_vec_layer = tf.keras.layers.TextVectorization()\n",
            ">>> text_vec_layer.adapt(train_data)\n",
            ">>> text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n",
            "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
            "array([[2, 1, 0, 0],\n",
            "       [6, 2, 1, 2]])>\n",
            "\n",
            "The  two  sentences  “Be  good!”  and  “Question:  be  or  be?”  were  encoded  as  [2,  1,\n",
            "0,  0]  and  [6,  2,  1,  2],  respectively.  The  vocabulary  was  learned  from  the  four\n",
            "sentences  in  the  training  data:  “be”  =  2,  “to”  =  3,  etc.  To  construct  the  vocabulary,\n",
            "the adapt() method first converted the training sentences to lowercase and removed\n",
            "punctuation,  which  is  why  “Be”,  “be”,  and  “be?”  are  all  encoded  as  “be”  =  2.  Next,\n",
            "the  sentences  were  split  on  whitespace,  and  the  resulting  words  were  sorted  by\n",
            "descending  frequency,  producing  the  final  vocabulary.  When  encoding  sentences,\n",
            "unknown words get encoded as 1s. Lastly, since the first sentence is shorter than the\n",
            "second, it was padded with 0s.\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "471\n",
            "\n",
            "\fThe TextVectorization layer has many options. For example, you\n",
            "can  preserve  the  case  and  punctuation  if  you  want,  by  setting\n",
            "standardize=None,  or  you  can  pass  any  standardization  function\n",
            "you please as the standardize argument. You can prevent splitting\n",
            "by setting split=None, or you can pass your own splitting function\n",
            "instead.  You  can  set  the  output_sequence_length  argument  to\n",
            "ensure that the output sequences all get cropped or padded to the\n",
            "desired length, or you can set ragged=True to get a ragged tensor\n",
            "instead of a regular tensor. Please check out the documentation for\n",
            "more options.\n",
            "\n",
            "The word IDs must be encoded, typically using an  Embedding layer: we will do this\n",
            "in Chapter 16. Alternatively, you can set the TextVectorization layer’s output_mode\n",
            "argument to \"multi_hot\" or \"count\" to get the corresponding encodings. However,\n",
            "simply counting words is usually not ideal: words like “to” and “the” are so frequent\n",
            "that  they  hardly  matter  at  all,  whereas,  rarer  words  such  as  “basketball”  are  much\n",
            "more informative. So, rather than setting output_mode to \"multi_hot\" or \"count\", it\n",
            "is usually preferable to set it to \"tf_idf\", which stands for term-frequency × inverse-\n",
            "document-frequency (TF-IDF). This is similar to the count encoding, but words that\n",
            "occur  frequently  in  the  training  data  are  downweighted,  and  conversely,  rare  words\n",
            "are upweighted. For example:\n",
            "\n",
            ">>> text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
            ">>> text_vec_layer.adapt(train_data)\n",
            ">>> text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n",
            "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
            "array([[0.96725637, 0.6931472 , 0. , 0. , 0. , 0.        ],\n",
            "       [0.96725637, 1.3862944 , 0. , 0. , 0. , 1.0986123 ]], dtype=float32)>\n",
            "\n",
            "There  are  many  TF-IDF  variants,  but  the  way  the  TextVectorization  layer  imple‐\n",
            "ments it is by multiplying each word count by a weight equal to log(1 + d / (f + 1)),\n",
            "where d is the total number of sentences (a.k.a., documents) in the training data and\n",
            "f counts how many of these training sentences contain the given word. For example,\n",
            "in this case there are d = 4 sentences in the training data, and the word “be” appears\n",
            "in f = 3 of these. Since the word “be” occurs twice in the sentence “Question: be or\n",
            "be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3)) ≈ 1.3862944. The word “question” only\n",
            "appears once, but since it is a less common word, its encoding is almost as high: 1 ×\n",
            "log(1 + 4 / (1 + 1)) ≈ 1.0986123. Note that the average weight is used for unknown\n",
            "words.\n",
            "\n",
            "472 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fThis approach to text encoding is straightforward to use and it can give fairly good\n",
            "results for basic natural language processing tasks, but it has several important lim‐\n",
            "itations:  it  only  works  with  languages  that  separate  words  with  spaces,  it  doesn’t\n",
            "distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it gives no hint\n",
            "to your model that words like “evolution” and “evolutionary” are related, etc. And if\n",
            "you use multi-hot, count, or TF-IDF encoding, then the order of the words is lost. So\n",
            "what are the other options?\n",
            "\n",
            "One option is to use the TensorFlow Text library, which provides more advanced text\n",
            "preprocessing  features  than  the  TextVectorization  layer.  For  example,  it  includes\n",
            "several  subword  tokenizers  capable  of  splitting  the  text  into  tokens  smaller  than\n",
            "words,  which  makes  it  possible  for  the  model  to  more  easily  detect  that  “evolution”\n",
            "and  “evolutionary”  have  something  in  common  (more  on  subword  tokenization  in\n",
            "Chapter 16).\n",
            "\n",
            "Yet another option is to use pretrained language model components. Let’s look at this\n",
            "now.\n",
            "\n",
            "Using Pretrained Language Model Components\n",
            "The  TensorFlow  Hub  library  makes  it  easy  to  reuse  pretrained  model  components\n",
            "in  your  own  models,  for  text,  image,  audio,  and  more.  These  model  components\n",
            "are  called  modules.  Simply  browse  the  TF  Hub  repository,  find  the  one  you  need,\n",
            "and copy the code example into your project, and the module will be automatically\n",
            "downloaded  and  bundled  into  a  Keras  layer  that  you  can  directly  include  in  your\n",
            "model.  Modules  typically  contain  both  preprocessing  code  and  pretrained  weights,\n",
            "and they generally require no extra training (but of course, the rest of your model will\n",
            "certainly require training).\n",
            "\n",
            "For  example,  some  powerful  pretrained  language  models  are  available.  The  most\n",
            "powerful  are  quite  large  (several  gigabytes),  so  for  a  quick  example  let’s  use  the\n",
            "nnlm-en-dim50 module, version 2, which is a fairly basic module that takes raw text\n",
            "as input and outputs 50-dimensional sentence embeddings. We’ll import TensorFlow\n",
            "Hub and use it to load the module, then use that module to encode two sentences to\n",
            "vectors:8\n",
            "\n",
            ">>> import tensorflow_hub as hub\n",
            ">>> hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
            ">>> sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
            ">>> sentence_embeddings.numpy().round(2)\n",
            "array([[-0.25,  0.28,  0.01,  0.1 ,  [...] ,  0.05,  0.31],\n",
            "       [-0.2 ,  0.2 , -0.08,  0.02,  [...] , -0.04,  0.15]], dtype=float32)\n",
            "\n",
            "8 TensorFlow Hub is not bundled with TensorFlow, but if you are running on Colab or if you followed the\n",
            "\n",
            "installation instructions at https://homl.info/install, then it’s already installed.\n",
            "\n",
            "Keras Preprocessing Layers \n",
            "\n",
            "| \n",
            "\n",
            "473\n",
            "\n",
            "\fThe hub.KerasLayer layer downloads the module from the given URL. This particu‐\n",
            "lar  module  is  a  sentence  encoder:  it  takes  strings  as  input  and  encodes  each  one  as\n",
            "a single vector (in this case, a 50-dimensional vector). Internally, it parses the string\n",
            "(splitting words on spaces) and embeds each word using an embedding matrix that\n",
            "was  pretrained  on  a  huge  corpus:  the  Google  News  7B  corpus  (seven  billion  words\n",
            "long!). Then it computes the mean of all the word embeddings, and the result is the\n",
            "sentence embedding.9\n",
            "\n",
            "You just need to include this hub_layer in your model, and you’re ready to go. Note\n",
            "that  this  particular  language  model  was  trained  on  the  English  language,  but  many\n",
            "other languages are available, as well as multilingual models.\n",
            "\n",
            "Last  but  not  least,  the  excellent  open  source  Transformers  library  by  Hugging  Face\n",
            "also makes it easy to include powerful language model components inside your own\n",
            "models. You can browse the Hugging Face Hub, choose the model you want, and use\n",
            "the provided code examples to get started. It used to contain only language models,\n",
            "but it has now expanded to include image models and more.\n",
            "\n",
            "We will come back to natural language processing in more depth in Chapter 16. Let’s\n",
            "now look at Keras’s image preprocessing layers.\n",
            "\n",
            "Image Preprocessing Layers\n",
            "The Keras preprocessing API includes three image preprocessing layers:\n",
            "\n",
            "• tf.keras.layers.Resizing  resizes  the  input  images  to  the  desired  size.  For\n",
            "•\n",
            "example,  Resizing(height=100,  width=200)  resizes  each  image  to  100  ×  200,\n",
            "possibly  distorting  the  image.  If  you  set  crop_to_aspect_ratio=True,  then  the\n",
            "image will be cropped to the target image ratio, to avoid distortion.\n",
            "\n",
            "• tf.keras.layers.Rescaling  rescales  the  pixel  values.  For  example,  Rescaling\n",
            "•\n",
            "\n",
            "(scale=2/255, offset=-1) scales the values from 0 → 255 to –1 → 1.\n",
            "\n",
            "• tf.keras.layers.CenterCrop  crops  the  image,  keeping  only  a  center  patch  of\n",
            "•\n",
            "\n",
            "the desired height and width.\n",
            "\n",
            "For example, let’s load a couple of sample images and center-crop them. For this, we\n",
            "will use Scikit-Learn’s load_sample_images() function; this loads two color images,\n",
            "one  of  a  Chinese  temple  and  the  other  of  a  flower  (this  requires  the  Pillow  library,\n",
            "which  should  already  be  installed  if  you  are  using  Colab  or  if  you  followed  the\n",
            "installation instructions):\n",
            "\n",
            "9 To be precise, the sentence embedding is equal to the mean word embedding multiplied by the square root of\n",
            "the number of words in the sentence. This compensates for the fact that the mean of n random vectors gets\n",
            "shorter as n grows.\n",
            "\n",
            "474 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\ffrom sklearn.datasets import load_sample_images\n",
            "\n",
            "images = load_sample_images()[\"images\"]\n",
            "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
            "cropped_images = crop_image_layer(images)\n",
            "\n",
            "Keras  also  includes  several  layers  for  data  augmentation,  such  as  RandomCrop,\n",
            "RandomFlip,  RandomTranslation,  RandomRotation,  RandomZoom,  RandomHeight,\n",
            "RandomWidth,  and  RandomContrast.  These  layers  are  only  active  during  training,\n",
            "and they randomly apply some transformation to the input images (their names are\n",
            "self-explanatory). Data augmentation will artificially increase the size of the training\n",
            "set, which often leads to improved performance, as long as the transformed images\n",
            "look like realistic (nonaugmented) images. We’ll cover image processing more closely\n",
            "in the next chapter.\n",
            "\n",
            "Under  the  hood,  the  Keras  preprocessing  layers  are  based  on\n",
            "TensorFlow’s  low-level  API.  For  example,  the  Normalization\n",
            "layer  uses  tf.nn.moments()  to  compute  both  the  mean  and  var‐\n",
            "iance,  the  Discretization  layer  uses  tf.raw_ops.Bucketize(),\n",
            "CategoricalEncoding  uses  tf.math.bincount(),  IntegerLookup\n",
            "and  StringLookup  use  the  tf.lookup  package,  Hashing  and\n",
            "TextVectorization use several ops from the tf.strings package,\n",
            "Embedding  uses  tf.nn.embedding_lookup(),  and  the  image  pre‐\n",
            "processing  layers  use  the  ops  from  the  tf.image  package.  If  the\n",
            "Keras  preprocessing  API  isn’t  sufficient  for  your  needs,  you  may\n",
            "occasionally need to use TensorFlow’s low-level API directly.\n",
            "\n",
            "Now let’s look at another way to load data easily and efficiently in TensorFlow.\n",
            "\n",
            "The TensorFlow Datasets Project\n",
            "The  TensorFlow  Datasets  (TFDS)  project  makes  it  very  easy  to  load  common  data‐\n",
            "sets, from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet\n",
            "(you will need quite a bit of disk space!). The list includes image datasets, text datasets\n",
            "(including  translation  datasets),  audio  and  video  datasets,  time  series,  and  much\n",
            "more. You can visit https://homl.info/tfds to view the full list, along with a description\n",
            "of each dataset. You can also check out Know Your Data, which is a tool to explore\n",
            "and understand many of the datasets provided by TFDS.\n",
            "\n",
            "The TensorFlow Datasets Project \n",
            "\n",
            "| \n",
            "\n",
            "475\n",
            "\n",
            "\fTFDS  is  not  bundled  with  TensorFlow,  but  if  you  are  running  on  Colab  or  if\n",
            "you followed the installation instructions at https://homl.info/install, then it’s already\n",
            "installed.  You  can  then  import  tensorflow_datasets,  usually  as  tfds,  then  call  the\n",
            "tfds.load() function, which will download the data you want (unless it was already\n",
            "downloaded  earlier)  and  return  the  data  as  a  dictionary  of  datasets  (typically  one\n",
            "for  training  and  one  for  testing,  but  this  depends  on  the  dataset  you  choose).  For\n",
            "example, let’s download MNIST:\n",
            "\n",
            "import tensorflow_datasets as tfds\n",
            "\n",
            "datasets = tfds.load(name=\"mnist\")\n",
            "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
            "\n",
            "You  can  then  apply  any  transformation  you  want  (typically  shuffling,  batching,  and\n",
            "prefetching), and you’re ready to train your model. Here is a simple example:\n",
            "\n",
            "for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
            "    images = batch[\"image\"]\n",
            "    labels = batch[\"label\"]\n",
            "    # [...] do something with the images and labels\n",
            "\n",
            "The  load()  function  can  shuffle  the  files  it  downloads:  just  set\n",
            "shuffle_files=True. However, this may be insufficient, so it’s best\n",
            "to shuffle the training data some more.\n",
            "\n",
            "Note that each item in the dataset is a dictionary containing both the features and the\n",
            "labels. But Keras expects each item to be a tuple containing two elements (again, the\n",
            "features and the labels). You could transform the dataset using the map() method, like\n",
            "this:\n",
            "\n",
            "mnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\n",
            "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
            "mnist_train = mnist_train.prefetch(1)\n",
            "\n",
            "But  it’s  simpler  to  ask  the  load()  function  to  do  this  for  you  by  setting\n",
            "as_supervised=True (obviously this works only for labeled datasets).\n",
            "\n",
            "Lastly, TFDS provides a convenient way to split the data using the split argument.\n",
            "For  example,  if  you  want  to  use  the  first  90%  of  the  training  set  for  training,  the\n",
            "remaining  10%  for  validation,  and  the  whole  test  set  for  testing,  then  you  can  set\n",
            "split=[\"train[:90%]\", \"train[90%:]\", \"test\"]. The load() function will return\n",
            "all  three  sets.  Here  is  a  complete  example,  loading  and  splitting  the  MNIST  dataset\n",
            "using TFDS, then using these sets to train and evaluate a simple Keras model:\n",
            "\n",
            "476 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\ftrain_set, valid_set, test_set = tfds.load(\n",
            "    name=\"mnist\",\n",
            "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
            "    as_supervised=True\n",
            ")\n",
            "train_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\n",
            "valid_set = valid_set.batch(32).cache()\n",
            "test_set = test_set.batch(32).cache()\n",
            "tf.random.set_seed(42)\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
            "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
            "])\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
            "test_loss, test_accuracy = model.evaluate(test_set)\n",
            "\n",
            "Congratulations, you’ve reached the end of this quite technical chapter! You may feel\n",
            "that  it  is  a  bit  far  from  the  abstract  beauty  of  neural  networks,  but  the  fact  is  deep\n",
            "learning often involves large amounts of data, and knowing how to load, parse, and\n",
            "preprocess  it  efficiently  is  a  crucial  skill  to  have.  In  the  next  chapter,  we  will  look\n",
            "at  convolutional  neural  networks,  which  are  among  the  most  successful  neural  net\n",
            "architectures for image processing and many other applications.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. Why would you want to use the tf.data API?\n",
            "\n",
            "2.\n",
            "2. What are the benefits of splitting a large dataset into multiple files?\n",
            "\n",
            "3.\n",
            "3. During  training,  how  can  you  tell  that  your  input  pipeline  is  the  bottleneck?\n",
            "\n",
            "What can you do to fix it?\n",
            "\n",
            "4. Can  you  save  any  binary  data  to  a  TFRecord  file,  or  only  serialized  protocol\n",
            "4.\n",
            "\n",
            "buffers?\n",
            "\n",
            "5. Why would you go through the hassle of converting all your data to the Example\n",
            "5.\n",
            "\n",
            "protobuf format? Why not use your own protobuf definition?\n",
            "\n",
            "6. When  using  TFRecords,  when  would  you  want  to  activate  compression?  Why\n",
            "6.\n",
            "\n",
            "not do it systematically?\n",
            "\n",
            "7. Data  can  be  preprocessed  directly  when  writing  the  data  files,  or  within  the\n",
            "7.\n",
            "tf.data pipeline, or in preprocessing layers within your model. Can you list a few\n",
            "pros and cons of each option?\n",
            "\n",
            "8.\n",
            "8. Name  a  few  common  ways  you  can  encode  categorical  integer  features.  What\n",
            "\n",
            "about text?\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "477\n",
            "\n",
            "\f9. Load  the  Fashion  MNIST  dataset  (introduced  in  Chapter  10);  split  it  into  a\n",
            "9.\n",
            "training set, a validation set, and a test set; shuffle the training set; and save each\n",
            "dataset  to  multiple  TFRecord  files.  Each  record  should  be  a  serialized  Example\n",
            "protobuf with two features: the serialized image (use tf.io.serialize_tensor()\n",
            "to  serialize  each  image),  and  the  label.10  Then  use  tf.data  to  create  an  efficient\n",
            "dataset for each set. Finally, use a Keras model to train these datasets, including\n",
            "a  preprocessing  layer  to  standardize  each  input  feature.  Try  to  make  the  input\n",
            "pipeline as efficient as possible, using TensorBoard to visualize profiling data.\n",
            "10. In  this  exercise  you  will  download  a  dataset,  split  it,  create  a  tf.data.Dataset\n",
            "10.\n",
            "to load it and preprocess it efficiently, then build and train a binary classification\n",
            "model containing an Embedding layer:\n",
            "\n",
            "a. Download  the  Large  Movie  Review  Dataset,  which  contains  50,000  movie\n",
            "a.\n",
            "reviews  from  the  Internet  Movie  Database  (IMDb).  The  data  is  organized\n",
            "in  two  directories,  train  and  test,  each  containing  a  pos  subdirectory  with\n",
            "12,500 positive reviews and a neg subdirectory with 12,500 negative reviews.\n",
            "Each review is stored in a separate text file. There are other files and folders\n",
            "(including  preprocessed  bag-of-words  versions),  but  we  will  ignore  them  in\n",
            "this exercise.\n",
            "\n",
            "b.\n",
            "b. Split the test set into a validation set (15,000) and a test set (10,000).\n",
            "\n",
            "c. Use tf.data to create an efficient dataset for each set.\n",
            "c.\n",
            "d. Create  a  binary  classification  model,  using  a  TextVectorization  layer  to\n",
            "d.\n",
            "\n",
            "preprocess each review.\n",
            "\n",
            "e. Add an  Embedding layer and compute the mean embedding for each review,\n",
            "e.\n",
            "multiplied by the square root of the number of words (see Chapter 16). This\n",
            "rescaled mean embedding can then be passed to the rest of your model.\n",
            "\n",
            "f. Train the model and see what accuracy you get. Try to optimize your pipelines\n",
            "f.\n",
            "\n",
            "to make training as fast as possible.\n",
            "\n",
            "g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\").\n",
            "g.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "10 For large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would\n",
            "\n",
            "lose a bit of image quality.\n",
            "\n",
            "478 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "\fCHAPTER 14\n",
            "Deep Computer Vision Using\n",
            "Convolutional Neural Networks\n",
            "\n",
            "Although  IBM’s  Deep  Blue  supercomputer  beat  the  chess  world  champion  Garry\n",
            "Kasparov  back  in  1996,  it  wasn’t  until  fairly  recently  that  computers  were  able  to\n",
            "reliably  perform  seemingly  trivial  tasks  such  as  detecting  a  puppy  in  a  picture  or\n",
            "recognizing  spoken  words.  Why  are  these  tasks  so  effortless  to  us  humans?  The\n",
            "answer  lies  in  the  fact  that  perception  largely  takes  place  outside  the  realm  of  our\n",
            "consciousness, within specialized visual, auditory, and other sensory modules in our\n",
            "brains.  By  the  time  sensory  information  reaches  our  consciousness,  it  is  already\n",
            "adorned  with  high-level  features;  for  example,  when  you  look  at  a  picture  of  a  cute\n",
            "puppy,  you  cannot  choose  not  to  see  the  puppy,  not  to  notice  its  cuteness.  Nor  can\n",
            "you explain how you recognize a cute puppy; it’s just obvious to you. Thus, we cannot\n",
            "trust our subjective experience: perception is not trivial at all, and to understand it we\n",
            "must look at how our sensory modules work.\n",
            "\n",
            "Convolutional  neural  networks  (CNNs)  emerged  from  the  study  of  the  brain’s  visual\n",
            "cortex,  and  they  have  been  used  in  computer  image  recognition  since  the  1980s.\n",
            "Over the last 10 years, thanks to the increase in computational power, the amount of\n",
            "available training data, and the tricks presented in Chapter 11 for training deep nets,\n",
            "CNNs  have  managed  to  achieve  superhuman  performance  on  some  complex  visual\n",
            "tasks. They power image search services, self-driving cars, automatic video classifica‐\n",
            "tion  systems,  and  more.  Moreover,  CNNs  are  not  restricted  to  visual  perception:\n",
            "they  are  also  successful  at  many  other  tasks,  such  as  voice  recognition  and  natural\n",
            "language processing. However, we will focus on visual applications for now.\n",
            "\n",
            "479\n",
            "\n",
            "\fIn  this  chapter  we  will  explore  where  CNNs  came  from,  what  their  building  blocks\n",
            "look  like,  and  how  to  implement  them  using  Keras.  Then  we  will  discuss  some  of\n",
            "the  best  CNN  architectures,  as  well  as  other  visual  tasks,  including  object  detection\n",
            "(classifying multiple objects in an image and placing bounding boxes around them)\n",
            "and semantic segmentation (classifying each pixel according to the class of the object\n",
            "it belongs to).\n",
            "\n",
            "The Architecture of the Visual Cortex\n",
            "David  H.  Hubel  and  Torsten  Wiesel  performed  a  series  of  experiments  on  cats  in\n",
            "19581 and 19592 (and a few years later on monkeys3), giving crucial insights into the\n",
            "structure of the visual cortex (the authors received the Nobel Prize in Physiology or\n",
            "Medicine  in  1981  for  their  work).  In  particular,  they  showed  that  many  neurons  in\n",
            "the  visual  cortex  have  a  small  local  receptive  field,  meaning  they  react  only  to  visual\n",
            "stimuli  located  in  a  limited  region  of  the  visual  field  (see  Figure  14-1,  in  which  the\n",
            "local receptive fields of five neurons are represented by dashed circles). The receptive\n",
            "fields of different neurons may overlap, and together they tile the whole visual field.\n",
            "\n",
            "Figure 14-1. Biological neurons in the visual cortex respond to specific patterns in small\n",
            "regions of the visual field called receptive fields; as the visual signal makes its way\n",
            "through consecutive brain modules, neurons respond to more complex patterns in larger\n",
            "receptive fields\n",
            "\n",
            "1 David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats”, The Journal of Physiology 147\n",
            "\n",
            "(1959): 226–238.\n",
            "\n",
            "2 David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex”, The\n",
            "\n",
            "Journal of Physiology 148 (1959): 574–591.\n",
            "\n",
            "3 David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate\n",
            "\n",
            "Cortex”, The Journal of Physiology 195 (1968): 215–243.\n",
            "\n",
            "480 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fMoreover, the authors showed that some neurons react only to images of horizontal\n",
            "lines,  while  others  react  only  to  lines  with  different  orientations  (two  neurons  may\n",
            "have  the  same  receptive  field  but  react  to  different  line  orientations).  They  also\n",
            "noticed that some neurons have larger receptive fields, and they react to more com‐\n",
            "plex  patterns  that  are  combinations  of  the  lower-level  patterns.  These  observations\n",
            "led to the idea that the higher-level neurons are based on the outputs of neighboring\n",
            "lower-level  neurons  (in  Figure  14-1,  notice  that  each  neuron  is  connected  only  to\n",
            "nearby neurons from the previous layer). This powerful architecture is able to detect\n",
            "all sorts of complex patterns in any area of the visual field.\n",
            "\n",
            "These  studies  of  the  visual  cortex  inspired  the  neocognitron,4  introduced  in  1980,\n",
            "which  gradually  evolved  into  what  we  now  call  convolutional  neural  networks.  An\n",
            "important  milestone  was  a  1998  paper5  by  Yann  LeCun  et  al.  that  introduced  the\n",
            "famous LeNet-5 architecture, which became widely used by banks to recognize hand‐\n",
            "written digits on checks. This architecture has some building blocks that you already\n",
            "know,  such  as  fully  connected  layers  and  sigmoid  activation  functions,  but  it  also\n",
            "introduces two new building blocks: convolutional layers and pooling layers. Let’s look\n",
            "at them now.\n",
            "\n",
            "Why  not  simply  use  a  deep  neural  network  with  fully  connec‐\n",
            "ted  layers  for  image  recognition  tasks?  Unfortunately,  although\n",
            "this  works  fine  for  small  images  (e.g.,  MNIST),  it  breaks  down\n",
            "for  larger  images  because  of  the  huge  number  of  parameters  it\n",
            "requires. For example, a 100 × 100–pixel image has 10,000 pixels,\n",
            "and if the first layer has just 1,000 neurons (which already severely\n",
            "restricts the amount of information transmitted to the next layer),\n",
            "this means a total of 10 million connections. And that’s just the first\n",
            "layer. CNNs solve this problem using partially connected layers and\n",
            "weight sharing.\n",
            "\n",
            "Convolutional Layers\n",
            "The most important building block of a CNN is the convolutional layer:6 neurons in\n",
            "the first convolutional layer are not connected to every single pixel in the input image\n",
            "(like they were in the layers discussed in previous chapters), but only to pixels in their\n",
            "\n",
            "4 Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern\n",
            "\n",
            "Recognition Unaffected by Shift in Position”, Biological Cybernetics 36 (1980): 193–202.\n",
            "\n",
            "5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition”, Proceedings of the IEEE 86,\n",
            "\n",
            "no. 11 (1998): 2278–2324.\n",
            "\n",
            "6 A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
            "their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform\n",
            "and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\n",
            "similar to convolutions (see https://homl.info/76 for more details).\n",
            "\n",
            "Convolutional Layers \n",
            "\n",
            "| \n",
            "\n",
            "481\n",
            "\n",
            "\freceptive  fields  (see  Figure  14-2).  In  turn,  each  neuron  in  the  second  convolutional\n",
            "layer is connected only to neurons located within a small rectangle in the first layer.\n",
            "This architecture allows the network to concentrate on small low-level features in the\n",
            "first  hidden  layer,  then  assemble  them  into  larger  higher-level  features  in  the  next\n",
            "hidden layer, and so on. This hierarchical structure is common in real-world images,\n",
            "which is one of the reasons why CNNs work so well for image recognition.\n",
            "\n",
            "Figure 14-2. CNN layers with rectangular local receptive fields\n",
            "\n",
            "All the multilayer neural networks we’ve looked at so far had layers\n",
            "composed  of  a  long  line  of  neurons,  and  we  had  to  flatten  input\n",
            "images to 1D before feeding them to the neural network. In a CNN\n",
            "each  layer  is  represented  in  2D,  which  makes  it  easier  to  match\n",
            "neurons with their corresponding inputs.\n",
            "\n",
            "A neuron located in row i, column j of a given layer is connected to the outputs of\n",
            "the neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw\n",
            "– 1, where fh and fw are the height and width of the receptive field (see Figure 14-3).\n",
            "In  order  for  a  layer  to  have  the  same  height  and  width  as  the  previous  layer,  it  is\n",
            "common to add zeros around the inputs, as shown in the diagram. This is called zero\n",
            "padding.\n",
            "\n",
            "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
            "the  receptive  fields,  as  shown  in  Figure  14-4.  This  dramatically  reduces  the  model’s\n",
            "computational  complexity.  The  horizontal  or  vertical  step  size  from  one  receptive\n",
            "field  to  the  next  is  called  the  stride.  In  the  diagram,  a  5  ×  7  input  layer  (plus  zero\n",
            "padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in\n",
            "this example the stride is the same in both directions, but it does not have to be so). A\n",
            "neuron located in row i, column j in the upper layer is connected to the outputs of the\n",
            "\n",
            "482 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fneurons in the previous layer located in rows i × sh to i × sh + fh – 1, columns j × sw to\n",
            "j × sw + fw – 1, where sh and sw are the vertical and horizontal strides.\n",
            "\n",
            "Figure 14-3. Connections between layers and zero padding\n",
            "\n",
            "Figure 14-4. Reducing dimensionality using a stride of 2\n",
            "\n",
            "Convolutional Layers \n",
            "\n",
            "| \n",
            "\n",
            "483\n",
            "\n",
            "\fFilters\n",
            "A  neuron’s  weights  can  be  represented  as  a  small  image  the  size  of  the  receptive\n",
            "field.  For  example,  Figure  14-5  shows  two  possible  sets  of  weights,  called  filters  (or\n",
            "convolution kernels, or just kernels). The first one is represented as a black square with\n",
            "a vertical white line in the middle (it’s a 7 × 7 matrix full of 0s except for the central\n",
            "column,  which  is  full  of  1s);  neurons  using  these  weights  will  ignore  everything\n",
            "in  their  receptive  field  except  for  the  central  vertical  line  (since  all  inputs  will  be\n",
            "multiplied by 0, except for the ones in the central vertical line). The second filter is a\n",
            "black square with a horizontal white line in the middle. Neurons using these weights\n",
            "will ignore everything in their receptive field except for the central horizontal line.\n",
            "\n",
            "Figure 14-5. Applying two different filters to get two feature maps\n",
            "\n",
            "Now  if  all  neurons  in  a  layer  use  the  same  vertical  line  filter  (and  the  same  bias\n",
            "term), and you feed the network the input image shown in Figure 14-5 (the bottom\n",
            "image),  the  layer  will  output  the  top-left  image.  Notice  that  the  vertical  white  lines\n",
            "get  enhanced  while  the  rest  gets  blurred.  Similarly,  the  upper-right  image  is  what\n",
            "you  get  if  all  neurons  use  the  same  horizontal  line  filter;  notice  that  the  horizontal\n",
            "white  lines  get  enhanced  while  the  rest  is  blurred  out.  Thus,  a  layer  full  of  neurons\n",
            "using  the  same  filter  outputs  a  feature  map,  which  highlights  the  areas  in  an  image\n",
            "that activate the filter the most. But don’t worry, you won’t have to define the filters\n",
            "manually: instead, during training the convolutional layer will automatically learn the\n",
            "most useful filters for its task, and the layers above will learn to combine them into\n",
            "more complex patterns.\n",
            "\n",
            "484 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fStacking Multiple Feature Maps\n",
            "Up to now, for simplicity, I have represented the output of each convolutional layer\n",
            "as a 2D layer, but in reality a convolutional layer has multiple filters (you decide how\n",
            "many) and outputs one feature map per filter, so it is more accurately represented in\n",
            "3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons\n",
            "within  a  given  feature  map  share  the  same  parameters  (i.e.,  the  same  kernel  and\n",
            "bias  term).  Neurons  in  different  feature  maps  use  different  parameters.  A  neuron’s\n",
            "receptive  field  is  the  same  as  described  earlier,  but  it  extends  across  all  the  feature\n",
            "maps  of  the  previous  layer.  In  short,  a  convolutional  layer  simultaneously  applies\n",
            "multiple trainable filters to its inputs, making it capable of detecting multiple features\n",
            "anywhere in its inputs.\n",
            "\n",
            "Figure 14-6. Two convolutional layers with multiple filters each (kernels), processing a\n",
            "color image with three color channels; each convolutional layer outputs one feature map\n",
            "per filter\n",
            "\n",
            "Convolutional Layers \n",
            "\n",
            "| \n",
            "\n",
            "485\n",
            "\n",
            "\fThe fact that all neurons in a feature map share the same parame‐\n",
            "ters dramatically reduces the number of parameters in the model.\n",
            "Once the CNN has learned to recognize a pattern in one location,\n",
            "it  can  recognize  it  in  any  other  location.  In  contrast,  once  a  fully\n",
            "connected neural network has learned to recognize a pattern in one\n",
            "location, it can only recognize it in that particular location.\n",
            "\n",
            "Input images are also composed of multiple sublayers: one per color channel. As men‐\n",
            "tioned in Chapter 9, there are typically three: red, green, and blue (RGB). Grayscale\n",
            "images have just one channel, but some images may have many more—for example,\n",
            "satellite images that capture extra light frequencies (such as infrared).\n",
            "\n",
            "Specifically,  a  neuron  located  in  row  i,  column  j  of  the  feature  map  k  in  a  given\n",
            "convolutional layer l is connected to the outputs of the neurons in the previous layer\n",
            "l – 1, located in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across\n",
            "all feature maps (in layer l – 1). Note that, within a layer, all neurons located in the\n",
            "same row i and column j but in different feature maps are connected to the outputs of\n",
            "the exact same neurons in the previous layer.\n",
            "\n",
            "Equation 14-1 summarizes the preceding explanations in one big mathematical equa‐\n",
            "tion: it shows how to compute the output of a given neuron in a convolutional layer.\n",
            "It is a bit ugly due to all the different indices, but all it does is calculate the weighted\n",
            "sum of all the inputs, plus the bias term.\n",
            "\n",
            "Equation 14-1. Computing the output of a neuron in a convolutional layer\n",
            "\n",
            "fℎ − 1\n",
            "zi, j, k = bk + ∑\n",
            "u = 0\n",
            "\n",
            "fw − 1\n",
            "∑\n",
            "v = 0\n",
            "\n",
            "fn′ − 1\n",
            "∑\n",
            "k′ = 0\n",
            "\n",
            "xi′, j′, k′ × wu, v, k′, k with\n",
            "\n",
            "i′ = i × sℎ + u\n",
            "j′ = j × sw + v\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• zi, j, k is the output of the neuron located in row i, column j in feature map k of the\n",
            "•\n",
            "\n",
            "convolutional layer (layer l).\n",
            "\n",
            "• As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\n",
            "•\n",
            "the height and width of the receptive field, and fn′ is the number of feature maps\n",
            "in the previous layer (layer l – 1).\n",
            "\n",
            "• xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\n",
            "•\n",
            "\n",
            "map k′ (or channel k′ if the previous layer is the input layer).\n",
            "\n",
            "•\n",
            "• bk is the bias term for feature map k (in layer l). You can think of it as a knob that\n",
            "\n",
            "tweaks the overall brightness of the feature map k.\n",
            "\n",
            "486 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\f• wu, v, k′, k is the connection weight between any neuron in feature map k of the layer\n",
            "•\n",
            "l and its input located at row u, column v (relative to the neuron’s receptive field),\n",
            "and feature map k′.\n",
            "\n",
            "Let’s see how to create and use a convolutional layer using Keras.\n",
            "\n",
            "Implementing Convolutional Layers with Keras\n",
            "First,  let’s  load  and  preprocess  a  couple  of  sample  images,  using  Scikit-Learn’s\n",
            "load_sample_image()  function  and  Keras’s  CenterCrop  and  Rescaling  layers  (all\n",
            "of which were introduced in Chapter 13):\n",
            "\n",
            "from sklearn.datasets import load_sample_images\n",
            "import tensorflow as tf\n",
            "\n",
            "images = load_sample_images()[\"images\"]\n",
            "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
            "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)\n",
            "\n",
            "Let’s look at the shape of the images tensor:\n",
            "\n",
            ">>> images.shape\n",
            "TensorShape([2, 70, 120, 3])\n",
            "\n",
            "Yikes,  it’s  a  4D  tensor;  we  haven’t  seen  this  before!  What  do  all  these  dimensions\n",
            "mean? Well, there are two sample images, which explains the first dimension. Then\n",
            "each  image  is  70  ×  120,  since  that’s  the  size  we  specified  when  creating  the  Center\n",
            "Crop layer (the original images were 427 × 640). This explains the second and third\n",
            "dimensions.  And  lastly,  each  pixel  holds  one  value  per  color  channel,  and  there  are\n",
            "three of them—red, green, and blue—which explains the last dimension.\n",
            "\n",
            "Now  let’s  create  a  2D  convolutional  layer  and  feed  it  these  images  to  see  what\n",
            "comes  out.  For  this,  Keras  provides  a  Convolution2D  layer,  alias  Conv2D.  Under\n",
            "the  hood,  this  layer  relies  on  TensorFlow’s  tf.nn.conv2d()  operation.  Let’s  create  a\n",
            "convolutional layer with 32 filters, each of size 7 × 7 (using kernel_size=7, which is\n",
            "equivalent to using kernel_size=(7 , 7)), and apply this layer to our small batch of\n",
            "two images:\n",
            "\n",
            "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
            "fmaps = conv_layer(images)\n",
            "\n",
            "When  we  talk  about  a  2D  convolutional  layer,  “2D”  refers  to  the\n",
            "number  of  spatial  dimensions  (height  and  width),  but  as  you  can\n",
            "see, the layer takes 4D inputs: as we saw, the two additional dimen‐\n",
            "sions  are  the  batch  size  (first  dimension)  and  the  channels  (last\n",
            "dimension).\n",
            "\n",
            "Convolutional Layers \n",
            "\n",
            "| \n",
            "\n",
            "487\n",
            "\n",
            "\fNow let’s look at the output’s shape:\n",
            "\n",
            ">>> fmaps.shape\n",
            "TensorShape([2, 64, 114, 32])\n",
            "\n",
            "The output shape is similar to the input shape, with two main differences. First, there\n",
            "are 32 channels instead of 3. This is because we set filters=32, so we get 32 output\n",
            "feature maps: instead of the intensity of red, green, and blue at each location, we now\n",
            "have the intensity of each feature at each location. Second, the height and width have\n",
            "both  shrunk  by  6  pixels.  This  is  due  to  the  fact  that  the  Conv2D  layer  does  not  use\n",
            "any  zero-padding  by  default,  which  means  that  we  lose  a  few  pixels  on  the  sides  of\n",
            "the  output  feature  maps,  depending  on  the  size  of  the  filters.  In  this  case,  since  the\n",
            "kernel size is 7, we lose 6 pixels horizontally and 6 pixels vertically (i.e., 3 pixels on\n",
            "each side).\n",
            "\n",
            "The default option is surprisingly named padding=\"valid\", which\n",
            "actually  means  no  zero-padding  at  all!  This  name  comes  from\n",
            "the  fact  that  in  this  case  every  neuron’s  receptive  field  lies  strictly\n",
            "within  valid  positions  inside  the  input  (it  does  not  go  out  of\n",
            "bounds).  It’s  not  a  Keras  naming  quirk:  everyone  uses  this  odd\n",
            "nomenclature.\n",
            "\n",
            "If instead we set padding=\"same\", then the inputs are padded with enough zeros on\n",
            "all  sides  to  ensure  that  the  output  feature  maps  end  up  with  the  same  size  as  the\n",
            "inputs (hence the name of this option):\n",
            "\n",
            ">>> conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
            "...                                     padding=\"same\")\n",
            "...\n",
            ">>> fmaps = conv_layer(images)\n",
            ">>> fmaps.shape\n",
            "TensorShape([2, 70, 120, 32])\n",
            "\n",
            "These  two  padding  options  are  illustrated  in  Figure  14-7.  For  simplicity,  only  the\n",
            "horizontal  dimension  is  shown  here,  but  of  course  the  same  logic  applies  to  the\n",
            "vertical dimension as well.\n",
            "\n",
            "If  the  stride  is  greater  than  1  (in  any  direction),  then  the  output  size  will  not  be\n",
            "equal to the input size, even if padding=\"same\". For example, if you set strides=2 (or\n",
            "equivalently strides=(2, 2)), then the output feature maps will be 35 × 60: halved\n",
            "both  vertically  and  horizontally.  Figure  14-8  shows  what  happens  when  strides=2,\n",
            "with both padding options.\n",
            "\n",
            "488 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFigure 14-7. The two padding options, when strides=1\n",
            "\n",
            "Figure 14-8. With strides greater than 1, the output is much smaller even when using\n",
            "\"same\" padding (and \"valid\" padding may ignore some inputs)\n",
            "\n",
            "If you are curious, this is how the output size is computed:\n",
            "\n",
            "• With  padding=\"valid\",  if  the  width  of  the  input  is  ih,  then  the  output  width  is\n",
            "•\n",
            "equal to (ih – fh + sh) / sh, rounded down. Recall that fh is the kernel width, and\n",
            "sh is the horizontal stride. Any remainder in the division corresponds to ignored\n",
            "columns  on  the  right  side  of  the  input  image.  The  same  logic  can  be  used  to\n",
            "compute the output height, and any ignored rows at the bottom of the image.\n",
            "• With padding=\"same\", the output width is equal to ih / sh, rounded up. To make\n",
            "•\n",
            "this possible, the appropriate number of zero columns are padded to the left and\n",
            "right  of  the  input  image  (an  equal  number  if  possible,  or  just  one  more  on  the\n",
            "right  side).  Assuming  the  output  width  is  ow,  then  the  number  of  padded  zero\n",
            "columns is (ow – 1) × sh + fh – ih. Again, the same logic can be used to compute the\n",
            "output height and the number of padded rows.\n",
            "\n",
            "Convolutional Layers \n",
            "\n",
            "| \n",
            "\n",
            "489\n",
            "\n",
            "\fNow  let’s  look  at  the  layer’s  weights  (which  were  noted  wu,  v,  k′,  k  and  bk  in  Equation\n",
            "14-1).  Just  like  a  Dense  layer,  a  Conv2D  layer  holds  all  the  layer’s  weights,  including\n",
            "the  kernels  and  biases.  The  kernels  are  initialized  randomly,  while  the  biases  are\n",
            "initialized  to  zero.  These  weights  are  accessible  as  TF  variables  via  the  weights\n",
            "attribute, or as NumPy arrays via the get_weights() method:\n",
            "\n",
            ">>> kernels, biases = conv_layer.get_weights()\n",
            ">>> kernels.shape\n",
            "(7, 7, 3, 32)\n",
            ">>> biases.shape\n",
            "(32,)\n",
            "\n",
            "The kernels array is 4D, and its shape is [kernel_height, kernel_width, input_channels,\n",
            "output_channels]. The biases array is 1D, with shape [output_channels]. The number\n",
            "of output channels is equal to the number of output feature maps, which is also equal\n",
            "to the number of filters.\n",
            "\n",
            "Most importantly, note that the height and width of the input images do not appear in\n",
            "the kernel’s shape: this is because all the neurons in the output feature maps share the\n",
            "same weights, as explained earlier. This means that you can feed images of any size to\n",
            "this layer, as long as they are at least as large as the kernels, and if they have the right\n",
            "number of channels (three in this case).\n",
            "\n",
            "Lastly, you will generally want to specify an activation function (such as ReLU) when\n",
            "creating  a  Conv2D  layer,  and  also  specify  the  corresponding  kernel  initializer  (such\n",
            "as He initialization). This is for the same reason as for Dense layers: a convolutional\n",
            "layer  performs  a  linear  operation,  so  if  you  stacked  multiple  convolutional  layers\n",
            "without  any  activation  functions  they  would  all  be  equivalent  to  a  single  convolu‐\n",
            "tional layer, and they wouldn’t be able to learn anything really complex.\n",
            "\n",
            "As  you  can  see,  convolutional  layers  have  quite  a  few  hyperparameters:  filters,\n",
            "kernel_size,  padding,  strides,  activation,  kernel_initializer,  etc.  As  always,\n",
            "you can use cross-validation to find the right hyperparameter values, but this is very\n",
            "time-consuming. We will discuss common CNN architectures later in this chapter, to\n",
            "give you some idea of which hyperparameter values work best in practice.\n",
            "\n",
            "Memory Requirements\n",
            "Another challenge with CNNs is that the convolutional layers require a huge amount\n",
            "of RAM. This is especially true during training, because the reverse pass of backpro‐\n",
            "pagation requires all the intermediate values computed during the forward pass.\n",
            "\n",
            "For example, consider a convolutional layer with 200 5 × 5 filters, with stride 1 and\n",
            "\"same\"  padding.  If  the  input  is  a  150  ×  100  RGB  image  (three  channels),  then  the\n",
            "number  of  parameters  is  (5  ×  5  ×  3  +  1)  ×  200  =  15,200  (the  +  1  corresponds  to\n",
            "\n",
            "490 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fthe bias terms), which is fairly small compared to a fully connected layer.7 However,\n",
            "each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons\n",
            "needs  to  compute  a  weighted  sum  of  its  5  ×  5  ×  3  =  75  inputs:  that’s  a  total  of  225\n",
            "million  float  multiplications.  Not  as  bad  as  a  fully  connected  layer,  but  still  quite\n",
            "computationally intensive. Moreover, if the feature maps are represented using 32-bit\n",
            "floats,  then  the  convolutional  layer’s  output  will  occupy  200  ×  150  ×  100  ×  32  =  96\n",
            "million  bits  (12  MB)  of  RAM.8  And  that’s  just  for  one  instance—if  a  training  batch\n",
            "contains 100 instances, then this layer will use up 1.2 GB of RAM!\n",
            "\n",
            "During  inference  (i.e.,  when  making  a  prediction  for  a  new  instance)  the  RAM\n",
            "occupied by one layer can be released as soon as the next layer has been computed,\n",
            "so  you  only  need  as  much  RAM  as  required  by  two  consecutive  layers.  But  during\n",
            "training everything computed during the forward pass needs to be preserved for the\n",
            "reverse  pass,  so  the  amount  of  RAM  needed  is  (at  least)  the  total  amount  of  RAM\n",
            "required by all layers.\n",
            "\n",
            "If training crashes because of an out-of-memory error, you can try\n",
            "reducing  the  mini-batch  size.  Alternatively,  you  can  try  reducing\n",
            "dimensionality  using  a  stride,  removing  a  few  layers,  using  16-bit\n",
            "floats instead of 32-bit floats, or distributing the CNN across multi‐\n",
            "ple devices (you will see how to do this in Chapter 19).\n",
            "\n",
            "Now let’s look at the second common building block of CNNs: the pooling layer.\n",
            "\n",
            "Pooling Layers\n",
            "Once  you  understand  how  convolutional  layers  work,  the  pooling  layers  are  quite\n",
            "easy  to  grasp.  Their  goal  is  to  subsample  (i.e.,  shrink)  the  input  image  in  order  to\n",
            "reduce  the  computational  load,  the  memory  usage,  and  the  number  of  parameters\n",
            "(thereby limiting the risk of overfitting).\n",
            "\n",
            "Just  like  in  convolutional  layers,  each  neuron  in  a  pooling  layer  is  connected  to  the\n",
            "outputs of a limited number of neurons in the previous layer, located within a small\n",
            "rectangular receptive field. You must define its size, the stride, and the padding type,\n",
            "just like before. However, a pooling neuron has no weights; all it does is aggregate the\n",
            "inputs using an aggregation function such as the max or mean. Figure 14-9 shows a\n",
            "max pooling layer, which is the most common type of pooling layer. In this example,\n",
            "\n",
            "7 To produce the same size outputs, a fully connected layer would need 200 × 150 × 100 neurons, each\n",
            "\n",
            "connected to all 150 × 100 × 3 inputs. It would have 200 × 150 × 100 × (150 × 100 × 3 + 1) ≈ 135 billion\n",
            "parameters!\n",
            "\n",
            "8 In the international system of units (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits. And\n",
            "\n",
            "1 MiB = 1,024 kiB = 1,024 × 1,024 bytes. So 12 MB ≈ 11.44 MiB.\n",
            "\n",
            "Pooling Layers \n",
            "\n",
            "| \n",
            "\n",
            "491\n",
            "\n",
            "\fwe use a 2 × 2 pooling kernel,9 with a stride of 2 and no padding. Only the max input\n",
            "value  in  each  receptive  field  makes  it  to  the  next  layer,  while  the  other  inputs  are\n",
            "dropped. For example, in the lower-left receptive field in Figure 14-9, the input values\n",
            "are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the\n",
            "stride of 2, the output image has half the height and half the width of the input image\n",
            "(rounded down since we use no padding).\n",
            "\n",
            "Figure 14-9. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\n",
            "\n",
            "A pooling layer typically works on every input channel independ‐\n",
            "ently, so the output depth (i.e., the number of channels) is the same\n",
            "as the input depth.\n",
            "\n",
            "Other  than  reducing  computations,  memory  usage,  and  the  number  of  parameters,\n",
            "a max pooling layer also introduces some level of invariance to small translations, as\n",
            "shown in Figure 14-10. Here we assume that the bright pixels have a lower value than\n",
            "dark  pixels,  and  we  consider  three  images  (A,  B,  C)  going  through  a  max  pooling\n",
            "layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but\n",
            "shifted  by  one  and  two  pixels  to  the  right.  As  you  can  see,  the  outputs  of  the  max\n",
            "pooling  layer  for  images  A  and  B  are  identical.  This  is  what  translation  invariance\n",
            "means.  For  image  C,  the  output  is  different:  it  is  shifted  one  pixel  to  the  right  (but\n",
            "there  is  still  50%  invariance).  By  inserting  a  max  pooling  layer  every  few  layers  in\n",
            "a  CNN,  it  is  possible  to  get  some  level  of  translation  invariance  at  a  larger  scale.\n",
            "Moreover,  max  pooling  offers  a  small  amount  of  rotational  invariance  and  a  slight\n",
            "scale  invariance.  Such  invariance  (even  if  it  is  limited)  can  be  useful  in  cases  where\n",
            "the prediction should not depend on these details, such as in classification tasks.\n",
            "\n",
            "9 Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding\n",
            "\n",
            "windows.\n",
            "\n",
            "492 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fHowever, max pooling has some downsides too. It’s obviously very destructive: even\n",
            "with  a  tiny  2  ×  2  kernel  and  a  stride  of  2,  the  output  will  be  two  times  smaller  in\n",
            "both  directions  (so  its  area  will  be  four  times  smaller),  simply  dropping  75%  of  the\n",
            "input  values.  And  in  some  applications,  invariance  is  not  desirable.  Take  semantic\n",
            "segmentation (the task of classifying each pixel in an image according to the object\n",
            "that pixel belongs to, which we’ll explore later in this chapter): obviously, if the input\n",
            "image is translated by one pixel to the right, the output should also be translated by\n",
            "one  pixel  to  the  right.  The  goal  in  this  case  is  equivariance,  not  invariance:  a  small\n",
            "change to the inputs should lead to a corresponding small change in the output.\n",
            "\n",
            "Figure 14-10. Invariance to small translations\n",
            "\n",
            "Implementing Pooling Layers with Keras\n",
            "The following code creates a MaxPooling2D layer, alias MaxPool2D, using a 2 × 2 ker‐\n",
            "nel. The strides default to the kernel size, so this layer uses a stride of 2 (horizontally\n",
            "and vertically). By default, it uses \"valid\" padding (i.e., no padding at all):\n",
            "\n",
            "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n",
            "\n",
            "To  create  an  average  pooling  layer,  just  use  AveragePooling2D,  alias  AvgPool2D,\n",
            "instead  of  MaxPool2D.  As  you  might  expect,  it  works  exactly  like  a  max  pooling\n",
            "layer, except it computes the mean rather than the max. Average pooling layers used\n",
            "to be very popular, but people mostly use max pooling layers now, as they generally\n",
            "perform better. This may seem surprising, since computing the mean generally loses\n",
            "less  information  than  computing  the  max.  But  on  the  other  hand,  max  pooling\n",
            "preserves  only  the  strongest  features,  getting  rid  of  all  the  meaningless  ones,  so  the\n",
            "\n",
            "Implementing Pooling Layers with Keras \n",
            "\n",
            "| \n",
            "\n",
            "493\n",
            "\n",
            "\fnext layers get a cleaner signal to work with. Moreover, max pooling offers stronger\n",
            "translation invariance than average pooling, and it requires slightly less compute.\n",
            "\n",
            "Note  that  max  pooling  and  average  pooling  can  be  performed  along  the  depth\n",
            "dimension instead of the spatial dimensions, although it’s not as common. This can\n",
            "allow the CNN to learn to be invariant to various features. For example, it could learn\n",
            "multiple filters, each detecting a different rotation of the same pattern (such as hand‐\n",
            "written digits; see Figure 14-11), and the depthwise max pooling layer would ensure\n",
            "that the output is the same regardless of the rotation. The CNN could similarly learn\n",
            "to be invariant to anything: thickness, brightness, skew, color, and so on.\n",
            "\n",
            "Figure 14-11. Depthwise max pooling can help the CNN learn to be invariant (to\n",
            "rotation in this case)\n",
            "\n",
            "Keras  does  not  include  a  depthwise  max  pooling  layer,  but  it’s  not  too  difficult  to\n",
            "implement a custom layer for that:\n",
            "\n",
            "class DepthPool(tf.keras.layers.Layer):\n",
            "    def __init__(self, pool_size=2, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.pool_size = pool_size\n",
            "\n",
            "    def call(self, inputs):\n",
            "        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n",
            "        groups = shape[-1] // self.pool_size  # number of channel groups\n",
            "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
            "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)\n",
            "\n",
            "494 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fThis  layer  reshapes  its  inputs  to  split  the  channels  into  groups  of  the  desired  size\n",
            "(pool_size), then it uses tf.reduce_max() to compute the max of each group. This\n",
            "implementation  assumes  that  the  stride  is  equal  to  the  pool  size,  which  is  generally\n",
            "what you want. Alternatively, you could use TensorFlow’s tf.nn.max_pool() opera‐\n",
            "tion, and wrap in a Lambda layer to use it inside a Keras model, but sadly this op does\n",
            "not implement depthwise pooling for the GPU, only for the CPU.\n",
            "\n",
            "One last type of pooling layer that you will often see in modern architectures is the\n",
            "global average pooling layer. It works very differently: all it does is compute the mean\n",
            "of  each  entire  feature  map  (it’s  like  an  average  pooling  layer  using  a  pooling  kernel\n",
            "with  the  same  spatial  dimensions  as  the  inputs).  This  means  that  it  just  outputs  a\n",
            "single number per feature map and per instance. Although this is of course extremely\n",
            "destructive (most of the information in the feature map is lost), it can be useful just\n",
            "before  the  output  layer,  as  you  will  see  later  in  this  chapter.  To  create  such  a  layer,\n",
            "simply use the GlobalAveragePooling2D class, alias GlobalAvgPool2D:\n",
            "\n",
            "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()\n",
            "\n",
            "It’s  equivalent  to  the  following  Lambda  layer,  which  computes  the  mean  over  the\n",
            "spatial dimensions (height and width):\n",
            "\n",
            "global_avg_pool = tf.keras.layers.Lambda(\n",
            "    lambda X: tf.reduce_mean(X, axis=[1, 2]))\n",
            "\n",
            "For example, if we apply this layer to the input images, we get the mean intensity of\n",
            "red, green, and blue for each image:\n",
            "\n",
            ">>> global_avg_pool(images)\n",
            "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "array([[0.64338624, 0.5971759 , 0.5824972 ],\n",
            "       [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>\n",
            "\n",
            "Now you know all the building blocks to create convolutional neural networks. Let’s\n",
            "see how to assemble them.\n",
            "\n",
            "CNN Architectures\n",
            "Typical  CNN  architectures  stack  a  few  convolutional  layers  (each  one  generally  fol‐\n",
            "lowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n",
            "(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\n",
            "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
            "with  more  feature  maps),  thanks  to  the  convolutional  layers  (see  Figure  14-12).  At\n",
            "the  top  of  the  stack,  a  regular  feedforward  neural  network  is  added,  composed  of  a\n",
            "few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\n",
            "softmax layer that outputs estimated class probabilities).\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "495\n",
            "\n",
            "\fFigure 14-12. Typical CNN architecture\n",
            "\n",
            "A common mistake is to use convolution kernels that are too large.\n",
            "For  example,  instead  of  using  a  convolutional  layer  with  a  5  ×\n",
            "5  kernel,  stack  two  layers  with  3  ×  3  kernels:  it  will  use  fewer\n",
            "parameters  and  require  fewer  computations,  and  it  will  usually\n",
            "perform better. One exception is for the first convolutional layer: it\n",
            "can typically have a large kernel (e.g., 5 × 5), usually with a stride\n",
            "of  2  or  more.  This  will  reduce  the  spatial  dimension  of  the  image\n",
            "without  losing  too  much  information,  and  since  the  input  image\n",
            "only has three channels in general, it will not be too costly.\n",
            "\n",
            "Here  is  how  you  can  implement  a  basic  CNN  to  tackle  the  Fashion  MNIST  dataset\n",
            "(introduced in Chapter 10):\n",
            "\n",
            "from functools import partial\n",
            "\n",
            "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
            "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
            "model = tf.keras.Sequential([\n",
            "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
            "    tf.keras.layers.MaxPool2D(),\n",
            "    DefaultConv2D(filters=128),\n",
            "    DefaultConv2D(filters=128),\n",
            "    tf.keras.layers.MaxPool2D(),\n",
            "    DefaultConv2D(filters=256),\n",
            "    DefaultConv2D(filters=256),\n",
            "    tf.keras.layers.MaxPool2D(),\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.Dropout(0.5),\n",
            "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
            "                          kernel_initializer=\"he_normal\"),\n",
            "    tf.keras.layers.Dropout(0.5),\n",
            "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "496 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fLet’s go through this code:\n",
            "\n",
            "• We use the functools.partial() function (introduced in Chapter 11) to define\n",
            "•\n",
            "DefaultConv2D, which acts just like Conv2D but with different default arguments:\n",
            "a  small  kernel  size  of  3,  \"same\"  padding,  the  ReLU  activation  function,  and  its\n",
            "corresponding He initializer.\n",
            "\n",
            "• Next,  we  create  the  Sequential  model.  Its  first  layer  is  a  DefaultConv2D  with\n",
            "•\n",
            "64  fairly  large  filters  (7  ×  7).  It  uses  the  default  stride  of  1  because  the  input\n",
            "images  are  not  very  large.  It  also  sets  input_shape=[28,  28,  1],  because  the\n",
            "images are 28 × 28 pixels, with a single color channel (i.e., grayscale). When you\n",
            "load the Fashion MNIST dataset, make sure each image has this shape: you may\n",
            "need to use np.reshape() or np.expanddims() to add the channels dimension.\n",
            "Alternatively, you could use a Reshape layer as the first layer in the model.\n",
            "\n",
            "•\n",
            "• We then add a max pooling layer that uses the default pool size of 2, so it divides\n",
            "\n",
            "each spatial dimension by a factor of 2.\n",
            "\n",
            "• Then we repeat the same structure twice: two convolutional layers followed by a\n",
            "•\n",
            "max pooling layer. For larger images, we could repeat this structure several more\n",
            "times. The number of repetitions is a hyperparameter you can tune.\n",
            "\n",
            "•\n",
            "• Note  that  the  number  of  filters  doubles  as  we  climb  up  the  CNN  toward  the\n",
            "output layer (it is initially 64, then 128, then 256): it makes sense for it to grow,\n",
            "since  the  number  of  low-level  features  is  often  fairly  low  (e.g.,  small  circles,\n",
            "horizontal lines), but there are many different ways to combine them into higher-\n",
            "level features. It is a common practice to double the number of filters after each\n",
            "pooling layer: since a pooling layer divides each spatial dimension by a factor of\n",
            "2, we can afford to double the number of feature maps in the next layer without\n",
            "fear  of  exploding  the  number  of  parameters,  memory  usage,  or  computational\n",
            "load.\n",
            "\n",
            "• Next  is  the  fully  connected  network,  composed  of  two  hidden  dense  layers  and\n",
            "•\n",
            "a  dense  output  layer.  Since  it’s  a  classification  task  with  10  classes,  the  output\n",
            "layer has 10 units, and it uses the softmax activation function. Note that we must\n",
            "flatten  the  inputs  just  before  the  first  dense  layer,  since  it  expects  a  1D  array  of\n",
            "features for each instance. We also add two dropout layers, with a dropout rate of\n",
            "50% each, to reduce overfitting.\n",
            "\n",
            "If you compile this model using the \"sparse_categorical_crossentropy\" loss and\n",
            "you  fit  the  model  to  the  Fashion  MNIST  training  set,  it  should  reach  over  92%\n",
            "accuracy on the test set. It’s not state of the art, but it is pretty good, and clearly much\n",
            "better than what we achieved with dense networks in Chapter 10.\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "497\n",
            "\n",
            "\fOver the years, variants of this fundamental architecture have been developed, lead‐\n",
            "ing  to  amazing  advances  in  the  field.  A  good  measure  of  this  progress  is  the  error\n",
            "rate  in  competitions  such  as  the  ILSVRC  ImageNet  challenge.  In  this  competition,\n",
            "the  top-five  error  rate  for  image  classification—that  is,  the  number  of  test  images\n",
            "for  which  the  system’s  top  five  predictions  did  not  include  the  correct  answer—fell\n",
            "from  over  26%  to  less  than  2.3%  in  just  six  years.  The  images  are  fairly  large  (e.g.,\n",
            "256  pixels  high)  and  there  are  1,000  classes,  some  of  which  are  really  subtle  (try\n",
            "distinguishing  120  dog  breeds).  Looking  at  the  evolution  of  the  winning  entries  is\n",
            "a  good  way  to  understand  how  CNNs  work,  and  how  research  in  deep  learning\n",
            "progresses.\n",
            "\n",
            "We  will  first  look  at  the  classical  LeNet-5  architecture  (1998),  then  several  winners\n",
            "of  the  ILSVRC  challenge:  AlexNet  (2012),  GoogLeNet  (2014),  ResNet  (2015),  and\n",
            "SENet (2017). Along the way, we will also look at a few more architectures, including\n",
            "Xception, ResNeXt, DenseNet, MobileNet, CSPNet, and EfficientNet.\n",
            "\n",
            "LeNet-5\n",
            "The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\n",
            "mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used\n",
            "for  handwritten  digit  recognition  (MNIST).  It  is  composed  of  the  layers  shown  in\n",
            "Table 14-1.\n",
            "\n",
            "Table 14-1. LeNet-5 architecture\n",
            "\n",
            "Kernel size\n",
            "–\n",
            "\n",
            "Stride Activation\n",
            "RBF\n",
            "–\n",
            "\n",
            "Layer\n",
            "Out\n",
            "\n",
            "Type\n",
            "Fully connected –\n",
            "\n",
            "Maps\n",
            "\n",
            "F6\n",
            "\n",
            "C5\n",
            "\n",
            "S4\n",
            "\n",
            "C3\n",
            "\n",
            "S2\n",
            "\n",
            "C1\n",
            "\n",
            "In\n",
            "\n",
            "Fully connected –\n",
            "\n",
            "Convolution\n",
            "\n",
            "Avg pooling\n",
            "\n",
            "Convolution\n",
            "\n",
            "Avg pooling\n",
            "\n",
            "Convolution\n",
            "\n",
            "Input\n",
            "\n",
            "120\n",
            "\n",
            "16\n",
            "\n",
            "16\n",
            "\n",
            "6\n",
            "\n",
            "6\n",
            "\n",
            "1\n",
            "\n",
            "Size\n",
            "10\n",
            "\n",
            "84\n",
            "\n",
            "1 × 1\n",
            "\n",
            "5 × 5\n",
            "\n",
            "–\n",
            "\n",
            "5 × 5\n",
            "\n",
            "2 × 2\n",
            "\n",
            "10 × 10\n",
            "\n",
            "5 × 5\n",
            "\n",
            "14 × 14\n",
            "\n",
            "2 × 2\n",
            "\n",
            "28 × 28\n",
            "\n",
            "5 × 5\n",
            "\n",
            "32 × 32 –\n",
            "\n",
            "–\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "–\n",
            "\n",
            "tanh\n",
            "\n",
            "tanh\n",
            "\n",
            "tanh\n",
            "\n",
            "tanh\n",
            "\n",
            "tanh\n",
            "\n",
            "tanh\n",
            "\n",
            "–\n",
            "\n",
            "As  you  can  see,  this  looks  pretty  similar  to  our  Fashion  MNIST  model:  a  stack  of\n",
            "convolutional  layers  and  pooling  layers,  followed  by  a  dense  network.  Perhaps  the\n",
            "main  difference  with  more  modern  classification  CNNs  is  the  activation  functions:\n",
            "today,  we  would  use  ReLU  instead  of  tanh  and  softmax  instead  of  RBF.  There  were\n",
            "\n",
            "10 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition”, Proceedings of the IEEE 86,\n",
            "\n",
            "no. 11 (1998): 2278–2324.\n",
            "\n",
            "498 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fseveral  other  minor  differences  that  don’t  really  matter  much,  but  in  case  you  are\n",
            "interested, they are listed in this chapter’s notebook at https://homl.info/colab3. Yann\n",
            "LeCun’s website also features great demos of LeNet-5 classifying digits.\n",
            "\n",
            "AlexNet\n",
            "The AlexNet CNN architecture11 won the 2012 ILSVRC challenge by a large margin:\n",
            "it  achieved  a  top-five  error  rate  of  17%,  while  the  second  best  competitor  achieved\n",
            "only 26%! AlexaNet was developed by Alex Krizhevsky (hence the name), Ilya Sutsk‐\n",
            "ever, and Geoffrey Hinton. It is similar to LeNet-5, only much larger and deeper, and\n",
            "it was the first to stack convolutional layers directly on top of one another, instead of\n",
            "stacking a pooling layer on top of each convolutional layer. Table 14-2 presents this\n",
            "architecture.\n",
            "\n",
            "Table 14-2. AlexNet architecture\n",
            "\n",
            "Layer\n",
            "Out\n",
            "\n",
            "Type\n",
            "Fully connected –\n",
            "\n",
            "Maps\n",
            "\n",
            "F10\n",
            "\n",
            "Fully connected –\n",
            "\n",
            "Fully connected –\n",
            "\n",
            "Max pooling\n",
            "\n",
            "Convolution\n",
            "\n",
            "Convolution\n",
            "\n",
            "Convolution\n",
            "\n",
            "Max pooling\n",
            "\n",
            "Convolution\n",
            "\n",
            "Max pooling\n",
            "\n",
            "Convolution\n",
            "\n",
            "256\n",
            "\n",
            "256\n",
            "\n",
            "384\n",
            "\n",
            "384\n",
            "\n",
            "256\n",
            "\n",
            "256\n",
            "\n",
            "96\n",
            "\n",
            "96\n",
            "\n",
            "F9\n",
            "\n",
            "S8\n",
            "\n",
            "C7\n",
            "\n",
            "C6\n",
            "\n",
            "C5\n",
            "\n",
            "S4\n",
            "\n",
            "C3\n",
            "\n",
            "S2\n",
            "\n",
            "C1\n",
            "\n",
            "In\n",
            "\n",
            "Size\n",
            "1,000\n",
            "\n",
            "4,096\n",
            "\n",
            "4,096\n",
            "\n",
            "6 × 6\n",
            "\n",
            "13 × 13\n",
            "\n",
            "13 × 13\n",
            "\n",
            "13 × 13\n",
            "\n",
            "13 × 13\n",
            "\n",
            "27 × 27\n",
            "\n",
            "27 × 27\n",
            "\n",
            "55 × 55\n",
            "\n",
            "Kernel size\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "3 × 3\n",
            "\n",
            "3 × 3\n",
            "\n",
            "3 × 3\n",
            "\n",
            "3 × 3\n",
            "\n",
            "3 × 3\n",
            "\n",
            "5 × 5\n",
            "\n",
            "3 × 3\n",
            "\n",
            "11 × 11\n",
            "\n",
            "Stride Padding Activation\n",
            "–\n",
            "\n",
            "Softmax\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "4\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "ReLU\n",
            "\n",
            "ReLU\n",
            "\n",
            "valid\n",
            "\n",
            "–\n",
            "\n",
            "same\n",
            "\n",
            "same\n",
            "\n",
            "same\n",
            "\n",
            "ReLU\n",
            "\n",
            "ReLU\n",
            "\n",
            "ReLU\n",
            "\n",
            "valid\n",
            "\n",
            "–\n",
            "\n",
            "same\n",
            "\n",
            "ReLU\n",
            "\n",
            "valid\n",
            "\n",
            "–\n",
            "\n",
            "valid\n",
            "\n",
            "ReLU\n",
            "\n",
            "–\n",
            "\n",
            "–\n",
            "\n",
            "Input\n",
            "\n",
            "3 (RGB)\n",
            "\n",
            "227 × 227 –\n",
            "\n",
            "To  reduce  overfitting,  the  authors  used  two  regularization  techniques.  First,  they\n",
            "applied dropout (introduced in Chapter 11) with a 50% dropout rate during training\n",
            "to  the  outputs  of  layers  F9  and  F10.  Second,  they  performed  data  augmentation  by\n",
            "randomly shifting the training images by various offsets, flipping them horizontally,\n",
            "and changing the lighting conditions.\n",
            "\n",
            "11 Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks”, Proceedings of\n",
            "\n",
            "the 25th International Conference on Neural Information Processing Systems 1 (2012): 1097–1105.\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "499\n",
            "\n",
            "\fData Augmentation\n",
            "Data  augmentation  artificially  increases  the  size  of  the  training  set  by  generating\n",
            "many realistic variants of each training instance. This reduces overfitting, making this\n",
            "a regularization technique. The generated instances should be as realistic as possible:\n",
            "ideally, given an image from the augmented training set, a human should not be able\n",
            "to tell whether it was augmented or not. Simply adding white noise will not help; the\n",
            "modifications should be learnable (white noise is not).\n",
            "\n",
            "For  example,  you  can  slightly  shift,  rotate,  and  resize  every  picture  in  the  training\n",
            "set  by  various  amounts  and  add  the  resulting  pictures  to  the  training  set  (see  Fig‐\n",
            "ure  14-13).  To  do  this,  you  can  use  Keras’s  data  augmentation  layers,  introduced  in\n",
            "Chapter  13  (e.g.,  RandomCrop,  RandomRotation,  etc.).  This  forces  the  model  to  be\n",
            "more tolerant to variations in the position, orientation, and size of the objects in the\n",
            "pictures. To produce a model that’s more tolerant of different lighting conditions, you\n",
            "can  similarly  generate  many  images  with  various  contrasts.  In  general,  you  can  also\n",
            "flip  the  pictures  horizontally  (except  for  text,  and  other  asymmetrical  objects).  By\n",
            "combining these transformations, you can greatly increase your training set size.\n",
            "\n",
            "Figure 14-13. Generating new training instances from existing ones\n",
            "\n",
            "Data augmentation is also useful when you have an unbalanced dataset: you can use\n",
            "it  to  generate  more  samples  of  the  less  frequent  classes.  This  is  called  the  synthetic\n",
            "minority oversampling technique, or SMOTE for short.\n",
            "\n",
            "500 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fAlexNet also uses a competitive normalization step immediately after the ReLU step\n",
            "of  layers  C1  and  C3,  called  local  response  normalization  (LRN):  the  most  strongly\n",
            "activated neurons inhibit other neurons located at the same position in neighboring\n",
            "feature  maps.  Such  competitive  activation  has  been  observed  in  biological  neurons.\n",
            "This encourages different feature maps to specialize, pushing them apart and forcing\n",
            "them to explore a wider range of features, ultimately improving generalization. Equa‐\n",
            "tion 14-2 shows how to apply LRN.\n",
            "\n",
            "Equation 14-2. Local response normalization (LRN)\n",
            "\n",
            "jhigh\n",
            "bi = ai k + α ∑\n",
            "\n",
            "j = jlow\n",
            "\n",
            "−β\n",
            "\n",
            "2\n",
            "\n",
            "aj\n",
            "\n",
            "with\n",
            "\n",
            "jhigh = min i +\n",
            "\n",
            "r\n",
            "2\n",
            "\n",
            ", fn − 1\n",
            "\n",
            "jlow = max 0, i −\n",
            "\n",
            "r\n",
            "2\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• bi is the normalized output of the neuron located in feature map i, at some row u\n",
            "•\n",
            "and column v (note that in this equation we consider only neurons located at this\n",
            "row and column, so u and v are not shown).\n",
            "\n",
            "•\n",
            "• ai is the activation of that neuron after the ReLU step, but before normalization.\n",
            "•\n",
            "• k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\n",
            "\n",
            "radius.\n",
            "\n",
            "•\n",
            "• fn is the number of feature maps.\n",
            "\n",
            "For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\n",
            "of the neurons located in the feature maps immediately above and below its own.\n",
            "\n",
            "In  AlexNet,  the  hyperparameters  are  set  as:  r  =  5,  α  =  0.0001,  β  =  0.75,  and  k  =  2.\n",
            "You can implement this step by using the tf.nn.local_response_normalization()\n",
            "function  (which  you  can  wrap  in  a  Lambda  layer  if  you  want  to  use  it  in  a  Keras\n",
            "model).\n",
            "\n",
            "A  variant  of  AlexNet  called  ZF  Net12  was  developed  by  Matthew  Zeiler  and  Rob\n",
            "Fergus  and  won  the  2013  ILSVRC  challenge.  It  is  essentially  AlexNet  with  a  few\n",
            "tweaked hyperparameters (number of feature maps, kernel size, stride, etc.).\n",
            "\n",
            "12 Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional Networks”, Proceedings of\n",
            "\n",
            "the European Conference on Computer Vision (2014): 818–833.\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "501\n",
            "\n",
            "\fGoogLeNet\n",
            "The GoogLeNet architecture was developed by Christian Szegedy et al. from Google\n",
            "Research,13 and it won the ILSVRC 2014 challenge by pushing the top-five error rate\n",
            "below 7%. This great performance came in large part from the fact that the network\n",
            "was much deeper than previous CNNs (as you’ll see in Figure 14-15). This was made\n",
            "possible  by  subnetworks  called  inception  modules,14  which  allow  GoogLeNet  to  use\n",
            "parameters  much  more  efficiently  than  previous  architectures:  GoogLeNet  actually\n",
            "has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).\n",
            "\n",
            "Figure 14-14 shows the architecture of an inception module. The notation “3 × 3 +\n",
            "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and \"same\" padding. The input\n",
            "signal is first fed to four different layers in parallel. All convolutional layers use the\n",
            "ReLU activation function. Note that the top convolutional layers use different kernel\n",
            "sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales.\n",
            "Also note that every single layer uses a stride of 1 and \"same\" padding (even the max\n",
            "pooling  layer),  so  their  outputs  all  have  the  same  height  and  width  as  their  inputs.\n",
            "This  makes  it  possible  to  concatenate  all  the  outputs  along  the  depth  dimension  in\n",
            "the  final  depth  concatenation  layer  (i.e.,  to  stack  the  feature  maps  from  all  four  top\n",
            "convolutional layers). It can be implemented using Keras’s  Concatenate layer, using\n",
            "the default axis=-1.\n",
            "\n",
            "Figure 14-14. Inception module\n",
            "\n",
            "13 Christian Szegedy et al., “Going Deeper with Convolutions”, Proceedings of the IEEE Conference on Computer\n",
            "\n",
            "Vision and Pattern Recognition (2015): 1–9.\n",
            "\n",
            "14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;\n",
            "\n",
            "hence the name of these modules.\n",
            "\n",
            "502 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fYou  may  wonder  why  inception  modules  have  convolutional  layers  with  1  ×  1  ker‐\n",
            "nels.  Surely  these  layers  cannot  capture  any  features  because  they  look  at  only  one\n",
            "pixel at a time, right? In fact, these layers serve three purposes:\n",
            "\n",
            "•\n",
            "• Although  they  cannot  capture  spatial  patterns,  they  can  capture  patterns  along\n",
            "\n",
            "the depth dimension (i.e., across channels).\n",
            "\n",
            "• They  are  configured  to  output  fewer  feature  maps  than  their  inputs,  so  they\n",
            "•\n",
            "serve  as  bottleneck  layers,  meaning  they  reduce  dimensionality.  This  cuts  the\n",
            "computational  cost  and  the  number  of  parameters,  speeding  up  training  and\n",
            "improving generalization.\n",
            "\n",
            "• Each  pair  of  convolutional  layers  ([1  ×  1,  3  ×  3]  and  [1  ×  1,  5  ×  5])  acts  like  a\n",
            "•\n",
            "single powerful convolutional layer, capable of capturing more complex patterns.\n",
            "A  convolutional  layer  is  equivalent  to  sweeping  a  dense  layer  across  the  image\n",
            "(at  each  location,  it  only  looks  at  a  small  receptive  field),  and  these  pairs  of\n",
            "convolutional layers are equivalent to sweeping two-layer neural networks across\n",
            "the image.\n",
            "\n",
            "In  short,  you  can  think  of  the  whole  inception  module  as  a  convolutional  layer  on\n",
            "steroids, able to output feature maps that capture complex patterns at various scales.\n",
            "\n",
            "Now  let’s  look  at  the  architecture  of  the  GoogLeNet  CNN  (see  Figure  14-15).  The\n",
            "number of feature maps output by each convolutional layer and each pooling layer is\n",
            "shown before the kernel size. The architecture is so deep that it has to be represented\n",
            "in three columns, but GoogLeNet is actually one tall stack, including nine inception\n",
            "modules  (the  boxes  with  the  spinning  tops).  The  six  numbers  in  the  inception\n",
            "modules represent the number of feature maps output by each convolutional layer in\n",
            "the  module  (in  the  same  order  as  in  Figure  14-14).  Note  that  all  the  convolutional\n",
            "layers use the ReLU activation function.\n",
            "\n",
            "Let’s go through this network:\n",
            "\n",
            "• The first two layers divide the image’s height and width by 4 (so its area is divided\n",
            "•\n",
            "by 16), to reduce the computational load. The first layer uses a large kernel size,\n",
            "7 × 7, so that much of the information is preserved.\n",
            "\n",
            "• Then the local response normalization layer ensures that the previous layers learn\n",
            "•\n",
            "\n",
            "a wide variety of features (as discussed earlier).\n",
            "\n",
            "•\n",
            "• Two  convolutional  layers  follow,  where  the  first  acts  like  a  bottleneck  layer.  As\n",
            "\n",
            "mentioned, you can think of this pair as a single smarter convolutional layer.\n",
            "\n",
            "• Again,  a  local  response  normalization  layer  ensures  that  the  previous  layers\n",
            "•\n",
            "\n",
            "capture a wide variety of patterns.\n",
            "\n",
            "• Next,  a  max  pooling  layer  reduces  the  image  height  and  width  by  2,  again  to\n",
            "•\n",
            "\n",
            "speed up computations.\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "503\n",
            "\n",
            "\f• Then  comes  the  CNN’s  backbone:  a  tall  stack  of  nine  inception  modules,  inter‐\n",
            "•\n",
            "leaved  with  a  couple  of  max  pooling  layers  to  reduce  dimensionality  and  speed\n",
            "up the net.\n",
            "\n",
            "•\n",
            "• Next,  the  global  average  pooling  layer  outputs  the  mean  of  each  feature  map:\n",
            "this drops any remaining spatial information, which is fine because there is not\n",
            "much  spatial  information  left  at  that  point.  Indeed,  GoogLeNet  input  images\n",
            "are  typically  expected  to  be  224  ×  224  pixels,  so  after  5  max  pooling  layers,\n",
            "each  dividing  the  height  and  width  by  2,  the  feature  maps  are  down  to  7  ×  7.\n",
            "Moreover, this is a classification task, not localization, so it doesn’t matter where\n",
            "the object is. Thanks to the dimensionality reduction brought by this layer, there\n",
            "is  no  need  to  have  several  fully  connected  layers  at  the  top  of  the  CNN  (like\n",
            "in  AlexNet),  and  this  considerably  reduces  the  number  of  parameters  in  the\n",
            "network and limits the risk of overfitting.\n",
            "\n",
            "• The  last  layers  are  self-explanatory:  dropout  for  regularization,  then  a  fully\n",
            "•\n",
            "connected  layer  with  1,000  units  (since  there  are  1,000  classes)  and  a  softmax\n",
            "activation function to output estimated class probabilities.\n",
            "\n",
            "Figure 14-15. GoogLeNet architecture\n",
            "\n",
            "504 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fThe  original  GoogLeNet  architecture  included  two  auxiliary  classifiers  plugged  on\n",
            "top of the third and sixth inception modules. They were both composed of one aver‐\n",
            "age pooling layer, one convolutional layer, two fully connected layers, and a softmax\n",
            "activation  layer.  During  training,  their  loss  (scaled  down  by  70%)  was  added  to  the\n",
            "overall loss. The goal was to fight the vanishing gradients problem and regularize the\n",
            "network, but it was later shown that their effect was relatively minor.\n",
            "\n",
            "Several  variants  of  the  GoogLeNet  architecture  were  later  proposed  by  Google\n",
            "researchers, including Inception-v3 and Inception-v4, using slightly different incep‐\n",
            "tion modules to reach even better performance.\n",
            "\n",
            "VGGNet\n",
            "The  runner-up  in  the  ILSVRC  2014  challenge  was  VGGNet,15  Karen  Simonyan  and\n",
            "Andrew Zisserman, from the Visual Geometry Group (VGG) research lab at Oxford\n",
            "University, developed a very simple and classical architecture; it had 2 or 3 convolu‐\n",
            "tional layers and a pooling layer, then again 2 or 3 convolutional layers and a pooling\n",
            "layer, and so on (reaching a total of 16 or 19 convolutional layers, depending on the\n",
            "VGG variant), plus a final dense network with 2 hidden layers and the output layer. It\n",
            "used small 3 × 3 filters, but it had many of them.\n",
            "\n",
            "ResNet\n",
            "Kaiming  He  et  al.  won  the  ILSVRC  2015  challenge  using  a  Residual  Network\n",
            "(ResNet)16 that delivered an astounding top-five error rate under 3.6%. The winning\n",
            "variant used an extremely deep CNN composed of 152 layers (other variants had 34,\n",
            "50,  and  101  layers).  It  confirmed  the  general  trend:  computer  vision  models  were\n",
            "getting deeper and deeper, with fewer and fewer parameters. The key to being able to\n",
            "train such a deep network is to use skip connections (also called shortcut connections):\n",
            "the signal feeding into a layer is also added to the output of a layer located higher up\n",
            "the stack. Let’s see why this is useful.\n",
            "\n",
            "When training a neural network, the goal is to make it model a target function h(x).\n",
            "If you add the input x to the output of the network (i.e., you add a skip connection),\n",
            "then  the  network  will  be  forced  to  model  f(x)  =  h(x)  –  x  rather  than  h(x).  This  is\n",
            "called residual learning (see Figure 14-16).\n",
            "\n",
            "15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐\n",
            "\n",
            "nition”, arXiv preprint arXiv:1409.1556 (2014).\n",
            "\n",
            "16 Kaiming He et al., “Deep Residual Learning for Image Recognition”, arXiv preprint arXiv:1512:03385 (2015).\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "505\n",
            "\n",
            "\fFigure 14-16. Residual learning\n",
            "\n",
            "When  you  initialize  a  regular  neural  network,  its  weights  are  close  to  zero,  so  the\n",
            "network just outputs values close to zero. If you add a skip connection, the resulting\n",
            "network  just  outputs  a  copy  of  its  inputs;  in  other  words,  it  initially  models  the\n",
            "identity function. If the target function is fairly close to the identity function (which is\n",
            "often the case), this will speed up training considerably.\n",
            "\n",
            "Moreover, if you add many skip connections, the network can start making progress\n",
            "even if several layers have not started learning yet (see Figure 14-17). Thanks to skip\n",
            "connections, the signal can easily make its way across the whole network. The deep\n",
            "residual network can be seen as a stack of residual units (RUs), where each residual\n",
            "unit is a small neural network with a skip connection.\n",
            "\n",
            "Now  let’s  look  at  ResNet’s  architecture  (see  Figure  14-18).  It  is  surprisingly  simple.\n",
            "It  starts  and  ends  exactly  like  GoogLeNet  (except  without  a  dropout  layer),  and  in\n",
            "between  is  just  a  very  deep  stack  of  residual  units.  Each  residual  unit  is  composed\n",
            "of  two  convolutional  layers  (and  no  pooling  layer!),  with  batch  normalization  (BN)\n",
            "and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions (stride 1,\n",
            "\"same\" padding).\n",
            "\n",
            "506 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFigure 14-17. Regular deep neural network (left) and deep residual network (right)\n",
            "\n",
            "Figure 14-18. ResNet architecture\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "507\n",
            "\n",
            "\fNote that the number of feature maps is doubled every few residual units, at the same\n",
            "time as their height and width are halved (using a convolutional layer with stride 2).\n",
            "When this happens, the inputs cannot be added directly to the outputs of the residual\n",
            "unit  because  they  don’t  have  the  same  shape  (for  example,  this  problem  affects  the\n",
            "skip  connection  represented  by  the  dashed  arrow  in  Figure  14-18).  To  solve  this\n",
            "problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and\n",
            "the right number of output feature maps (see Figure 14-19).\n",
            "\n",
            "Figure 14-19. Skip connection when changing feature map size and depth\n",
            "\n",
            "Different  variations  of  the  architecture  exist,  with  different  numbers  of  layers.\n",
            "ResNet-34  is  a  ResNet  with  34  layers  (only  counting  the  convolutional  layers  and\n",
            "the fully connected layer)17 containing 3 RUs that output 64 feature maps, 4 RUs with\n",
            "128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will implement this\n",
            "architecture later in this chapter.\n",
            "\n",
            "Google’s  Inception-v418  architecture  merged  the  ideas  of  GoogLe‐\n",
            "Net and ResNet and achieved a top-five error rate of close to 3% on\n",
            "ImageNet classification.\n",
            "\n",
            "ResNets  deeper  than  that,  such  as  ResNet-152,  use  slightly  different  residual  units.\n",
            "Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three\n",
            "convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4 ×\n",
            "less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer with\n",
            "64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature maps\n",
            "(4  times  64)  that  restores  the  original  depth.  ResNet-152  contains  3  such  RUs  that\n",
            "\n",
            "17 It is a common practice when describing a neural network to count only layers with parameters.\n",
            "\n",
            "18 Christian Szegedy et al., “Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learn‐\n",
            "\n",
            "ing”, arXiv preprint arXiv:1602.07261 (2016).\n",
            "\n",
            "508 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\foutput 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024 maps,\n",
            "and finally 3 RUs with 2,048 maps.\n",
            "\n",
            "Xception\n",
            "Another  variant  of  the  GoogLeNet  architecture  is  worth  noting:  Xception19  (which\n",
            "stands for Extreme Inception) was proposed in 2016 by François Chollet (the author\n",
            "of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350\n",
            "million  images  and  17,000  classes).  Just  like  Inception-v4,  it  merges  the  ideas  of\n",
            "GoogLeNet  and  ResNet,  but  it  replaces  the  inception  modules  with  a  special  type\n",
            "of layer called a depthwise separable convolution layer (or separable convolution layer\n",
            "for short20). These layers had been used before in some CNN architectures, but they\n",
            "were  not  as  central  as  in  the  Xception  architecture.  While  a  regular  convolutional\n",
            "layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and\n",
            "cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional\n",
            "layer  makes  the  strong  assumption  that  spatial  patterns  and  cross-channel  patterns\n",
            "can be modeled separately (see Figure 14-20). Thus, it is composed of two parts: the\n",
            "first part applies a single spatial filter to each input feature map, then the second part\n",
            "looks  exclusively  for  cross-channel  patterns—it  is  just  a  regular  convolutional  layer\n",
            "with 1 × 1 filters.\n",
            "\n",
            "Since  separable  convolutional  layers  only  have  one  spatial  filter  per  input  channel,\n",
            "you should avoid using them after layers that have too few channels, such as the input\n",
            "layer (granted, that’s what Figure 14-20 represents, but it is just for illustration pur‐\n",
            "poses). For this reason, the Xception architecture starts with 2 regular convolutional\n",
            "layers,  but  then  the  rest  of  the  architecture  uses  only  separable  convolutions  (34  in\n",
            "all), plus a few max pooling layers and the usual final layers (a global average pooling\n",
            "layer and a dense output layer).\n",
            "\n",
            "You might wonder why Xception is considered a variant of GoogLeNet, since it con‐\n",
            "tains no inception modules at all. Well, as discussed earlier, an inception module con‐\n",
            "tains convolutional layers with 1 × 1 filters: these look exclusively for cross-channel\n",
            "patterns. However, the convolutional layers that sit on top of them are regular convo‐\n",
            "lutional layers that look both for spatial and cross-channel patterns. So, you can think\n",
            "of  an  inception  module  as  an  intermediate  between  a  regular  convolutional  layer\n",
            "(which considers spatial patterns and cross-channel patterns jointly) and a separable\n",
            "convolutional  layer  (which  considers  them  separately).  In  practice,  it  seems  that\n",
            "separable convolutional layers often perform better.\n",
            "\n",
            "19 François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions”, arXiv preprint\n",
            "\n",
            "arXiv:1610.02357 (2016).\n",
            "\n",
            "20 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\n",
            "\n",
            "convolutions” as well.\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "509\n",
            "\n",
            "\fFigure 14-20. Depthwise separable convolutional layer\n",
            "\n",
            "Separable convolutional layers use fewer parameters, less memory,\n",
            "and  fewer  computations  than  regular  convolutional  layers,  and\n",
            "they often perform better. Consider using them by default, except\n",
            "after layers with few channels (such as the input channel). In Keras,\n",
            "just use SeparableConv2D instead of Conv2D: it’s a drop-in replace‐\n",
            "ment.  Keras  also  offers  a  DepthwiseConv2D  layer  that  implements\n",
            "the  first  part  of  a  depthwise  separable  convolutional  layer  (i.e.,\n",
            "applying one spatial filter per input feature map).\n",
            "\n",
            "SENet\n",
            "The  winning  architecture  in  the  ILSVRC  2017  challenge  was  the  Squeeze-and-\n",
            "Excitation  Network  (SENet).21  This  architecture  extends  existing  architectures  such\n",
            "as  inception  networks  and  ResNets,  and  boosts  their  performance.  This  allowed\n",
            "SENet  to  win  the  competition  with  an  astonishing  2.25%  top-five  error  rate!  The\n",
            "extended  versions  of  inception  networks  and  ResNets  are  called  SE-Inception  and\n",
            "SE-ResNet,  respectively.  The  boost  comes  from  the  fact  that  a  SENet  adds  a  small\n",
            "neural network, called an SE block, to every inception module or residual unit in the\n",
            "original architecture, as shown in Figure 14-21.\n",
            "\n",
            "21 Jie Hu et al., “Squeeze-and-Excitation Networks”, Proceedings of the IEEE Conference on Computer Vision and\n",
            "\n",
            "Pattern Recognition (2018): 7132–7141.\n",
            "\n",
            "510 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFigure 14-21. SE-Inception module (left) and SE-ResNet unit (right)\n",
            "\n",
            "An SE block analyzes the output of the unit it is attached to, focusing exclusively on\n",
            "the  depth  dimension  (it  does  not  look  for  any  spatial  pattern),  and  it  learns  which\n",
            "features are usually most active together. It then uses this information to recalibrate\n",
            "the feature maps, as shown in Figure 14-22. For example, an SE block may learn that\n",
            "mouths, noses, and eyes usually appear together in pictures: if you see a mouth and\n",
            "a nose, you should expect to see eyes as well. So, if the block sees a strong activation\n",
            "in the mouth and nose feature maps, but only mild activation in the eye feature map,\n",
            "it  will  boost  the  eye  feature  map  (more  accurately,  it  will  reduce  irrelevant  feature\n",
            "maps).  If  the  eyes  were  somewhat  confused  with  something  else,  this  feature  map\n",
            "recalibration will help resolve the ambiguity.\n",
            "\n",
            "Figure 14-22. An SE block performs feature map recalibration\n",
            "\n",
            "An SE block is composed of just three layers: a global average pooling layer, a hidden\n",
            "dense  layer  using  the  ReLU  activation  function,  and  a  dense  output  layer  using  the\n",
            "sigmoid activation function (see Figure 14-23).\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "511\n",
            "\n",
            "\fFigure 14-23. SE block architecture\n",
            "\n",
            "As  earlier,  the  global  average  pooling  layer  computes  the  mean  activation  for  each\n",
            "feature  map:  for  example,  if  its  input  contains  256  feature  maps,  it  will  output  256\n",
            "numbers  representing  the  overall  level  of  response  for  each  filter.  The  next  layer  is\n",
            "where  the  “squeeze”  happens:  this  layer  has  significantly  fewer  than  256  neurons—\n",
            "typically  16  times  fewer  than  the  number  of  feature  maps  (e.g.,  16  neurons)—so\n",
            "the  256  numbers  get  compressed  into  a  small  vector  (e.g.,  16  dimensions).  This\n",
            "is  a  low-dimensional  vector  representation  (i.e.,  an  embedding)  of  the  distribution\n",
            "of  feature  responses.  This  bottleneck  step  forces  the  SE  block  to  learn  a  general\n",
            "representation of the feature combinations (we will see this principle in action again\n",
            "when  we  discuss  autoencoders  in  Chapter  17).  Finally,  the  output  layer  takes  the\n",
            "embedding  and  outputs  a  recalibration  vector  containing  one  number  per  feature\n",
            "map (e.g., 256), each between 0 and 1. The feature maps are then multiplied by this\n",
            "recalibration vector, so irrelevant features (with a low recalibration score) get scaled\n",
            "down while relevant features (with a recalibration score close to 1) are left alone.\n",
            "\n",
            "Other Noteworthy Architectures\n",
            "There are many other CNN architectures to explore. Here’s a brief overview of some\n",
            "of the most noteworthy:\n",
            "\n",
            "ResNeXt22\n",
            "\n",
            "ResNeXt  improves  the  residual  units  in  ResNet.  Whereas  the  residual  units  in\n",
            "the  best  ResNet  models  just  contain  3  convolutional  layers  each,  the  ResNeXt\n",
            "residual  units  are  composed  of  many  parallel  stacks  (e.g.,  32  stacks),  with  3\n",
            "convolutional layers each. However, the first two layers in each stack only use a\n",
            "few filters (e.g., just four), so the overall number of parameters remains the same\n",
            "as in ResNet. Then the outputs of all the stacks are added together, and the result\n",
            "is passed to the next residual unit (along with the skip connection).\n",
            "\n",
            "22 Saining Xie et al., “Aggregated Residual Transformations for Deep Neural Networks”, arXiv preprint\n",
            "\n",
            "arXiv:1611.05431 (2016).\n",
            "\n",
            "512 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fDenseNet23\n",
            "\n",
            "A DenseNet is composed of several dense blocks, each made up of a few densely\n",
            "connected  convolutional  layers.  This  architecture  achieved  excellent  accuracy\n",
            "while  using  comparatively  few  parameters.  What  does  “densely  connected”\n",
            "mean?  The  output  of  each  layer  is  fed  as  input  to  every  layer  after  it  within\n",
            "the  same  block.  For  example,  layer  4  in  a  block  takes  as  input  the  depthwise\n",
            "concatenation of the outputs of layers 1, 2, and 3 in that block. Dense blocks are\n",
            "separated by a few transition layers.\n",
            "\n",
            "MobileNet24\n",
            "\n",
            "MobileNets are streamlined models designed to be lightweight and fast, making\n",
            "them popular in mobile and web applications. They are based on depthwise sepa‐\n",
            "rable convolutional layers, like Xception. The authors proposed several variants,\n",
            "trading a bit of accuracy for faster and smaller models.\n",
            "\n",
            "CSPNet25\n",
            "\n",
            "A  Cross  Stage  Partial  Network  (CSPNet)  is  similar  to  a  DenseNet,  but  part  of\n",
            "each  dense  block’s  input  is  concatenated  directly  to  that  block’s  output,  without\n",
            "going through the block.\n",
            "\n",
            "EfficientNet26\n",
            "\n",
            "EfficientNet  is  arguably  the  most  important  model  in  this  list.  The  authors\n",
            "proposed a method to scale any CNN efficiently, by jointly increasing the depth\n",
            "(number  of  layers),  width  (number  of  filters  per  layer),  and  resolution  (size  of\n",
            "the input image) in a principled way. This is called compound scaling. They used\n",
            "neural architecture search to find a good architecture for a scaled-down version\n",
            "of ImageNet (with smaller and fewer images), and then used compound scaling\n",
            "to create larger and larger versions of this architecture. When EfficientNet mod‐\n",
            "els  came  out,  they  vastly  outperformed  all  existing  models,  across  all  compute\n",
            "budgets, and they remain among the best models out there today.\n",
            "\n",
            "Understanding  EfficientNet’s  compound  scaling  method  is  helpful  to  gain  a  deeper\n",
            "understanding  of  CNNs,  especially  if  you  ever  need  to  scale  a  CNN  architecture.  It\n",
            "is based on a logarithmic measure of the compute budget, noted ϕ: if your compute\n",
            "budget doubles, then ϕ increases by 1. In other words, the number of floating-point\n",
            "operations available for training is proportional to 2ϕ. Your CNN architecture’s depth,\n",
            "\n",
            "23 Gao Huang et al., “Densely Connected Convolutional Networks”, arXiv preprint arXiv:1608.06993 (2016).\n",
            "\n",
            "24 Andrew G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applica‐\n",
            "\n",
            "tions”, arXiv preprint arxiv:1704.04861 (2017).\n",
            "\n",
            "25 Chien-Yao Wang et al., “CSPNet: A New Backbone That Can Enhance Learning Capability of CNN”, arXiv\n",
            "\n",
            "preprint arXiv:1911.11929 (2019).\n",
            "\n",
            "26 Mingxing Tan and Quoc V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”,\n",
            "\n",
            "arXiv preprint arXiv:1905.11946 (2019).\n",
            "\n",
            "CNN Architectures \n",
            "\n",
            "| \n",
            "\n",
            "513\n",
            "\n",
            "\fwidth,  and  resolution  should  scale  as  αϕ,  βϕ,  and  γϕ,  respectively.  The  factors  α,  β,\n",
            "and  γ  must  be  greater  than  1,  and  α  +  β2  +  γ2  should  be  close  to  2.  The  optimal\n",
            "values for these factors depend on the CNN’s architecture. To find the optimal values\n",
            "for  the  EfficientNet  architecture,  the  authors  started  with  a  small  baseline  model\n",
            "(EfficientNetB0), fixed ϕ = 1, and simply ran a grid search: they found α = 1.2, β =\n",
            "1.1,  and  γ  =  1.1.  They  then  used  these  factors  to  create  several  larger  architectures,\n",
            "named EfficientNetB1 to EfficientNetB7, for increasing values of ϕ.\n",
            "\n",
            "Choosing the Right CNN Architecture\n",
            "With  so  many  CNN  architectures,  how  do  you  choose  which  one  is  best  for  your\n",
            "project?  Well,  it  depends  on  what  matters  most  to  you:  Accuracy?  Model  size  (e.g.,\n",
            "for deployment to a mobile device)? Inference speed on CPU? On GPU? Table 14-3\n",
            "lists  the  best  pretrained  models  currently  available  in  Keras  (you’ll  see  how  to  use\n",
            "them later in this chapter), sorted by model size. You can find the full list at https://\n",
            "keras.io/api/applications.  For  each  model,  the  table  shows  the  Keras  class  name  to\n",
            "use  (in  the  tf.keras.applications  package),  the  model’s  size  in  MB,  the  top-1\n",
            "and  top-5  validation  accuracy  on  the  ImageNet  dataset,  the  number  of  parameters\n",
            "(millions),  and  the  inference  time  on  CPU  and  GPU  in  ms,  using  batches  of  32\n",
            "images on reasonably powerful hardware.27 For each column, the best value is high‐\n",
            "lighted.  As  you  can  see,  larger  models  are  generally  more  accurate,  but  not  always;\n",
            "for  example,  EfficientNetB2  outperforms  InceptionV3  both  in  size  and  accuracy.  I\n",
            "only kept InceptionV3 in the list because it is almost twice as fast as EfficientNetB2\n",
            "on  a  CPU.  Similarly,  InceptionResNetV2  is  fast  on  a  CPU,  and  ResNet50V2  and\n",
            "ResNet101V2 are blazingly fast on a GPU.\n",
            "\n",
            "Table 14-3. Pretrained models available in Keras\n",
            "\n",
            "Class name\n",
            "MobileNetV2\n",
            "\n",
            "MobileNet\n",
            "\n",
            "NASNetMobile\n",
            "\n",
            "EfficientNetB0\n",
            "\n",
            "EfficientNetB1\n",
            "\n",
            "EfficientNetB2\n",
            "\n",
            "EfficientNetB3\n",
            "\n",
            "EfficientNetB4\n",
            "\n",
            "InceptionV3\n",
            "\n",
            "ResNet50V2\n",
            "\n",
            "Size (MB)\n",
            "14\n",
            "\n",
            "Top-1 acc\n",
            "71.3%\n",
            "\n",
            "Top-5 acc Params\n",
            "90.1%\n",
            "\n",
            "3.5M\n",
            "\n",
            "CPU (ms) GPU (ms)\n",
            "25.9\n",
            "\n",
            "3.8\n",
            "\n",
            "16\n",
            "\n",
            "23\n",
            "\n",
            "29\n",
            "\n",
            "31\n",
            "\n",
            "36\n",
            "\n",
            "48\n",
            "\n",
            "75\n",
            "\n",
            "92\n",
            "\n",
            "98\n",
            "\n",
            "70.4%\n",
            "\n",
            "74.4%\n",
            "\n",
            "77.1%\n",
            "\n",
            "79.1%\n",
            "\n",
            "80.1%\n",
            "\n",
            "81.6%\n",
            "\n",
            "82.9%\n",
            "\n",
            "77.9%\n",
            "\n",
            "76.0%\n",
            "\n",
            "89.5%\n",
            "\n",
            "91.9%\n",
            "\n",
            "93.3%\n",
            "\n",
            "94.4%\n",
            "\n",
            "94.9%\n",
            "\n",
            "95.7%\n",
            "\n",
            "96.4%\n",
            "\n",
            "93.7%\n",
            "\n",
            "93.0%\n",
            "\n",
            "4.3M\n",
            "\n",
            "5.3M\n",
            "\n",
            "5.3M\n",
            "\n",
            "7.9M\n",
            "\n",
            "9.2M\n",
            "\n",
            "12.3M\n",
            "\n",
            "19.5M\n",
            "\n",
            "23.9M\n",
            "\n",
            "25.6M\n",
            "\n",
            "22.6\n",
            "\n",
            "27.0\n",
            "\n",
            "46.0\n",
            "\n",
            "60.2\n",
            "\n",
            "80.8\n",
            "\n",
            "140.0\n",
            "\n",
            "308.3\n",
            "\n",
            "42.2\n",
            "\n",
            "45.6\n",
            "\n",
            "3.4\n",
            "\n",
            "6.7\n",
            "\n",
            "4.9\n",
            "\n",
            "5.6\n",
            "\n",
            "6.5\n",
            "\n",
            "8.8\n",
            "\n",
            "15.1\n",
            "\n",
            "6.9\n",
            "\n",
            "4.4\n",
            "\n",
            "27 A 92-core AMD EPYC CPU with IBPB, 1.7 TB of RAM, and an Nvidia Tesla A100 GPU.\n",
            "\n",
            "514 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fClass name\n",
            "EfficientNetB5\n",
            "\n",
            "EfficientNetB6\n",
            "\n",
            "ResNet101V2\n",
            "\n",
            "InceptionResNetV2\n",
            "\n",
            "EfficientNetB7\n",
            "\n",
            "Size (MB)\n",
            "118\n",
            "\n",
            "Top-1 acc\n",
            "83.6%\n",
            "\n",
            "Top-5 acc Params\n",
            "96.7%\n",
            "\n",
            "30.6M\n",
            "\n",
            "CPU (ms) GPU (ms)\n",
            "579.2\n",
            "\n",
            "25.3\n",
            "\n",
            "166\n",
            "\n",
            "171\n",
            "\n",
            "215\n",
            "\n",
            "256\n",
            "\n",
            "84.0%\n",
            "\n",
            "77.2%\n",
            "\n",
            "80.3%\n",
            "\n",
            "84.3%\n",
            "\n",
            "96.8%\n",
            "\n",
            "93.8%\n",
            "\n",
            "95.3%\n",
            "\n",
            "97.0%\n",
            "\n",
            "43.3M\n",
            "\n",
            "44.7M\n",
            "\n",
            "55.9M\n",
            "\n",
            "66.7M\n",
            "\n",
            "958.1\n",
            "\n",
            "72.7\n",
            "\n",
            "130.2\n",
            "\n",
            "1578.9\n",
            "\n",
            "40.4\n",
            "\n",
            "5.4\n",
            "\n",
            "10.0\n",
            "\n",
            "61.6\n",
            "\n",
            "I  hope  you  enjoyed  this  deep  dive  into  the  main  CNN  architectures!  Now  let’s  see\n",
            "how to implement one of them using Keras.\n",
            "\n",
            "Implementing a ResNet-34 CNN Using Keras\n",
            "Most CNN architectures described so far can be implemented pretty naturally using\n",
            "Keras (although generally you would load a pretrained network instead, as you will\n",
            "see).  To  illustrate  the  process,  let’s  implement  a  ResNet-34  from  scratch  with  Keras.\n",
            "First, we’ll create a ResidualUnit layer:\n",
            "\n",
            "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
            "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
            "                        use_bias=False)\n",
            "\n",
            "class ResidualUnit(tf.keras.layers.Layer):\n",
            "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.activation = tf.keras.activations.get(activation)\n",
            "        self.main_layers = [\n",
            "            DefaultConv2D(filters, strides=strides),\n",
            "            tf.keras.layers.BatchNormalization(),\n",
            "            self.activation,\n",
            "            DefaultConv2D(filters),\n",
            "            tf.keras.layers.BatchNormalization()\n",
            "        ]\n",
            "        self.skip_layers = []\n",
            "        if strides > 1:\n",
            "            self.skip_layers = [\n",
            "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
            "                tf.keras.layers.BatchNormalization()\n",
            "            ]\n",
            "\n",
            "    def call(self, inputs):\n",
            "        Z = inputs\n",
            "        for layer in self.main_layers:\n",
            "            Z = layer(Z)\n",
            "        skip_Z = inputs\n",
            "        for layer in self.skip_layers:\n",
            "            skip_Z = layer(skip_Z)\n",
            "        return self.activation(Z + skip_Z)\n",
            "\n",
            "Implementing a ResNet-34 CNN Using Keras \n",
            "\n",
            "| \n",
            "\n",
            "515\n",
            "\n",
            "\fAs you can see, this code matches Figure 14-19 pretty closely. In the constructor, we\n",
            "create  all  the  layers  we  will  need:  the  main  layers  are  the  ones  on  the  right  side  of\n",
            "the diagram, and the skip layers are the ones on the left (only needed if the stride is\n",
            "greater than 1). Then in the call() method, we make the inputs go through the main\n",
            "layers and the skip layers (if any), and we add both outputs and apply the activation\n",
            "function.\n",
            "\n",
            "Now we can build a ResNet-34 using a Sequential model, since it’s really just a long\n",
            "sequence of layers—we can treat each residual unit as a single layer now that we have\n",
            "the ResidualUnit class. The code closely matches Figure 14-18:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Activation(\"relu\"),\n",
            "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
            "])\n",
            "prev_filters = 64\n",
            "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
            "    strides = 1 if filters == prev_filters else 2\n",
            "    model.add(ResidualUnit(filters, strides=strides))\n",
            "    prev_filters = filters\n",
            "\n",
            "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
            "model.add(tf.keras.layers.Flatten())\n",
            "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
            "\n",
            "The only tricky part in this code is the loop that adds the ResidualUnit layers to the\n",
            "model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have\n",
            "128 filters, and so on. At each iteration, we must set the stride to 1 when the number\n",
            "of  filters  is  the  same  as  in  the  previous  RU,  or  else  we  set  it  to  2;  then  we  add  the\n",
            "ResidualUnit, and finally we update prev_filters.\n",
            "\n",
            "It  is  amazing  that  in  about  40  lines  of  code,  we  can  build  the  model  that  won  the\n",
            "ILSVRC  2015  challenge!  This  demonstrates  both  the  elegance  of  the  ResNet  model\n",
            "and the expressiveness of the Keras API. Implementing the other CNN architectures\n",
            "is  a  bit  longer,  but  not  much  harder.  However,  Keras  comes  with  several  of  these\n",
            "architectures built in, so why not use them instead?\n",
            "\n",
            "Using Pretrained Models from Keras\n",
            "In general, you won’t have to implement standard models like GoogLeNet or ResNet\n",
            "manually, since pretrained networks are readily available with a single line of code in\n",
            "the tf.keras.applications package.\n",
            "\n",
            "516 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFor  example,  you  can  load  the  ResNet-50  model,  pretrained  on  ImageNet,  with  the\n",
            "following line of code:\n",
            "\n",
            "model = tf.keras.applications.ResNet50(weights=\"imagenet\")\n",
            "\n",
            "That’s  all!  This  will  create  a  ResNet-50  model  and  download  weights  pretrained  on\n",
            "the  ImageNet  dataset.  To  use  it,  you  first  need  to  ensure  that  the  images  have  the\n",
            "right  size.  A  ResNet-50  model  expects  224  ×  224–pixel  images  (other  models  may\n",
            "expect other sizes, such as 299 × 299), so let’s use Keras’s Resizing layer (introduced\n",
            "in Chapter 13) to resize two sample images (after cropping them to the target aspect\n",
            "ratio):\n",
            "\n",
            "images = load_sample_images()[\"images\"]\n",
            "images_resized = tf.keras.layers.Resizing(height=224, width=224,\n",
            "                                          crop_to_aspect_ratio=True)(images)\n",
            "\n",
            "The pretrained models assume that the images are preprocessed in a specific way. In\n",
            "some  cases  they  may  expect  the  inputs  to  be  scaled  from  0  to  1,  or  from  –1  to  1,\n",
            "and so on. Each model provides a preprocess_input() function that you can use to\n",
            "preprocess your images. These functions assume that the original pixel values range\n",
            "from 0 to 255, which is the case here:\n",
            "\n",
            "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)\n",
            "\n",
            "Now we can use the pretrained model to make predictions:\n",
            "\n",
            ">>> Y_proba = model.predict(inputs)\n",
            ">>> Y_proba.shape\n",
            "(2, 1000)\n",
            "\n",
            "As  usual,  the  output  Y_proba  is  a  matrix  with  one  row  per  image  and  one  column\n",
            "per  class  (in  this  case,  there  are  1,000  classes).  If  you  want  to  display  the  top  K\n",
            "predictions, including the class name and the estimated probability of each predicted\n",
            "class,  use  the  decode_predictions()  function.  For  each  image,  it  returns  an  array\n",
            "containing  the  top  K  predictions,  where  each  prediction  is  represented  as  an  array\n",
            "containing the class identifier,28 its name, and the corresponding confidence score:\n",
            "\n",
            "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
            "for image_index in range(len(images)):\n",
            "    print(f\"Image #{image_index}\")\n",
            "    for class_id, name, y_proba in top_K[image_index]:\n",
            "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")\n",
            "\n",
            "The output looks like this:\n",
            "\n",
            "28 In the ImageNet dataset, each image is mapped to a word in the WordNet dataset: the class ID is just a\n",
            "\n",
            "WordNet ID.\n",
            "\n",
            "Using Pretrained Models from Keras \n",
            "\n",
            "| \n",
            "\n",
            "517\n",
            "\n",
            "\fImage #0\n",
            "  n03877845 - palace       54.69%\n",
            "  n03781244 - monastery    24.72%\n",
            "  n02825657 - bell_cote    18.55%\n",
            "Image #1\n",
            "  n04522168 - vase         32.66%\n",
            "  n11939491 - daisy        17.81%\n",
            "  n03530642 - honeycomb    12.06%\n",
            "\n",
            "The correct classes are palace and dahlia, so the model is correct for the first image\n",
            "but  wrong  for  the  second.  However,  that’s  because  dahlia  is  not  one  of  the  1,000\n",
            "ImageNet classes. With that in mind, vase is a reasonable guess (perhaps the flower\n",
            "is in a vase?), and daisy is not a bad choice either, since dahlias and daisies are both\n",
            "from the same Compositae family.\n",
            "\n",
            "As  you  can  see,  it  is  very  easy  to  create  a  pretty  good  image  classifier  using  a\n",
            "pretrained model. As you saw in Table 14-3, many other vision models are available\n",
            "in  tf.keras.applications,  from  lightweight  and  fast  models  to  large  and  accurate\n",
            "ones.\n",
            "\n",
            "But what if you want to use an image classifier for classes of images that are not part\n",
            "of ImageNet? In that case, you may still benefit from the pretrained models by using\n",
            "them to perform transfer learning.\n",
            "\n",
            "Pretrained Models for Transfer Learning\n",
            "If you want to build an image classifier but you do not have enough data to train it\n",
            "from  scratch,  then  it  is  often  a  good  idea  to  reuse  the  lower  layers  of  a  pretrained\n",
            "model,  as  we  discussed  in  Chapter  11.  For  example,  let’s  train  a  model  to  classify\n",
            "pictures of flowers, reusing a pretrained Xception model. First, we’ll load the flowers\n",
            "dataset using TensorFlow Datasets (introduced in Chapter 13):\n",
            "\n",
            "import tensorflow_datasets as tfds\n",
            "\n",
            "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
            "dataset_size = info.splits[\"train\"].num_examples  # 3670\n",
            "class_names = info.features[\"label\"].names  # [\"dandelion\", \"daisy\", ...]\n",
            "n_classes = info.features[\"label\"].num_classes  # 5\n",
            "\n",
            "Note that you can get information about the dataset by setting with_info=True. Here,\n",
            "we  get  the  dataset  size  and  the  names  of  the  classes.  Unfortunately,  there  is  only  a\n",
            "\"train\" dataset, no test set or validation set, so we need to split the training set. Let’s\n",
            "call  tfds.load()  again,  but  this  time  taking  the  first  10%  of  the  dataset  for  testing,\n",
            "the next 15% for validation, and the remaining 75% for training:\n",
            "\n",
            "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
            "    \"tf_flowers\",\n",
            "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
            "    as_supervised=True)\n",
            "\n",
            "518 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fAll  three  datasets  contain  individual  images.  We  need  to  batch  them,  but  first\n",
            "we  need  to  ensure  they  all  have  the  same  size,  or  batching  will  fail.  We  can\n",
            "use  a  Resizing  layer  for  this.  We  must  also  call  the  tf.keras.applications.\n",
            "xception.preprocess_input()  function  to  preprocess  the  images  appropriately  for\n",
            "the Xception model. Lastly, we’ll also shuffle the training set and use prefetching:\n",
            "\n",
            "batch_size = 32\n",
            "preprocess = tf.keras.Sequential([\n",
            "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
            "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
            "])\n",
            "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
            "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
            "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
            "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
            "\n",
            "Now  each  batch  contains  32  images,  all  of  them  224  ×  224  pixels,  with  pixel  values\n",
            "ranging from –1 to 1. Perfect!\n",
            "\n",
            "Since the dataset is not very large, a bit of data augmentation will certainly help. Let’s\n",
            "create  a  data  augmentation  model  that  we  will  embed  in  our  final  model.  During\n",
            "training,  it  will  randomly  flip  the  images  horizontally,  rotate  them  a  little  bit,  and\n",
            "tweak the contrast:\n",
            "\n",
            "data_augmentation = tf.keras.Sequential([\n",
            "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
            "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
            "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
            "])\n",
            "\n",
            "The  tf.keras.preprocessing.image.ImageDataGenerator  class\n",
            "makes  it  easy  to  load  images  from  disk  and  augment  them  in\n",
            "various  ways:  you  can  shift  each  image,  rotate  it,  rescale  it,  flip\n",
            "it  horizontally  or  vertically,  shear  it,  or  apply  any  transformation\n",
            "function you want to it. This is very convenient for simple projects.\n",
            "However, a tf.data pipeline is not much more complicated, and it’s\n",
            "generally faster. Moreover, if you have a GPU and you include the\n",
            "preprocessing or data augmentation layers inside your model, they\n",
            "will benefit from GPU acceleration during training.\n",
            "\n",
            "Next  let’s  load  an  Xception  model,  pretrained  on  ImageNet.  We  exclude  the  top  of\n",
            "the network by setting include_top=False. This excludes the global average pooling\n",
            "layer and the dense output layer. We then add our own global average pooling layer\n",
            "(feeding it the output of the base model), followed by a dense output layer with one\n",
            "unit  per  class,  using  the  softmax  activation  function.  Finally,  we  wrap  all  this  in  a\n",
            "Keras Model:\n",
            "\n",
            "Pretrained Models for Transfer Learning \n",
            "\n",
            "| \n",
            "\n",
            "519\n",
            "\n",
            "\fbase_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
            "                                                     include_top=False)\n",
            "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
            "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
            "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
            "\n",
            "As  explained  in  Chapter  11,  it’s  usually  a  good  idea  to  freeze  the  weights  of  the\n",
            "pretrained layers, at least at the beginning of training:\n",
            "\n",
            "for layer in base_model.layers:\n",
            "    layer.trainable = False\n",
            "\n",
            "Since  our  model  uses  the  base  model’s  layers  directly,  rather  than\n",
            "the base_model object itself, setting base_model.trainable=False\n",
            "would have no effect.\n",
            "\n",
            "Finally, we can compile the model and start training:\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
            "              metrics=[\"accuracy\"])\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=3)\n",
            "\n",
            "If  you  are  running  in  Colab,  make  sure  the  runtime  is  using  a\n",
            "GPU:  select  Runtime  →  “Change  runtime  type”,  choose  “GPU”  in\n",
            "the  “Hardware  accelerator”  drop-down  menu,  then  click  Save.  It’s\n",
            "possible  to  train  the  model  without  a  GPU,  but  it  will  be  terribly\n",
            "slow (minutes per epoch, as opposed to seconds).\n",
            "\n",
            "After training the model for a few epochs, its validation accuracy should reach a bit\n",
            "over  80%  and  then  stop  improving.  This  means  that  the  top  layers  are  now  pretty\n",
            "well trained, and we are ready to unfreeze some of the base model’s top layers, then\n",
            "continue training. For example, let’s unfreeze layers 56 and above (that’s the start of\n",
            "residual unit 7 out of 14, as you can see if you list the layer names):\n",
            "\n",
            "for layer in base_model.layers[56:]:\n",
            "    layer.trainable = True\n",
            "\n",
            "Don’t forget to compile the model whenever you freeze or unfreeze layers. Also make\n",
            "sure to use a much lower learning rate to avoid damaging the pretrained weights:\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
            "              metrics=[\"accuracy\"])\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=10)\n",
            "\n",
            "This model should reach around 92% accuracy on the test set, in just a few minutes\n",
            "of  training  (with  a  GPU).  If  you  tune  the  hyperparameters,  lower  the  learning  rate,\n",
            "\n",
            "520 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fand train for quite a bit longer, you should be able to reach 95% to 97%. With that,\n",
            "you can start training amazing image classifiers on your own images and classes! But\n",
            "there’s more to computer vision than just classification. For example, what if you also\n",
            "want to know where the flower is in a picture? Let’s look at this now.\n",
            "\n",
            "Classification and Localization\n",
            "Localizing  an  object  in  a  picture  can  be  expressed  as  a  regression  task,  as  discussed\n",
            "in Chapter 10: to predict a bounding box around the object, a common approach is\n",
            "to predict the horizontal and vertical coordinates of the object’s center, as well as its\n",
            "height and width. This means we have four numbers to predict. It does not require\n",
            "much change to the model; we just need to add a second dense output layer with four\n",
            "units (typically on top of the global average pooling layer), and it can be trained using\n",
            "the MSE loss:\n",
            "\n",
            "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
            "                                                     include_top=False)\n",
            "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
            "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
            "loc_output = tf.keras.layers.Dense(4)(avg)\n",
            "model = tf.keras.Model(inputs=base_model.input,\n",
            "                       outputs=[class_output, loc_output])\n",
            "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
            "              loss_weights=[0.8, 0.2],  # depends on what you care most about\n",
            "              optimizer=optimizer, metrics=[\"accuracy\"])\n",
            "\n",
            "But  now  we  have  a  problem:  the  flowers  dataset  does  not  have  bounding  boxes\n",
            "around  the  flowers.  So,  we  need  to  add  them  ourselves.  This  is  often  one  of  the\n",
            "hardest  and  most  costly  parts  of  a  machine  learning  project:  getting  the  labels.  It’s\n",
            "a  good  idea  to  spend  time  looking  for  the  right  tools.  To  annotate  images  with\n",
            "bounding boxes, you may want to use an open source image labeling tool like VGG\n",
            "Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial tool\n",
            "like LabelBox or Supervisely. You may also want to consider crowdsourcing platforms\n",
            "such  as  Amazon  Mechanical  Turk  if  you  have  a  very  large  number  of  images  to\n",
            "annotate.  However,  it  is  quite  a  lot  of  work  to  set  up  a  crowdsourcing  platform,\n",
            "prepare  the  form  to  be  sent  to  the  workers,  supervise  them,  and  ensure  that  the\n",
            "quality  of  the  bounding  boxes  they  produce  is  good,  so  make  sure  it  is  worth  the\n",
            "effort.  Adriana  Kovashka  et  al.  wrote  a  very  practical  paper29  about  crowdsourcing\n",
            "in  computer  vision.  I  recommend  you  check  it  out,  even  if  you  do  not  plan  to  use\n",
            "crowdsourcing. If there are just a few hundred or a even a couple thousand images\n",
            "to  label,  and  you  don’t  plan  to  do  this  frequently,  it  may  be  preferable  to  do  it\n",
            "\n",
            "29 Adriana Kovashka et al., “Crowdsourcing in Computer Vision”, Foundations and Trends in Computer Graphics\n",
            "\n",
            "and Vision 10, no. 3 (2014): 177–243.\n",
            "\n",
            "Classification and Localization \n",
            "\n",
            "| \n",
            "\n",
            "521\n",
            "\n",
            "\fyourself: with the right tools, it will only take a few days, and you’ll also gain a better\n",
            "understanding of your dataset and task.\n",
            "\n",
            "Now let’s suppose you’ve obtained the bounding boxes for every image in the flowers\n",
            "dataset (for now we will assume there is a single bounding box per image). You then\n",
            "need  to  create  a  dataset  whose  items  will  be  batches  of  preprocessed  images  along\n",
            "with their class labels and their bounding boxes. Each item should be a tuple of the\n",
            "form  (images,  (class_labels,  bounding_boxes)).  Then  you  are  ready  to  train\n",
            "your model!\n",
            "\n",
            "The  bounding  boxes  should  be  normalized  so  that  the  horizontal\n",
            "and vertical coordinates, as well as the height and width, all range\n",
            "from  0  to  1.  Also,  it  is  common  to  predict  the  square  root  of  the\n",
            "height  and  width  rather  than  the  height  and  width  directly:  this\n",
            "way, a 10-pixel error for a large bounding box will not be penalized\n",
            "as much as a 10-pixel error for a small bounding box.\n",
            "\n",
            "The  MSE  often  works  fairly  well  as  a  cost  function  to  train  the  model,  but  it  is\n",
            "not  a  great  metric  to  evaluate  how  well  the  model  can  predict  bounding  boxes.\n",
            "The  most  common  metric  for  this  is  the  intersection  over  union  (IoU):  the  area  of\n",
            "overlap  between  the  predicted  bounding  box  and  the  target  bounding  box,  divided\n",
            "by  the  area  of  their  union  (see  Figure  14-24).  In  Keras,  it  is  implemented  by  the\n",
            "tf.keras.metrics.MeanIoU class.\n",
            "\n",
            "Classifying  and  localizing  a  single  object  is  nice,  but  what  if  the  images  contain\n",
            "multiple objects (as is often the case in the flowers dataset)?\n",
            "\n",
            "Figure 14-24. IoU metric for bounding boxes\n",
            "\n",
            "522 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fObject Detection\n",
            "The  task  of  classifying  and  localizing  multiple  objects  in  an  image  is  called  object\n",
            "detection.  Until  a  few  years  ago,  a  common  approach  was  to  take  a  CNN  that  was\n",
            "trained  to  classify  and  locate  a  single  object  roughly  centered  in  the  image,  then\n",
            "slide  this  CNN  across  the  image  and  make  predictions  at  each  step.  The  CNN  was\n",
            "generally  trained  to  predict  not  only  class  probabilities  and  a  bounding  box,  but\n",
            "also  an  objectness  score:  this  is  the  estimated  probability  that  the  image  does  indeed\n",
            "contain  an  object  centered  near  the  middle.  This  is  a  binary  classification  output;\n",
            "it  can  be  produced  by  a  dense  output  layer  with  a  single  unit,  using  the  sigmoid\n",
            "activation function and trained using the binary cross-entropy loss.\n",
            "\n",
            "Instead  of  an  objectness  score,  a  “no-object”  class  was  sometimes\n",
            "added, but in general this did not work as well: the questions “Is an\n",
            "object present?” and “What type of object is it?” are best answered\n",
            "separately.\n",
            "\n",
            "This sliding-CNN approach is illustrated in Figure 14-25. In this example, the image\n",
            "was chopped into a 5 × 7 grid, and we see a CNN—the thick black rectangle—sliding\n",
            "across all 3 × 3 regions and making predictions at each step.\n",
            "\n",
            "Figure 14-25. Detecting multiple objects by sliding a CNN across the image\n",
            "\n",
            "Object Detection \n",
            "\n",
            "| \n",
            "\n",
            "523\n",
            "\n",
            "\fIn this figure, the CNN has already made predictions for three of these 3 × 3 regions:\n",
            "\n",
            "• When looking at the top-left 3 × 3 region (centered on the red-shaded grid cell\n",
            "•\n",
            "located  in  the  second  row  and  second  column),  it  detected  the  leftmost  rose.\n",
            "Notice  that  the  predicted  bounding  box  exceeds  the  boundary  of  this  3  ×  3\n",
            "region.  That’s  absolutely  fine:  even  though  the  CNN  could  not  see  the  bottom\n",
            "part  of  the  rose,  it  was  able  to  make  a  reasonable  guess  as  to  where  it  might\n",
            "be.  It  also  predicted  class  probabilities,  giving  a  high  probability  to  the  “rose”\n",
            "class.  Lastly,  it  predicted  a  fairly  high  objectness  score,  since  the  center  of  the\n",
            "bounding box lies within the central grid cell (in this figure, the objectness score\n",
            "is represented by the thickness of the bounding box).\n",
            "\n",
            "•\n",
            "• When  looking  at  the  next  3  ×  3  region,  one  grid  cell  to  the  right  (centered  on\n",
            "the shaded blue square), it did not detect any flower centered in that region, so it\n",
            "predicted a very low objectness score; therefore, the predicted bounding box and\n",
            "class probabilities can safely be ignored. You can see that the predicted bounding\n",
            "box was no good anyway.\n",
            "\n",
            "• finally,  when  looking  at  the  next  3  ×  3  region,  again  one  grid  cell  to  the  right\n",
            "•\n",
            "(centered  on  the  shaded  green  cell),  it  detected  the  rose  at  the  top,  although\n",
            "not  perfectly:  this  rose  is  not  well  centered  within  this  region,  so  the  predicted\n",
            "objectness score was not very high.\n",
            "\n",
            "You can imagine how sliding the CNN across the whole image would give you a total\n",
            "of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding box\n",
            "accompanied  by  its  estimated  class  probabilities  and  objectness  score.  Since  objects\n",
            "can have varying sizes, you may then want to slide the CNN again across larger 4 × 4\n",
            "regions as well, to get even more bounding boxes.\n",
            "\n",
            "This  technique  is  fairly  straightforward,  but  as  you  can  see  it  will  often  detect  the\n",
            "same  object  multiple  times,  at  slightly  different  positions.  Some  postprocessing  is\n",
            "needed  to  get  rid  of  all  the  unnecessary  bounding  boxes.  A  common  approach  for\n",
            "this is called non-max suppression. Here’s how it works:\n",
            "\n",
            "1. First,  get  rid  of  all  the  bounding  boxes  for  which  the  objectness  score  is  below\n",
            "1.\n",
            "some  threshold:  since  the  CNN  believes  there’s  no  object  at  that  location,  the\n",
            "bounding box is useless.\n",
            "\n",
            "2.\n",
            "2. Find the remaining bounding box with the highest objectness score, and get rid\n",
            "of all the other remaining bounding boxes that overlap a lot with it (e.g., with an\n",
            "IoU greater than 60%). For example, in Figure 14-25, the bounding box with the\n",
            "max objectness score is the thick bounding box over the leftmost rose. The other\n",
            "bounding box that touches this same rose overlaps a lot with the max bounding\n",
            "box, so we will get rid of it (although in this example it would already have been\n",
            "removed in the previous step).\n",
            "\n",
            "524 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\f3.\n",
            "3. Repeat step 2 until there are no more bounding boxes to get rid of.\n",
            "\n",
            "This  simple  approach  to  object  detection  works  pretty  well,  but  it  requires  running\n",
            "the CNN many times (15 times in this example), so it is quite slow. Fortunately, there\n",
            "is  a  much  faster  way  to  slide  a  CNN  across  an  image:  using  a  fully  convolutional\n",
            "network (FCN).\n",
            "\n",
            "Fully Convolutional Networks\n",
            "The idea of FCNs was first introduced in a 2015 paper30 by Jonathan Long et al., for\n",
            "semantic  segmentation  (the  task  of  classifying  every  pixel  in  an  image  according  to\n",
            "the class of the object it belongs to). The authors pointed out that you could replace\n",
            "the dense layers at the top of a CNN with convolutional layers. To understand this,\n",
            "let’s  look  at  an  example:  suppose  a  dense  layer  with  200  neurons  sits  on  top  of  a\n",
            "convolutional  layer  that  outputs  100  feature  maps,  each  of  size  7  ×  7  (this  is  the\n",
            "feature  map  size,  not  the  kernel  size).  Each  neuron  will  compute  a  weighted  sum\n",
            "of  all  100  ×  7  ×  7  activations  from  the  convolutional  layer  (plus  a  bias  term).  Now\n",
            "let’s see what happens if we replace the dense layer with a convolutional layer using\n",
            "200 filters, each of size 7 × 7, and with \"valid\" padding. This layer will output 200\n",
            "feature maps, each 1 × 1 (since the kernel is exactly the size of the input feature maps\n",
            "and we are using \"valid\" padding). In other words, it will output 200 numbers, just\n",
            "like the dense layer did; and if you look closely at the computations performed by a\n",
            "convolutional layer, you will notice that these numbers will be precisely the same as\n",
            "those  the  dense  layer  produced.  The  only  difference  is  that  the  dense  layer’s  output\n",
            "was  a  tensor  of  shape  [batch  size,  200],  while  the  convolutional  layer  will  output  a\n",
            "tensor of shape [batch size, 1, 1, 200].\n",
            "\n",
            "To  convert  a  dense  layer  to  a  convolutional  layer,  the  number  of\n",
            "filters  in  the  convolutional  layer  must  be  equal  to  the  number  of\n",
            "units in the dense layer, the filter size must be equal to the size of\n",
            "the  input  feature  maps,  and  you  must  use  \"valid\"  padding.  The\n",
            "stride may be set to 1 or more, as you will see shortly.\n",
            "\n",
            "Why is this important? Well, while a dense layer expects a specific input size (since it\n",
            "has one weight per input feature), a convolutional layer will happily process images\n",
            "of any size31 (however, it does expect its inputs to have a specific number of channels,\n",
            "since  each  kernel  contains  a  different  set  of  weights  for  each  input  channel).  Since\n",
            "\n",
            "30 Jonathan Long et al., “Fully Convolutional Networks for Semantic Segmentation”, Proceedings of the IEEE\n",
            "\n",
            "Conference on Computer Vision and Pattern Recognition (2015): 3431–3440.\n",
            "\n",
            "31 There is one small exception: a convolutional layer using \"valid\" padding will complain if the input size is\n",
            "\n",
            "smaller than the kernel size.\n",
            "\n",
            "Object Detection \n",
            "\n",
            "| \n",
            "\n",
            "525\n",
            "\n",
            "\fan FCN contains only convolutional layers (and pooling layers, which have the same\n",
            "property), it can be trained and executed on images of any size!\n",
            "\n",
            "For example, suppose we’d already trained a CNN for flower classification and locali‐\n",
            "zation. It was trained on 224 × 224 images, and it outputs 10 numbers:\n",
            "\n",
            "•\n",
            "• Outputs  0  to  4  are  sent  through  the  softmax  activation  function,  and  this  gives\n",
            "\n",
            "the class probabilities (one per class).\n",
            "\n",
            "•\n",
            "• Output  5  is  sent  through  the  sigmoid  activation  function,  and  this  gives  the\n",
            "\n",
            "objectness score.\n",
            "\n",
            "• Outputs  6  and  7  represent  the  bounding  box’s  center  coordinates;  they  also  go\n",
            "•\n",
            "\n",
            "through a sigmoid activation function to ensure they range from 0 to 1.\n",
            "\n",
            "• Lastly,  outputs  8  and  9  represent  the  bounding  box’s  height  and  width;  they  do\n",
            "•\n",
            "not  go  through  any  activation  function  to  allow  the  bounding  boxes  to  extend\n",
            "beyond the borders of the image.\n",
            "\n",
            "We can now convert the CNN’s dense layers to convolutional layers. In fact, we don’t\n",
            "even  need  to  retrain  it;  we  can  just  copy  the  weights  from  the  dense  layers  to  the\n",
            "convolutional  layers!  Alternatively,  we  could  have  converted  the  CNN  into  an  FCN\n",
            "before training.\n",
            "\n",
            "Now  suppose  the  last  convolutional  layer  before  the  output  layer  (also  called  the\n",
            "bottleneck  layer)  outputs  7  ×  7  feature  maps  when  the  network  is  fed  a  224  ×  224\n",
            "image  (see  the  left  side  of  Figure  14-26).  If  we  feed  the  FCN  a  448  ×  448  image\n",
            "(see  the  right  side  of  Figure  14-26),  the  bottleneck  layer  will  now  output  14  ×  14\n",
            "feature  maps.32  Since  the  dense  output  layer  was  replaced  by  a  convolutional  layer\n",
            "using 10 filters of size 7 × 7, with  \"valid\" padding and stride 1, the output will be\n",
            "composed  of  10  features  maps,  each  of  size  8  ×  8  (since  14  –  7  +  1  =  8).  In  other\n",
            "words,  the  FCN  will  process  the  whole  image  only  once,  and  it  will  output  an  8  ×\n",
            "8 grid where each cell contains 10 numbers (5 class probabilities, 1 objectness score,\n",
            "and 4 bounding box coordinates). It’s exactly like taking the original CNN and sliding\n",
            "it  across  the  image  using  8  steps  per  row  and  8  steps  per  column.  To  visualize  this,\n",
            "imagine chopping the original image into a 14 × 14 grid, then sliding a 7 × 7 window\n",
            "across this grid; there will be 8 × 8 = 64 possible locations for the window, hence 8 ×\n",
            "8 predictions. However, the FCN approach is much more efficient, since the network\n",
            "only looks at the image once. In fact, You Only Look Once (YOLO) is the name of a\n",
            "very popular object detection architecture, which we’ll look at next.\n",
            "\n",
            "32 This assumes we used only \"same\" padding in the network: \"valid\" padding would reduce the size of the\n",
            "\n",
            "feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any rounding\n",
            "error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\n",
            "feature maps may end up being smaller.\n",
            "\n",
            "526 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFigure 14-26. The same fully convolutional network processing a small image (left) and a\n",
            "large one (right)\n",
            "\n",
            "You Only Look Once\n",
            "YOLO  is  a  fast  and  accurate  object  detection  architecture  proposed  by  Joseph  Red‐\n",
            "mon et al. in a 2015 paper.33 It is so fast that it can run in real time on a video, as seen\n",
            "in Redmon’s demo. YOLO’s architecture is quite similar to the one we just discussed,\n",
            "but with a few important differences:\n",
            "\n",
            "• For each grid cell, YOLO only considers objects whose bounding box center lies\n",
            "•\n",
            "within  that  cell.  The  bounding  box  coordinates  are  relative  to  that  cell,  where\n",
            "(0,  0)  means  the  top-left  corner  of  the  cell  and  (1,  1)  means  the  bottom  right.\n",
            "However, the bounding box’s height and width may extend well beyond the cell.\n",
            "\n",
            "33 Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection”, Proceedings of the IEEE\n",
            "\n",
            "Conference on Computer Vision and Pattern Recognition (2016): 779–788.\n",
            "\n",
            "Object Detection \n",
            "\n",
            "| \n",
            "\n",
            "527\n",
            "\n",
            "\f• It  outputs  two  bounding  boxes  for  each  grid  cell  (instead  of  just  one),  which\n",
            "•\n",
            "allows  the  model  to  handle  cases  where  two  objects  are  so  close  to  each  other\n",
            "that their bounding box centers lie within the same cell. Each bounding box also\n",
            "comes with its own objectness score.\n",
            "\n",
            "• YOLO also outputs a class probability distribution for each grid cell, predicting\n",
            "•\n",
            "20 class probabilities per grid cell since YOLO was trained on the PASCAL VOC\n",
            "dataset, which contains 20 classes. This produces a coarse class probability map.\n",
            "Note that the model predicts one class probability distribution per grid cell, not\n",
            "per bounding box. However, it’s possible to estimate class probabilities for each\n",
            "bounding box during postprocessing, by measuring how well each bounding box\n",
            "matches each class in the class probability map. For example, imagine a picture\n",
            "of  a  person  standing  in  front  of  a  car.  There  will  be  two  bounding  boxes:  one\n",
            "large horizontal one for the car, and a smaller vertical one for the person. These\n",
            "bounding  boxes  may  have  their  centers  within  the  same  grid  cell.  So  how  can\n",
            "we  tell  which  class  should  be  assigned  to  each  bounding  box?  Well,  the  class\n",
            "probability  map  will  contain  a  large  region  where  the  “car”  class  is  dominant,\n",
            "and inside it there will be a smaller region where the “person” class is dominant.\n",
            "Hopefully, the car’s bounding box will roughly match the “car” region, while the\n",
            "person’s bounding box will roughly match the “person” region: this will allow the\n",
            "correct class to be assigned to each bounding box.\n",
            "\n",
            "YOLO was originally developed using Darknet, an open source deep learning frame‐\n",
            "work initially developed in C by Joseph Redmon, but it was soon ported to Tensor‐\n",
            "Flow, Keras, PyTorch, and more. It was continuously improved over the years, with\n",
            "YOLOv2,  YOLOv3,  and  YOLO9000  (again  by  Joseph  Redmon  et  al.),  YOLOv4  (by\n",
            "Alexey  Bochkovskiy  et  al.),  YOLOv5  (by  Glenn  Jocher),  and  PP-YOLO  (by  Xiang\n",
            "Long et al.).\n",
            "\n",
            "Each  version  brought  some  impressive  improvements  in  speed  and  accuracy,  using\n",
            "a  variety  of  techniques;  for  example,  YOLOv3  boosted  accuracy  in  part  thanks  to\n",
            "anchor priors, exploiting the fact that some bounding box shapes are more likely than\n",
            "others,  depending  on  the  class  (e.g.,  people  tend  to  have  vertical  bounding  boxes,\n",
            "while cars usually don’t). They also increased the number of bounding boxes per grid\n",
            "cell,  they  trained  on  different  datasets  with  many  more  classes  (up  to  9,000  classes\n",
            "organized in a hierarchy in the case of YOLO9000), they added skip connections to\n",
            "recover  some  of  the  spatial  resolution  that  is  lost  in  the  CNN  (we  will  discuss  this\n",
            "shortly,  when  we  look  at  semantic  segmentation),  and  much  more.  There  are  many\n",
            "variants of these models too, such as YOLOv4-tiny, which is optimized to be trained\n",
            "on less powerful machines and which can run extremely fast (at over 1,000 frames per\n",
            "second!), but with a slightly lower mean average precision (mAP).\n",
            "\n",
            "528 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fMean Average Precision\n",
            "A very common metric used in object detection tasks is the mean average precision.\n",
            "“Mean  average”  sounds  a  bit  redundant,  doesn’t  it?  To  understand  this  metric,  let’s\n",
            "go back to two classification metrics we discussed in Chapter 3: precision and recall.\n",
            "Remember  the  trade-off:  the  higher  the  recall,  the  lower  the  precision.  You  can\n",
            "visualize  this  in  a  precision/recall  curve  (see  Figure  3-6).  To  summarize  this  curve\n",
            "into a single number, we could compute its area under the curve (AUC). But note that\n",
            "the precision/recall curve may contain a few sections where precision actually goes up\n",
            "when recall increases, especially at low recall values (you can see this at the top left of\n",
            "Figure 3-6). This is one of the motivations for the mAP metric.\n",
            "\n",
            "Suppose  the  classifier  has  90%  precision  at  10%  recall,  but  96%  precision  at  20%\n",
            "recall. There’s really no trade-off here: it simply makes more sense to use the classifier\n",
            "at 20% recall rather than at 10% recall, as you will get both higher recall and higher\n",
            "precision.  So  instead  of  looking  at  the  precision  at  10%  recall,  we  should  really  be\n",
            "looking  at  the  maximum  precision  that  the  classifier  can  offer  with  at  least  10%\n",
            "recall. It would be 96%, not 90%. Therefore, one way to get a fair idea of the model’s\n",
            "performance  is  to  compute  the  maximum  precision  you  can  get  with  at  least  0%\n",
            "recall,  then  10%  recall,  20%,  and  so  on  up  to  100%,  and  then  calculate  the  mean\n",
            "of these maximum precisions. This is called the average precision (AP) metric. Now\n",
            "when  there  are  more  than  two  classes,  we  can  compute  the  AP  for  each  class,  and\n",
            "then compute the mean AP (mAP). That’s it!\n",
            "\n",
            "In  an  object  detection  system,  there  is  an  additional  level  of  complexity:  what  if\n",
            "the  system  detected  the  correct  class,  but  at  the  wrong  location  (i.e.,  the  bounding\n",
            "box  is  completely  off)?  Surely  we  should  not  count  this  as  a  positive  prediction.\n",
            "One  approach  is  to  define  an  IoU  threshold:  for  example,  we  may  consider  that  a\n",
            "prediction is correct only if the IoU is greater than, say, 0.5, and the predicted class\n",
            "is  correct.  The  corresponding  mAP  is  generally  noted  mAP@0.5  (or  mAP@50%,  or\n",
            "sometimes  just  AP50).  In  some  competitions  (such  as  the  PASCAL  VOC  challenge),\n",
            "this is what is done. In others (such as the COCO competition), the mAP is computed\n",
            "for  different  IoU  thresholds  (0.50,  0.55,  0.60,  …,  0.95),  and  the  final  metric  is  the\n",
            "mean  of  all  these  mAPs  (noted  mAP@[.50:.95]  or  mAP@[.50:0.05:.95]).  Yes,  that’s  a\n",
            "mean mean average.\n",
            "\n",
            "Object Detection \n",
            "\n",
            "| \n",
            "\n",
            "529\n",
            "\n",
            "\fMany  object  detection  models  are  available  on  TensorFlow  Hub,  often  with  pre‐\n",
            "trained weights, such as YOLOv5,34 SSD,35 Faster R-CNN,36 and EfficentDet.37\n",
            "\n",
            "SSD and EfficientDet are “look once” detection models, similar to YOLO. Efficient‐\n",
            "Det  is  based  on  the  EfficientNet  convolutional  architecture.  Faster  R-CNN  is  more\n",
            "complex: the image first goes through a CNN, then the output is passed to a region\n",
            "proposal network (RPN) that proposes bounding boxes that are most likely to contain\n",
            "an object; a classifier is then run for each bounding box, based on the cropped output\n",
            "of the CNN. The best place to start using these models is TensorFlow Hub’s excellent\n",
            "object detection tutorial.\n",
            "\n",
            "So  far,  we’ve  only  considered  detecting  objects  in  single  images.  But  what  about\n",
            "videos? Objects must not only be detected in each frame, they must also be tracked\n",
            "over time. Let’s take a quick look at object tracking now.\n",
            "\n",
            "Object Tracking\n",
            "Object tracking is a challenging task: objects move, they may grow or shrink as they\n",
            "get closer to or further away from the camera, their appearance may change as they\n",
            "turn  around  or  move  to  different  lighting  conditions  or  backgrounds,  they  may  be\n",
            "temporarily occluded by other objects, and so on.\n",
            "\n",
            "One  of  the  most  popular  object  tracking  systems  is  DeepSORT.38  It  is  based  on  a\n",
            "combination of classical algorithms and deep learning:\n",
            "\n",
            "•\n",
            "• It  uses  Kalman  filters  to  estimate  the  most  likely  current  position  of  an  object\n",
            "given  prior  detections,  and  assuming  that  objects  tend  to  move  at  a  constant\n",
            "speed.\n",
            "\n",
            "•\n",
            "• It  uses  a  deep  learning  model  to  measure  the  resemblance  between  new  detec‐\n",
            "\n",
            "tions and existing tracked objects.\n",
            "\n",
            "34 You can find YOLOv3, YOLOv4, and their tiny variants in the TensorFlow Models project at https://homl.info/\n",
            "\n",
            "yolotf.\n",
            "\n",
            "35 Wei Liu et al., “SSD: Single Shot Multibox Detector”, Proceedings of the 14th European Conference on Computer\n",
            "\n",
            "Vision 1 (2016): 21–37.\n",
            "\n",
            "36 Shaoqing Ren et al., “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”,\n",
            "Proceedings of the 28th International Conference on Neural Information Processing Systems 1 (2015): 91–99.\n",
            "\n",
            "37 Mingxing Tan et al., “EfficientDet: Scalable and Efficient Object Detection”, arXiv preprint arXiv:1911.09070\n",
            "\n",
            "(2019).\n",
            "\n",
            "38 Nicolai Wojke et al., “Simple Online and Realtime Tracking with a Deep Association Metric”, arXiv preprint\n",
            "\n",
            "arXiv:1703.07402 (2017).\n",
            "\n",
            "530 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\f• Lastly,  it  uses  the  Hungarian  algorithm  to  map  new  detections  to  existing\n",
            "•\n",
            "tracked  objects  (or  to  new  tracked  objects):  this  algorithm  efficiently  finds  the\n",
            "combination  of  mappings  that  minimizes  the  distance  between  the  detections\n",
            "and the predicted positions of tracked objects, while also minimizing the appear‐\n",
            "ance discrepancy.\n",
            "\n",
            "For  example,  imagine  a  red  ball  that  just  bounced  off  a  blue  ball  traveling  in  the\n",
            "opposite  direction.  Based  on  the  previous  positions  of  the  balls,  the  Kalman  filter\n",
            "will predict that the balls will go through each other: indeed, it assumes that objects\n",
            "move at a constant speed, so it will not expect the bounce. If the Hungarian algorithm\n",
            "only  considered  positions,  then  it  would  happily  map  the  new  detections  to  the\n",
            "wrong  balls,  as  if  they  had  just  gone  through  each  other  and  swapped  colors.  But\n",
            "thanks to the resemblance measure, the Hungarian algorithm will notice the problem.\n",
            "Assuming the balls are not too similar, the algorithm will map the new detections to\n",
            "the correct balls.\n",
            "\n",
            "There are a few DeepSORT implementations available on GitHub,\n",
            "including a TensorFlow implementation of YOLOv4 + DeepSORT:\n",
            "https://github.com/theAIGuysCode/yolov4-deepsort.\n",
            "\n",
            "So  far  we  have  located  objects  using  bounding  boxes.  This  is  often  sufficient,  but\n",
            "sometimes  you  need  to  locate  objects  with  much  more  precision—for  example,  to\n",
            "remove the background behind a person during a videoconference call. Let’s see how\n",
            "to go down to the pixel level.\n",
            "\n",
            "Semantic Segmentation\n",
            "In semantic segmentation, each pixel is classified according to the class of the object\n",
            "it  belongs  to  (e.g.,  road,  car,  pedestrian,  building,  etc.),  as  shown  in  Figure  14-27.\n",
            "Note  that  different  objects  of  the  same  class  are  not  distinguished.  For  example,  all\n",
            "the  bicycles  on  the  right  side  of  the  segmented  image  end  up  as  one  big  lump  of\n",
            "pixels. The main difficulty in this task is that when images go through a regular CNN,\n",
            "they gradually lose their spatial resolution (due to the layers with strides greater than\n",
            "1);  so,  a  regular  CNN  may  end  up  knowing  that  there’s  a  person  somewhere  in  the\n",
            "bottom left of the image, but it will not be much more precise than that.\n",
            "\n",
            "Semantic Segmentation \n",
            "\n",
            "| \n",
            "\n",
            "531\n",
            "\n",
            "\fFigure 14-27. Semantic segmentation\n",
            "\n",
            "Just  like  for  object  detection,  there  are  many  different  approaches  to  tackle  this\n",
            "problem,  some  quite  complex.  However,  a  fairly  simple  solution  was  proposed  in\n",
            "the  2015  paper  by  Jonathan  Long  et  al.  I  mentioned  earlier,  on  fully  convolutional\n",
            "networks. The authors start by taking a pretrained CNN and turning it into an FCN.\n",
            "The  CNN  applies  an  overall  stride  of  32  to  the  input  image  (i.e.,  if  you  add  up  all\n",
            "the  strides  greater  than  1),  meaning  the  last  layer  outputs  feature  maps  that  are  32\n",
            "times smaller than the input image. This is clearly too coarse, so they added a single\n",
            "upsampling layer that multiplies the resolution by 32.\n",
            "\n",
            "There are several solutions available for upsampling (increasing the size of an image),\n",
            "such  as  bilinear  interpolation,  but  that  only  works  reasonably  well  up  to  ×4  or  ×8.\n",
            "Instead, they use a transposed convolutional layer:39 this is equivalent to first stretching\n",
            "the  image  by  inserting  empty  rows  and  columns  (full  of  zeros),  then  performing  a\n",
            "regular convolution (see Figure 14-28). Alternatively, some people prefer to think of\n",
            "it  as  a  regular  convolutional  layer  that  uses  fractional  strides  (e.g.,  the  stride  is  1/2\n",
            "in  Figure  14-28).  The  transposed  convolutional  layer  can  be  initialized  to  perform\n",
            "something close to linear interpolation, but since it is a trainable layer, it will learn to\n",
            "do better during training. In Keras, you can use the Conv2DTranspose layer.\n",
            "\n",
            "In  a  transposed  convolutional  layer,  the  stride  defines  how  much\n",
            "the  input  will  be  stretched,  not  the  size  of  the  filter  steps,  so  the\n",
            "larger  the  stride,  the  larger  the  output  (unlike  for  convolutional\n",
            "layers or pooling layers).\n",
            "\n",
            "39 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐\n",
            "\n",
            "cians call a deconvolution, so this name should be avoided.\n",
            "\n",
            "532 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fFigure 14-28. Upsampling using a transposed convolutional layer\n",
            "\n",
            "Other Keras Convolutional Layers\n",
            "\n",
            "Keras also offers a few other kinds of convolutional layers:\n",
            "\n",
            "tf.keras.layers.Conv1D\n",
            "\n",
            "A  convolutional  layer  for  1D  inputs,  such  as  time  series  or  text  (sequences  of\n",
            "letters or words), as you will see in Chapter 15.\n",
            "\n",
            "tf.keras.layers.Conv3D\n",
            "\n",
            "A convolutional layer for 3D inputs, such as 3D PET scans.\n",
            "\n",
            "dilation_rate\n",
            "\n",
            "Setting the dilation_rate hyperparameter of any convolutional layer to a value\n",
            "of 2 or more creates an à-trous convolutional layer (“à trous” is French for “with\n",
            "holes”).  This  is  equivalent  to  using  a  regular  convolutional  layer  with  a  filter\n",
            "dilated  by  inserting  rows  and  columns  of  zeros  (i.e.,  holes).  For  example,  a  1  ×\n",
            "3 filter equal to  [[1,2,3]] may be dilated with a dilation rate of 4, resulting in\n",
            "a dilated filter of [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This lets the convolutional\n",
            "layer have a larger receptive field at no computational price and using no extra\n",
            "parameters.\n",
            "\n",
            "Using transposed convolutional layers for upsampling is OK, but still too imprecise.\n",
            "To do better, Long et al. added skip connections from lower layers: for example, they\n",
            "upsampled  the  output  image  by  a  factor  of  2  (instead  of  32),  and  they  added  the\n",
            "output  of  a  lower  layer  that  had  this  double  resolution.  Then  they  upsampled  the\n",
            "result by a factor of 16, leading to a total upsampling factor of 32 (see Figure 14-29).\n",
            "This recovered some of the spatial resolution that was lost in earlier pooling layers.\n",
            "\n",
            "Semantic Segmentation \n",
            "\n",
            "| \n",
            "\n",
            "533\n",
            "\n",
            "\fIn their best architecture, they used a second similar skip connection to recover even\n",
            "finer details from an even lower layer. In short, the output of the original CNN goes\n",
            "through the following extra steps: upsample ×2, add the output of a lower layer (of\n",
            "the appropriate scale), upsample ×2, add the output of an even lower layer, and finally\n",
            "upsample  ×8.  It  is  even  possible  to  scale  up  beyond  the  size  of  the  original  image:\n",
            "this  can  be  used  to  increase  the  resolution  of  an  image,  which  is  a  technique  called\n",
            "super-resolution.\n",
            "\n",
            "Figure 14-29. Skip layers recover some spatial resolution from lower layers\n",
            "\n",
            "Instance  segmentation  is  similar  to  semantic  segmentation,  but  instead  of  merging\n",
            "all  objects  of  the  same  class  into  one  big  lump,  each  object  is  distinguished  from\n",
            "the  others  (e.g.,  it  identifies  each  individual  bicycle).  For  example  the  Mask  R-CNN\n",
            "architecture,  proposed  in  a  2017  paper40  by  Kaiming  He  et  al.,  extends  the  Faster\n",
            "R-CNN  model  by  additionally  producing  a  pixel  mask  for  each  bounding  box.  So,\n",
            "not only do you get a bounding box around each object, with a set of estimated class\n",
            "probabilities,  but  you  also  get  a  pixel  mask  that  locates  pixels  in  the  bounding  box\n",
            "that  belong  to  the  object.  This  model  is  available  on  TensorFlow  Hub,  pretrained\n",
            "on  the  COCO  2017  dataset.  The  field  is  moving  fast,  though  so  if  you  want  to  try\n",
            "the latest and greatest models, please check out the state-of-the-art section of https://\n",
            "paperswithcode.com.\n",
            "\n",
            "As you can see, the field of deep computer vision is vast and fast-paced, with all sorts\n",
            "of architectures popping up every year. Almost all of them are based on convolutional\n",
            "neural  networks,  but  since  2020  another  neural  net  architecture  has  entered  the\n",
            "computer  vision  space:  transformers  (which  we  will  discuss  in  Chapter  16).  The\n",
            "progress  made  over  the  last  decade  has  been  astounding,  and  researchers  are  now\n",
            "focusing on harder and harder problems, such as adversarial learning (which attempts\n",
            "to  make  the  network  more  resistant  to  images  designed  to  fool  it),  explainability\n",
            "(understanding why the network makes a specific classification), realistic image gen‐\n",
            "eration (which we will come back to in Chapter 17), single-shot learning (a system that\n",
            "\n",
            "40 Kaiming He et al., “Mask R-CNN”, arXiv preprint arXiv:1703.06870 (2017).\n",
            "\n",
            "534 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fcan recognize an object after it has seen it just once), predicting the next frames in a\n",
            "video, combining text and image tasks, and more.\n",
            "\n",
            "Now on to the next chapter, where we will look at how to process sequential data such\n",
            "as time series using recurrent neural networks and convolutional neural networks.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1. What  are  the  advantages  of  a  CNN  over  a  fully  connected  DNN  for  image\n",
            "1.\n",
            "\n",
            "classification?\n",
            "\n",
            "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
            "2.\n",
            "a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the\n",
            "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
            "images of 200 × 300 pixels:\n",
            "\n",
            "a.\n",
            "a. What is the total number of parameters in the CNN?\n",
            "\n",
            "b.\n",
            "b. If we are using 32-bit floats, at least how much RAM will this network require\n",
            "\n",
            "when making a prediction for a single instance?\n",
            "\n",
            "c.\n",
            "c. What about when training on a mini-batch of 50 images?\n",
            "\n",
            "3.\n",
            "3. If your GPU runs out of memory while training a CNN, what are five things you\n",
            "\n",
            "could try to solve the problem?\n",
            "\n",
            "4.\n",
            "4. Why  would  you  want  to  add  a  max  pooling  layer  rather  than  a  convolutional\n",
            "\n",
            "layer with the same stride?\n",
            "\n",
            "5.\n",
            "5. When would you want to add a local response normalization layer?\n",
            "\n",
            "6. Can  you  name  the  main  innovations  in  AlexNet,  as  compared  to  LeNet-5?\n",
            "6.\n",
            "What about the main innovations in GoogLeNet, ResNet, SENet, Xception, and\n",
            "EfficientNet?\n",
            "\n",
            "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
            "7.\n",
            "\n",
            "convolutional layer?\n",
            "\n",
            "8.\n",
            "8. What is the main technical difficulty of semantic segmentation?\n",
            "\n",
            "9.\n",
            "9. Build  your  own  CNN  from  scratch  and  try  to  achieve  the  highest  possible\n",
            "\n",
            "accuracy on MNIST.\n",
            "\n",
            "10. Use transfer learning for large image classification, going through these steps:\n",
            "10.\n",
            "\n",
            "a. Create a training set containing at least 100 images per class. For example, you\n",
            "a.\n",
            "could classify your own pictures based on the location (beach, mountain, city,\n",
            "etc.),  or  alternatively  you  can  use  an  existing  dataset  (e.g.,  from  TensorFlow\n",
            "Datasets).\n",
            "\n",
            "b. Split it into a training set, a validation set, and a test set.\n",
            "b.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "535\n",
            "\n",
            "\fc.\n",
            "c. Build the input pipeline, apply the appropriate preprocessing operations, and\n",
            "\n",
            "optionally add data augmentation.\n",
            "\n",
            "d.\n",
            "d. Fine-tune a pretrained model on this dataset.\n",
            "\n",
            "11.\n",
            "11. Go through TensorFlow’s Style Transfer tutorial. This is a fun way to generate art\n",
            "\n",
            "using deep learning.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "536 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
            "\n",
            "\fCHAPTER 15\n",
            "Processing Sequences Using\n",
            "RNNs and CNNs\n",
            "\n",
            "Predicting  the  future  is  something  you  do  all  the  time,  whether  you  are  finishing  a\n",
            "friend’s  sentence  or  anticipating  the  smell  of  coffee  at  breakfast.  In  this  chapter  we\n",
            "will  discuss  recurrent  neural  networks  (RNNs)—a  class  of  nets  that  can  predict  the\n",
            "future (well, up to a point). RNNs can analyze time series data, such as the number\n",
            "of daily active users on your website, the hourly temperature in your city, your home’s\n",
            "daily  power  consumption,  the  trajectories  of  nearby  cars,  and  more.  Once  an  RNN\n",
            "learns past patterns in the data, it is able to use its knowledge to forecast the future,\n",
            "assuming of course that past patterns still hold in the future.\n",
            "\n",
            "More  generally,  RNNs  can  work  on  sequences  of  arbitrary  lengths,  rather  than\n",
            "on  fixed-sized  inputs.  For  example,  they  can  take  sentences,  documents,  or  audio\n",
            "samples  as  input,  making  them  extremely  useful  for  natural  language  processing\n",
            "applications such as automatic translation or speech-to-text.\n",
            "\n",
            "In this chapter, we will first go through the fundamental concepts underlying RNNs\n",
            "and how to train them using backpropagation through time. Then, we will use them\n",
            "to forecast a time series. Along the way, we will look at the popular ARMA family of\n",
            "models, often used to forecast time series, and use them as baselines to compare with\n",
            "our RNNs. After that, we’ll explore the two main difficulties that RNNs face:\n",
            "\n",
            "• Unstable gradients (discussed in Chapter 11), which can be alleviated using vari‐\n",
            "•\n",
            "ous techniques, including recurrent dropout and recurrent layer normalization.\n",
            "\n",
            "•\n",
            "• A  (very)  limited  short-term  memory,  which  can  be  extended  using  LSTM  and\n",
            "\n",
            "GRU cells.\n",
            "\n",
            "537\n",
            "\n",
            "\fRNNs are not the only types of neural networks capable of handling sequential data.\n",
            "For  small  sequences,  a  regular  dense  network  can  do  the  trick,  and  for  very  long\n",
            "sequences, such as audio samples or text, convolutional neural networks can actually\n",
            "work  quite  well  too.  We  will  discuss  both  of  these  possibilities,  and  we  will  finish\n",
            "this  chapter  by  implementing  a  WaveNet—a  CNN  architecture  capable  of  handling\n",
            "sequences of tens of thousands of time steps. Let’s get started!\n",
            "\n",
            "Recurrent Neurons and Layers\n",
            "Up  to  now  we  have  focused  on  feedforward  neural  networks,  where  the  activations\n",
            "flow  only  in  one  direction,  from  the  input  layer  to  the  output  layer.  A  recurrent\n",
            "neural network looks very much like a feedforward neural network, except it also has\n",
            "connections pointing backward.\n",
            "\n",
            "Let’s  look  at  the  simplest  possible  RNN,  composed  of  one  neuron  receiving  inputs,\n",
            "producing an output, and sending that output back to itself, as shown in Figure 15-1\n",
            "(left). At each time step t (also called a frame), this recurrent neuron receives the inputs\n",
            "x(t)  as  well  as  its  own  output  from  the  previous  time  step,  ŷ(t–1).  Since  there  is  no\n",
            "previous output at the first time step, it is generally set to 0. We can represent this tiny\n",
            "network against the time axis, as shown in Figure 15-1 (right). This is called unrolling\n",
            "the  network  through  time  (it’s  the  same  recurrent  neuron  represented  once  per  time\n",
            "step).\n",
            "\n",
            "Figure 15-1. A recurrent neuron (left) unrolled through time (right)\n",
            "\n",
            "You can easily create a layer of recurrent neurons. At each time step t, every neuron\n",
            "receives both the input vector x(t) and the output vector from the previous time step\n",
            "ŷ(t–1), as shown in Figure 15-2. Note that both the inputs and outputs are now vectors\n",
            "(when there was just a single neuron, the output was a scalar).\n",
            "\n",
            "538 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fFigure 15-2. A layer of recurrent neurons (left) unrolled through time (right)\n",
            "\n",
            "Each recurrent neuron has two sets of weights: one for the inputs x(t) and the other for\n",
            "the outputs of the previous time step, ŷ(t–1). Let’s call these weight vectors wx and wŷ.\n",
            "If we consider the whole recurrent layer instead of just one recurrent neuron, we can\n",
            "place all the weight vectors in two weight matrices: Wx and Wŷ.\n",
            "\n",
            "The output vector of the whole recurrent layer can then be computed pretty much as\n",
            "you might expect, as shown in Equation 15-1, where b is the bias vector and ϕ(·) is\n",
            "the activation function (e.g., ReLU1).\n",
            "\n",
            "Equation 15-1. Output of a recurrent layer for a single instance\n",
            "\n",
            "ŷ t = ϕ Wx\n",
            "\n",
            "⊺x t + Wŷ\n",
            "\n",
            "⊺\n",
            "\n",
            "ŷ t − 1 + b\n",
            "\n",
            "Just as with feedforward neural networks, we can compute a recurrent layer’s output\n",
            "in  one  shot  for  an  entire  mini-batch  by  placing  all  the  inputs  at  time  step  t  into  an\n",
            "input matrix X(t) (see Equation 15-2).\n",
            "\n",
            "Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a pass:\n",
            "[mini-batch\n",
            "\n",
            "Ŷ t = ϕ X t Wx + Ŷ t − 1 Wŷ + b\n",
            "\n",
            "= ϕ X t Ŷ t − 1 W + b with W =\n",
            "\n",
            "Wx\n",
            "Wŷ\n",
            "\n",
            "1 Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather\n",
            "\n",
            "than the ReLU activation function. For example, see Vu Pham et al.’s 2013 paper “Dropout Improves Recur‐\n",
            "rent Neural Networks for Handwriting Recognition”. ReLU-based RNNs are also possible, as shown in Quoc\n",
            "V. Le et al.’s 2015 paper “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”.\n",
            "\n",
            "Recurrent Neurons and Layers \n",
            "\n",
            "| \n",
            "\n",
            "539\n",
            "\n",
            "\fIn this equation:\n",
            "\n",
            "• Ŷ(t) is an m × nneurons matrix containing the layer’s outputs at time step t for each\n",
            "•\n",
            "instance in the mini-batch (m is the number of instances in the mini-batch and\n",
            "nneurons is the number of neurons).\n",
            "\n",
            "• X(t)  is  an  m  ×  ninputs  matrix  containing  the  inputs  for  all  instances  (ninputs  is  the\n",
            "•\n",
            "\n",
            "number of input features).\n",
            "\n",
            "•\n",
            "• Wx is an ninputs × nneurons matrix containing the connection weights for the inputs\n",
            "\n",
            "of the current time step.\n",
            "\n",
            "•\n",
            "• Wŷ  is  an  nneurons  ×  nneurons  matrix  containing  the  connection  weights  for  the\n",
            "\n",
            "outputs of the previous time step.\n",
            "\n",
            "• b is a vector of size nneurons containing each neuron’s bias term.\n",
            "•\n",
            "• The  weight  matrices  Wx  and  Wŷ  are  often  concatenated  vertically  into  a  single\n",
            "•\n",
            "weight  matrix  W  of  shape  (ninputs  +  nneurons)  ×  nneurons  (see  the  second  line  of\n",
            "Equation 15-2).\n",
            "\n",
            "•\n",
            "• The notation [X(t) Ŷ(t–1)] represents the horizontal concatenation of the matrices\n",
            "\n",
            "X(t) and Ŷ(t–1).\n",
            "\n",
            "Notice that Ŷ(t) is a function of X(t) and Ŷ(t–1), which is a function of X(t–1) and Ŷ(t–2),\n",
            "which is a function of X(t–2) and Ŷ(t–3), and so on. This makes Ŷ(t) a function of all the\n",
            "inputs since time t = 0 (that is, X(0), X(1), …, X(t)). At the first time step, t = 0, there are\n",
            "no previous outputs, so they are typically assumed to be all zeros.\n",
            "\n",
            "Memory Cells\n",
            "Since  the  output  of  a  recurrent  neuron  at  time  step  t  is  a  function  of  all  the  inputs\n",
            "from previous time steps, you could say it has a form of memory. A part of a neural\n",
            "network that preserves some state across time steps is called a memory cell (or simply\n",
            "a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell,\n",
            "capable of learning only short patterns (typically about 10 steps long, but this varies\n",
            "depending on the task). Later in this chapter, we will look at some more complex and\n",
            "powerful types of cells capable of learning longer patterns (roughly 10 times longer,\n",
            "but again, this depends on the task).\n",
            "\n",
            "A cell’s state at time step t, denoted h(t) (the “h” stands for “hidden”), is a function of\n",
            "some inputs at that time step and its state at the previous time step: h(t) = f(x(t), h(t–1)).\n",
            "Its output at time step t, denoted ŷ(t), is also a function of the previous state and the\n",
            "current inputs. In the case of the basic cells we have discussed so far, the output is just\n",
            "equal to the state, but in more complex cells this is not always the case, as shown in\n",
            "Figure 15-3.\n",
            "\n",
            "540 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fFigure 15-3. A cell’s hidden state and its output may be different\n",
            "\n",
            "Input and Output Sequences\n",
            "An  RNN  can  simultaneously  take  a  sequence  of  inputs  and  produce  a  sequence  of\n",
            "outputs  (see  the  top-left  network  in  Figure  15-4).  This  type  of  sequence-to-sequence\n",
            "network is useful to forecast time series, such as your home’s daily power consump‐\n",
            "tion:  you  feed  it  the  data  over  the  last  N  days,  and  you  train  it  to  output  the\n",
            "power consumption shifted by one day into the future (i.e., from N – 1 days ago to\n",
            "tomorrow).\n",
            "\n",
            "Alternatively, you could feed the network a sequence of inputs and ignore all outputs\n",
            "except for the last one (see the top-right network in Figure 15-4). This is a sequence-\n",
            "to-vector  network.  For  example,  you  could  feed  the  network  a  sequence  of  words\n",
            "corresponding  to  a  movie  review,  and  the  network  would  output  a  sentiment  score\n",
            "(e.g., from 0 [hate] to 1 [love]).\n",
            "\n",
            "Conversely,  you  could  feed  the  network  the  same  input  vector  over  and  over  again\n",
            "at  each  time  step  and  let  it  output  a  sequence  (see  the  bottom-left  network  of\n",
            "Figure 15-4). This is a vector-to-sequence network. For example, the input could be an\n",
            "image (or the output of a CNN), and the output could be a caption for that image.\n",
            "\n",
            "Lastly,  you  could  have  a  sequence-to-vector  network,  called  an  encoder,  followed\n",
            "by  a  vector-to-sequence  network,  called  a  decoder  (see  the  bottom-right  network  of\n",
            "Figure  15-4).  For  example,  this  could  be  used  for  translating  a  sentence  from  one\n",
            "language  to  another.  You  would  feed  the  network  a  sentence  in  one  language,  the\n",
            "encoder would convert this sentence into a single vector representation, and then the\n",
            "decoder would decode this vector into a sentence in another language. This two-step\n",
            "model,  called  an  encoder–decoder,2  works  much  better  than  trying  to  translate  on\n",
            "the fly with a single sequence-to-sequence RNN (like the one represented at the top\n",
            "left): the last words of a sentence can affect the first words of the translation, so you\n",
            "\n",
            "2 Nal Kalchbrenner and Phil Blunsom, “Recurrent Continuous Translation Models”, Proceedings of the 2013\n",
            "\n",
            "Conference on Empirical Methods in Natural Language Processing (2013): 1700–1709.\n",
            "\n",
            "Recurrent Neurons and Layers \n",
            "\n",
            "| \n",
            "\n",
            "541\n",
            "\n",
            "\fneed to wait until you have seen the whole sentence before translating it. We will go\n",
            "through the implementation of an encoder–decoder in Chapter 16 (as you will see, it\n",
            "is a bit more complex than what Figure 15-4 suggests).\n",
            "\n",
            "Figure 15-4. Sequence-to-sequence (top left), sequence-to-vector (top right), vector-to-\n",
            "sequence (bottom left), and encoder–decoder (bottom right) networks\n",
            "\n",
            "This versatility sounds promising, but how do you train a recurrent neural network?\n",
            "\n",
            "Training RNNs\n",
            "To  train  an  RNN,  the  trick  is  to  unroll  it  through  time  (like  we  just  did)  and  then\n",
            "use regular backpropagation (see Figure 15-5). This strategy is called backpropagation\n",
            "through time (BPTT).\n",
            "\n",
            "Just like in regular backpropagation, there is a first forward pass through the unrolled\n",
            "network (represented by the dashed arrows). Then the output sequence is evaluated\n",
            "using  a  loss  function  ℒ(Y(0),  Y(1),  …,  Y(T);  Ŷ(0),  Ŷ(1),  …,  Ŷ(T))  (where  Y(i)  is  the  ith\n",
            "target,  Ŷ(i)  is  the  ith  prediction,  and  T  is  the  max  time  step).  Note  that  this  loss\n",
            "function  may  ignore  some  outputs.  For  example,  in  a  sequence-to-vector  RNN,  all\n",
            "outputs are ignored except for the very last one. In Figure 15-5, the loss function is\n",
            "computed based on the last three outputs only. The gradients of that loss function are\n",
            "then  propagated  backward  through  the  unrolled  network  (represented  by  the  solid\n",
            "arrows).  In  this  example,  since  the  outputs  Ŷ(0)  and  Ŷ(1)  are  not  used  to  compute\n",
            "the loss, the gradients do not flow backward through them; they only flow through\n",
            "\n",
            "542 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fŶ(2),  Ŷ(3),  and  Ŷ(4).  Moreover,  since  the  same  parameters  W  and  b  are  used  at  each\n",
            "time step, their gradients will be tweaked multiple times during backprop. Once the\n",
            "backward  phase  is  complete  and  all  the  gradients  have  been  computed,  BPTT  can\n",
            "perform  a  gradient  descent  step  to  update  the  parameters  (this  is  no  different  from\n",
            "regular backprop).\n",
            "\n",
            "Figure 15-5. Backpropagation through time\n",
            "\n",
            "Fortunately,  Keras  takes  care  of  all  of  this  complexity  for  you,  as  you  will  see.  But\n",
            "before we get there, let’s load a time series and start analyzing it using classical tools\n",
            "to better understand what we’re dealing with, and to get some baseline metrics.\n",
            "\n",
            "Forecasting a Time Series\n",
            "All right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s Transit\n",
            "Authority.  Your  first  task  is  to  build  a  model  capable  of  forecasting  the  number\n",
            "of  passengers  that  will  ride  on  bus  and  rail  the  next  day.  You  have  access  to  daily\n",
            "ridership  data  since  2001.  Let’s  walk  through  together  how  you  would  handle  this.\n",
            "We’ll start by loading and cleaning up the data:3\n",
            "\n",
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "\n",
            "path = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\n",
            "df = pd.read_csv(path, parse_dates=[\"service_date\"])\n",
            "df.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]  # shorter names\n",
            "df = df.sort_values(\"date\").set_index(\"date\")\n",
            "\n",
            "3 The latest data from the Chicago Transit Authority is available at the Chicago Data Portal.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "543\n",
            "\n",
            "\fdf = df.drop(\"total\", axis=1)  # no need for total, it's just bus + rail\n",
            "df = df.drop_duplicates()  # remove duplicated months (2011-10 and 2014-07)\n",
            "\n",
            "We  load  the  CSV  file,  set  short  column  names,  sort  the  rows  by  date,  remove  the\n",
            "redundant total column, and drop duplicate rows. Now let’s check what the first few\n",
            "rows look like:\n",
            "\n",
            ">>> df.head()\n",
            "           day_type     bus    rail\n",
            "date\n",
            "2001-01-01        U  297192  126455\n",
            "2001-01-02        W  780827  501952\n",
            "2001-01-03        W  824923  536432\n",
            "2001-01-04        W  870021  550011\n",
            "2001-01-05        W  890426  557917\n",
            "\n",
            "On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded\n",
            "a  train.  The  day_type  column  contains  W  for  Weekdays,  A  for  Saturdays,  and  U  for\n",
            "Sundays or holidays.\n",
            "\n",
            "Now  let’s  plot  the  bus  and  rail  ridership  figures  over  a  few  months  in  2019,  to  see\n",
            "what it looks like (see Figure 15-6):\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "df[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\n",
            "plt.show()\n",
            "\n",
            "Figure 15-6. Daily ridership in Chicago\n",
            "\n",
            "544 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fNote that Pandas includes both the start and end month in the range, so this plots the\n",
            "data from the 1st of March all the way up to the 31st of May. This is a time series: data\n",
            "with values at different time steps, usually at regular intervals. More specifically, since\n",
            "there are multiple values per time step, this is called a multivariate time series. If we\n",
            "only looked at the bus column, it would be a univariate time series, with a single value\n",
            "per time step. Predicting future values (i.e., forecasting) is the most typical task when\n",
            "dealing with time series, and this is what we will focus on in this chapter. Other tasks\n",
            "include imputation (filling in missing past values), classification, anomaly detection,\n",
            "and more.\n",
            "\n",
            "Looking  at  Figure  15-6,  we  can  see  that  a  similar  pattern  is  clearly  repeated  every\n",
            "week.  This  is  called  a  weekly  seasonality.  In  fact,  it’s  so  strong  in  this  case  that\n",
            "forecasting tomorrow’s ridership by just copying the values from a week earlier will\n",
            "yield  reasonably  good  results.  This  is  called  naive  forecasting:  simply  copying  a  past\n",
            "value to make our forecast. Naive forecasting is often a great baseline, and it can even\n",
            "be tricky to beat in some cases.\n",
            "\n",
            "In general, naive forecasting means copying the latest known value\n",
            "(e.g.,  forecasting  that  tomorrow  will  be  the  same  as  today).  How‐\n",
            "ever, in our case, copying the value from the previous week works\n",
            "better, due to the strong weekly seasonality.\n",
            "\n",
            "To visualize these naive forecasts, let’s overlay the two time series (for bus and rail) as\n",
            "well as the same time series lagged by one week (i.e., shifted toward the right) using\n",
            "dotted  lines.  We’ll  also  plot  the  difference  between  the  two  (i.e.,  the  value  at  time  t\n",
            "minus the value at time t – 7); this is called differencing (see Figure 15-7):\n",
            "\n",
            "diff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\n",
            "\n",
            "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\n",
            "df.plot(ax=axs[0], legend=False, marker=\".\")  # original time series\n",
            "df.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")  # lagged\n",
            "diff_7.plot(ax=axs[1], grid=True, marker=\".\")  # 7-day difference time series\n",
            "plt.show()\n",
            "\n",
            "Not  too  bad!  Notice  how  closely  the  lagged  time  series  track  the  actual  time  series.\n",
            "When a time series is correlated with a lagged version of itself, we say that the time\n",
            "series is autocorrelated. As you can see, most of the differences are fairly small, except\n",
            "at the end of May. Maybe there was a holiday at that time? Let’s check the day_type\n",
            "column:\n",
            "\n",
            ">>> list(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])\n",
            "['A', 'U', 'U']\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "545\n",
            "\n",
            "\fFigure 15-7. Time series overlaid with 7-day lagged time series (top), and difference\n",
            "between t and t – 7 (bottom)\n",
            "\n",
            "Indeed,  there  was  a  long  weekend  back  then:  the  Monday  was  the  Memorial  Day\n",
            "holiday.  We  could  use  this  column  to  improve  our  forecasts,  but  for  now  let’s  just\n",
            "measure the mean absolute error over the three-month period we’re arbitrarily focus‐\n",
            "ing on—March, April, and May 2019—to get a rough idea:\n",
            "\n",
            ">>> diff_7.abs().mean()\n",
            "bus     43915.608696\n",
            "rail    42143.271739\n",
            "dtype: float64\n",
            "\n",
            "Our  naive  forecasts  get  an  MAE  of  about  43,916  bus  riders,  and  about  42,143  rail\n",
            "riders.  It’s  hard  to  tell  at  a  glance  how  good  or  bad  this  is,  so  let’s  put  the  forecast\n",
            "errors into perspective by dividing them by the target values:\n",
            "\n",
            ">>> targets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\n",
            ">>> (diff_7 / targets).abs().mean()\n",
            "bus     0.082938\n",
            "rail    0.089948\n",
            "dtype: float64\n",
            "\n",
            "What we just computed is called the mean absolute percentage error (MAPE): it looks\n",
            "like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0% for rail. It’s\n",
            "interesting  to  note  that  the  MAE  for  the  rail  forecasts  looks  slightly  better  than  the\n",
            "MAE  for  the  bus  forecasts,  while  the  opposite  is  true  for  the  MAPE.  That’s  because\n",
            "the bus ridership is larger than the rail ridership, so naturally the forecast errors are\n",
            "\n",
            "546 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\falso  larger,  but  when  we  put  the  errors  into  perspective,  it  turns  out  that  the  bus\n",
            "forecasts are actually slightly better than the rail forecasts.\n",
            "\n",
            "The MAE, MAPE, and MSE are among the most common metrics\n",
            "you  can  use  to  evaluate  your  forecasts.  As  always,  choosing  the\n",
            "right  metric  depends  on  the  task.  For  example,  if  your  project\n",
            "suffers quadratically more from large errors than from small ones,\n",
            "then  the  MSE  may  be  preferable,  as  it  strongly  penalizes  large\n",
            "errors.\n",
            "\n",
            "Looking at the time series, there doesn’t appear to be any significant monthly season‐\n",
            "ality, but let’s check whether there’s any yearly seasonality. We’ll look at the data from\n",
            "2001 to 2019. To reduce the risk of data snooping, we’ll ignore more recent data for\n",
            "now. Let’s also plot a 12-month rolling average for each series to visualize long-term\n",
            "trends (see Figure 15-8):\n",
            "\n",
            "period = slice(\"2001\", \"2019\")\n",
            "df_monthly = df.resample('M').mean()  # compute the mean for each month\n",
            "rolling_average_12_months = df_monthly[period].rolling(window=12).mean()\n",
            "\n",
            "fig, ax = plt.subplots(figsize=(8, 4))\n",
            "df_monthly[period].plot(ax=ax, marker=\".\")\n",
            "rolling_average_12_months.plot(ax=ax, grid=True, legend=False)\n",
            "plt.show()\n",
            "\n",
            "Figure 15-8. Yearly seasonality and long-term trends\n",
            "\n",
            "Yep! There’s definitely some yearly seasonality as well, although it is noisier than the\n",
            "weekly  seasonality,  and  more  visible  for  the  rail  series  than  the  bus  series:  we  see\n",
            "peaks and troughs at roughly the same dates each year. Let’s check what we get if we\n",
            "plot the 12-month difference (see Figure 15-9):\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "547\n",
            "\n",
            "\fdf_monthly.diff(12)[period].plot(grid=True, marker=\".\", figsize=(8, 3))\n",
            "plt.show()\n",
            "\n",
            "Figure 15-9. The 12-month difference\n",
            "\n",
            "Notice how differencing not only removed the yearly seasonality, but it also removed\n",
            "the  long-term  trends.  For  example,  the  linear  downward  trend  present  in  the  time\n",
            "series from 2016 to 2019 became a roughly constant negative value in the differenced\n",
            "time  series.  In  fact,  differencing  is  a  common  technique  used  to  remove  trend  and\n",
            "seasonality  from  a  time  series:  it’s  easier  to  study  a  stationary  time  series,  meaning\n",
            "one whose statistical properties remain constant over time, without any seasonality or\n",
            "trends. Once you’re able to make accurate forecasts on the differenced time series, it’s\n",
            "easy to turn them into forecasts for the actual time series by just adding back the past\n",
            "values that were previously subtracted.\n",
            "\n",
            "You  may  be  thinking  that  we’re  only  trying  to  predict  tomorrow’s  ridership,  so  the\n",
            "long-term patterns matter much less than the short-term ones. You’re right, but still,\n",
            "we  may  be  able  to  improve  performance  slightly  by  taking  long-term  patterns  into\n",
            "account. For example, daily bus ridership dropped by about 2,500 in October 2017,\n",
            "which represents about 570 fewer passengers each week, so if we were at the end of\n",
            "October 2017, it would make sense to forecast tomorrow’s ridership by copying the\n",
            "value from last week, minus 570. Accounting for the trend will make your forecasts a\n",
            "bit more accurate on average.\n",
            "\n",
            "Now that you’re familiar with the ridership time series, as well as some of the most\n",
            "important concepts in time series analysis, including seasonality, trend, differencing,\n",
            "and  moving  averages,  let’s  take  a  quick  look  at  a  very  popular  family  of  statistical\n",
            "models that are commonly used to analyze time series.\n",
            "\n",
            "548 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fThe ARMA Model Family\n",
            "We’ll  start  with  the  autoregressive  moving  average  (ARMA)  model,  developed  by\n",
            "Herman  Wold  in  the  1930s:  it  computes  its  forecasts  using  a  simple  weighted  sum\n",
            "of lagged values and corrects these forecasts by adding a moving average, very much\n",
            "like we just discussed. Specifically, the moving average component is computed using\n",
            "a weighted sum of the last few forecast errors. Equation 15-3 shows how the model\n",
            "makes its forecasts.\n",
            "\n",
            "Equation 15-3. Forecasting using an ARMA model\n",
            "\n",
            "p\n",
            "\n",
            "y t = ∑i = 1\n",
            "with ϵ t = y t − y t\n",
            "\n",
            "αi y t − i + ∑i = 1\n",
            "\n",
            "θi ϵ t − i\n",
            "\n",
            "q\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• ŷ(t) is the model’s forecast for time step t.\n",
            "•\n",
            "•\n",
            "• y(t) is the time series’ value at time step t.\n",
            "•\n",
            "• The  first  sum  is  the  weighted  sum  of  the  past  p  values  of  the  time  series,  using\n",
            "the  learned  weights  αi.  The  number  p  is  a  hyperparameter,  and  it  determines\n",
            "how far back into the past the model should look. This sum is the autoregressive\n",
            "component of the model: it performs regression based on past values.\n",
            "\n",
            "•\n",
            "• The second sum is the weighted sum over the past q forecast errors ε(t), using the\n",
            "learned  weights  θi.  The  number  q  is  a  hyperparameter.  This  sum  is  the  moving\n",
            "average component of the model.\n",
            "\n",
            "Importantly,  this  model  assumes  that  the  time  series  is  stationary.  If  it  is  not,  then\n",
            "differencing  may  help.  Using  differencing  over  a  single  time  step  will  produce  an\n",
            "approximation  of  the  derivative  of  the  time  series:  indeed,  it  will  give  the  slope\n",
            "of  the  series  at  each  time  step.  This  means  that  it  will  eliminate  any  linear  trend,\n",
            "transforming it into a constant value. For example, if you apply one-step differencing\n",
            "to the series [3, 5, 7, 9, 11], you get the differenced series [2, 2, 2, 2].\n",
            "\n",
            "If the original time series has a quadratic trend instead of a linear trend, then a single\n",
            "round of differencing will not be enough. For example, the series [1, 4, 9, 16, 25, 36]\n",
            "becomes  [3,  5,  7,  9,  11]  after  one  round  of  differencing,  but  if  you  run  differencing\n",
            "for a second round, then you get [2, 2, 2, 2]. So, running two rounds of differencing\n",
            "will  eliminate  quadratic  trends.  More  generally,  running  d  consecutive  rounds  of\n",
            "differencing computes an approximation of the dth order derivative of the time series,\n",
            "so it will eliminate polynomial trends up to degree d. This hyperparameter d is called\n",
            "the order of integration.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "549\n",
            "\n",
            "\fDifferencing is the central contribution of the autoregressive integrated moving average\n",
            "(ARIMA)  model,  introduced  in  1970  by  George  Box  and  Gwilym  Jenkins  in  their\n",
            "book Time Series Analysis (Wiley): this model runs d rounds of differencing to make\n",
            "the time series more stationary, then it applies a regular ARMA model. When making\n",
            "forecasts, it uses this ARMA model, then it adds back the terms that were subtracted\n",
            "by differencing.\n",
            "\n",
            "One  last  member  of  the  ARMA  family  is  the  seasonal  ARIMA  (SARIMA)  model:\n",
            "it  models  the  time  series  in  the  same  way  as  ARIMA,  but  it  additionally  models\n",
            "a  seasonal  component  for  a  given  frequency  (e.g.,  weekly),  using  the  exact  same\n",
            "ARIMA  approach.  It  has  a  total  of  seven  hyperparameters:  the  same  p,  d,  and  q\n",
            "hyperparameters as ARIMA, plus additional P, D, and Q hyperparameters to model\n",
            "the  seasonal  pattern,  and  lastly  the  period  of  the  seasonal  pattern,  noted  s.  The\n",
            "hyperparameters P, D, and Q are just like p, d, and q, but they are used to model the\n",
            "time series at t – s, t – 2s, t – 3s, etc.\n",
            "\n",
            "Let’s  see  how  to  fit  a  SARIMA  model  to  the  rail  time  series,  and  use  it  to  make  a\n",
            "forecast for tomorrow’s ridership. We’ll pretend today is the last day of May 2019, and\n",
            "we want to forecast the rail ridership for “tomorrow”, the 1st of June, 2019. For this,\n",
            "we can use the statsmodels library, which contains many different statistical models,\n",
            "including the ARMA model and its variants, implemented by the ARIMA class:\n",
            "\n",
            "from statsmodels.tsa.arima.model import ARIMA\n",
            "\n",
            "origin, today = \"2019-01-01\", \"2019-05-31\"\n",
            "rail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\n",
            "model = ARIMA(rail_series,\n",
            "              order=(1, 0, 0),\n",
            "              seasonal_order=(0, 1, 1, 7))\n",
            "model = model.fit()\n",
            "y_pred = model.forecast()  # returns 427,758.6\n",
            "\n",
            "In this code example:\n",
            "\n",
            "• We start by importing the ARIMA class, then we take the rail ridership data from\n",
            "•\n",
            "the  start  of  2019  up  to  “today”,  and  we  use  asfreq(\"D\")  to  set  the  time  series’\n",
            "frequency to daily: this doesn’t change the data at all in this case, since it’s already\n",
            "daily, but without this the ARIMA class would have to guess the frequency, and it\n",
            "would display a warning.\n",
            "\n",
            "• Next, we create an ARIMA instance, passing it all the data until “today”, and we set\n",
            "•\n",
            "the model hyperparameters: order=(1, 0, 0) means that p = 1, d = 0, q = 0, and\n",
            "seasonal_order=(0, 1, 1, 7) means that P = 0, D = 1, Q = 1, and s = 7. Notice\n",
            "that the statsmodels API differs a bit from Scikit-Learn’s API, since we pass the\n",
            "data to the model at construction time, instead of passing it to the fit() method.\n",
            "\n",
            "550 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\f•\n",
            "• Next, we fit the model, and we use it to make a forecast for “tomorrow”, the 1st of\n",
            "\n",
            "June, 2019.\n",
            "\n",
            "The  forecast  is  427,759  passengers,  when  in  fact  there  were  379,044.  Yikes,  we’re\n",
            "12.9% off—that’s pretty bad. It’s actually slightly worse than naive forecasting, which\n",
            "forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day? To check\n",
            "this, we can run the same code in a loop to make forecasts for every day in March,\n",
            "April, and May, and compute the MAE over that period:\n",
            "\n",
            "origin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\n",
            "time_period = pd.date_range(start_date, end_date)\n",
            "rail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\n",
            "y_preds = []\n",
            "for today in time_period.shift(-1):\n",
            "    model = ARIMA(rail_series[origin:today],  # train on data up to \"today\"\n",
            "                  order=(1, 0, 0),\n",
            "                  seasonal_order=(0, 1, 1, 7))\n",
            "    model = model.fit()  # note that we retrain the model every day!\n",
            "    y_pred = model.forecast()[0]\n",
            "    y_preds.append(y_pred)\n",
            "\n",
            "y_preds = pd.Series(y_preds, index=time_period)\n",
            "mae = (y_preds - rail_series[time_period]).abs().mean()  # returns 32,040.7\n",
            "\n",
            "Ah,  that’s  much  better!  The  MAE  is  about  32,041,  which  is  significantly  lower  than\n",
            "the MAE we got with naive forecasting (42,143). So although the model is not perfect,\n",
            "it still beats naive forecasting by a large margin, on average.\n",
            "\n",
            "At  this  point,  you  may  be  wondering  how  to  pick  good  hyperparameters  for  the\n",
            "SARIMA  model.  There  are  several  methods,  but  the  simplest  to  understand  and\n",
            "to  get  started  with  is  the  brute-force  approach:  just  run  a  grid  search.  For  each\n",
            "model  you  want  to  evaluate  (i.e.,  each  hyperparameter  combination),  you  can  run\n",
            "the preceding code example, changing only the hyperparameter values. Good p, q, P,\n",
            "and Q values are usually fairly small (typically 0 to 2, sometimes up to 5 or 6), and d\n",
            "and D are typically 0 or 1, sometimes 2. As for s, it’s just the main seasonal pattern’s\n",
            "period: in our case it’s 7 since there’s a strong weekly seasonality. The model with the\n",
            "lowest MAE wins. Of course, you can replace the MAE with another metric if it better\n",
            "matches your business objective. And that’s it!4\n",
            "\n",
            "4 There are other more principled approaches to selecting good hyperparameters, based on analyzing the\n",
            "\n",
            "autocorrelation function (ACF) and partial autocorrelation function (PACF), or minimizing the AIC or BIC\n",
            "metrics (introduced in Chapter 9) to penalize models that use too many parameters and reduce the risk of\n",
            "overfitting the data, but grid search is a good place to start. For more details on the ACF-PACF approach,\n",
            "check out this very nice post by Jason Brownlee.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "551\n",
            "\n",
            "\fPreparing the Data for Machine Learning Models\n",
            "Now that we have two baselines, naive forecasting and SARIMA, let’s try to use the\n",
            "machine  learning  models  we’ve  covered  so  far  to  forecast  this  time  series,  starting\n",
            "with  a  basic  linear  model.  Our  goal  will  be  to  forecast  tomorrow’s  ridership  based\n",
            "on  the  ridership  of  the  past  8  weeks  of  data  (56  days).  The  inputs  to  our  model\n",
            "will  therefore  be  sequences  (usually  a  single  sequence  per  day  once  the  model  is  in\n",
            "production),  each  containing  56  values  from  time  steps  t  –  55  to  t.  For  each  input\n",
            "sequence, the model will output a single value: the forecast for time step t + 1.\n",
            "\n",
            "But what will we use as training data? Well, that’s the trick: we will use every 56-day\n",
            "window  from  the  past  as  training  data,  and  the  target  for  each  window  will  be  the\n",
            "value immediately following it.\n",
            "\n",
            "Keras  actually  has  a  nice  utility  function  called  tf.keras.utils.timeseries_\n",
            "dataset_from_array()  to  help  us  prepare  the  training  set.  It  takes  a  time  series\n",
            "as  input,  and  it  builds  a  tf.data.Dataset  (introduced  in  Chapter  13)  containing  all\n",
            "the  windows  of  the  desired  length,  as  well  as  their  corresponding  targets.  Here’s  an\n",
            "example that takes a time series containing the numbers 0 to 5 and creates a dataset\n",
            "containing  all  the  windows  of  length  3,  with  their  corresponding  targets,  grouped\n",
            "into batches of size 2:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "my_series = [0, 1, 2, 3, 4, 5]\n",
            "my_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    my_series,\n",
            "    targets=my_series[3:],  # the targets are 3 steps into the future\n",
            "    sequence_length=3,\n",
            "    batch_size=2\n",
            ")\n",
            "\n",
            "Let’s inspect the contents of this dataset:\n",
            "\n",
            ">>> list(my_dataset)\n",
            "[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "  array([[0, 1, 2],\n",
            "         [1, 2, 3]], dtype=int32)>,\n",
            "  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>),\n",
            " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]], dtype=int32)>,\n",
            "  <tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>)]\n",
            "\n",
            "Each  sample  in  the  dataset  is  a  window  of  length  3,  along  with  its  corresponding\n",
            "target  (i.e.,  the  value  immediately  after  the  window).  The  windows  are  [0,  1,  2],  [1,\n",
            "2, 3], and [2, 3, 4], and their respective targets are 3, 4, and 5. Since there are three\n",
            "windows in total, which is not a multiple of the batch size, the last batch only contains\n",
            "one window instead of two.\n",
            "\n",
            "552 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fAnother way to get the same result is to use the window() method of tf.data’s Dataset\n",
            "class. It’s more complex, but it gives you full control, which will come in handy later\n",
            "in this chapter, so let’s see how it works. The  window() method returns a dataset of\n",
            "window datasets:\n",
            "\n",
            ">>> for window_dataset in tf.data.Dataset.range(6).window(4, shift=1):\n",
            "...     for element in window_dataset:\n",
            "...         print(f\"{element}\", end=\" \")\n",
            "...     print()\n",
            "...\n",
            "0 1 2 3\n",
            "1 2 3 4\n",
            "2 3 4 5\n",
            "3 4 5\n",
            "4 5\n",
            "5\n",
            "\n",
            "In this example, the dataset contains six windows, each shifted by one step compared\n",
            "to the previous one, and the last three windows are smaller because they’ve reached\n",
            "the  end  of  the  series.  In  general  you’ll  want  to  get  rid  of  these  smaller  windows  by\n",
            "passing drop_remainder=True to the window() method.\n",
            "\n",
            "The  window()  method  returns  a  nested  dataset,  analogous  to  a  list  of  lists.  This  is\n",
            "useful when you want to transform each window by calling its dataset methods (e.g.,\n",
            "to shuffle them or batch them). However, we cannot use a nested dataset directly for\n",
            "training, as our model will expect tensors as input, not datasets.\n",
            "\n",
            "Therefore,  we  must  call  the  flat_map()  method:  it  converts  a  nested  dataset  into  a\n",
            "flat  dataset  (one  that  contains  tensors,  not  datasets).  For  example,  suppose  {1,  2,  3}\n",
            "represents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the\n",
            "nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}.\n",
            "\n",
            "Moreover, the flat_map() method takes a function as an argument, which allows you\n",
            "to transform each dataset in the nested dataset before flattening. For example, if you\n",
            "pass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the\n",
            "nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset\n",
            "containing 3 tensors, each of size 2.\n",
            "\n",
            "With that in mind, we are ready to flatten our dataset:\n",
            "\n",
            ">>> dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)\n",
            ">>> dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))\n",
            ">>> for window_tensor in dataset:\n",
            "...     print(f\"{window_tensor}\")\n",
            "...\n",
            "[0 1 2 3]\n",
            "[1 2 3 4]\n",
            "[2 3 4 5]\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "553\n",
            "\n",
            "\fSince each window dataset contains exactly four items, calling batch(4) on a window\n",
            "produces a single tensor of size 4. Great! We now have a dataset containing consecu‐\n",
            "tive  windows  represented  as  tensors.  Let’s  create  a  little  helper  function  to  make  it\n",
            "easier to extract windows from a dataset:\n",
            "\n",
            "def to_windows(dataset, length):\n",
            "    dataset = dataset.window(length, shift=1, drop_remainder=True)\n",
            "    return dataset.flat_map(lambda window_ds: window_ds.batch(length))\n",
            "\n",
            "The last step is to split each window into inputs and targets, using the map() method.\n",
            "We can also group the resulting windows into batches of size 2:\n",
            "\n",
            ">>> dataset = to_windows(tf.data.Dataset.range(6), 4)  # 3 inputs + 1 target = 4\n",
            ">>> dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
            ">>> list(dataset.batch(2))\n",
            "[(<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
            "  array([[0, 1, 2],\n",
            "         [1, 2, 3]])>,\n",
            "  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])>),\n",
            " (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]])>,\n",
            "  <tf.Tensor: shape=(1,), dtype=int64, numpy=array([5])>)]\n",
            "\n",
            "As  you  can  see,  we  now  have  the  same  output  as  we  got  earlier  with  the\n",
            "timeseries_dataset_from_array()  function  (with  a  bit  more  effort,  but  it  will  be\n",
            "worthwhile soon).\n",
            "\n",
            "Now,  before  we  start  training,  we  need  to  split  our  data  into  a  training  period,  a\n",
            "validation period, and a test period. We will focus on the rail ridership for now. We\n",
            "will also scale it down by a factor of one million, to ensure the values are near the 0–1\n",
            "range; this plays nicely with the default weight initialization and learning rate:\n",
            "\n",
            "rail_train = df[\"rail\"][\"2016-01\":\"2018-12\"] / 1e6\n",
            "rail_valid = df[\"rail\"][\"2019-01\":\"2019-05\"] / 1e6\n",
            "rail_test = df[\"rail\"][\"2019-06\":] / 1e6\n",
            "\n",
            "When  dealing  with  time  series,  you  generally  want  to  split  across\n",
            "time. However, in some cases you may be able to split along other\n",
            "dimensions,  which  will  give  you  a  longer  time  period  to  train\n",
            "on.  For  example,  if  you  have  data  about  the  financial  health  of\n",
            "10,000  companies  from  2001  to  2019,  you  might  be  able  to  split\n",
            "this  data  across  the  different  companies.  It’s  very  likely  that  many\n",
            "of these companies will be strongly correlated, though (e.g., whole\n",
            "economic  sectors  may  go  up  or  down  jointly),  and  if  you  have\n",
            "correlated companies across the training set and the test set, your\n",
            "test  set  will  not  be  as  useful,  as  its  measure  of  the  generalization\n",
            "error will be optimistically biased.\n",
            "\n",
            "554 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fNext, let’s use timeseries_dataset_from_array() to create datasets for training and\n",
            "validation.  Since  gradient  descent  expects  the  instances  in  the  training  set  to  be\n",
            "independent and identically distributed (IID), as we saw in Chapter 4, we must set the\n",
            "argument shuffle=True to shuffle the training windows (but not their contents):\n",
            "\n",
            "seq_length = 56\n",
            "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    rail_train.to_numpy(),\n",
            "    targets=rail_train[seq_length:],\n",
            "    sequence_length=seq_length,\n",
            "    batch_size=32,\n",
            "    shuffle=True,\n",
            "    seed=42\n",
            ")\n",
            "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    rail_valid.to_numpy(),\n",
            "    targets=rail_valid[seq_length:],\n",
            "    sequence_length=seq_length,\n",
            "    batch_size=32\n",
            ")\n",
            "\n",
            "And now we’re ready to build and train any regression model we want!\n",
            "\n",
            "Forecasting Using a Linear Model\n",
            "Let’s  try  a  basic  linear  model  first.  We  will  use  the  Huber  loss,  which  usually  works\n",
            "better than minimizing the MAE directly, as discussed in Chapter 10. We’ll also use\n",
            "early stopping:\n",
            "\n",
            "tf.random.set_seed(42)\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(1, input_shape=[seq_length])\n",
            "])\n",
            "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
            "    monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
            "opt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
            "model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n",
            "history = model.fit(train_ds, validation_data=valid_ds, epochs=500,\n",
            "                    callbacks=[early_stopping_cb])\n",
            "\n",
            "This model reaches a validation MAE of about 37,866 (your mileage may vary). That’s\n",
            "better than naive forecasting, but worse than the SARIMA model.5\n",
            "\n",
            "Can we do better with an RNN? Let’s see!\n",
            "\n",
            "5 Note that the validation period starts on the 1st of January 2019, so the first prediction is for the 26th of\n",
            "\n",
            "February 2019, eight weeks later. When we evaluated the baseline models we used predictions starting on the\n",
            "1st of March instead, but this should be close enough.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "555\n",
            "\n",
            "\fForecasting Using a Simple RNN\n",
            "Let’s  try  the  most  basic  RNN,  containing  a  single  recurrent  layer  with  just  one\n",
            "recurrent neuron, as we saw in Figure 15-1:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
            "])\n",
            "\n",
            "All recurrent layers in Keras expect 3D inputs of shape [batch size, time steps, dimen‐\n",
            "sionality], where dimensionality is 1 for univariate time series and more for multivari‐\n",
            "ate  time  series.  Recall  that  the  input_shape  argument  ignores  the  first  dimension\n",
            "(i.e.,  the  batch  size),  and  since  recurrent  layers  can  accept  input  sequences  of  any\n",
            "length, we can set the second dimension to None, which means “any size”. Lastly, since\n",
            "we’re dealing with a univariate time series, we need the last dimension’s size to be 1.\n",
            "This is why we specified the input shape [None, 1]: it means “univariate sequences\n",
            "of any length”. Note that the datasets actually contain inputs of shape [batch size, time\n",
            "steps], so we’re missing the last dimension, of size 1, but Keras is kind enough to add\n",
            "it for us in this case.\n",
            "\n",
            "This  model  works  exactly  as  we  saw  earlier:  the  initial  state  h(init)  is  set  to  0,  and  it\n",
            "is  passed  to  a  single  recurrent  neuron,  along  with  the  value  of  the  first  time  step,\n",
            "x(0). The neuron computes a weighted sum of these values plus the bias term, and it\n",
            "applies the activation function to the result, using the hyperbolic tangent function by\n",
            "default. The result is the first output, y0. In a simple RNN, this output is also the new\n",
            "state  h0.  This  new  state  is  passed  to  the  same  recurrent  neuron  along  with  the  next\n",
            "input value, x(1), and the process is repeated until the last time step. At the end, the\n",
            "layer just outputs the last value: in our case the sequences are 56 steps long, so the last\n",
            "value is y55. All of this is performed simultaneously for every sequence in the batch, of\n",
            "which there are 32 in this case.\n",
            "\n",
            "By  default,  recurrent  layers  in  Keras  only  return  the  final  output.\n",
            "To  make  them  return  one  output  per  time  step,  you  must  set\n",
            "return_sequences=True, as you will see.\n",
            "\n",
            "So  that’s  our  first  recurrent  model!  It’s  a  sequence-to-vector  model.  Since  there’s  a\n",
            "single output neuron, the output vector has a size of 1.\n",
            "\n",
            "Now if you compile, train, and evaluate this model just like the previous model, you\n",
            "will  find  that  it’s  no  good  at  all:  its  validation  MAE  is  greater  than  100,000!  Ouch.\n",
            "That was to be expected, for two reasons:\n",
            "\n",
            "556 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\f1. The  model  only  has  a  single  recurrent  neuron,  so  the  only  data  it  can  use  to\n",
            "1.\n",
            "make  a  prediction  at  each  time  step  is  the  input  value  at  the  current  time  step\n",
            "and the output value from the previous time step. That’s not much to go on! In\n",
            "other  words,  the  RNN’s  memory  is  extremely  limited:  it’s  just  a  single  number,\n",
            "its previous output. And let’s count how many parameters this model has: since\n",
            "there’s  just  one  recurrent  neuron  with  only  two  input  values,  the  whole  model\n",
            "only has three parameters (two weights plus a bias term). That’s far from enough\n",
            "for this time series. In contrast, our previous model could look at all 56 previous\n",
            "values at once, and it had a total of 57 parameters.\n",
            "\n",
            "2. The time series contains values from 0 to about 1.4, but since the default activa‐\n",
            "2.\n",
            "tion function is tanh, the recurrent layer can only output values between –1 and\n",
            "+1. There’s no way it can predict values between 1.0 and 1.4.\n",
            "\n",
            "Let’s  fix  both  of  these  issues:  we  will  create  a  model  with  a  larger  recurrent  layer,\n",
            "containing  32  recurrent  neurons,  and  we  will  add  a  dense  output  layer  on  top  of  it\n",
            "with  a  single  output  neuron  and  no  activation  function.  The  recurrent  layer  will  be\n",
            "able to carry much more information from one time step to the next, and the dense\n",
            "output layer will project the final output from 32 dimensions down to 1, without any\n",
            "value range constraints:\n",
            "\n",
            "univar_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\n",
            "    tf.keras.layers.Dense(1)  # no activation function by default\n",
            "])\n",
            "\n",
            "Now if you compile, fit, and evaluate this model just like the previous one, you will\n",
            "find that its validation MAE reaches 27,703. That’s the best model we’ve trained so far,\n",
            "and it even beats the SARIMA model: we’re doing pretty well!\n",
            "\n",
            "We’ve  only  normalized  the  time  series,  without  removing  trend\n",
            "and seasonality, and yet the model still performs well. This is con‐\n",
            "venient,  as  it  makes  it  possible  to  quickly  search  for  promising\n",
            "models without worrying too much about preprocessing. However,\n",
            "to get the best performance, you may want to try making the time\n",
            "series more stationary; for example, using differencing.\n",
            "\n",
            "Forecasting Using a Deep RNN\n",
            "It  is  quite  common  to  stack  multiple  layers  of  cells,  as  shown  in  Figure  15-10.  This\n",
            "gives you a deep RNN.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "557\n",
            "\n",
            "\fFigure 15-10. A deep RNN (left) unrolled through time (right)\n",
            "\n",
            "Implementing a deep RNN with Keras is straightforward: just stack recurrent layers.\n",
            "In the following example, we use three SimpleRNN layers (but we could use any other\n",
            "type  of  recurrent  layer  instead,  such  as  an  LSTM  layer  or  a  GRU  layer,  which  we  will\n",
            "discuss  shortly).  The  first  two  are  sequence-to-sequence  layers,  and  the  last  one  is\n",
            "a  sequence-to-vector  layer.  Finally,  the  Dense  layer  produces  the  model’s  forecast\n",
            "(you can think of it as a vector-to-vector layer). So this model is just like the model\n",
            "represented in Figure 15-10, except the outputs Ŷ(0) to Ŷ(t–1_) are ignored, and there’s a\n",
            "dense layer on top of Ŷ(t), which outputs the actual forecast:\n",
            "\n",
            "deep_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n",
            "    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",
            "    tf.keras.layers.SimpleRNN(32),\n",
            "    tf.keras.layers.Dense(1)\n",
            "])\n",
            "\n",
            "Make  sure  to  set  return_sequences=True  for  all  recurrent  layers\n",
            "(except the last one, if you only care about the last output). If you\n",
            "forget to set this parameter for one recurrent layer, it will output a\n",
            "2D array containing only the output of the last time step, instead of\n",
            "a 3D array containing outputs for all time steps. The next recurrent\n",
            "layer  will  complain  that  you  are  not  feeding  it  sequences  in  the\n",
            "expected 3D format.\n",
            "\n",
            "If  you  train  and  evaluate  this  model,  you  will  find  that  it  reaches  an  MAE  of  about\n",
            "31,211. That’s better than both baselines, but it doesn’t beat our “shallower” RNN. It\n",
            "looks like this RNN is a bit too large for our task.\n",
            "\n",
            "558 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fForecasting Multivariate Time Series\n",
            "A great quality of neural networks is their flexibility: in particular, they can deal with\n",
            "multivariate  time  series  with  almost  no  change  to  their  architecture.  For  example,\n",
            "let’s  try  to  forecast  the  rail  time  series  using  both  the  bus  and  rail  data  as  input.  In\n",
            "fact, let’s also throw in the day type! Since we can always know in advance whether\n",
            "tomorrow is going to be a weekday, a weekend, or a holiday, we can shift the day type\n",
            "series one day into the future, so that the model is given tomorrow’s day type as input.\n",
            "For simplicity, we’ll do this processing using Pandas:\n",
            "\n",
            "df_mulvar = df[[\"bus\", \"rail\"]] / 1e6  # use both bus & rail series as input\n",
            "df_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)  # we know tomorrow's type\n",
            "df_mulvar = pd.get_dummies(df_mulvar)  # one-hot encode the day type\n",
            "\n",
            "Now df_mulvar is a DataFrame with five columns: the bus and rail data, plus three\n",
            "columns containing the one-hot encoding of the next day’s type (recall that there are\n",
            "three possible day types, W, A, and U). Next we can proceed much like we did earlier.\n",
            "First we split the data into three periods, for training, validation, and testing:\n",
            "\n",
            "mulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\n",
            "mulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\n",
            "mulvar_test = df_mulvar[\"2019-06\":]\n",
            "\n",
            "Then we create the datasets:\n",
            "\n",
            "train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    mulvar_train.to_numpy(),  # use all 5 columns as input\n",
            "    targets=mulvar_train[\"rail\"][seq_length:],  # forecast only the rail series\n",
            "    [...]  # the other 4 arguments are the same as earlier\n",
            ")\n",
            "valid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    mulvar_valid.to_numpy(),\n",
            "    targets=mulvar_valid[\"rail\"][seq_length:],\n",
            "    [...]  # the other 2 arguments are the same as earlier\n",
            ")\n",
            "\n",
            "And finally we create the RNN:\n",
            "\n",
            "mulvar_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n",
            "    tf.keras.layers.Dense(1)\n",
            "])\n",
            "\n",
            "Notice  that  the  only  difference  from  the  univar_model  RNN  we  built  earlier  is  the\n",
            "input shape: at each time step, the model now receives five inputs instead of one. This\n",
            "model actually reaches a validation MAE of 22,062. Now we’re making big progress!\n",
            "\n",
            "In  fact,  it’s  not  too  hard  to  make  the  RNN  forecast  both  the  bus  and  rail  rid‐\n",
            "ership.  You  just  need  to  change  the  targets  when  creating  the  datasets,  setting\n",
            "them  to  mulvar_train[[\"bus\",  \"rail\"]][seq_length:]  for  the  training  set,  and\n",
            "mulvar_valid[[\"bus\",  \"rail\"]][seq_length:]  for  the  validation  set.  You  must\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "559\n",
            "\n",
            "\falso  add  an  extra  neuron  in  the  output  Dense  layer,  since  it  must  now  make  two\n",
            "forecasts: one for tomorrow’s bus ridership, and the other for rail. That’s all there is to\n",
            "it!\n",
            "\n",
            "As we discussed in Chapter 10, using a single model for multiple related tasks often\n",
            "results in better performance than using a separate model for each task, since features\n",
            "learned  for  one  task  may  be  useful  for  the  other  tasks,  and  also  because  having  to\n",
            "perform  well  across  multiple  tasks  prevents  the  model  from  overfitting  (it’s  a  form\n",
            "of  regularization).  However,  it  depends  on  the  task,  and  in  this  particular  case  the\n",
            "multitask  RNN  that  forecasts  both  the  bus  and  the  rail  ridership  doesn’t  perform\n",
            "quite as well as dedicated models that forecast one or the other (using all five columns\n",
            "as  input).  Still,  it  reaches  a  validation  MAE  of  25,330  for  rail  and  26,369  for  bus,\n",
            "which is pretty good.\n",
            "\n",
            "Forecasting Several Time Steps Ahead\n",
            "So  far  we  have  only  predicted  the  value  at  the  next  time  step,  but  we  could  just  as\n",
            "easily have predicted the value several steps ahead by changing the targets appropri‐\n",
            "ately (e.g., to predict the ridership 2 weeks from now, we could just change the targets\n",
            "to be the value 14 days ahead instead of 1 day ahead). But what if we want to predict\n",
            "the next 14 values?\n",
            "\n",
            "The first option is to take the univar_model RNN we trained earlier for the rail time\n",
            "series, make it predict the next value, and add that value to the inputs, acting as if the\n",
            "predicted value had actually occurred; we would then use the model again to predict\n",
            "the following value, and so on, as in the following code:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]\n",
            "for step_ahead in range(14):\n",
            "    y_pred_one = univar_model.predict(X)\n",
            "    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)\n",
            "\n",
            "In  this  code,  we  take  the  rail  ridership  of  the  first  56  days  of  the  validation  period,\n",
            "and  we  convert  the  data  to  a  NumPy  array  of  shape  [1,  56,  1]  (recall  that  recurrent\n",
            "layers expect 3D inputs). Then we repeatedly use the model to forecast the next value,\n",
            "and  we  append  each  forecast  to  the  input  series,  along  the  time  axis  (axis=1).  The\n",
            "resulting forecasts are plotted in Figure 15-11.\n",
            "\n",
            "If  the  model  makes  an  error  at  one  time  step,  then  the  forecasts\n",
            "for the following time steps are impacted as well: the errors tend to\n",
            "accumulate. So, it’s preferable to use this technique only for a small\n",
            "number of steps.\n",
            "\n",
            "560 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fFigure 15-11. Forecasting 14 steps ahead, 1 step at a time\n",
            "\n",
            "The second option is to train an RNN to predict the next 14 values in one shot. We\n",
            "can  still  use  a  sequence-to-vector  model,  but  it  will  output  14  values  instead  of  1.\n",
            "However,  we  first  need  to  change  the  targets  to  be  vectors  containing  the  next  14\n",
            "values.  To  do  this,  we  can  use  timeseries_dataset_from_array()  again,  but  this\n",
            "time  asking  it  to  create  datasets  without  targets  (targets=None)  and  with  longer\n",
            "sequences, of length seq_length + 14. Then we can use the datasets’ map() method\n",
            "to apply a custom function to each batch of sequences, splitting them into inputs and\n",
            "targets.  In  this  example,  we  use  the  multivariate  time  series  as  input  (using  all  five\n",
            "columns), and we forecast the rail ridership for the next 14 days:6\n",
            "\n",
            "def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\n",
            "    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\n",
            "\n",
            "ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    mulvar_train.to_numpy(),\n",
            "    targets=None,\n",
            "    sequence_length=seq_length + 14,\n",
            "    [...]  # the other 3 arguments are the same as earlier\n",
            ").map(split_inputs_and_targets)\n",
            "ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
            "    mulvar_valid.to_numpy(),\n",
            "    targets=None,\n",
            "    sequence_length=seq_length + 14,\n",
            "    batch_size=32\n",
            ").map(split_inputs_and_targets)\n",
            "\n",
            "Now we just need the output layer to have 14 units instead of 1:\n",
            "\n",
            "6 Feel free to play around with this model. For example, you can try forecasting both the bus and rail ridership\n",
            "\n",
            "for the next 14 days. You’ll need to tweak the targets to include both, and make your model output 28\n",
            "forecasts instead of 14.\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "561\n",
            "\n",
            "\fahead_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n",
            "    tf.keras.layers.Dense(14)\n",
            "])\n",
            "\n",
            "After training this model, you can predict the next 14 values at once like this:\n",
            "\n",
            "X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\n",
            "Y_pred = ahead_model.predict(X)  # shape [1, 14]\n",
            "\n",
            "This  approach  works  quite  well.  Its  forecasts  for  the  next  day  are  obviously  better\n",
            "than its forecasts for 14 days into the future, but it doesn’t accumulate errors like the\n",
            "previous approach did. However, we can still do better, using a sequence-to-sequence\n",
            "(or seq2seq) model.\n",
            "\n",
            "Forecasting Using a Sequence-to-Sequence Model\n",
            "Instead of training the model to forecast the next 14 values only at the very last time\n",
            "step, we can train it to forecast the next 14 values at each and every time step. In other\n",
            "words, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN.\n",
            "The advantage of this technique is that the loss will contain a term for the output of\n",
            "the RNN at each and every time step, not just for the output at the last time step.\n",
            "\n",
            "This means there will be many more error gradients flowing through the model, and\n",
            "they won’t have to flow through time as much since they will come from the output of\n",
            "each time step, not just the last one. This will both stabilize and speed up training.\n",
            "\n",
            "To be clear, at time step 0 the model will output a vector containing the forecasts for\n",
            "time steps 1 to 14, then at time step 1 the model will forecast time steps 2 to 15, and\n",
            "so on. In other words, the targets are sequences of consecutive windows, shifted by\n",
            "one time step at each time step. The target is not a vector anymore, but a sequence of\n",
            "the same length as the inputs, containing a 14-dimensional vector at each step.\n",
            "\n",
            "Preparing  the  datasets  is  not  trivial,  since  each  instance  has  a  window  as  input  and\n",
            "a  sequence  of  windows  as  output.  One  way  to  do  this  is  to  use  the  to_windows()\n",
            "utility  function  we  created  earlier,  twice  in  a  row,  to  get  windows  of  consecutive\n",
            "windows. For example, let’s turn the series of numbers 0 to 6 into a dataset containing\n",
            "sequences of 4 consecutive windows, each of length 3:\n",
            "\n",
            ">>> my_series = tf.data.Dataset.range(7)\n",
            ">>> dataset = to_windows(to_windows(my_series, 3), 4)\n",
            ">>> list(dataset)\n",
            "[<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n",
            " array([[0, 1, 2],\n",
            "        [1, 2, 3],\n",
            "        [2, 3, 4],\n",
            "        [3, 4, 5]])>,\n",
            " <tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n",
            " array([[1, 2, 3],\n",
            "        [2, 3, 4],\n",
            "\n",
            "562 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\f        [3, 4, 5],\n",
            "        [4, 5, 6]])>]\n",
            "\n",
            "Now we can use the map() method to split these windows of windows into inputs and\n",
            "targets:\n",
            "\n",
            ">>> dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))\n",
            ">>> list(dataset)\n",
            "[(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])>,\n",
            "  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
            "  array([[1, 2],\n",
            "         [2, 3],\n",
            "         [3, 4],\n",
            "         [4, 5]])>),\n",
            " (<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4])>,\n",
            "  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
            "  array([[2, 3],\n",
            "         [3, 4],\n",
            "         [4, 5],\n",
            "         [5, 6]])>)]\n",
            "\n",
            "Now the dataset contains sequences of length 4 as inputs, and the targets are sequen‐\n",
            "ces  containing  the  next  two  steps,  for  each  time  step.  For  example,  the  first  input\n",
            "sequence is [0, 1, 2, 3], and its corresponding targets are [[1, 2], [2, 3], [3, 4], [4, 5]],\n",
            "which are the next two values for each time step. If you’re like me, you will probably\n",
            "need a few minutes to wrap your head around this. Take your time!\n",
            "\n",
            "It may be surprising that the targets contain values that appear in\n",
            "the inputs. Isn’t that cheating? Fortunately, not at all: at each time\n",
            "step,  an  RNN  only  knows  about  past  time  steps;  it  cannot  look\n",
            "ahead. It is said to be a causal model.\n",
            "\n",
            "Let’s create another little utility function to prepare the datasets for our sequence-to-\n",
            "sequence model. It will also take care of shuffling (optional) and batching:\n",
            "\n",
            "def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,\n",
            "                       batch_size=32, shuffle=False, seed=None):\n",
            "    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)\n",
            "    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))\n",
            "    if shuffle:\n",
            "        ds = ds.shuffle(8 * batch_size, seed=seed)\n",
            "    return ds.batch(batch_size)\n",
            "\n",
            "Now we can use this function to create the datasets:\n",
            "\n",
            "seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)\n",
            "seq2seq_valid = to_seq2seq_dataset(mulvar_valid)\n",
            "\n",
            "Forecasting a Time Series \n",
            "\n",
            "| \n",
            "\n",
            "563\n",
            "\n",
            "\fAnd lastly, we can build the sequence-to-sequence model:\n",
            "\n",
            "seq2seq_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),\n",
            "    tf.keras.layers.Dense(14)\n",
            "])\n",
            "\n",
            "It  is  almost  identical  to  our  previous  model:  the  only  difference  is  that  we  set\n",
            "return_sequences=True in the SimpleRNN layer. This way, it will output a sequence of\n",
            "vectors (each of size 32), instead of outputting a single vector at the last time step. The\n",
            "Dense layer is smart enough to handle sequences as input: it will be applied at each\n",
            "time step, taking a 32-dimensional vector as input and outputting a 14-dimensional\n",
            "vector. In fact, another way to get the exact same result is to use a Conv1D layer with a\n",
            "kernel size of 1: Conv1D(14, kernel_size=1).\n",
            "\n",
            "Keras  offers  a  TimeDistributed  layer  that  lets  you  apply  any\n",
            "vector-to-vector  layer  to  every  vector  in  the  input  sequences,  at\n",
            "every  time  step.  It  does  this  efficiently,  by  reshaping  the  inputs  so\n",
            "that each time step is treated as a separate instance, then it reshapes\n",
            "the layer’s outputs to recover the time dimension. In our case, we\n",
            "don’t  need  it  since  the  Dense  layer  already  supports  sequences  as\n",
            "inputs.\n",
            "\n",
            "The  training  code  is  the  same  as  usual.  During  training,  all  the  model’s  outputs  are\n",
            "used, but after training only the output of the very last time step matters, and the rest\n",
            "can be ignored. For example, we can forecast the rail ridership for the next 14 days\n",
            "like this:\n",
            "\n",
            "X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]\n",
            "y_pred_14 = seq2seq_model.predict(X)[0, -1]  # only the last time step's output\n",
            "\n",
            "If  you  evaluate  this  model’s  forecasts  for  t  +  1,  you  will  find  a  validation  MAE  of\n",
            "25,519. For t + 2 it’s 26,274, and the performance continues to drop gradually as the\n",
            "model tries to forecast further into the future. At t + 14, the MAE is 34,322.\n",
            "\n",
            "You  can  combine  both  approaches  to  forecasting  multiple  steps\n",
            "ahead:  for  example,  you  can  train  a  model  that  forecasts  14  days\n",
            "ahead,  then  take  its  output  and  append  it  to  the  inputs,  then  run\n",
            "the  model  again  to  get  forecasts  for  the  following  14  days,  and\n",
            "possibly repeat the process.\n",
            "\n",
            "Simple  RNNs  can  be  quite  good  at  forecasting  time  series  or  handling  other  kinds\n",
            "of sequences, but they do not perform as well on long time series or sequences. Let’s\n",
            "discuss why and see what we can do about it.\n",
            "\n",
            "564 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fHandling Long Sequences\n",
            "To  train  an  RNN  on  long  sequences,  we  must  run  it  over  many  time  steps,  making\n",
            "the  unrolled  RNN  a  very  deep  network.  Just  like  any  deep  neural  network  it  may\n",
            "suffer  from  the  unstable  gradients  problem,  discussed  in  Chapter  11:  it  may  take\n",
            "forever  to  train,  or  training  may  be  unstable.  Moreover,  when  an  RNN  processes  a\n",
            "long  sequence,  it  will  gradually  forget  the  first  inputs  in  the  sequence.  Let’s  look  at\n",
            "both these problems, starting with the unstable gradients problem.\n",
            "\n",
            "Fighting the Unstable Gradients Problem\n",
            "Many of the tricks we used in deep nets to alleviate the unstable gradients problem\n",
            "can also be used for RNNs: good parameter initialization, faster optimizers, dropout,\n",
            "and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as\n",
            "much here. In fact, they may actually lead the RNN to be even more unstable during\n",
            "training.  Why?  Well,  suppose  gradient  descent  updates  the  weights  in  a  way  that\n",
            "increases the outputs slightly at the first time step. Because the same weights are used\n",
            "at every time step, the outputs at the second time step may also be slightly increased,\n",
            "and  those  at  the  third,  and  so  on  until  the  outputs  explode—and  a  nonsaturating\n",
            "activation function does not prevent that.\n",
            "\n",
            "You can reduce this risk by using a smaller learning rate, or you can use a saturating\n",
            "activation function like the hyperbolic tangent (this explains why it’s the default).\n",
            "\n",
            "In  much  the  same  way,  the  gradients  themselves  can  explode.  If  you  notice  that\n",
            "training  is  unstable,  you  may  want  to  monitor  the  size  of  the  gradients  (e.g.,  using\n",
            "TensorBoard) and perhaps use gradient clipping.\n",
            "\n",
            "Moreover,  batch  normalization  cannot  be  used  as  efficiently  with  RNNs  as  with\n",
            "deep  feedforward  nets.  In  fact,  you  cannot  use  it  between  time  steps,  only  between\n",
            "recurrent layers.\n",
            "\n",
            "To be more precise, it is technically possible to add a BN layer to a memory cell (as\n",
            "you  will  see  shortly)  so  that  it  will  be  applied  at  each  time  step  (both  on  the  inputs\n",
            "for  that  time  step  and  on  the  hidden  state  from  the  previous  step).  However,  the\n",
            "same  BN  layer  will  be  used  at  each  time  step,  with  the  same  parameters,  regardless\n",
            "of the actual scale and offset of the inputs and hidden state. In practice, this does not\n",
            "yield good results, as was demonstrated by César Laurent et al. in a 2015 paper:7 the\n",
            "authors found that BN was slightly beneficial only when it was applied to the layer’s\n",
            "inputs,  not  to  the  hidden  states.  In  other  words,  it  was  slightly  better  than  nothing\n",
            "when applied between recurrent layers (i.e., vertically in Figure 15-10), but not within\n",
            "\n",
            "7 César Laurent et al., “Batch Normalized Recurrent Neural Networks”, Proceedings of the IEEE International\n",
            "\n",
            "Conference on Acoustics, Speech, and Signal Processing (2016): 2657–2661.\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "565\n",
            "\n",
            "\frecurrent layers (i.e., horizontally). In Keras, you can apply BN between layers simply\n",
            "by  adding  a  BatchNormalization  layer  before  each  recurrent  layer,  but  it  will  slow\n",
            "down training, and it may not help much.\n",
            "\n",
            "Another  form  of  normalization  often  works  better  with  RNNs:  layer  normalization.\n",
            "This  idea  was  introduced  by  Jimmy  Lei  Ba  et  al.  in  a  2016  paper:8  it  is  very  similar\n",
            "to batch normalization, but instead of normalizing across the batch dimension, layer\n",
            "normalization normalizes across the features dimension. One advantage is that it can\n",
            "compute  the  required  statistics  on  the  fly,  at  each  time  step,  independently  for  each\n",
            "instance. This also means that it behaves the same way during training and testing (as\n",
            "opposed to BN), and it does not need to use exponential moving averages to estimate\n",
            "the  feature  statistics  across  all  instances  in  the  training  set,  like  BN  does.  Like  BN,\n",
            "layer normalization learns a scale and an offset parameter for each input. In an RNN,\n",
            "it  is  typically  used  right  after  the  linear  combination  of  the  inputs  and  the  hidden\n",
            "states.\n",
            "\n",
            "Let’s use Keras to implement layer normalization within a simple memory cell. To do\n",
            "this, we need to define a custom memory cell, which is just like a regular layer, except\n",
            "its call() method takes two arguments: the inputs at the current time step and the\n",
            "hidden states from the previous time step.\n",
            "\n",
            "Note that the states argument is a list containing one or more tensors. In the case\n",
            "of a simple RNN cell it contains a single tensor equal to the outputs of the previous\n",
            "time  step,  but  other  cells  may  have  multiple  state  tensors  (e.g.,  an  LSTMCell  has  a\n",
            "long-term  state  and  a  short-term  state,  as  you  will  see  shortly).  A  cell  must  also\n",
            "have  a  state_size  attribute  and  an  output_size  attribute.  In  a  simple  RNN,  both\n",
            "are  simply  equal  to  the  number  of  units.  The  following  code  implements  a  custom\n",
            "memory  cell  that  will  behave  like  a  SimpleRNNCell,  except  it  will  also  apply  layer\n",
            "normalization at each time step:\n",
            "\n",
            "class LNSimpleRNNCell(tf.keras.layers.Layer):\n",
            "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.state_size = units\n",
            "        self.output_size = units\n",
            "        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,\n",
            "                                                             activation=None)\n",
            "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
            "        self.activation = tf.keras.activations.get(activation)\n",
            "\n",
            "    def call(self, inputs, states):\n",
            "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
            "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
            "        return norm_outputs, [norm_outputs]\n",
            "\n",
            "8 Jimmy Lei Ba et al., “Layer Normalization”, arXiv preprint arXiv:1607.06450 (2016).\n",
            "\n",
            "566 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fLet’s walk through this code:\n",
            "\n",
            "• Our LNSimpleRNNCell class inherits from the tf.keras.layers.Layer class, just\n",
            "•\n",
            "\n",
            "like any custom layer.\n",
            "\n",
            "• The  constructor  takes  the  number  of  units  and  the  desired  activation  function\n",
            "•\n",
            "and  sets  the  state_size  and  output_size  attributes,  then  creates  a  SimpleRNN\n",
            "Cell with no activation function (because we want to perform layer normaliza‐\n",
            "tion  after  the  linear  operation  but  before  the  activation  function).9  Then  the\n",
            "constructor  creates  the  LayerNormalization  layer,  and  finally  it  fetches  the\n",
            "desired activation function.\n",
            "\n",
            "• The  call()  method  starts  by  applying  the  simpleRNNCell,  which  computes  a\n",
            "•\n",
            "linear  combination  of  the  current  inputs  and  the  previous  hidden  states,  and  it\n",
            "returns the result twice (indeed, in a  SimpleRNNCell, the outputs are just equal\n",
            "to  the  hidden  states:  in  other  words,  new_states[0]  is  equal  to  outputs,  so  we\n",
            "can safely ignore new_states in the rest of the call() method). Next, the call()\n",
            "method applies layer normalization, followed by the activation function. Finally,\n",
            "it  returns  the  outputs  twice:  once  as  the  outputs,  and  once  as  the  new  hidden\n",
            "states. To use this custom cell, all we need to do is create a tf.keras.layers.RNN\n",
            "layer, passing it a cell instance:\n",
            "\n",
            "custom_ln_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,\n",
            "                        input_shape=[None, 5]),\n",
            "    tf.keras.layers.Dense(14)\n",
            "])\n",
            "\n",
            "Similarly,  you  could  create  a  custom  cell  to  apply  dropout  between  each  time  step.\n",
            "But  there’s  a  simpler  way:  most  recurrent  layers  and  cells  provided  by  Keras  have\n",
            "dropout  and  recurrent_dropout  hyperparameters:  the  former  defines  the  dropout\n",
            "rate  to  apply  to  the  inputs,  and  the  latter  defines  the  dropout  rate  for  the  hidden\n",
            "states, between time steps. So, there’s no need to create a custom cell to apply dropout\n",
            "at each time step in an RNN.\n",
            "\n",
            "With  these  techniques,  you  can  alleviate  the  unstable  gradients  problem  and  train\n",
            "an  RNN  much  more  efficiently.  Now  let’s  look  at  how  to  deal  with  the  short-term\n",
            "memory problem.\n",
            "\n",
            "9 It would have been simpler to inherit from SimpleRNNCell instead so that we wouldn’t have to create an\n",
            "\n",
            "internal SimpleRNNCell or handle the state_size and output_size attributes, but the goal here was to show\n",
            "how to create a custom cell from scratch.\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "567\n",
            "\n",
            "\fWhen forecasting time series, it is often useful to have some error\n",
            "bars along with your predictions. For this, one approach is to use\n",
            "MC  dropout,  introduced  in  Chapter  11:  use  recurrent_dropout\n",
            "during  training,  then  keep  dropout  active  at  inference  time  by\n",
            "calling  the  model  using  model(X,  training=True).  Repeat  this\n",
            "several times to get multiple slightly different forecasts, then com‐\n",
            "pute the mean and standard deviation of these predictions for each\n",
            "time step.\n",
            "\n",
            "Tackling the Short-Term Memory Problem\n",
            "Due  to  the  transformations  that  the  data  goes  through  when  traversing  an  RNN,\n",
            "some  information  is  lost  at  each  time  step.  After  a  while,  the  RNN’s  state  contains\n",
            "virtually  no  trace  of  the  first  inputs.  This  can  be  a  showstopper.  Imagine  Dory  the\n",
            "fish10 trying to translate a long sentence; by the time she’s finished reading it, she has\n",
            "no clue how it started. To tackle this problem, various types of cells with long-term\n",
            "memory  have  been  introduced.  They  have  proven  so  successful  that  the  basic  cells\n",
            "are  not  used  much  anymore.  Let’s  first  look  at  the  most  popular  of  these  long-term\n",
            "memory cells: the LSTM cell.\n",
            "\n",
            "LSTM cells\n",
            "\n",
            "The long short-term memory (LSTM) cell was proposed in 199711 by Sepp Hochreiter\n",
            "and Jürgen Schmidhuber and gradually improved over the years by several research‐\n",
            "ers, such as Alex Graves, Haşim Sak,12 and Wojciech Zaremba.13 If you consider the\n",
            "LSTM  cell  as  a  black  box,  it  can  be  used  very  much  like  a  basic  cell,  except  it  will\n",
            "perform  much  better;  training  will  converge  faster,  and  it  will  detect  longer-term\n",
            "patterns  in  the  data.  In  Keras,  you  can  simply  use  the  LSTM  layer  instead  of  the\n",
            "SimpleRNN layer:\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\n",
            "    tf.keras.layers.Dense(14)\n",
            "])\n",
            "\n",
            "Alternatively,  you  could  use  the  general-purpose  tf.keras.layers.RNN  layer,  giv‐\n",
            "ing  it  an  LSTMCell  as  an  argument.  However,  the  LSTM  layer  uses  an  optimized\n",
            "implementation when running on a GPU (see Chapter 19), so in general it is prefera‐\n",
            "\n",
            "10 A character from the animated movies Finding Nemo and Finding Dory who has short-term memory loss.\n",
            "\n",
            "11 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory”, Neural Computation 9, no. 8 (1997):\n",
            "\n",
            "1735–1780.\n",
            "\n",
            "12 Haşim Sak et al., “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large\n",
            "\n",
            "Vocabulary Speech Recognition”, arXiv preprint arXiv:1402.1128 (2014).\n",
            "\n",
            "13 Wojciech Zaremba et al., “Recurrent Neural Network Regularization”, arXiv preprint arXiv:1409.2329 (2014).\n",
            "\n",
            "568 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fble to use it (the RNN layer is mostly useful when you define custom cells, as we did\n",
            "earlier).\n",
            "\n",
            "So  how  does  an  LSTM  cell  work?  Its  architecture  is  shown  in  Figure  15-12.  If  you\n",
            "don’t  look  at  what’s  inside  the  box,  the  LSTM  cell  looks  exactly  like  a  regular  cell,\n",
            "except that its state is split into two vectors: h(t) and c(t) (“c” stands for “cell”). You can\n",
            "think of h(t) as the short-term state and c(t) as the long-term state.\n",
            "\n",
            "Figure 15-12. An LSTM cell\n",
            "\n",
            "Now let’s open the box! The key idea is that the network can learn what to store in the\n",
            "long-term state, what to throw away, and what to read from it. As the long-term state\n",
            "c(t–1) traverses the network from left to right, you can see that it first goes through a\n",
            "forget gate, dropping some memories, and then it adds some new memories via the\n",
            "addition  operation  (which  adds  the  memories  that  were  selected  by  an  input  gate).\n",
            "The  result  c(t)  is  sent  straight  out,  without  any  further  transformation.  So,  at  each\n",
            "time  step,  some  memories  are  dropped  and  some  memories  are  added.  Moreover,\n",
            "after  the  addition  operation,  the  long-term  state  is  copied  and  passed  through  the\n",
            "tanh  function,  and  then  the  result  is  filtered  by  the  output  gate.  This  produces  the\n",
            "short-term  state  h(t)  (which  is  equal  to  the  cell’s  output  for  this  time  step,  y(t)).  Now\n",
            "let’s look at where new memories come from and how the gates work.\n",
            "\n",
            "First,  the  current  input  vector  x(t)  and  the  previous  short-term  state  h(t–1)  are  fed  to\n",
            "four different fully connected layers. They all serve a different purpose:\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "569\n",
            "\n",
            "\f• The main layer is the one that outputs g(t). It has the usual role of analyzing the\n",
            "•\n",
            "current inputs x(t) and the previous (short-term) state h(t–1). In a basic cell, there\n",
            "is  nothing  other  than  this  layer,  and  its  output  goes  straight  out  to  y(t)  and  h(t).\n",
            "But in an LSTM cell, this layer’s output does not go straight out; instead its most\n",
            "important parts are stored in the long-term state (and the rest is dropped).\n",
            "\n",
            "• The  three  other  layers  are  gate  controllers.  Since  they  use  the  logistic  activation\n",
            "•\n",
            "function,  the  outputs  range  from  0  to  1.  As  you  can  see,  the  gate  controllers’\n",
            "outputs are fed to element-wise multiplication operations: if they output 0s they\n",
            "close the gate, and if they output 1s they open it. Specifically:\n",
            "\n",
            "—\n",
            "— The forget gate (controlled by f(t)) controls which parts of the long-term state\n",
            "\n",
            "should be erased.\n",
            "\n",
            "—\n",
            "— The input gate (controlled by i(t)) controls which parts of g(t) should be added\n",
            "\n",
            "to the long-term state.\n",
            "\n",
            "—\n",
            "— Finally,  the  output  gate  (controlled  by  o(t))  controls  which  parts  of  the  long-\n",
            "term state should be read and output at this time step, both to h(t) and to y(t).\n",
            "\n",
            "In short, an LSTM cell can learn to recognize an important input (that’s the role of the\n",
            "input gate), store it in the long-term state, preserve it for as long as it is needed (that’s\n",
            "the  role  of  the  forget  gate),  and  extract  it  whenever  it  is  needed.  This  explains  why\n",
            "these  cells  have  been  amazingly  successful  at  capturing  long-term  patterns  in  time\n",
            "series, long texts, audio recordings, and more.\n",
            "\n",
            "Equation 15-4 summarizes how to compute the cell’s long-term state, its short-term\n",
            "state, and its output at each time step for a single instance (the equations for a whole\n",
            "mini-batch are very similar).\n",
            "\n",
            "i t = σ Wxi\n",
            "\n",
            "Equation 15-4. LSTM computations\n",
            "⊺h t − 1 + bi\n",
            "⊺x t + Wℎi\n",
            "⊺h t − 1 + b f\n",
            "⊺x t + Wℎ f\n",
            "⊺h t − 1 + bo\n",
            "⊺x t + Wℎo\n",
            "\n",
            "o t = σ Wxo\n",
            "\n",
            "f t = σ Wx f\n",
            "\n",
            "⊺x t + Wℎg\n",
            "g t = tanh Wxg\n",
            "c t = f t ⊗ c t − 1 + i t ⊗ g t\n",
            "y t = h t = o t ⊗ tanh c t\n",
            "\n",
            "⊺h t − 1 + bg\n",
            "\n",
            "In this equation:\n",
            "\n",
            "•\n",
            "• Wxi, Wxf, Wxo, and Wxg are the weight matrices of each of the four layers for their\n",
            "\n",
            "connection to the input vector x(t).\n",
            "\n",
            "570 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\f•\n",
            "• Whi, Whf, Who, and Whg are the weight matrices of each of the four layers for their\n",
            "\n",
            "connection to the previous short-term state h(t–1).\n",
            "\n",
            "• bi, bf, bo, and bg are the bias terms for each of the four layers. Note that Tensor‐\n",
            "•\n",
            "Flow  initializes  bf  to  a  vector  full  of  1s  instead  of  0s.  This  prevents  forgetting\n",
            "everything at the beginning of training.\n",
            "\n",
            "There  are  several  variants  of  the  LSTM  cell.  One  particularly  popular  variant  is  the\n",
            "GRU cell, which we will look at now.\n",
            "\n",
            "GRU cells\n",
            "\n",
            "The gated recurrent unit (GRU) cell (see Figure 15-13) was proposed by Kyunghyun\n",
            "Cho  et  al.  in  a  2014  paper14  that  also  introduced  the  encoder–decoder  network  we\n",
            "discussed earlier.\n",
            "\n",
            "Figure 15-13. GRU cell\n",
            "\n",
            "14 Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder–Decoder for Statistical\n",
            "\n",
            "Machine Translation”, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n",
            "(2014): 1724–1734.\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "571\n",
            "\n",
            "\fThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as\n",
            "well15 (which explains its growing popularity). These are the main simplifications:\n",
            "\n",
            "•\n",
            "• Both state vectors are merged into a single vector h(t).\n",
            "• A  single  gate  controller  z(t)  controls  both  the  forget  gate  and  the  input  gate.  If\n",
            "•\n",
            "the  gate  controller  outputs  a  1,  the  forget  gate  is  open  (=  1)  and  the  input  gate\n",
            "is  closed  (1  –  1  =  0).  If  it  outputs  a  0,  the  opposite  happens.  In  other  words,\n",
            "whenever a memory must be stored, the location where it will be stored is erased\n",
            "first. This is actually a frequent variant to the LSTM cell in and of itself.\n",
            "\n",
            "• There is no output gate; the full state vector is output at every time step. How‐\n",
            "•\n",
            "ever,  there  is  a  new  gate  controller  r(t)  that  controls  which  part  of  the  previous\n",
            "state will be shown to the main layer (g(t)).\n",
            "\n",
            "Equation  15-5  summarizes  how  to  compute  the  cell’s  state  at  each  time  step  for  a\n",
            "single instance.\n",
            "\n",
            "Equation 15-5. GRU computations\n",
            "\n",
            "z t = σ Wxz\n",
            "\n",
            "r t = σ Wxr\n",
            "\n",
            "⊺x t + Wℎz\n",
            "⊺x t + Wℎr\n",
            "\n",
            "⊺h t − 1 + bz\n",
            "⊺h t − 1 + br\n",
            "\n",
            "g t = tanh Wxg\n",
            "h t = z t ⊗ h t − 1 + 1 − z t ⊗ g t\n",
            "\n",
            "⊺x t + Wℎg\n",
            "\n",
            "⊺ r t ⊗ h t − 1 + bg\n",
            "\n",
            "Keras  provides  a  tf.keras.layers.GRU  layer:  using  it  is  just  a  matter  of  replacing\n",
            "SimpleRNN  or  LSTM  with  GRU.  It  also  provides  a  tf.keras.layers.GRUCell,  in  case\n",
            "you want to create a custom cell based on a GRU cell.\n",
            "\n",
            "LSTM  and  GRU  cells  are  one  of  the  main  reasons  behind  the  success  of  RNNs.\n",
            "Yet  while  they  can  tackle  much  longer  sequences  than  simple  RNNs,  they  still  have\n",
            "a  fairly  limited  short-term  memory,  and  they  have  a  hard  time  learning  long-term\n",
            "patterns  in  sequences  of  100  time  steps  or  more,  such  as  audio  samples,  long  time\n",
            "series, or long sentences. One way to solve this is to shorten the input sequences; for\n",
            "example, using 1D convolutional layers.\n",
            "\n",
            "15 See Klaus Greff et al., “LSTM: A Search Space Odyssey”, IEEE Transactions on Neural Networks and Learning\n",
            "Systems 28, no. 10 (2017): 2222–2232.This paper seems to show that all LSTM variants perform roughly the\n",
            "same.\n",
            "\n",
            "572 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fUsing 1D convolutional layers to process sequences\n",
            "\n",
            "In  Chapter  14,  we  saw  that  a  2D  convolutional  layer  works  by  sliding  several  fairly\n",
            "small  kernels  (or  filters)  across  an  image,  producing  multiple  2D  feature  maps\n",
            "(one  per  kernel).  Similarly,  a  1D  convolutional  layer  slides  several  kernels  across  a\n",
            "sequence,  producing  a  1D  feature  map  per  kernel.  Each  kernel  will  learn  to  detect\n",
            "a  single  very  short  sequential  pattern  (no  longer  than  the  kernel  size).  If  you  use\n",
            "10  kernels,  then  the  layer’s  output  will  be  composed  of  10  1D  sequences  (all  of  the\n",
            "same length), or equivalently you can view this output as a single 10D sequence. This\n",
            "means that you can build a neural network composed of a mix of recurrent layers and\n",
            "1D  convolutional  layers  (or  even  1D  pooling  layers).  If  you  use  a  1D  convolutional\n",
            "layer with a stride of 1 and \"same\" padding, then the output sequence will have the\n",
            "same length as the input sequence. But if you use \"valid\" padding or a stride greater\n",
            "than  1,  then  the  output  sequence  will  be  shorter  than  the  input  sequence,  so  make\n",
            "sure you adjust the targets accordingly.\n",
            "\n",
            "For  example,  the  following  model  is  the  same  as  earlier,  except  it  starts  with  a  1D\n",
            "convolutional  layer  that  downsamples  the  input  sequence  by  a  factor  of  2,  using  a\n",
            "stride  of  2.  The  kernel  size  is  larger  than  the  stride,  so  all  inputs  will  be  used  to\n",
            "compute the layer’s output, and therefore the model can learn to preserve the useful\n",
            "information, dropping only the unimportant details. By shortening the sequences the\n",
            "convolutional layer may help the GRU layers detect longer patterns, so we can afford\n",
            "to double the input sequence length to 112 days. Note that we must also crop off the\n",
            "first  three  time  steps  in  the  targets:  indeed,  the  kernel’s  size  is  4,  so  the  first  output\n",
            "of the convolutional layer will be based on the input time steps 0 to 3, and the first\n",
            "forecasts will be for time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we\n",
            "must downsample the targets by a factor of 2, because of the stride:\n",
            "\n",
            "conv_rnn_model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2,\n",
            "                           activation=\"relu\", input_shape=[None, 5]),\n",
            "    tf.keras.layers.GRU(32, return_sequences=True),\n",
            "    tf.keras.layers.Dense(14)\n",
            "])\n",
            "\n",
            "longer_train = to_seq2seq_dataset(mulvar_train, seq_length=112,\n",
            "                                       shuffle=True, seed=42)\n",
            "longer_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\n",
            "downsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))\n",
            "downsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))\n",
            "[...]  # compile and fit the model using the downsampled datasets\n",
            "\n",
            "If  you  train  and  evaluate  this  model,  you  will  find  that  it  outperforms  the  previous\n",
            "model (by a small margin). In fact, it is actually possible to use only 1D convolutional\n",
            "layers and drop the recurrent layers entirely!\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "573\n",
            "\n",
            "\fWaveNet\n",
            "\n",
            "In a 2016 paper,16 Aaron van den Oord and other DeepMind researchers introduced\n",
            "a novel architecture called WaveNet. They stacked 1D convolutional layers, doubling\n",
            "the dilation rate (how spread apart each neuron’s inputs are) at every layer: the first\n",
            "convolutional layer gets a glimpse of just two time steps at a time, while the next one\n",
            "sees four time steps (its receptive field is four time steps long), the next one sees eight\n",
            "time steps, and so on (see Figure 15-14). This way, the lower layers learn short-term\n",
            "patterns,  while  the  higher  layers  learn  long-term  patterns.  Thanks  to  the  doubling\n",
            "dilation rate, the network can process extremely large sequences very efficiently.\n",
            "\n",
            "Figure 15-14. WaveNet architecture\n",
            "\n",
            "The authors of the paper actually stacked 10 convolutional layers with dilation rates\n",
            "of  1,  2,  4,  8,  …,  256,  512,  then  they  stacked  another  group  of  10  identical  layers\n",
            "(also  with  dilation  rates  1,  2,  4,  8,  …,  256,  512),  then  again  another  identical  group\n",
            "of  10  layers.  They  justified  this  architecture  by  pointing  out  that  a  single  stack  of\n",
            "10 convolutional layers with these dilation rates will act like a super-efficient convolu‐\n",
            "tional layer with a kernel of size 1,024 (except way faster, more powerful, and using\n",
            "significantly  fewer  parameters).  They  also  left-padded  the  input  sequences  with  a\n",
            "number  of  zeros  equal  to  the  dilation  rate  before  every  layer,  to  preserve  the  same\n",
            "sequence length throughout the network.\n",
            "\n",
            "Here  is  how  to  implement  a  simplified  WaveNet  to  tackle  the  same  sequences  as\n",
            "earlier:17\n",
            "\n",
            "wavenet_model = tf.keras.Sequential()\n",
            "wavenet_model.add(tf.keras.layers.Input(shape=[None, 5]))\n",
            "\n",
            "16 Aaron van den Oord et al., “WaveNet: A Generative Model for Raw Audio”, arXiv preprint arXiv:1609.03499\n",
            "\n",
            "(2016).\n",
            "\n",
            "17 The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and gated activation\n",
            "\n",
            "units similar to those found in a GRU cell. See this chapter’s notebook for more details.\n",
            "\n",
            "574 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\ffor rate in (1, 2, 4, 8) * 2:\n",
            "    wavenet_model.add(tf.keras.layers.Conv1D(\n",
            "        filters=32, kernel_size=2, padding=\"causal\", activation=\"relu\",\n",
            "        dilation_rate=rate))\n",
            "wavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))\n",
            "\n",
            "This Sequential model starts with an explicit input layer—this is simpler than trying\n",
            "to set input_shape only on the first layer. Then it continues with a 1D convolutional\n",
            "layer  using  \"causal\"  padding,  which  is  like  \"same\"  padding  except  that  the  zeros\n",
            "are  appended  only  at  the  start  of  the  input  sequence,  instead  of  on  both  sides.  This\n",
            "ensures  that  the  convolutional  layer  does  not  peek  into  the  future  when  making\n",
            "predictions. Then we add similar pairs of layers using growing dilation rates: 1, 2, 4,\n",
            "8,  and  again  1,  2,  4,  8.  Finally,  we  add  the  output  layer:  a  convolutional  layer  with\n",
            "14  filters  of  size  1  and  without  any  activation  function.  As  we  saw  earlier,  such  a\n",
            "convolutional layer is equivalent to a Dense layer with 14 units. Thanks to the causal\n",
            "padding, every convolutional layer outputs a sequence of the same length as its input\n",
            "sequence, so the targets we use during training can be the full 112-day sequences: no\n",
            "need to crop them or downsample them.\n",
            "\n",
            "The models we’ve discussed in this section offer similar performance for the ridership\n",
            "forecasting  task,  but  they  may  vary  significantly  depending  on  the  task  and  the\n",
            "amount of available data. In the WaveNet paper, the authors achieved state-of-the-art\n",
            "performance on various audio tasks (hence the name of the architecture), including\n",
            "text-to-speech  tasks,  producing  incredibly  realistic  voices  across  several  languages.\n",
            "They also used the model to generate music, one audio sample at a time. This feat is\n",
            "all  the  more  impressive  when  you  realize  that  a  single  second  of  audio  can  contain\n",
            "tens  of  thousands  of  time  steps—even  LSTMs  and  GRUs  cannot  handle  such  long\n",
            "sequences.\n",
            "\n",
            "If  you  evaluate  our  best  Chicago  ridership  models  on  the  test\n",
            "period,  starting  in  2020,  you  will  find  that  they  perform  much\n",
            "worse than expected! Why is that? Well, that’s when the Covid-19\n",
            "pandemic started, which greatly affected public transportation. As\n",
            "mentioned earlier, these models will only work well if the patterns\n",
            "they  learned  from  the  past  continue  in  the  future.  In  any  case,\n",
            "before  deploying  a  model  to  production,  verify  that  it  works  well\n",
            "on recent data. And once it’s in production, make sure to monitor\n",
            "its performance regularly.\n",
            "\n",
            "With that, you can now tackle all sorts of time series! In Chapter 16, we will continue\n",
            "to explore RNNs, and we will see how they can tackle various NLP tasks as well.\n",
            "\n",
            "Handling Long Sequences \n",
            "\n",
            "| \n",
            "\n",
            "575\n",
            "\n",
            "\fExercises\n",
            "\n",
            "1.\n",
            "1. Can  you  think  of  a  few  applications  for  a  sequence-to-sequence  RNN?  What\n",
            "\n",
            "about a sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
            "\n",
            "2.\n",
            "2. How many dimensions must the inputs of an RNN layer have? What does each\n",
            "\n",
            "dimension represent? What about its outputs?\n",
            "\n",
            "3.\n",
            "3. If  you  want  to  build  a  deep  sequence-to-sequence  RNN,  which  RNN  layers\n",
            "should have return_sequences=True? What about a sequence-to-vector RNN?\n",
            "\n",
            "4. Suppose you have a daily univariate time series, and you want to forecast the next\n",
            "4.\n",
            "\n",
            "seven days. Which RNN architecture should you use?\n",
            "\n",
            "5.\n",
            "5. What are the main difficulties when training RNNs? How can you handle them?\n",
            "\n",
            "6.\n",
            "6. Can you sketch the LSTM cell’s architecture?\n",
            "\n",
            "7.\n",
            "7. Why would you want to use 1D convolutional layers in an RNN?\n",
            "\n",
            "8.\n",
            "8. Which neural network architecture could you use to classify videos?\n",
            "\n",
            "9.\n",
            "9. Train a classification model for the SketchRNN dataset, available in TensorFlow\n",
            "\n",
            "Datasets.\n",
            "\n",
            "10.\n",
            "10. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales\n",
            "composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long,\n",
            "and each time step contains 4 integers, where each integer corresponds to a note’s\n",
            "index  on  a  piano  (except  for  the  value  0,  which  means  that  no  note  is  played).\n",
            "Train a model—recurrent, convolutional, or both—that can predict the next time\n",
            "step  (four  notes),  given  a  sequence  of  time  steps  from  a  chorale.  Then  use  this\n",
            "model to generate Bach-like music, one note at a time: you can do this by giving\n",
            "the model the start of a chorale and asking it to predict the next time step, then\n",
            "appending these time steps to the input sequence and asking the model for the\n",
            "next  note,  and  so  on.  Also  make  sure  to  check  out  Google’s  Coconet  model,\n",
            "which was used for a nice Google doodle about Bach.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "576 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
            "\n",
            "\fCHAPTER 16\n",
            "Natural Language Processing\n",
            "with RNNs and Attention\n",
            "\n",
            "When Alan Turing imagined his famous Turing test1 in 1950, he proposed a way to\n",
            "evaluate  a  machine’s  ability  to  match  human  intelligence.  He  could  have  tested  for\n",
            "many  things,  such  as  the  ability  to  recognize  cats  in  pictures,  play  chess,  compose\n",
            "music,  or  escape  a  maze,  but,  interestingly,  he  chose  a  linguistic  task.  More  specifi‐\n",
            "cally,  he  devised  a  chatbot  capable  of  fooling  its  interlocutor  into  thinking  it  was\n",
            "human.2 This test does have its weaknesses: a set of hardcoded rules can fool unsus‐\n",
            "pecting or naive humans (e.g., the machine could give vague predefined answers in\n",
            "response to some keywords, it could pretend that it is joking or drunk to get a pass on\n",
            "its weirdest answers, or it could escape difficult questions by answering them with its\n",
            "own questions), and many aspects of human intelligence are utterly ignored (e.g., the\n",
            "ability to interpret nonverbal communication such as facial expressions, or to learn a\n",
            "manual task). But the test does highlight the fact that mastering language is arguably\n",
            "Homo sapiens’s greatest cognitive ability.\n",
            "\n",
            "Can  we  build  a  machine  that  can  master  written  and  spoken  language?  This  is  the\n",
            "ultimate  goal  of  NLP  research,  but  it’s  a  bit  too  broad,  so  in  practice  researchers\n",
            "focus  on  more  specific  tasks,  such  as  text  classification,  translation,  summarization,\n",
            "question answering, and many more.\n",
            "\n",
            "1 Alan Turing, “Computing Machinery and Intelligence”, Mind 49 (1950): 433–460.\n",
            "\n",
            "2 Of course, the word chatbot came much later. Turing called his test the imitation game: machine A and human\n",
            "B chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is\n",
            "the machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try\n",
            "to help the interrogator.\n",
            "\n",
            "577\n",
            "\n",
            "\fA common approach for natural language tasks is to use recurrent neural networks.\n",
            "We will therefore continue to explore RNNs (introduced in Chapter 15), starting with\n",
            "a  character  RNN,  or  char-RNN,  trained  to  predict  the  next  character  in  a  sentence.\n",
            "This  will  allow  us  to  generate  some  original  text.  We  will  first  use  a  stateless  RNN\n",
            "(which learns on random portions of text at each iteration, without any information\n",
            "on the rest of the text), then we will build a stateful RNN (which preserves the hidden\n",
            "state  between  training  iterations  and  continues  reading  where  it  left  off,  allowing  it\n",
            "to learn longer patterns). Next, we will build an RNN to perform sentiment analysis\n",
            "(e.g., reading movie reviews and extracting the rater’s feeling about the movie), this\n",
            "time  treating  sentences  as  sequences  of  words,  rather  than  characters.  Then  we  will\n",
            "show  how  RNNs  can  be  used  to  build  an  encoder–decoder  architecture  capable  of\n",
            "performing neural machine translation (NMT), translating English to Spanish.\n",
            "\n",
            "In  the  second  part  of  this  chapter,  we  will  explore  attention  mechanisms.  As  their\n",
            "name suggests, these are neural network components that learn to select the part of\n",
            "the inputs that the rest of the model should focus on at each time step. First, we will\n",
            "boost the performance of an RNN-based encoder–decoder architecture using atten‐\n",
            "tion.  Next,  we  will  drop  RNNs  altogether  and  use  a  very  successful  attention-only\n",
            "architecture, called the transformer, to build a translation model. We will then discuss\n",
            "some of the most important advances in NLP in the last few years, including incredi‐\n",
            "bly powerful language models such as GPT and BERT, both based on transformers.\n",
            "Lastly, I will show you how to get started with the excellent Transformers library by\n",
            "Hugging Face.\n",
            "\n",
            "Let’s start with a simple and fun model that can write like Shakespeare (sort of).\n",
            "\n",
            "Generating Shakespearean Text Using a Character RNN\n",
            "In  a  famous  2015  blog  post  titled  “The  Unreasonable  Effectiveness  of  Recurrent\n",
            "Neural Networks”, Andrej Karpathy showed how to train an RNN to predict the next\n",
            "character in a sentence. This char-RNN can then be used to generate novel text, one\n",
            "character at a time. Here is a small sample of the text generated by a char-RNN model\n",
            "after it was trained on all of Shakespeare’s works:\n",
            "\n",
            "PANDARUS:\n",
            "\n",
            "Alas, I think he shall be come approached and the day\n",
            "\n",
            "When little srain would be attain’d into being never fed,\n",
            "\n",
            "And who is but a chain and subjects of his death,\n",
            "\n",
            "I should not sleep.\n",
            "\n",
            "Not  exactly  a  masterpiece,  but  it  is  still  impressive  that  the  model  was  able  to  learn\n",
            "words, grammar, proper punctuation, and more, just by learning to predict the next\n",
            "character  in  a  sentence.  This  is  our  first  example  of  a  language  model;  similar  (but\n",
            "\n",
            "578 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fmuch more powerful) language models, discussed later in this chapter, are at the core\n",
            "of modern NLP. In the remainder of this section we’ll build a char-RNN step by step,\n",
            "starting with the creation of the dataset.\n",
            "\n",
            "Creating the Training Dataset\n",
            "First, using Keras’s handy tf.keras.utils.get_file() function, let’s download all of\n",
            "Shakespeare’s works. The data is loaded from Andrej Karpathy’s char-rnn project:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
            "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
            "with open(filepath) as f:\n",
            "    shakespeare_text = f.read()\n",
            "\n",
            "Let’s print the first few lines:\n",
            "\n",
            ">>> print(shakespeare_text[:80])\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "Looks like Shakespeare all right!\n",
            "\n",
            "Next,  we’ll  use  a  tf.keras.layers.TextVectorization  layer  (introduced  in  Chap‐\n",
            "ter 13) to encode this text. We set split=\"character\" to get character-level encoding\n",
            "rather  than  the  default  word-level  encoding,  and  we  use  standardize=\"lower\"  to\n",
            "convert the text to lowercase (which will simplify the task):\n",
            "\n",
            "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
            "                                                   standardize=\"lower\")\n",
            "text_vec_layer.adapt([shakespeare_text])\n",
            "encoded = text_vec_layer([shakespeare_text])[0]\n",
            "\n",
            "Each  character  is  now  mapped  to  an  integer,  starting  at  2.  The  TextVectorization\n",
            "layer  reserved  the  value  0  for  padding  tokens,  and  it  reserved  1  for  unknown  char‐\n",
            "acters.  We  won’t  need  either  of  these  tokens  for  now,  so  let’s  subtract  2  from  the\n",
            "character IDs and compute the number of distinct characters and the total number of\n",
            "characters:\n",
            "\n",
            "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
            "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
            "dataset_size = len(encoded)  # total number of chars = 1,115,394\n",
            "\n",
            "Next,  just  like  we  did  in  Chapter  15,  we  can  turn  this  very  long  sequence  into  a\n",
            "dataset of windows that we can then use to train a sequence-to-sequence RNN. The\n",
            "targets will be similar to the inputs, but shifted by one time step into the “future”. For\n",
            "example, one sample in the dataset may be a sequence of character IDs representing\n",
            "\n",
            "Generating Shakespearean Text Using a Character RNN \n",
            "\n",
            "| \n",
            "\n",
            "579\n",
            "\n",
            "\fthe  text  “to  be  or  not  to  b”  (without  the  final  “e”),  and  the  corresponding  target—a\n",
            "sequence  of  character  IDs  representing  the  text  “o  be  or  not  to  be”  (with  the  final\n",
            "“e”, but without the leading “t”). Let’s write a small utility function to convert a long\n",
            "sequence of character IDs into a dataset of input/target window pairs:\n",
            "\n",
            "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
            "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
            "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
            "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
            "    if shuffle:\n",
            "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
            "    ds = ds.batch(batch_size)\n",
            "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
            "\n",
            "This function starts much like the to_windows() custom utility function we created\n",
            "in Chapter 15:\n",
            "\n",
            "•\n",
            "• It takes a sequence as input (i.e., the encoded text), and creates a dataset contain‐\n",
            "\n",
            "ing all the windows of the desired length.\n",
            "\n",
            "•\n",
            "• It increases the length by one, since we need the next character for the target.\n",
            "\n",
            "•\n",
            "• Then, it shuffles the windows (optionally), batches them, splits them into input/\n",
            "\n",
            "output pairs, and activates prefetching.\n",
            "\n",
            "Figure 16-1 summarizes the dataset preparation steps: it shows windows of length 11,\n",
            "and a batch size of 3. The start index of each window is indicated next to it.\n",
            "\n",
            "Figure 16-1. Preparing a dataset of shuffled windows\n",
            "\n",
            "580 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fNow we’re ready to create the training set, the validation set, and the test set. We will\n",
            "use roughly 90% of the text for training, 5% for validation, and 5% for testing:\n",
            "\n",
            "length = 100\n",
            "tf.random.set_seed(42)\n",
            "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
            "                       seed=42)\n",
            "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
            "test_set = to_dataset(encoded[1_060_000:], length=length)\n",
            "\n",
            "We  set  the  window  length  to  100,  but  you  can  try  tuning  it:  it’s\n",
            "easier and faster to train RNNs on shorter input sequences, but the\n",
            "RNN  will  not  be  able  to  learn  any  pattern  longer  than  length,  so\n",
            "don’t make it too small.\n",
            "\n",
            "That’s it! Preparing the dataset was the hardest part. Now let’s create the model.\n",
            "\n",
            "Building and Training the Char-RNN Model\n",
            "Since our dataset is reasonably large, and modeling language is quite a difficult task,\n",
            "we need more than a simple RNN with a few recurrent neurons. Let’s build and train\n",
            "a model with one GRU layer composed of 128 units (you can try tweaking the number\n",
            "of layers and units later, if needed):\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
            "    tf.keras.layers.GRU(128, return_sequences=True),\n",
            "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
            "])\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
            "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
            "                    callbacks=[model_ckpt])\n",
            "\n",
            "Let’s go over this code:\n",
            "\n",
            "• We use an Embedding layer as the first layer, to encode the character IDs (embed‐\n",
            "•\n",
            "dings  were  introduced  in  Chapter  13).  The  Embedding  layer’s  number  of  input\n",
            "dimensions  is  the  number  of  distinct  character  IDs,  and  the  number  of  output\n",
            "dimensions is a hyperparameter you can tune—we’ll set it to 16 for now. Whereas\n",
            "the inputs of the Embedding layer will be 2D tensors of shape [batch size, window\n",
            "length], the output of the Embedding layer will be a 3D tensor of shape [batch size,\n",
            "window length, embedding size].\n",
            "\n",
            "• We  use  a  Dense  layer  for  the  output  layer:  it  must  have  39  units  (n_tokens)\n",
            "•\n",
            "because  there  are  39  distinct  characters  in  the  text,  and  we  want  to  output  a\n",
            "\n",
            "Generating Shakespearean Text Using a Character RNN \n",
            "\n",
            "| \n",
            "\n",
            "581\n",
            "\n",
            "\fprobability for each possible character (at each time step). The 39 output proba‐\n",
            "bilities should sum up to 1 at each time step, so we apply the softmax activation\n",
            "function to the outputs of the Dense layer.\n",
            "\n",
            "• Lastly,  we  compile  this  model,  using  the  \"sparse_categorical_crossentropy\"\n",
            "•\n",
            "loss  and  a  Nadam  optimizer,  and  we  train  the  model  for  several  epochs,3  using\n",
            "a ModelCheckpoint callback to save the best model (in terms of validation accu‐\n",
            "racy) as training progresses.\n",
            "\n",
            "If you are running this code on Colab with a GPU activated, then\n",
            "training  should  take  roughly  one  to  two  hours.  You  can  reduce\n",
            "the  number  of  epochs  if  you  don’t  want  to  wait  that  long,  but  of\n",
            "course  the  model’s  accuracy  will  probably  be  lower.  If  the  Colab\n",
            "session times out, make sure to reconnect quickly, or else the Colab\n",
            "runtime will be destroyed.\n",
            "\n",
            "This  model  does  not  handle  text  preprocessing,  so  let’s  wrap  it  in  a  final  model\n",
            "containing  the  tf.keras.layers.TextVectorization  layer  as  the  first  layer,  plus  a\n",
            "tf.keras.layers.Lambda layer to subtract 2 from the character IDs since we’re not\n",
            "using the padding and unknown tokens for now:\n",
            "\n",
            "shakespeare_model = tf.keras.Sequential([\n",
            "    text_vec_layer,\n",
            "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
            "    model\n",
            "])\n",
            "\n",
            "And now let’s use it to predict the next character in a sentence:\n",
            "\n",
            ">>> y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
            ">>> y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
            ">>> text_vec_layer.get_vocabulary()[y_pred + 2]\n",
            "'e'\n",
            "\n",
            "Great,  the  model  correctly  predicted  the  next  character.  Now  let’s  use  this  model  to\n",
            "pretend we’re Shakespeare!\n",
            "\n",
            "Generating Fake Shakespearean Text\n",
            "To  generate  new  text  using  the  char-RNN  model,  we  could  feed  it  some  text,  make\n",
            "the model predict the most likely next letter, add it to the end of the text, then give\n",
            "the extended text to the model to guess the next letter, and so on. This is called greedy\n",
            "decoding. But in practice this often leads to the same words being repeated over and\n",
            "\n",
            "3 Since the input windows overlap, the concept of epoch is not so clear in this case: during each epoch (as\n",
            "\n",
            "implemented by Keras), the model will actually see the same character multiple times.\n",
            "\n",
            "582 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fover  again.  Instead,  we  can  sample  the  next  character  randomly,  with  a  probability\n",
            "equal  to  the  estimated  probability,  using  TensorFlow’s  tf.random.categorical()\n",
            "function.  This  will  generate  more  diverse  and  interesting  text.  The  categorical()\n",
            "function samples random class indices, given the class log probabilities (logits). For\n",
            "example:\n",
            "\n",
            ">>> log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
            ">>> tf.random.set_seed(42)\n",
            ">>> tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples\n",
            "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>\n",
            "\n",
            "To have more control over the diversity of the generated text, we can divide the logits\n",
            "by a number called the temperature, which we can tweak as we wish. A temperature\n",
            "close  to  zero  favors  high-probability  characters,  while  a  high  temperature  gives  all\n",
            "characters  an  equal  probability.  Lower  temperatures  are  typically  preferred  when\n",
            "generating fairly rigid and precise text, such as mathematical equations, while higher\n",
            "temperatures  are  preferred  when  generating  more  diverse  and  creative  text.  The\n",
            "following  next_char()  custom  helper  function  uses  this  approach  to  pick  the  next\n",
            "character to add to the input text:\n",
            "\n",
            "def next_char(text, temperature=1):\n",
            "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
            "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
            "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
            "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
            "\n",
            "Next,  we  can  write  another  small  helper  function  that  will  repeatedly  call\n",
            "next_char() to get the next character and append it to the given text:\n",
            "\n",
            "def extend_text(text, n_chars=50, temperature=1):\n",
            "    for _ in range(n_chars):\n",
            "        text += next_char(text, temperature)\n",
            "    return text\n",
            "\n",
            "We are now ready to generate some text! Let’s try with different temperature values:\n",
            "\n",
            ">>> tf.random.set_seed(42)\n",
            ">>> print(extend_text(\"To be or not to be\", temperature=0.01))\n",
            "To be or not to be the duke\n",
            "as it is a proper strange death,\n",
            "and the\n",
            ">>> print(extend_text(\"To be or not to be\", temperature=1))\n",
            "To be or not to behold?\n",
            "\n",
            "second push:\n",
            "gremio, lord all, a sistermen,\n",
            ">>> print(extend_text(\"To be or not to be\", temperature=100))\n",
            "To be or not to bef ,mt'&o3fpadm!$\n",
            "wh!nse?bws3est--vgerdjw?c-y-ewznq\n",
            "\n",
            "Generating Shakespearean Text Using a Character RNN \n",
            "\n",
            "| \n",
            "\n",
            "583\n",
            "\n",
            "\fShakespeare  seems  to  be  suffering  from  a  heatwave.  To  generate  more  convincing\n",
            "text, a common technique is to sample only from the top k characters, or only from\n",
            "the smallest set of top characters whose total probability exceeds some threshold (this\n",
            "is called nucleus sampling). Alternatively, you could try using beam search, which we\n",
            "will  discuss  later  in  this  chapter,  or  using  more  GRU  layers  and  more  neurons  per\n",
            "layer,  training  for  longer,  and  adding  some  regularization  if  needed.  Also  note  that\n",
            "the  model  is  currently  incapable  of  learning  patterns  longer  than  length,  which  is\n",
            "just  100  characters.  You  could  try  making  this  window  larger,  but  it  will  also  make\n",
            "training harder, and even LSTM and GRU cells cannot handle very long sequences.\n",
            "An alternative approach is to use a stateful RNN.\n",
            "\n",
            "Stateful RNN\n",
            "Until  now,  we  have  only  used  stateless  RNNs:  at  each  training  iteration  the  model\n",
            "starts  with  a  hidden  state  full  of  zeros,  then  it  updates  this  state  at  each  time  step,\n",
            "and  after  the  last  time  step,  it  throws  it  away  as  it  is  not  needed  anymore.  What  if\n",
            "we  instructed  the  RNN  to  preserve  this  final  state  after  processing  a  training  batch\n",
            "and  use  it  as  the  initial  state  for  the  next  training  batch?  This  way  the  model  could\n",
            "learn long-term patterns despite only backpropagating through short sequences. This\n",
            "is called a stateful RNN. Let’s go over how to build one.\n",
            "\n",
            "First,  note  that  a  stateful  RNN  only  makes  sense  if  each  input  sequence  in  a  batch\n",
            "starts exactly where the corresponding sequence in the previous batch left off. So the\n",
            "first thing we need to do to build a stateful RNN is to use sequential and nonoverlap‐\n",
            "ping  input  sequences  (rather  than  the  shuffled  and  overlapping  sequences  we  used\n",
            "to train stateless RNNs). When creating the tf.data.Dataset, we must therefore use\n",
            "shift=length (instead of shift=1) when calling the window() method. Moreover, we\n",
            "must not call the shuffle() method.\n",
            "\n",
            "Unfortunately,  batching  is  much  harder  when  preparing  a  dataset  for  a  stateful\n",
            "RNN  than  it  is  for  a  stateless  RNN.  Indeed,  if  we  were  to  call  batch(32),  then  32\n",
            "consecutive windows would be put in the same batch, and the following batch would\n",
            "not  continue  each  of  these  windows  where  it  left  off.  The  first  batch  would  contain\n",
            "windows  1  to  32  and  the  second  batch  would  contain  windows  33  to  64,  so  if  you\n",
            "consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that\n",
            "they are not consecutive. The simplest solution to this problem is to just use a batch\n",
            "size  of  1.  The  following  to_dataset_for_stateful_rnn()  custom  utility  function\n",
            "uses this strategy to prepare a dataset for a stateful RNN:\n",
            "\n",
            "584 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fdef to_dataset_for_stateful_rnn(sequence, length):\n",
            "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
            "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
            "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
            "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
            "\n",
            "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
            "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n",
            "                                                 length)\n",
            "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)\n",
            "\n",
            "Figure 16-2 summarizes the main steps of this function.\n",
            "\n",
            "Figure 16-2. Preparing a dataset of consecutive sequence fragments for a stateful RNN\n",
            "\n",
            "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s\n",
            "text  into  32  texts  of  equal  length,  create  one  dataset  of  consecutive  input  sequen‐\n",
            "ces for each of them, and finally use  tf.data.Dataset.zip(datasets).map(lambda\n",
            "*windows: tf.stack(windows)) to create proper consecutive batches, where the nth\n",
            "input sequence in a batch starts off exactly where the nth input sequence ended in the\n",
            "previous batch (see the notebook for the full code).\n",
            "\n",
            "Now,  let’s  create  the  stateful  RNN.  We  need  to  set  the  stateful  argument  to  True\n",
            "when  creating  each  recurrent  layer,  and  because  the  stateful  RNN  needs  to  know\n",
            "the  batch  size  (since  it  will  preserve  a  state  for  each  input  sequence  in  the  batch).\n",
            "Therefore we must set the batch_input_shape argument in the first layer. Note that\n",
            "we can leave the second dimension unspecified, since the input sequences could have\n",
            "any length:\n",
            "\n",
            "Generating Shakespearean Text Using a Character RNN \n",
            "\n",
            "| \n",
            "\n",
            "585\n",
            "\n",
            "\fmodel = tf.keras.Sequential([\n",
            "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
            "                              batch_input_shape=[1, None]),\n",
            "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
            "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
            "])\n",
            "\n",
            "At  the  end  of  each  epoch,  we  need  to  reset  the  states  before  we  go  back  to  the\n",
            "beginning of the text. For this, we can use a small custom Keras callback:\n",
            "\n",
            "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
            "    def on_epoch_begin(self, epoch, logs):\n",
            "        self.model.reset_states()\n",
            "\n",
            "And now we can compile the model and train it using our callback:\n",
            "\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
            "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])\n",
            "\n",
            "After this model is trained, it will only be possible to use it to make\n",
            "predictions for batches of the same size as were used during train‐\n",
            "ing. To avoid this restriction, create an identical stateless model, and\n",
            "copy the stateful model’s weights to this model.\n",
            "\n",
            "Interestingly, although a char-RNN model is just trained to predict the next character,\n",
            "this  seemingly  simple  task  actually  requires  it  to  learn  some  higher-level  tasks  as\n",
            "well. For example, to find the next character after “Great movie, I really”, it’s helpful\n",
            "to  understand  that  the  sentence  is  positive,  so  what  follows  is  more  likely  to  be\n",
            "the  letter  “l”  (for  “loved”)  rather  than  “h”  (for  “hated”).  In  fact,  a  2017  paper4  by\n",
            "Alec Radford and other OpenAI researchers describes how the authors trained a big\n",
            "char-RNN-like model on a large dataset, and found that one of the neurons acted as\n",
            "an excellent sentiment analysis classifier: although the model was trained without any\n",
            "labels, the sentiment neuron—as they called it—reached state-of-the-art performance\n",
            "on  sentiment  analysis  benchmarks.  This  foreshadowed  and  motivated  unsupervised\n",
            "pretraining in NLP.\n",
            "\n",
            "But before we explore unsupervised pretraining, let’s turn our attention to word-level\n",
            "models  and  how  to  use  them  in  a  supervised  fashion  for  sentiment  analysis.  In  the\n",
            "process, you will learn how to handle sequences of variable lengths using masking.\n",
            "\n",
            "4 Alec Radford et al., “Learning to Generate Reviews and Discovering Sentiment”, arXiv preprint\n",
            "\n",
            "arXiv:1704.01444 (2017).\n",
            "\n",
            "586 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fSentiment Analysis\n",
            "Generating text can be fun and instructive, but in real-life projects, one of the most\n",
            "common  applications  of  NLP  is  text  classification—especially  sentiment  analysis.  If\n",
            "image classification on the MNIST dataset is the “Hello world!” of computer vision,\n",
            "then sentiment analysis on the IMDb reviews dataset is the “Hello world!” of natural\n",
            "language processing. The IMDb dataset consists of 50,000 movie reviews in English\n",
            "(25,000  for  training,  25,000  for  testing)  extracted  from  the  famous  Internet  Movie\n",
            "Database, along with a simple binary target for each review indicating whether it is\n",
            "negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for\n",
            "good reasons: it is simple enough to be tackled on a laptop in a reasonable amount of\n",
            "time, but challenging enough to be fun and rewarding.\n",
            "\n",
            "Let’s  load  the  IMDb  dataset  using  the  TensorFlow  Datasets  library  (introduced  in\n",
            "Chapter 13). We’ll use the first 90% of the training set for training, and the remaining\n",
            "10% for validation:\n",
            "\n",
            "import tensorflow_datasets as tfds\n",
            "\n",
            "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
            "    name=\"imdb_reviews\",\n",
            "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
            "    as_supervised=True\n",
            ")\n",
            "tf.random.set_seed(42)\n",
            "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
            "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
            "test_set = raw_test_set.batch(32).prefetch(1)\n",
            "\n",
            "Keras  also  includes  a  function  for  loading  the  IMDb  dataset,  if\n",
            "you  prefer:  tf.keras.datasets.imdb.load_data().  The  reviews\n",
            "are already preprocessed as sequences of word IDs.\n",
            "\n",
            "Let’s inspect a few reviews:\n",
            "\n",
            ">>> for review, label in raw_train_set.take(4):\n",
            "...     print(review.numpy().decode(\"utf-8\"))\n",
            "...     print(\"Label:\", label.numpy())\n",
            "...\n",
            "This was an absolutely terrible movie. Don't be lured in by Christopher [...]\n",
            "Label: 0\n",
            "I have been known to fall asleep during films, but this is usually due to [...]\n",
            "Label: 0\n",
            "Mann photographs the Alberta Rocky Mountains in a superb fashion, and [...]\n",
            "Label: 0\n",
            "This is the kind of film for a snowy Sunday afternoon when the rest of the [...]\n",
            "Label: 1\n",
            "\n",
            "Sentiment Analysis \n",
            "\n",
            "| \n",
            "\n",
            "587\n",
            "\n",
            "\fSome  reviews  are  easy  to  classify.  For  example,  the  first  review  includes  the  words\n",
            "“terrible  movie”  in  the  very  first  sentence.  But  in  many  cases  things  are  not  that\n",
            "simple. For example, the third review starts off positively, even though it’s ultimately a\n",
            "negative review (label 0).\n",
            "\n",
            "To  build  a  model  for  this  task,  we  need  to  preprocess  the  text,  but  this  time  we\n",
            "will  chop  it  into  words  instead  of  characters.  For  this,  we  can  use  the  tf.keras.\n",
            "layers.TextVectorization  layer  again.  Note  that  it  uses  spaces  to  identify  word\n",
            "boundaries,  which  will  not  work  well  in  some  languages.  For  example,  Chinese\n",
            "writing  does  not  use  spaces  between  words,  Vietnamese  uses  spaces  even  within\n",
            "words, and German often attaches multiple words together, without spaces. Even in\n",
            "English, spaces are not always the best way to tokenize text: think of “San Francisco”\n",
            "or “#ILoveDeepLearning”.\n",
            "\n",
            "Fortunately, there are solutions to address these issues. In a 2016 paper,5 Rico Senn‐\n",
            "rich  et  al.  from  the  University  of  Edinburgh  explored  several  methods  to  tokenize\n",
            "and  detokenize  text  at  the  subword  level.  This  way,  even  if  your  model  encounters\n",
            "a  rare  word  it  has  never  seen  before,  it  can  still  reasonably  guess  what  it  means.\n",
            "For  example,  even  if  the  model  never  saw  the  word  “smartest”  during  training,  if  it\n",
            "learned  the  word  “smart”  and  it  also  learned  that  the  suffix  “est”  means  “the  most”,\n",
            "it  can  infer  the  meaning  of  “smartest”.  One  of  the  techniques  the  authors  evaluated\n",
            "is  byte  pair  encoding  (BPE).  BPE  works  by  splitting  the  whole  training  set  into\n",
            "individual characters (including spaces), then repeatedly merging the most frequent\n",
            "adjacent pairs until the vocabulary reaches the desired size.\n",
            "\n",
            "A  2018  paper6  by  Taku  Kudo  at  Google  further  improved  subword  tokenization,\n",
            "often  removing  the  need  for  language-specific  preprocessing  prior  to  tokenization.\n",
            "Moreover, the paper proposed a novel regularization technique called subword regula‐\n",
            "rization, which improves accuracy and robustness by introducing some randomness\n",
            "in  tokenization  during  training:  for  example,  “New  England”  may  be  tokenized  as\n",
            "“New”  +  “England”,  or  “New”  +  “Eng”  +  “land”,  or  simply  “New  England”  (just\n",
            "one token). Google’s SentencePiece project provides an open source implementation,\n",
            "which is described in a paper7 by Taku Kudo and John Richardson.\n",
            "\n",
            "5 Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units”, Proceedings of the 54th\n",
            "\n",
            "Annual Meeting of the Association for Computational Linguistics 1 (2016): 1715–1725.\n",
            "\n",
            "6 Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword\n",
            "\n",
            "Candidates”, arXiv preprint arXiv:1804.10959 (2018).\n",
            "\n",
            "7 Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer\n",
            "\n",
            "and Detokenizer for Neural Text Processing”, arXiv preprint arXiv:1808.06226 (2018).\n",
            "\n",
            "588 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fThe TensorFlow Text library also implements various tokenization strategies, includ‐\n",
            "ing  WordPiece8  (a  variant  of  BPE),  and  last  but  not  least,  the  Tokenizers  library  by\n",
            "Hugging Face implements a wide range of extremely fast tokenizers.\n",
            "\n",
            "However, for the IMDb task in English, using spaces for token boundaries should be\n",
            "good enough. So let’s go ahead with creating a TextVectorization layer and adapting\n",
            "it to the training set. We will limit the vocabulary to 1,000 tokens, including the most\n",
            "frequent  998  words  plus  a  padding  token  and  a  token  for  unknown  words,  since\n",
            "it’s  unlikely  that  very  rare  words  will  be  important  for  this  task,  and  limiting  the\n",
            "vocabulary size will reduce the number of parameters the model needs to learn:\n",
            "\n",
            "vocab_size = 1000\n",
            "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
            "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\n",
            "\n",
            "Finally, we can create the model and train it:\n",
            "\n",
            "embed_size = 128\n",
            "tf.random.set_seed(42)\n",
            "model = tf.keras.Sequential([\n",
            "    text_vec_layer,\n",
            "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
            "    tf.keras.layers.GRU(128),\n",
            "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
            "])\n",
            "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=2)\n",
            "\n",
            "The  first  layer  is  the  TextVectorization  layer  we  just  prepared,  followed  by  an\n",
            "Embedding layer that will convert word IDs into embeddings. The embedding matrix\n",
            "needs to have one row per token in the vocabulary (vocab_size) and one column per\n",
            "embedding dimension (this example uses 128 dimensions, but this is a hyperparame‐\n",
            "ter you could tune). Next we use a GRU layer and a Dense layer with a single neuron\n",
            "and  the  sigmoid  activation  function,  since  this  is  a  binary  classification  task:  the\n",
            "model’s output will be the estimated probability that the review expresses a positive\n",
            "sentiment  regarding  the  movie.  We  then  compile  the  model,  and  we  fit  it  on  the\n",
            "dataset we prepared earlier for a couple of epochs (or you can train for longer to get\n",
            "better results).\n",
            "\n",
            "8 Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and\n",
            "\n",
            "Machine Translation”, arXiv preprint arXiv:1609.08144 (2016).\n",
            "\n",
            "Sentiment Analysis \n",
            "\n",
            "| \n",
            "\n",
            "589\n",
            "\n",
            "\fSadly,  if  you  run  this  code,  you  will  generally  find  that  the  model  fails  to  learn\n",
            "anything  at  all:  the  accuracy  remains  close  to  50%,  no  better  than  random  chance.\n",
            "Why  is  that?  The  reviews  have  different  lengths,  so  when  the  TextVectorization\n",
            "layer converts them to sequences of token IDs, it pads the shorter sequences using the\n",
            "padding token (with ID 0) to make them as long as the longest sequence in the batch.\n",
            "As  a  result,  most  sequences  end  with  many  padding  tokens—often  dozens  or  even\n",
            "hundreds  of  them.  Even  though  we’re  using  a  GRU  layer,  which  is  much  better  than\n",
            "a SimpleRNN layer, its short-term memory is still not great, so when it goes through\n",
            "many padding tokens, it ends up forgetting what the review was about! One solution\n",
            "is  to  feed  the  model  with  batches  of  equal-length  sentences  (which  also  speeds  up\n",
            "training). Another solution is to make the RNN ignore the padding tokens. This can\n",
            "be done using masking.\n",
            "\n",
            "Masking\n",
            "Making  the  model  ignore  padding  tokens  is  trivial  using  Keras:  simply  add\n",
            "mask_zero=True when creating the Embedding layer. This means that padding tokens\n",
            "(whose ID is 0) will be ignored by all downstream layers. That’s all! If you retrain the\n",
            "previous  model  for  a  few  epochs,  you  will  find  that  the  validation  accuracy  quickly\n",
            "reaches over 80%.\n",
            "\n",
            "The  way  this  works  is  that  the  Embedding  layer  creates  a  mask  tensor  equal  to\n",
            "tf.math.not_equal(inputs,  0):  it  is  a  Boolean  tensor  with  the  same  shape  as  the\n",
            "inputs, and it is equal to False anywhere the token IDs are 0, or True otherwise. This\n",
            "mask tensor is then automatically propagated by the model to the next layer. If that\n",
            "layer’s call() method has a mask argument, then it automatically receives the mask.\n",
            "This  allows  the  layer  to  ignore  the  appropriate  time  steps.  Each  layer  may  handle\n",
            "the mask differently, but in general they simply ignore masked time steps (i.e., time\n",
            "steps for which the mask is False). For example, when a recurrent layer encounters a\n",
            "masked time step, it simply copies the output from the previous time step.\n",
            "\n",
            "Next,  if  the  layer’s  supports_masking  attribute  is  True,  then  the  mask  is  automati‐\n",
            "cally  propagated  to  the  next  layer.  It  keeps  propagating  this  way  for  as  long  as  the\n",
            "layers  have  supports_masking=True.  As  an  example,  a  recurrent  layer’s  supports_\n",
            "masking attribute is True when return_sequences=True, but it’s False when return_\n",
            "sequences=False  since  there’s  no  need  for  a  mask  anymore  in  this  case.  So  if  you\n",
            "have  a  model  with  several  recurrent  layers  with  return_sequences=True,  followed\n",
            "by a recurrent layer with return_sequences=False, then the mask will automatically\n",
            "propagate  up  to  the  last  recurrent  layer:  that  layer  will  use  the  mask  to  ignore\n",
            "masked  steps,  but  it  will  not  propagate  the  mask  any  further.  Similarly,  if  you  set\n",
            "mask_zero=True when creating the Embedding layer in the sentiment analysis model\n",
            "we just built, then the GRU layer will receive and use the mask automatically, but it will\n",
            "not propagate it any further, since return_sequences is not set to True.\n",
            "\n",
            "590 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fSome  layers  need  to  update  the  mask  before  propagating  it  to\n",
            "the  next  layer:  they  do  so  by  implementing  the  compute_mask()\n",
            "method,  which  takes  two  arguments:  the  inputs  and  the  previous\n",
            "mask.  It  then  computes  the  updated  mask  and  returns  it.  The\n",
            "default  implementation  of  compute_mask()  just  returns  the  previ‐\n",
            "ous mask unchanged.\n",
            "\n",
            "Many  Keras  layers  support  masking:  SimpleRNN,  GRU,  LSTM,  Bidirectional,  Dense,\n",
            "TimeDistributed,  Add,  and  a  few  others  (all  in  the  tf.keras.layers  package).\n",
            "However,  convolutional  layers  (including  Conv1D)  do  not  support  masking—it’s  not\n",
            "obvious how they would do so anyway.\n",
            "\n",
            "If the mask propagates all the way to the output, then it gets applied to the losses as\n",
            "well, so the masked time steps will not contribute to the loss (their loss will be 0). This\n",
            "assumes  that  the  model  outputs  sequences,  which  is  not  the  case  in  our  sentiment\n",
            "analysis model.\n",
            "\n",
            "The  LSTM  and  GRU  layers  have  an  optimized  implementation  for\n",
            "GPUs, based on Nvidia’s cuDNN library. However, this implemen‐\n",
            "tation  only  supports  masking  if  all  the  padding  tokens  are  at  the\n",
            "end of the sequences. It also requires you to use the default values\n",
            "for  several  hyperparameters:  activation,  recurrent_activation,\n",
            "recurrent_dropout,  unroll,  use_bias, and  reset_after. If that’s\n",
            "not  the  case,  then  these  layers  will  fall  back  to  the  (much  slower)\n",
            "default GPU implementation.\n",
            "\n",
            "If you want to implement your own custom layer with masking support, you should\n",
            "add  a  mask  argument  to  the  call()  method,  and  obviously  make  the  method  use\n",
            "the  mask.  Additionally,  if  the  mask  must  be  propagated  to  the  next  layers,  then\n",
            "you  should  set  self.supports_masking=True  in  the  constructor.  If  the  mask  must\n",
            "be  updated  before  it  is  propagated,  then  you  must  implement  the  compute_mask()\n",
            "method.\n",
            "\n",
            "If  your  model  does  not  start  with  an  Embedding  layer,  you  may  use  the  tf.\n",
            "keras.layers.Masking  layer  instead:  by  default,  it  sets  the  mask  to  tf.math.\n",
            "reduce_any(tf.math.not_equal(X,  0),  axis=-1),  meaning  that  time  steps  where\n",
            "the last dimension is full of zeros will be masked out in subsequent layers.\n",
            "\n",
            "Sentiment Analysis \n",
            "\n",
            "| \n",
            "\n",
            "591\n",
            "\n",
            "\fUsing masking layers and automatic mask propagation works best for simple models.\n",
            "It  will  not  always  work  for  more  complex  models,  such  as  when  you  need  to  mix\n",
            "Conv1D layers with recurrent layers. In such cases, you will need to explicitly compute\n",
            "the  mask  and  pass  it  to  the  appropriate  layers,  using  either  the  functional  API  or\n",
            "the subclassing API. For example, the following model is equivalent to the previous\n",
            "model, except it is built using the functional API and handles masking manually. It\n",
            "also adds a bit of dropout since the previous model was overfitting slightly:\n",
            "\n",
            "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
            "token_ids = text_vec_layer(inputs)\n",
            "mask = tf.math.not_equal(token_ids, 0)\n",
            "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
            "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
            "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
            "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
            "\n",
            "One last approach to masking is to feed the model with ragged tensors.9 In practice,\n",
            "all you need to do is to set ragged=True when creating the TextVectorization layer,\n",
            "so that the input sequences are represented as ragged tensors:\n",
            "\n",
            ">>> text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
            "...     max_tokens=vocab_size, ragged=True)\n",
            "...\n",
            ">>> text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
            ">>> text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])\n",
            "<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>\n",
            "\n",
            "Compare  this  ragged  tensor  representation  with  the  regular  tensor  representation,\n",
            "which uses padding tokens:\n",
            "\n",
            ">>> text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])\n",
            "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[ 86,  18,   0,   0,   0],\n",
            "       [ 11,   7,   1, 116, 217]])>\n",
            "\n",
            "Keras’s  recurrent  layers  have  built-in  support  for  ragged  tensors,  so  there’s  nothing\n",
            "else  you  need  to  do:  just  use  this  TextVectorization  layer  in  your  model.  There’s\n",
            "no need to pass mask_zero=True or handle masks explicitly—it’s all implemented for\n",
            "you. That’s convenient! However, as of early 2022, the support for ragged tensors in\n",
            "Keras is still fairly recent, so there are a few rough edges. For example, it is currently\n",
            "not possible to use ragged tensors as targets when running on the GPU (but this may\n",
            "be resolved by the time you read these lines).\n",
            "\n",
            "Whichever masking approach you prefer, after training this model for a few epochs,\n",
            "it  will  become  quite  good  at  judging  whether  a  review  is  positive  or  not.  If  you  use\n",
            "the tf.keras.callbacks.TensorBoard() callback, you can visualize the embeddings\n",
            "\n",
            "9 Ragged tensors were introduced in Chapter 12, and they are detailed in Appendix C.\n",
            "\n",
            "592 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fin TensorBoard as they are being learned: it is fascinating to see words like “awesome”\n",
            "and “amazing” gradually cluster on one side of the embedding space, while words like\n",
            "“awful” and “terrible” cluster on the other side. Some words are not as positive as you\n",
            "might expect (at least with this model), such as the word “good”, presumably because\n",
            "many negative reviews contain the phrase “not good”.\n",
            "\n",
            "Reusing Pretrained Embeddings and Language Models\n",
            "It’s impressive that the model is able to learn useful word embeddings based on just\n",
            "25,000 movie reviews. Imagine how good the embeddings would be if we had billions\n",
            "of  reviews  to  train  on!  Unfortunately,  we  don’t,  but  perhaps  we  can  reuse  word\n",
            "embeddings  trained  on  some  other  (very)  large  text  corpus  (e.g.,  Amazon  reviews,\n",
            "available  on  TensorFlow  Datasets),  even  if  it  is  not  composed  of  movie  reviews?\n",
            "After all, the word “amazing” generally has the same meaning whether you use it to\n",
            "talk about movies or anything else. Moreover, perhaps embeddings would be useful\n",
            "for  sentiment  analysis  even  if  they  were  trained  on  another  task:  since  words  like\n",
            "“awesome”  and  “amazing”  have  a  similar  meaning,  they  will  likely  cluster  in  the\n",
            "embedding  space  even  for  tasks  such  as  predicting  the  next  word  in  a  sentence.  If\n",
            "all positive words and all negative words form clusters, then this will be helpful for\n",
            "sentiment analysis. So, instead of training word embeddings, we could just download\n",
            "and  use  pretrained  embeddings,  such  as  Google’s  Word2vec  embeddings,  Stanford’s\n",
            "GloVe embeddings, or Facebook’s FastText embeddings.\n",
            "\n",
            "Using pretrained word embeddings was popular for several years, but this approach\n",
            "has its limits. In particular, a word has a single representation, no matter the context.\n",
            "For example, the word “right” is encoded the same way in “left and right” and “right\n",
            "and wrong”, even though it means two very different things. To address this limita‐\n",
            "tion, a 2018 paper10 by Matthew Peters introduced Embeddings from Language Models\n",
            "(ELMo): these are contextualized word embeddings learned from the internal states\n",
            "of a deep bidirectional language model. Instead of just using pretrained embeddings\n",
            "in your model, you reuse part of a pretrained language model.\n",
            "\n",
            "At  roughly  the  same  time,  the  Universal  Language  Model  Fine-Tuning  (ULMFiT)\n",
            "paper11  by  Jeremy  Howard  and  Sebastian  Ruder  demonstrated  the  effectiveness  of\n",
            "unsupervised  pretraining  for  NLP  tasks:  the  authors  trained  an  LSTM  language\n",
            "model  on  a  huge  text  corpus  using  self-supervised  learning  (i.e.,  generating  the\n",
            "labels  automatically  from  the  data),  then  they  fine-tuned  it  on  various  tasks.  Their\n",
            "model  outperformed  the  state  of  the  art  on  six  text  classification  tasks  by  a  large\n",
            "\n",
            "10 Matthew Peters et al., “Deep Contextualized Word Representations”, Proceedings of the 2018 Conference of\n",
            "\n",
            "the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1\n",
            "(2018): 2227–2237.\n",
            "\n",
            "11 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification”, Pro‐\n",
            "ceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328–339.\n",
            "\n",
            "Sentiment Analysis \n",
            "\n",
            "| \n",
            "\n",
            "593\n",
            "\n",
            "\fmargin  (reducing  the  error  rate  by  18–24%  in  most  cases).  Moreover,  the  authors\n",
            "showed  a  pretrained  model  fine-tuned  on  just  100  labeled  examples  could  achieve\n",
            "the  same  performance  as  one  trained  from  scratch  on  10,000  examples.  Before  the\n",
            "ULMFiT  paper,  using  pretrained  models  was  only  the  norm  in  computer  vision;  in\n",
            "the context of NLP, pretraining was limited to word embeddings. This paper marked\n",
            "the beginning of a new era in NLP: today, reusing pretrained language models is the\n",
            "norm.\n",
            "\n",
            "For example, let’s build a classifier based on the Universal Sentence Encoder, a model\n",
            "architecture  introduced  in  a  2018  paper12  by  a  team  of  Google  researchers.  This\n",
            "model  is  based  on  the  transformer  architecture,  which  we  will  look  at  later  in  this\n",
            "chapter. Conveniently, the model is available on TensorFlow Hub:\n",
            "\n",
            "import os\n",
            "import tensorflow_hub as hub\n",
            "\n",
            "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
            "model = tf.keras.Sequential([\n",
            "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
            "                   trainable=True, dtype=tf.string, input_shape=[]),\n",
            "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
            "])\n",
            "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "model.fit(train_set, validation_data=valid_set, epochs=10)\n",
            "\n",
            "This model is quite large—close to 1 GB in size—so it may take a\n",
            "while to download. By default, TensorFlow Hub modules are saved\n",
            "to a temporary directory, and they get downloaded again and again\n",
            "every  time  you  run  your  program.  To  avoid  that,  you  must  set\n",
            "the  TFHUB_CACHE_DIR  environment  variable  to  a  directory  of  your\n",
            "choice: the modules will then be saved there, and only downloaded\n",
            "once.\n",
            "\n",
            "Note  that  the  last  part  of  the  TensorFlow  Hub  module  URL  specifies  that  we  want\n",
            "version  4  of  the  model.  This  versioning  ensures  that  if  a  new  module  version  is\n",
            "released on TF Hub, it will not break our model. Conveniently, if you just enter this\n",
            "URL in a web browser, you will get the documentation for this module.\n",
            "\n",
            "12 Daniel Cer et al., “Universal Sentence Encoder”, arXiv preprint arXiv:1803.11175 (2018).\n",
            "\n",
            "594 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fAlso note that we set trainable=True when creating the hub.KerasLayer. This way,\n",
            "the pretrained Universal Sentence Encoder is fine-tuned during training: some of its\n",
            "weights are tweaked via backprop. Not all TensorFlow Hub modules are fine-tunable,\n",
            "so make sure to check the documentation for each pretrained module you’re interes‐\n",
            "ted in.\n",
            "\n",
            "After  training,  this  model  should  reach  a  validation  accuracy  of  over  90%.  That’s\n",
            "actually really good: if you try to perform the task yourself, you will probably do only\n",
            "marginally better since many reviews contain both positive and negative comments.\n",
            "Classifying these ambiguous reviews is like flipping a coin.\n",
            "\n",
            "So  far  we  have  looked  at  text  generation  using  a  char-RNN,  and  sentiment  analysis\n",
            "with word-level RNN models (based on trainable embeddings) and using a powerful\n",
            "pretrained language model from TensorFlow Hub. In the next section, we will explore\n",
            "another important NLP task: neural machine translation (NMT).\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine\n",
            "Translation\n",
            "Let’s begin with a simple NMT model13 that will translate English sentences to Span‐\n",
            "ish (see Figure 16-3).\n",
            "\n",
            "In  short,  the  architecture  is  as  follows:  English  sentences  are  fed  as  inputs  to  the\n",
            "encoder,  and  the  decoder  outputs  the  Spanish  translations.  Note  that  the  Spanish\n",
            "translations are also used as inputs to the decoder during training, but shifted back\n",
            "by one step. In other words, during training the decoder is given as input the word\n",
            "that it should have output at the previous step, regardless of what it actually output.\n",
            "This  is  called  teacher  forcing—a  technique  that  significantly  speeds  up  training  and\n",
            "improves the model’s performance. For the very first word, the decoder is given the\n",
            "start-of-sequence (SOS) token, and the decoder is expected to end the sentence with\n",
            "an end-of-sequence (EOS) token.\n",
            "\n",
            "Each word is initially represented by its ID (e.g., 854 for the word “soccer”). Next, an\n",
            "Embedding layer returns the word embedding. These word embeddings are then fed\n",
            "to the encoder and the decoder.\n",
            "\n",
            "At each step, the decoder outputs a score for each word in the output vocabulary (i.e.,\n",
            "Spanish),  then  the  softmax  activation  function  turns  these  scores  into  probabilities.\n",
            "For example, at the first step the word “Me” may have a probability of 7%, “Yo” may\n",
            "have a probability of 1%, and so on. The word with the highest probability is output.\n",
            "This  is  very  much  like  a  regular  classification  task,  and  indeed  you  can  train  the\n",
            "\n",
            "13 Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks”, arXiv preprint (2014).\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine Translation \n",
            "\n",
            "| \n",
            "\n",
            "595\n",
            "\n",
            "\fmodel using the \"sparse_categorical_crossentropy\" loss, much like we did in the\n",
            "char-RNN model.\n",
            "\n",
            "Figure 16-3. A simple machine translation model\n",
            "\n",
            "Note that at inference time (after training), you will not have the target sentence to\n",
            "feed to the decoder. Instead, you need to feed it the word that it has just output at the\n",
            "previous step, as shown in Figure 16-4 (this will require an embedding lookup that is\n",
            "not shown in the diagram).\n",
            "\n",
            "In a 2015 paper,14 Samy Bengio et al. proposed gradually switching\n",
            "from feeding the decoder the previous target token to feeding it the\n",
            "previous output token during training.\n",
            "\n",
            "14 Samy Bengio et al., “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks”, arXiv\n",
            "\n",
            "preprint arXiv:1506.03099 (2015).\n",
            "\n",
            "596 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fFigure 16-4. At inference time, the decoder is fed as input the word it just output at the\n",
            "previous time step\n",
            "\n",
            "Let’s  build  and  train  this  model!  First,  we  need  to  download  a  dataset  of  English/\n",
            "Spanish sentence pairs:15\n",
            "\n",
            "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
            "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
            "                               extract=True)\n",
            "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n",
            "\n",
            "Each  line  contains  an  English  sentence  and  the  corresponding  Spanish  translation,\n",
            "separated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”, which\n",
            "the  TextVectorization  layer  doesn’t  handle,  then  we  will  parse  the  sentence  pairs\n",
            "and shuffle them. Finally, we will split them into two separate lists, one per language:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
            "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
            "np.random.shuffle(pairs)\n",
            "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists\n",
            "\n",
            "Let’s take a look at the first three sentence pairs:\n",
            "\n",
            ">>> for i in range(3):\n",
            "...     print(sentences_en[i], \"=>\", sentences_es[i])\n",
            "...\n",
            "How boring! => Qué aburrimiento!\n",
            "I love sports. => Adoro el deporte.\n",
            "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n",
            "\n",
            "15 This dataset is composed of sentence pairs created by contributors of the Tatoeba project. About 120,000\n",
            "\n",
            "sentence pairs were selected by the authors of the website https://manythings.org/anki. This dataset is released\n",
            "under the Creative Commons Attribution 2.0 France license. Other language pairs are available.\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine Translation \n",
            "\n",
            "| \n",
            "\n",
            "597\n",
            "\n",
            "\fNext, let’s create two TextVectorization layers—one per language—and adapt them\n",
            "to the text:\n",
            "\n",
            "vocab_size = 1000\n",
            "max_length = 50\n",
            "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
            "    vocab_size, output_sequence_length=max_length)\n",
            "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
            "    vocab_size, output_sequence_length=max_length)\n",
            "text_vec_layer_en.adapt(sentences_en)\n",
            "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
            "\n",
            "There are a few things to note here:\n",
            "\n",
            "•\n",
            "• We  limit  the  vocabulary  size  to  1,000,  which  is  quite  small.  That’s  because  the\n",
            "training  set  is  not  very  large,  and  because  using  a  small  value  will  speed  up\n",
            "training. State-of-the-art translation models typically use a much larger vocabu‐\n",
            "lary  (e.g.,  30,000),  a  much  larger  training  set  (gigabytes),  and  a  much  larger\n",
            "model (hundreds or even thousands of megabytes). For example, check out the\n",
            "Opus-MT  models  by  the  University  of  Helsinki,  or  the  M2M-100  model  by\n",
            "Facebook.\n",
            "\n",
            "•\n",
            "• Since  all  sentences  in  the  dataset  have  a  maximum  of  50  words,  we  set\n",
            "output_sequence_length to 50: this way the input sequences will automatically\n",
            "be padded with zeros until they are all 50 tokens long. If there was any sentence\n",
            "longer than 50 tokens in the training set, it would be cropped to 50 tokens.\n",
            "\n",
            "•\n",
            "• For the Spanish text, we add “startofseq” and “endofseq” to each sentence when\n",
            "adapting the TextVectorization layer: we will use these words as SOS and EOS\n",
            "tokens.  You  could  use  any  other  words,  as  long  as  they  are  not  actual  Spanish\n",
            "words.\n",
            "\n",
            "Let’s  inspect  the  first  10  tokens  in  both  vocabularies.  They  start  with  the  padding\n",
            "token,  the  unknown  token,  the  SOS  and  EOS  tokens  (only  in  the  Spanish  vocabu‐\n",
            "lary), then the actual words, sorted by decreasing frequency:\n",
            "\n",
            ">>> text_vec_layer_en.get_vocabulary()[:10]\n",
            "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
            ">>> text_vec_layer_es.get_vocabulary()[:10]\n",
            "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n",
            "\n",
            "Next,  let’s  create  the  training  set  and  the  validation  set  (you  could  also  create  a  test\n",
            "set if you needed it). We will use the first 100,000 sentence pairs for training, and the\n",
            "rest for validation. The decoder’s inputs are the Spanish sentences plus an SOS token\n",
            "prefix. The targets are the Spanish sentences plus an EOS suffix:\n",
            "\n",
            "X_train = tf.constant(sentences_en[:100_000])\n",
            "X_valid = tf.constant(sentences_en[100_000:])\n",
            "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
            "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
            "\n",
            "598 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fY_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
            "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n",
            "\n",
            "OK, we’re now ready to build our translation model. We will use the functional API\n",
            "for  that  since  the  model  is  not  sequential.  It  requires  two  text  inputs—one  for  the\n",
            "encoder and one for the decoder—so let’s start with that:\n",
            "\n",
            "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
            "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
            "\n",
            "Next,  we  need  to  encode  these  sentences  using  the  TextVectorization  layers\n",
            "we  prepared  earlier,  followed  by  an  Embedding  layer  for  each  language,  with\n",
            "mask_zero=True  to  ensure  masking  is  handled  automatically.  The  embedding  size\n",
            "is a hyperparameter you can tune, as always:\n",
            "\n",
            "embed_size = 128\n",
            "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
            "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
            "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
            "                                                    mask_zero=True)\n",
            "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
            "                                                    mask_zero=True)\n",
            "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
            "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
            "\n",
            "When the languages share many words, you may get better perfor‐\n",
            "mance  using  the  same  embedding  layer  for  both  the  encoder  and\n",
            "the decoder.\n",
            "\n",
            "Now let’s create the encoder and pass it the embedded inputs:\n",
            "\n",
            "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
            "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
            "\n",
            "To  keep  things  simple,  we  just  used  a  single  LSTM  layer,  but  you  could  stack  several\n",
            "of  them.  We  also  set  return_state=True  to  get  a  reference  to  the  layer’s  final  state.\n",
            "Since we’re using an LSTM layer, there are actually two states: the short-term state and\n",
            "the long-term state. The layer returns these states separately, which is why we had to\n",
            "write *encoder_state to group both states in a list.16 Now we can use this (double)\n",
            "state as the initial state of the decoder:\n",
            "\n",
            "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
            "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
            "\n",
            "16 In Python, if you run a, *b = [1, 2, 3, 4], then a equals 1 and b equals [2, 3, 4].\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine Translation \n",
            "\n",
            "| \n",
            "\n",
            "599\n",
            "\n",
            "\fNext,  we  can  pass  the  decoder’s  outputs  through  a  Dense  layer  with  the  softmax\n",
            "activation function to get the word probabilities for each step:\n",
            "\n",
            "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
            "Y_proba = output_layer(decoder_outputs)\n",
            "\n",
            "Optimizing the Output Layer\n",
            "When  the  output  vocabulary  is  large,  outputting  a  probability  for  each  and  every\n",
            "possible  word  can  be  quite  slow.  If  the  target  vocabulary  contained,  say,  50,000\n",
            "Spanish words instead of 1,000, then the decoder would output 50,000-dimensional\n",
            "vectors, and computing the softmax function over such a large vector would be very\n",
            "computationally intensive. To avoid this, one solution is to look only at the logits out‐\n",
            "put by the model for the correct word and for a random sample of incorrect words,\n",
            "then compute an approximation of the loss based only on these logits. This sampled\n",
            "softmax  technique  was  introduced  in  2015  by  Sébastien  Jean  et  al.17  In  TensorFlow\n",
            "you  can  use  the  tf.nn.sampled_softmax_loss()  function  for  this  during  training\n",
            "and use the normal softmax function at inference time (sampled softmax cannot be\n",
            "used at inference time because it requires knowing the target).\n",
            "\n",
            "Another  thing  you  can  do  to  speed  up  training—which  is  compatible  with  sampled\n",
            "softmax—is  to  tie  the  weights  of  the  output  layer  to  the  transpose  of  the  decoder’s\n",
            "embedding matrix (you will see how to tie weights in Chapter 17). This significantly\n",
            "reduces the number of model parameters, which speeds up training and may some‐\n",
            "times  improve  the  model’s  accuracy  as  well,  especially  if  you  don’t  have  a  lot  of\n",
            "training data. The embedding matrix is equivalent to one-hot encoding followed by\n",
            "a  linear  layer  with  no  bias  term  and  no  activation  function  that  maps  the  one-hot\n",
            "vectors to the embedding space. The output layer does the reverse. So, if the model\n",
            "can find an embedding matrix whose transpose is close to its inverse (such a matrix is\n",
            "called an orthogonal matrix), then there’s no need to learn a separate set of weights for\n",
            "the output layer.\n",
            "\n",
            "And that’s it! We just need to create the Keras Model, compile it, and train it:\n",
            "\n",
            "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
            "                       outputs=[Y_proba])\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
            "          validation_data=((X_valid, X_valid_dec), Y_valid))\n",
            "\n",
            "17 Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation”, Proceedings\n",
            "of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint\n",
            "Conference on Natural Language Processing of the Asian Federation of Natural Language Processing 1 (2015):\n",
            "1–10.\n",
            "\n",
            "600 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fAfter training, we can use the model to translate new English sentences to Spanish.\n",
            "But  it’s  not  as  simple  as  calling  model.predict(),  because  the  decoder  expects  as\n",
            "input  the  word  that  was  predicted  at  the  previous  time  step.  One  way  to  do  this  is\n",
            "to  write  a  custom  memory  cell  that  keeps  track  of  the  previous  output  and  feeds  it\n",
            "to the encoder at the next time step. However, to keep things simple, we can just call\n",
            "the model multiple times, predicting one extra word at each round. Let’s write a little\n",
            "utility function for that:\n",
            "\n",
            "def translate(sentence_en):\n",
            "    translation = \"\"\n",
            "    for word_idx in range(max_length):\n",
            "        X = np.array([sentence_en])  # encoder input\n",
            "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
            "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
            "        predicted_word_id = np.argmax(y_proba)\n",
            "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
            "        if predicted_word == \"endofseq\":\n",
            "            break\n",
            "        translation += \" \" + predicted_word\n",
            "    return translation.strip()\n",
            "\n",
            "The  function  simply  keeps  predicting  one  word  at  a  time,  gradually  completing  the\n",
            "translation, and it stops once it reaches the EOS token. Let’s give it a try!\n",
            "\n",
            ">>> translate(\"I like soccer\")\n",
            "'me gusta el fútbol'\n",
            "\n",
            "Hurray,  it  works!  Well,  at  least  it  does  with  very  short  sentences.  If  you  try  playing\n",
            "with this model for a while, you will find that it’s not bilingual yet, and in particular it\n",
            "really struggles with longer sentences. For example:\n",
            "\n",
            ">>> translate(\"I like soccer and also going to the beach\")\n",
            "'me gusta el fútbol y a veces mismo al bus'\n",
            "\n",
            "The  translation  says  “I  like  soccer  and  sometimes  even  the  bus”.  So  how  can  you\n",
            "improve it? One way is to increase the training set size and add more LSTM layers in\n",
            "both  the  encoder  and  the  decoder.  But  this  will  only  get  you  so  far,  so  let’s  look  at\n",
            "more sophisticated techniques, starting with bidirectional recurrent layers.\n",
            "\n",
            "Bidirectional RNNs\n",
            "At  each  time  step,  a  regular  recurrent  layer  only  looks  at  past  and  present  inputs\n",
            "before  generating  its  output.  In  other  words,  it  is  causal,  meaning  it  cannot  look\n",
            "into  the  future.  This  type  of  RNN  makes  sense  when  forecasting  time  series,  or\n",
            "in  the  decoder  of  a  sequence-to-sequence  (seq2seq)  model.  But  for  tasks  like  text\n",
            "classification,  or  in  the  encoder  of  a  seq2seq  model,  it  is  often  preferable  to  look\n",
            "ahead at the next words before encoding a given word.\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine Translation \n",
            "\n",
            "| \n",
            "\n",
            "601\n",
            "\n",
            "\fFor example, consider the phrases “the right arm”, “the right person”, and “the right to\n",
            "criticize”: to properly encode the word “right”, you need to look ahead. One solution\n",
            "is  to  run  two  recurrent  layers  on  the  same  inputs,  one  reading  the  words  from  left\n",
            "to right and the other reading them from right to left, then combine their outputs at\n",
            "each time step, typically by concatenating them. This is what a bidirectional recurrent\n",
            "layer does (see Figure 16-5).\n",
            "\n",
            "Figure 16-5. A bidirectional recurrent layer\n",
            "\n",
            "To implement a bidirectional recurrent layer in Keras, just wrap a recurrent layer in\n",
            "a tf.keras.layers.Bidirectional layer. For example, the following Bidirectional\n",
            "layer could be used as the encoder in our translation model:\n",
            "\n",
            "encoder = tf.keras.layers.Bidirectional(\n",
            "    tf.keras.layers.LSTM(256, return_state=True))\n",
            "\n",
            "The  Bidirectional  layer  will  create  a  clone  of  the  GRU  layer  (but\n",
            "in the reverse direction), and it will run both and concatenate their\n",
            "outputs. So although the GRU layer has 10 units, the Bidirectional\n",
            "layer will output 20 values per time step.\n",
            "\n",
            "There’s  just  one  problem.  This  layer  will  now  return  four  states  instead  of  two:  the\n",
            "final short-term and long-term states of the forward LSTM layer, and the final short-\n",
            "term and long-term states of the backward LSTM layer. We cannot use this quadruple\n",
            "state  directly  as  the  initial  state  of  the  decoder’s  LSTM  layer,  since  it  expects  just  two\n",
            "states (short-term and long-term). We cannot make the decoder bidirectional, since it\n",
            "must remain causal: otherwise it would cheat during training and it would not work.\n",
            "Instead, we can concatenate the two short-term states, and also concatenate the two\n",
            "long-term states:\n",
            "\n",
            "602 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fencoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
            "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
            "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
            "\n",
            "Now let’s look at another popular technique that can greatly improve the performance\n",
            "of a translation model at inference time: beam search.\n",
            "\n",
            "Beam Search\n",
            "Suppose you have trained an encoder–decoder model, and you use it to translate the\n",
            "sentence  “I  like  soccer”  to  Spanish.  You  are  hoping  that  it  will  output  the  proper\n",
            "translation  “me  gusta  el  fútbol”,  but  unfortunately  it  outputs  “me  gustan  los  juga‐\n",
            "dores”, which means “I like the players”. Looking at the training set, you notice many\n",
            "sentences such as “I like cars”, which translates to “me gustan los autos”, so it wasn’t\n",
            "absurd for the model to output “me gustan los” after seeing “I like”. Unfortunately, in\n",
            "this case it was a mistake since “soccer” is singular. The model could not go back and\n",
            "fix it, so it tried to complete the sentence as best it could, in this case using the word\n",
            "“jugadores”. How can we give the model a chance to go back and fix mistakes it made\n",
            "earlier? One of the most common solutions is beam search: it keeps track of a short\n",
            "list of the k most promising sentences (say, the top three), and at each decoder step\n",
            "it  tries  to  extend  them  by  one  word,  keeping  only  the  k  most  likely  sentences.  The\n",
            "parameter k is called the beam width.\n",
            "\n",
            "For  example,  suppose  you  use  the  model  to  translate  the  sentence  “I  like  soccer”\n",
            "using  beam  search  with  a  beam  width  of  3  (see  Figure  16-6).  At  the  first  decoder\n",
            "step,  the  model  will  output  an  estimated  probability  for  each  possible  first  word\n",
            "in  the  translated  sentence.  Suppose  the  top  three  words  are  “me”  (75%  estimated\n",
            "probability), “a” (3%), and “como” (1%). That’s our short list so far. Next, we use the\n",
            "model to find the next word for each sentence. For the first sentence (“me”), perhaps\n",
            "the  model  outputs  a  probability  of  36%  for  the  word  “gustan”,  32%  for  the  word\n",
            "“gusta”, 16% for the word “encanta”, and so on. Note that these are actually conditional\n",
            "probabilities, given that the sentence starts with “me”. For the second sentence (“a”),\n",
            "the  model  might  output  a  conditional  probability  of  50%  for  the  word  “mi”,  and  so\n",
            "on. Assuming the vocabulary has 1,000 words, we will end up with 1,000 probabilities\n",
            "per sentence.\n",
            "\n",
            "Next,  we  compute  the  probabilities  of  each  of  the  3,000  two-word  sentences  we\n",
            "considered (3 × 1,000). We do this by multiplying the estimated conditional proba‐\n",
            "bility  of  each  word  by  the  estimated  probability  of  the  sentence  it  completes.  For\n",
            "example, the estimated probability of the sentence “me” was 75%, while the estimated\n",
            "conditional  probability  of  the  word  “gustan”  (given  that  the  first  word  is  “me”)  was\n",
            "36%, so the estimated probability of the sentence “me gustan” is 75% × 36% = 27%.\n",
            "After  computing  the  probabilities  of  all  3,000  two-word  sentences,  we  keep  only\n",
            "the  top  3.  In  this  example  they  all  start  with  the  word  “me”:  “me  gustan”  (27%),\n",
            "\n",
            "An Encoder–Decoder Network for Neural Machine Translation \n",
            "\n",
            "| \n",
            "\n",
            "603\n",
            "\n",
            "\f“me  gusta”  (24%),  and  “me  encanta”  (12%).  Right  now,  the  sentence  “me  gustan”  is\n",
            "winning, but “me gusta” has not been eliminated.\n",
            "\n",
            "Figure 16-6. Beam search, with a beam width of 3\n",
            "\n",
            "Then we repeat the same process: we use the model to predict the next word in each\n",
            "of  these  three  sentences,  and  we  compute  the  probabilities  of  all  3,000  three-word\n",
            "sentences we considered. Perhaps the top three are now “me gustan los” (10%), “me\n",
            "gusta el” (8%), and “me gusta mucho” (2%). At the next step we may get “me gusta el\n",
            "fútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte” (0.2%). Notice that\n",
            "“me gustan” was eliminated, and the correct translation is now ahead. We boosted our\n",
            "encoder–decoder model’s performance without any extra training, simply by using it\n",
            "more wisely.\n",
            "\n",
            "The  TensorFlow  Addons  library  includes  a  full  seq2seq  API  that\n",
            "lets  you  build  encoder–decoder  models  with  attention,  including\n",
            "beam  search,  and  more.  However,  its  documentation  is  currently\n",
            "very limited. Implementing beam search is a good exercise, so give\n",
            "it a try! Check out this chapter’s notebook for a possible solution.\n",
            "\n",
            "With  all  this,  you  can  get  reasonably  good  translations  for  fairly  short  sentences.\n",
            "Unfortunately,  this  model  will  be  really  bad  at  translating  long  sentences.  Once\n",
            "again, the problem comes from the limited short-term memory of RNNs. Attention\n",
            "mechanisms are the game-changing innovation that addressed this problem.\n",
            "\n",
            "Attention Mechanisms\n",
            "Consider  the  path  from  the  word  “soccer”  to  its  translation  “fútbol”  back  in  Fig‐\n",
            "ure 16-3: it is quite long! This means that a representation of this word (along with all\n",
            "\n",
            "604 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fthe other words) needs to be carried over many steps before it is actually used. Can’t\n",
            "we make this path shorter?\n",
            "\n",
            "This  was  the  core  idea  in  a  landmark  2014  paper18  by  Dzmitry  Bahdanau  et  al.,\n",
            "where  the  authors  introduced  a  technique  that  allowed  the  decoder  to  focus  on  the\n",
            "appropriate  words  (as  encoded  by  the  encoder)  at  each  time  step.  For  example,  at\n",
            "the  time  step  where  the  decoder  needs  to  output  the  word  “fútbol”,  it  will  focus  its\n",
            "attention  on  the  word  “soccer”.  This  means  that  the  path  from  an  input  word  to  its\n",
            "translation is now much shorter, so the short-term memory limitations of RNNs have\n",
            "much less impact. Attention mechanisms revolutionized neural machine translation\n",
            "(and deep learning in general), allowing a significant improvement in the state of the\n",
            "art, especially for long sentences (e.g., over 30 words).\n",
            "\n",
            "The  most  common  metric  used  in  NMT  is  the  bilingual  evalua‐\n",
            "tion  understudy  (BLEU)  score,  which  compares  each  translation\n",
            "produced by the model with several good translations produced by\n",
            "humans: it counts the number of n-grams (sequences of n words)\n",
            "that  appear  in  any  of  the  target  translations  and  adjusts  the  score\n",
            "to take into account the frequency of the produced n-grams in the\n",
            "target translations.\n",
            "\n",
            "Figure 16-7 shows our encoder–decoder model with an added attention mechanism.\n",
            "On  the  left,  you  have  the  encoder  and  the  decoder.  Instead  of  just  sending  the\n",
            "encoder’s final hidden state to the decoder, as well as the previous target word at each\n",
            "step  (which  is  still  done,  although  it  is  not  shown  in  the  figure),  we  now  send  all\n",
            "of  the  encoder’s  outputs  to  the  decoder  as  well.  Since  the  decoder  cannot  deal  with\n",
            "all these encoder outputs at once, they need to be aggregated: at each time step, the\n",
            "decoder’s  memory  cell  computes  a  weighted  sum  of  all  the  encoder  outputs.  This\n",
            "determines  which  words  it  will  focus  on  at  this  step.  The  weight  α(t,i)  is  the  weight\n",
            "of  the  ith  encoder  output  at  the  tth  decoder  time  step.  For  example,  if  the  weight\n",
            "α(3,2) is much larger than the weights α(3,0) and α(3,1), then the decoder will pay much\n",
            "more attention to the encoder’s output for word #2 (“soccer”) than to the other two\n",
            "outputs,  at  least  at  this  time  step.  The  rest  of  the  decoder  works  just  like  earlier:  at\n",
            "each time step the memory cell receives the inputs we just discussed, plus the hidden\n",
            "state  from  the  previous  time  step,  and  finally  (although  it  is  not  represented  in  the\n",
            "diagram) it receives the target word from the previous time step (or at inference time,\n",
            "the output from the previous time step).\n",
            "\n",
            "18 Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate”, arXiv\n",
            "\n",
            "preprint arXiv:1409.0473 (2014).\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "605\n",
            "\n",
            "\fFigure 16-7. Neural machine translation using an encoder–decoder network with an\n",
            "attention model\n",
            "\n",
            "But  where  do  these  α(t,i)  weights  come  from?  Well,  they  are  generated  by  a  small\n",
            "neural  network  called  an  alignment  model  (or  an  attention  layer),  which  is  trained\n",
            "jointly with the rest of the encoder–decoder model. This alignment model is illustra‐\n",
            "ted on the righthand side of Figure 16-7. It starts with a Dense layer composed of a\n",
            "single neuron that processes each of the encoder’s outputs, along with the decoder’s\n",
            "previous  hidden  state  (e.g.,  h(2)).  This  layer  outputs  a  score  (or  energy)  for  each\n",
            "encoder  output  (e.g.,  e(3,  2)):  this  score  measures  how  well  each  output  is  aligned\n",
            "with  the  decoder’s  previous  hidden  state.  For  example,  in  Figure  16-7,  the  model\n",
            "has  already  output  “me  gusta  el”  (meaning  “I  like”),  so  it’s  now  expecting  a  noun:\n",
            "the word “soccer” is the one that best aligns with the current state, so it gets a high\n",
            "score.  Finally,  all  the  scores  go  through  a  softmax  layer  to  get  a  final  weight  for\n",
            "each  encoder  output  (e.g.,  α(3,2)).  All  the  weights  for  a  given  decoder  time  step  add\n",
            "up  to  1.  This  particular  attention  mechanism  is  called  Bahdanau  attention  (named\n",
            "after  the  2014  paper’s  first  author).  Since  it  concatenates  the  encoder  output  with\n",
            "the decoder’s previous hidden state, it is sometimes called concatenative attention (or\n",
            "additive attention).\n",
            "\n",
            "606 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fIf the input sentence is n words long, and assuming the output sen‐\n",
            "tence is about as long, then this model will need to compute about\n",
            "n2 weights. Fortunately, this quadratic computational complexity is\n",
            "still tractable because even long sentences don’t have thousands of\n",
            "words.\n",
            "\n",
            "Another  common  attention  mechanism,  known  as  Luong  attention  or  multiplicative\n",
            "attention, was proposed shortly after, in 2015,19 by Minh-Thang Luong et al. Because\n",
            "the  goal  of  the  alignment  model  is  to  measure  the  similarity  between  one  of  the\n",
            "encoder’s  outputs  and  the  decoder’s  previous  hidden  state,  the  authors  proposed  to\n",
            "simply compute the dot product (see Chapter 4) of these two vectors, as this is often a\n",
            "fairly good similarity measure, and modern hardware can compute it very efficiently.\n",
            "For  this  to  be  possible,  both  vectors  must  have  the  same  dimensionality.  The  dot\n",
            "product gives a score, and all the scores (at a given decoder time step) go through a\n",
            "softmax layer to give the final weights, just like in Bahdanau attention. Another sim‐\n",
            "plification Luong et al. proposed was to use the decoder’s hidden state at the current\n",
            "time step rather than at the previous time step (i.e., h(t) rather than h(t–1)), then to use\n",
            "the output of the attention mechanism (noted h t ) directly to compute the decoder’s\n",
            "predictions, rather than using it to compute the decoder’s current hidden state. The\n",
            "researchers also proposed a variant of the dot product mechanism where the encoder\n",
            "outputs  first  go  through  a  fully  connected  layer  (without  a  bias  term)  before  the\n",
            "dot  products  are  computed.  This  is  called  the  “general”  dot  product  approach.  The\n",
            "researchers compared both dot product approaches with the concatenative attention\n",
            "mechanism  (adding  a  rescaling  parameter  vector  v),  and  they  observed  that  the\n",
            "dot product variants performed better than concatenative attention. For this reason,\n",
            "concatenative attention is much less used now. The equations for these three attention\n",
            "mechanisms are summarized in Equation 16-1.\n",
            "\n",
            "Equation 16-1. Attention mechanisms\n",
            "\n",
            "h t = ∑\n",
            "i\n",
            "\n",
            "α t, i y i\n",
            "\n",
            " with α t, i =\n",
            "\n",
            " and e t, i =\n",
            "\n",
            "exp e t, i\n",
            "∑\n",
            "exp e t, i′\n",
            "i′\n",
            "⊺y i\n",
            "h t\n",
            "⊺ Wy i\n",
            "\n",
            "h t\n",
            "v⊺tanh W h t ; y i\n",
            "\n",
            "dot\n",
            "\n",
            "general\n",
            "\n",
            "concat\n",
            "\n",
            "19 Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation”, Proceedings\n",
            "\n",
            "of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412–1421.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "607\n",
            "\n",
            "\fKeras  provides  a  tf.keras.layers.Attention  layer  for  Luong  attention,  and  an\n",
            "AdditiveAttention  layer  for  Bahdanau  attention.  Let’s  add  Luong  attention  to  our\n",
            "encoder–decoder  model.  Since  we  will  need  to  pass  all  the  encoder’s  outputs  to  the\n",
            "Attention  layer,  we  first  need  to  set  return_sequences=True  when  creating  the\n",
            "encoder:\n",
            "\n",
            "encoder = tf.keras.layers.Bidirectional(\n",
            "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))\n",
            "\n",
            "Next,  we  need  to  create  the  attention  layer  and  pass  it  the  decoder’s  states  and  the\n",
            "encoder’s outputs. However, to access the decoder’s states at each step we would need\n",
            "to write a custom memory cell. For simplicity, let’s use the decoder’s outputs instead\n",
            "of  its  states:  in  practice  this  works  well  too,  and  it’s  much  easier  to  code.  Then  we\n",
            "just pass the attention layer’s outputs directly to the output layer, as suggested in the\n",
            "Luong attention paper:\n",
            "\n",
            "attention_layer = tf.keras.layers.Attention()\n",
            "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
            "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
            "Y_proba = output_layer(attention_outputs)\n",
            "\n",
            "And that’s it! If you train this model, you will find that it now handles much longer\n",
            "sentences. For example:\n",
            "\n",
            ">>> translate(\"I like soccer and also going to the beach\")\n",
            "'me gusta el fútbol y también ir a la playa'\n",
            "\n",
            "In short, the attention layer provides a way to focus the attention of the model on part\n",
            "of the inputs. But there’s another way to think of this layer: it acts as a differentiable\n",
            "memory retrieval mechanism.\n",
            "\n",
            "For  example,  let’s  suppose  the  encoder  analyzed  the  input  sentence  “I  like  soccer”,\n",
            "and it managed to understand that the word “I” is the subject and the word “like” is\n",
            "the verb, so it encoded this information in its outputs for these words. Now suppose\n",
            "the  decoder  has  already  translated  the  subject,  and  it  thinks  that  it  should  translate\n",
            "the  verb  next.  For  this,  it  needs  to  fetch  the  verb  from  the  input  sentence.  This\n",
            "is  analogous  to  a  dictionary  lookup:  it’s  as  if  the  encoder  had  created  a  dictionary\n",
            "{\"subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value\n",
            "that corresponds to the key “verb”.\n",
            "\n",
            "However, the model does not have discrete tokens to represent the keys (like “subject”\n",
            "or “verb”); instead, it has vectorized representations of these concepts that it learned\n",
            "during training, so the query it will use for the lookup will not perfectly match any\n",
            "key  in  the  dictionary.  The  solution  is  to  compute  a  similarity  measure  between  the\n",
            "query and each key in the dictionary, and then use the softmax function to convert\n",
            "these  similarity  scores  to  weights  that  add  up  to  1.  As  we  saw  earlier,  that’s  exactly\n",
            "what  the  attention  layer  does.  If  the  key  that  represents  the  verb  is  by  far  the  most\n",
            "similar to the query, then that key’s weight will be close to 1.\n",
            "\n",
            "608 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fNext, the attention layer computes a weighted sum of the corresponding values: if the\n",
            "weight of the “verb” key is close to 1, then the weighted sum will be very close to the\n",
            "representation of the word “played”.\n",
            "\n",
            "This is why the Keras Attention and AdditiveAttention layers both expect a list as\n",
            "input, containing two or three items: the queries, the keys, and optionally the values. If\n",
            "you do not pass any values, then they are automatically equal to the keys. So, looking\n",
            "at  the  previous  code  example  again,  the  decoder  outputs  are  the  queries,  and  the\n",
            "encoder outputs are both the keys and the values. For each decoder output (i.e., each\n",
            "query),  the  attention  layer  returns  a  weighted  sum  of  the  encoder  outputs  (i.e.,  the\n",
            "keys/values) that are most similar to the decoder output.\n",
            "\n",
            "The  bottom  line  is  that  an  attention  mechanism  is  a  trainable  memory  retrieval\n",
            "system. It is so powerful that you can actually build state-of-the-art models using only\n",
            "attention mechanisms. Enter the transformer architecture.\n",
            "\n",
            "Attention Is All You Need: The Original Transformer Architecture\n",
            "In  a  groundbreaking  2017  paper,20  a  team  of  Google  researchers  suggested  that\n",
            "“Attention  Is  All  You  Need”.  They  created  an  architecture  called  the  transformer,\n",
            "which  significantly  improved  the  state-of-the-art  in  NMT  without  using  any  recur‐\n",
            "rent  or  convolutional  layers,21  just  attention  mechanisms  (plus  embedding  layers,\n",
            "dense  layers,  normalization  layers,  and  a  few  other  bits  and  pieces).  Because  the\n",
            "model  is  not  recurrent,  it  doesn’t  suffer  as  much  from  the  vanishing  or  exploding\n",
            "gradients problems as RNNs, it can be trained in fewer steps, it’s easier to parallelize\n",
            "across multiple GPUs, and it can better capture long-range patterns than RNNs. The\n",
            "original 2017 transformer architecture is represented in Figure 16-8.\n",
            "\n",
            "In  short,  the  left  part  of  Figure  16-8  is  the  encoder,  and  the  right  part  is  the\n",
            "decoder.  Each  embedding  layer  outputs  a  3D  tensor  of  shape  [batch  size,  sequence\n",
            "length, embedding size]. After that, the tensors are gradually transformed as they flow\n",
            "through the transformer, but their shape remains the same.\n",
            "\n",
            "20 Ashish Vaswani et al., “Attention Is All You Need”, Proceedings of the 31st International Conference on Neural\n",
            "\n",
            "Information Processing Systems (2017): 6000–6010.\n",
            "\n",
            "21 Since the transformer uses time-distributed dense layers, you could argue that it uses 1D convolutional layers\n",
            "\n",
            "with a kernel size of 1.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "609\n",
            "\n",
            "\fFigure 16-8. The original 2017 transformer architecture22\n",
            "\n",
            "If you use the transformer for NMT, then during training you must feed the English\n",
            "sentences to the encoder and the corresponding Spanish translations to the decoder,\n",
            "with an extra SOS token inserted at the start of each sentence. At inference time, you\n",
            "must  call  the  transformer  multiple  times,  producing  the  translations  one  word  at  a\n",
            "time and feeding the partial translations to the decoder at each round, just like we did\n",
            "earlier in the translate() function.\n",
            "\n",
            "22 This is figure 1 from the “Attention Is All You Need” paper, reproduced with the kind permission of the\n",
            "\n",
            "authors.\n",
            "\n",
            "610 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fThe encoder’s role is to gradually transform the inputs—word representations of the\n",
            "English sentence—until each word’s representation perfectly captures the meaning of\n",
            "the  word,  in  the  context  of  the  sentence.  For  example,  if  you  feed  the  encoder  with\n",
            "the  sentence  “I  like  soccer”,  then  the  word  “like”  will  start  off  with  a  rather  vague\n",
            "representation,  since  this  word  could  mean  different  things  in  different  contexts:\n",
            "think  of  “I  like  soccer”  versus  “It’s  like  that”.  But  after  going  through  the  encoder,\n",
            "the  word’s  representation  should  capture  the  correct  meaning  of  “like”  in  the  given\n",
            "sentence (i.e., to be fond of), as well as any other information that may be required\n",
            "for translation (e.g., it’s a verb).\n",
            "\n",
            "The  decoder’s  role  is  to  gradually  transform  each  word  representation  in  the  trans‐\n",
            "lated  sentence  into  a  word  representation  of  the  next  word  in  the  translation.  For\n",
            "example, if the sentence to translate is “I like soccer”, and the decoder’s input sentence\n",
            "is  “<SOS>  me  gusta  el  fútbol”,  then  after  going  through  the  decoder,  the  word\n",
            "representation of the word “el” will end up transformed into a representation of the\n",
            "word “fútbol”. Similarly, the representation of the word “fútbol” will be transformed\n",
            "into a representation of the EOS token.\n",
            "\n",
            "After  going  through  the  decoder,  each  word  representation  goes  through  a  final\n",
            "Dense  layer  with  a  softmax  activation  function,  which  will  hopefully  output  a  high\n",
            "probability for the correct next word and a low probability for all other words. The\n",
            "predicted sentence should be “me gusta el fútbol <EOS>”.\n",
            "\n",
            "That was the big picture; now let’s walk through Figure 16-8 in more detail:\n",
            "\n",
            "•\n",
            "• First,  notice  that  both  the  encoder  and  the  decoder  contain  modules  that  are\n",
            "stacked N times. In the paper, N = 6. The final outputs of the whole encoder stack\n",
            "are fed to the decoder at each of these N levels.\n",
            "\n",
            "•\n",
            "• Zooming  in,  you  can  see  that  you  are  already  familiar  with  most  components:\n",
            "there are two embedding layers; several skip connections, each of them followed\n",
            "by  a  layer  normalization  layer;  several  feedforward  modules  that  are  composed\n",
            "of  two  dense  layers  each  (the  first  one  using  the  ReLU  activation  function,  the\n",
            "second with no activation function); and finally the output layer is a dense layer\n",
            "using  the  softmax  activation  function.  You  can  also  sprinkle  a  bit  of  dropout\n",
            "after  the  attention  layers  and  the  feedforward  modules,  if  needed.  Since  all  of\n",
            "these layers are time-distributed, each word is treated independently from all the\n",
            "others. But how can we translate a sentence by looking at the words completely\n",
            "separately? Well, we can’t, so that’s where the new components come in:\n",
            "\n",
            "— The encoder’s multi-head attention layer updates each word representation by\n",
            "—\n",
            "attending  to  (i.e.,  paying  attention  to)  all  other  words  in  the  same  sentence.\n",
            "That’s  where  the  vague  representation  of  the  word  “like”  becomes  a  richer\n",
            "and more accurate representation, capturing its precise meaning in the given\n",
            "sentence. We will discuss exactly how this works shortly.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "611\n",
            "\n",
            "\f— The  decoder’s  masked  multi-head  attention  layer  does  the  same  thing,  but\n",
            "—\n",
            "when it processes a word, it doesn’t attend to words located after it: it’s a causal\n",
            "layer. For example, when it processes the word “gusta”, it only attends to the\n",
            "words  “<SOS>  me  gusta”,  and  it  ignores  the  words  “el  fútbol”  (or  else  that\n",
            "would be cheating).\n",
            "\n",
            "— The  decoder’s  upper  multi-head  attention  layer  is  where  the  decoder  pays\n",
            "—\n",
            "attention  to  the  words  in  the  English  sentence.  This  is  called  cross-attention,\n",
            "not self-attention in this case. For example, the decoder will probably pay close\n",
            "attention to the word “soccer” when it processes the word “el” and transforms\n",
            "its representation into a representation of the word “fútbol”.\n",
            "\n",
            "—\n",
            "— The  positional  encodings  are  dense  vectors  (much  like  word  embeddings)\n",
            "that  represent  the  position  of  each  word  in  the  sentence.  The  nth  positional\n",
            "encoding  is  added  to  the  word  embedding  of  the  nth  word  in  each  sentence.\n",
            "This is needed because all layers in the transformer architecture ignore word\n",
            "positions:  without  positional  encodings,  you  could  shuffle  the  input  sequen‐\n",
            "ces, and it would just shuffle the output sequences in the same way. Obviously,\n",
            "the order of words matters, which is why we need to give positional informa‐\n",
            "tion  to  the  transformer  somehow:  adding  positional  encodings  to  the  word\n",
            "representations is a good way to achieve this.\n",
            "\n",
            "The  first  two  arrows  going  into  each  multi-head  attention  layer\n",
            "in  Figure  16-8  represent  the  keys  and  values,  and  the  third  arrow\n",
            "represents  the  queries.  In  the  self-attention  layers,  all  three  are\n",
            "equal  to  the  word  representations  output  by  the  previous  layer,\n",
            "while in the decoder’s upper attention layer, the keys and values are\n",
            "equal  to  the  encoder’s  final  word  representations,  and  the  queries\n",
            "are equal to the word representations output by the previous layer.\n",
            "\n",
            "Let’s go through the novel components of the transformer architecture in more detail,\n",
            "starting with the positional encodings.\n",
            "\n",
            "Positional encodings\n",
            "\n",
            "A positional encoding is a dense vector that encodes the position of a word within a\n",
            "sentence: the ith positional encoding is added to the word embedding of the ith word\n",
            "in the sentence. The easiest way to implement this is to use an Embedding layer and\n",
            "make it encode all the positions from 0 to the maximum sequence length in the batch,\n",
            "then  add  the  result  to  the  word  embeddings.  The  rules  of  broadcasting  will  ensure\n",
            "that the positional encodings get applied to every input sequence. For example, here\n",
            "is how to add positional encodings to the encoder and decoder inputs:\n",
            "\n",
            "612 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fmax_length = 50  # max length in the whole training set\n",
            "embed_size = 128\n",
            "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
            "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
            "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
            "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
            "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\n",
            "\n",
            "Note that this implementation assumes that the embeddings are represented as reg‐\n",
            "ular  tensors,  not  ragged  tensors.23  The  encoder  and  the  decoder  share  the  same\n",
            "Embedding  layer  for  the  positional  encodings,  since  they  have  the  same  embedding\n",
            "size (this is often the case).\n",
            "\n",
            "Instead of using trainable positional encodings, the authors of the transformer paper\n",
            "chose  to  use  fixed  positional  encodings,  based  on  the  sine  and  cosine  functions  at\n",
            "different frequencies. The positional encoding matrix P is defined in Equation 16-2\n",
            "and represented at the top of Figure 16-9 (transposed), where Pp,i is the ith component\n",
            "of the encoding for the word located at the pth position in the sentence.\n",
            "\n",
            "Equation 16-2. Sine/cosine positional encodings\n",
            "\n",
            "Pp, i =\n",
            "\n",
            "sin p/10000\n",
            "\n",
            "i/d\n",
            "\n",
            "if i is even\n",
            "\n",
            "cos p/10000\n",
            "\n",
            "i − 1 /d\n",
            "\n",
            "if i is odd\n",
            "\n",
            "Figure 16-9. Sine/cosine positional encoding matrix (transposed, top) with a focus on\n",
            "two values of i (bottom)\n",
            "\n",
            "23 It’s possible to use ragged tensors instead, if you are using the latest version of TensorFlow.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "613\n",
            "\n",
            "\fThis solution can give the same performance as trainable positional encodings, and it\n",
            "can extend to arbitrarily long sentences without adding any parameters to the model\n",
            "(however,  when  there  is  a  large  amount  of  pretraining  data,  trainable  positional\n",
            "encodings  are  usually  favored).  After  these  positional  encodings  are  added  to  the\n",
            "word  embeddings,  the  rest  of  the  model  has  access  to  the  absolute  position  of  each\n",
            "word in the sentence because there is a unique positional encoding for each position\n",
            "(e.g., the positional encoding for the word located at the 22nd position in a sentence\n",
            "is represented by the vertical dashed line at the top left of Figure 16-9, and you can\n",
            "see  that  it  is  unique  to  that  position).  Moreover,  the  choice  of  oscillating  functions\n",
            "(sine and cosine) makes it possible for the model to learn relative positions as well.\n",
            "For  example,  words  located  38  words  apart  (e.g.,  at  positions  p  =  22  and  p  =  60)\n",
            "always have the same positional encoding values in the encoding dimensions i = 100\n",
            "and i = 101, as you can see in Figure 16-9. This explains why we need both the sine\n",
            "and the cosine for each frequency: if we only used the sine (the blue wave at i = 100),\n",
            "the model would not be able to distinguish positions p = 22 and p = 35 (marked by a\n",
            "cross).\n",
            "\n",
            "There  is  no  PositionalEncoding  layer  in  TensorFlow,  but  it  is  not  too  hard  to\n",
            "create  one.  For  efficiency  reasons,  we  precompute  the  positional  encoding  matrix\n",
            "in  the  constructor.  The  call()  method  just  truncates  this  encoding  matrix  to  the\n",
            "max  length  of  the  input  sequences,  and  it  adds  them  to  the  inputs.  We  also  set\n",
            "supports_masking=True to propagate the input’s automatic mask to the next layer:\n",
            "\n",
            "class PositionalEncoding(tf.keras.layers.Layer):\n",
            "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
            "        super().__init__(dtype=dtype, **kwargs)\n",
            "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
            "        p, i = np.meshgrid(np.arange(max_length),\n",
            "                           2 * np.arange(embed_size // 2))\n",
            "        pos_emb = np.empty((1, max_length, embed_size))\n",
            "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
            "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
            "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
            "        self.supports_masking = True\n",
            "\n",
            "    def call(self, inputs):\n",
            "        batch_max_length = tf.shape(inputs)[1]\n",
            "        return inputs + self.pos_encodings[:, :batch_max_length]\n",
            "\n",
            "Let’s use this layer to add the positional encoding to the encoder’s inputs:\n",
            "\n",
            "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
            "encoder_in = pos_embed_layer(encoder_embeddings)\n",
            "decoder_in = pos_embed_layer(decoder_embeddings)\n",
            "\n",
            "Now  let’s  look  deeper  into  the  heart  of  the  transformer  model,  at  the  multi-head\n",
            "attention layer.\n",
            "\n",
            "614 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fMulti-head attention\n",
            "\n",
            "To  understand  how  a  multi-head  attention  layer  works,  we  must  first  understand\n",
            "the  scaled  dot-product  attention  layer,  which  it  is  based  on.  Its  equation  is  shown  in\n",
            "Equation  16-3,  in  a  vectorized  form.  It’s  the  same  as  Luong  attention,  except  for  a\n",
            "scaling factor.\n",
            "\n",
            "Equation 16-3. Scaled dot-product attention\n",
            "\n",
            "Attention Q, K, V = softmax\n",
            "\n",
            "QK⊺\n",
            "dkeys\n",
            "\n",
            "V\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• Q  is  a  matrix  containing  one  row  per  query.  Its  shape  is  [nqueries,  dkeys],  where\n",
            "•\n",
            "nqueries  is  the  number  of  queries  and  dkeys  is  the  number  of  dimensions  of  each\n",
            "query and each key.\n",
            "\n",
            "•\n",
            "• K is a matrix containing one row per key. Its shape is [nkeys, dkeys], where nkeys is\n",
            "\n",
            "the number of keys and values.\n",
            "\n",
            "•\n",
            "• V is a matrix containing one row per value. Its shape is [nkeys, dvalues], where dvalues\n",
            "\n",
            "is the number of dimensions of each value.\n",
            "\n",
            "• The  shape  of  Q  K⊺  is  [nqueries,  nkeys]:  it  contains  one  similarity  score  for  each\n",
            "•\n",
            "query/key pair. To prevent this matrix from being huge, the input sequences must\n",
            "not  be  too  long  (we  will  discuss  how  to  overcome  this  limitation  later  in  this\n",
            "chapter).  The  output  of  the  softmax  function  has  the  same  shape,  but  all  rows\n",
            "sum up to 1. The final output has a shape of [nqueries, dvalues]: there is one row per\n",
            "query, where each row represents the query result (a weighted sum of the values).\n",
            "• The scaling factor 1 / ( dkeys) scales down the similarity scores to avoid saturat‐\n",
            "•\n",
            "\n",
            "ing the softmax function, which would lead to tiny gradients.\n",
            "\n",
            "• It  is  possible  to  mask  out  some  key/value  pairs  by  adding  a  very  large  negative\n",
            "•\n",
            "value to the corresponding similarity scores, just before computing the softmax.\n",
            "This is useful in the masked multi-head attention layer.\n",
            "\n",
            "If you set use_scale=True when creating a tf.keras.layers.Attention layer, then\n",
            "it will create an additional parameter that lets the layer learn how to properly down‐\n",
            "scale the similarity scores. The scaled dot-product attention used in the transformer\n",
            "model  is  almost  the  same,  except  it  always  scales  the  similarity  scores  by  the  same\n",
            "factor, 1 / ( dkeys).\n",
            "Note that the Attention layer’s inputs are just like Q, K, and V, except with an extra\n",
            "batch dimension (the first dimension). Internally, the layer computes all the attention\n",
            "scores for all sentences in the batch with just one call to tf.matmul(queries, keys):\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "615\n",
            "\n",
            "\fthis  makes  it  extremely  efficient.  Indeed,  in  TensorFlow,  if  A  and  B  are  tensors  with\n",
            "more than two dimensions—say, of shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively—\n",
            "then tf.matmul(A, B) will treat these tensors as 2 × 3 arrays where each cell contains\n",
            "a matrix, and it will multiply the corresponding matrices: the matrix at the ith row and\n",
            "jth column in A will be multiplied by the matrix at the ith row and jth column in B. Since\n",
            "the product of a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6 matrix, tf.matmul(A, B)\n",
            "will return an array of shape [2, 3, 4, 6].\n",
            "\n",
            "Now we’re ready to look at the multi-head attention layer. Its architecture is shown in\n",
            "Figure 16-10.\n",
            "\n",
            "Figure 16-10. Multi-head attention layer architecture24\n",
            "\n",
            "As you can see, it is just a bunch of scaled dot-product attention layers, each preceded\n",
            "by  a  linear  transformation  of  the  values,  keys,  and  queries  (i.e.,  a  time-distributed\n",
            "dense layer with no activation function). All the outputs are simply concatenated, and\n",
            "they go through a final linear transformation (again, time-distributed).\n",
            "\n",
            "But  why?  What  is  the  intuition  behind  this  architecture?  Well,  consider  once  again\n",
            "the  word  “like”  in  the  sentence  “I  like  soccer”.  The  encoder  was  smart  enough  to\n",
            "\n",
            "24 This is the righthand part of figure 2 from “Attention Is All You Need”, reproduced with the kind authoriza‐\n",
            "\n",
            "tion of the authors.\n",
            "\n",
            "616 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fencode the fact that it is a verb. But the word representation also includes its position\n",
            "in the text, thanks to the positional encodings, and it probably includes many other\n",
            "features  that  are  useful  for  its  translation,  such  as  the  fact  that  it  is  in  the  present\n",
            "tense. In short, the word representation encodes many different characteristics of the\n",
            "word.  If  we  just  used  a  single  scaled  dot-product  attention  layer,  we  would  only  be\n",
            "able to query all of these characteristics in one shot.\n",
            "\n",
            "This  is  why  the  multi-head  attention  layer  applies  multiple  different  linear  transfor‐\n",
            "mations of the values, keys, and queries: this allows the model to apply many different\n",
            "projections  of  the  word  representation  into  different  subspaces,  each  focusing  on  a\n",
            "subset  of  the  word’s  characteristics.  Perhaps  one  of  the  linear  layers  will  project  the\n",
            "word representation into a subspace where all that remains is the information that the\n",
            "word is a verb, another linear layer will extract just the fact that it is present tense, and\n",
            "so on. Then the scaled dot-product attention layers implement the lookup phase, and\n",
            "finally we concatenate all the results and project them back to the original space.\n",
            "\n",
            "Keras  includes  a  tf.keras.layers.MultiHeadAttention  layer,  so  we  now  have\n",
            "everything  we  need  to  build  the  rest  of  the  transformer.  Let’s  start  with  the  full\n",
            "encoder, which is exactly like in Figure 16-8, except we use a stack of two blocks (N =\n",
            "2) instead of six, since we don’t have a huge training set, and we add a bit of dropout\n",
            "as well:\n",
            "\n",
            "N = 2  # instead of 6\n",
            "num_heads = 8\n",
            "dropout_rate = 0.1\n",
            "n_units = 128  # for the first dense layer in each feedforward block\n",
            "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
            "Z = encoder_in\n",
            "for _ in range(N):\n",
            "    skip = Z\n",
            "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
            "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
            "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
            "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
            "    skip = Z\n",
            "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
            "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
            "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
            "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
            "\n",
            "This code should be mostly straightforward, except for one thing: masking. As of the\n",
            "time of writing, the MultiHeadAttention layer does not support automatic masking,25\n",
            "so we must handle it manually. How can we do that?\n",
            "\n",
            "25 This will most likely change by the time you read this; check out Keras issue #16248 for more details. When\n",
            "this happens, there will be no need to set the attention_mask argument, and therefore no need to create\n",
            "encoder_pad_mask.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "617\n",
            "\n",
            "\fThe  MultiHeadAttention  layer  accepts  an  attention_mask  argument,  which  is  a\n",
            "Boolean  tensor  of  shape  [batch  size,  max  query  length,  max  value  length]:  for  every\n",
            "token in every query sequence, this mask indicates which tokens in the correspond‐\n",
            "ing  value  sequence  should  be  attended  to.  We  want  to  tell  the  MultiHeadAttention\n",
            "layer to ignore all the padding tokens in the values. So, we first compute the padding\n",
            "mask  using  tf.math.not_equal(encoder_input_ids,  0).  This  returns  a  Boolean\n",
            "tensor  of  shape  [batch  size,  max  sequence  length].  We  then  insert  a  second  axis\n",
            "using [:, tf.newaxis], to get a mask of shape [batch size, 1, max sequence length].\n",
            "This  allows  us  to  use  this  mask  as  the  attention_mask  when  calling  the  MultiHead\n",
            "Attention layer: thanks to broadcasting, the same mask will be used for all tokens in\n",
            "each query. This way, the padding tokens in the values will be ignored correctly.\n",
            "\n",
            "However,  the  layer  will  compute  outputs  for  every  single  query  token,  including\n",
            "the  padding  tokens.  We  need  to  mask  the  outputs  that  correspond  to  these  pad‐\n",
            "ding  tokens.  Recall  that  we  used  mask_zero  in  the  Embedding  layers,  and  we  set\n",
            "supports_masking to True in the PositionalEncoding layer, so the automatic mask\n",
            "was propagated all the way to the  MultiHeadAttention layer’s inputs (encoder_in).\n",
            "We  can  use  this  to  our  advantage  in  the  skip  connection:  indeed,  the  Add  layer\n",
            "supports  automatic  masking,  so  when  we  add  Z  and  skip  (which  is  initially  equal\n",
            "to  encoder_in),  the  outputs  get  automatically  masked  correctly.26  Yikes!  Masking\n",
            "required much more explanation than code.\n",
            "\n",
            "Now on to the decoder! Once again, masking is going to be the only tricky part, so\n",
            "let’s  start  with  that.  The  first  multi-head  attention  layer  is  a  self-attention  layer,  like\n",
            "in the encoder, but it is a masked multi-head attention layer, meaning it is causal: it\n",
            "should ignore all tokens in the future. So, we need two masks: a padding mask and a\n",
            "causal mask. Let’s create them:\n",
            "\n",
            "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
            "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
            "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\n",
            "\n",
            "The padding mask is exactly like the one we created for the encoder, except it’s based\n",
            "on  the  decoder’s  inputs  rather  than  the  encoder’s.  The  causal  mask  is  created  using\n",
            "the tf.linalg.band_part() function, which takes a tensor and returns a copy with\n",
            "all  the  values  outside  a  diagonal  band  set  to  zero.  With  these  arguments,  we  get  a\n",
            "square matrix of size batch_max_len_dec (the max length of the input sequences in\n",
            "the batch), with 1s in the lower-left triangle and 0s in the upper right. If we use this\n",
            "mask as the attention mask, we will get exactly what we want: the first query token\n",
            "will only attend to the first value token, the second will only attend to the first two,\n",
            "\n",
            "26 Currently Z + skip does not support automatic masking, which is why we had to write tf.keras.\n",
            "\n",
            "layers.Add()([Z, skip]) instead. Again, this may change by the time you read this.\n",
            "\n",
            "618 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fthe third will only attend to the first three, and so on. In other words, query tokens\n",
            "cannot attend to any value token in the future.\n",
            "\n",
            "Let’s now build the decoder:\n",
            "\n",
            "encoder_outputs = Z  # let's save the encoder's final outputs\n",
            "Z = decoder_in  # the decoder starts with its own inputs\n",
            "for _ in range(N):\n",
            "    skip = Z\n",
            "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
            "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
            "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
            "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
            "    skip = Z\n",
            "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
            "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
            "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
            "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
            "    skip = Z\n",
            "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
            "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
            "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
            "\n",
            "For the first attention layer, we use causal_mask & decoder_pad_mask to mask both\n",
            "the padding tokens and future tokens. The causal mask only has two dimensions: it’s\n",
            "missing  the  batch  dimension,  but  that’s  okay  since  broadcasting  ensures  that  it  gets\n",
            "copied across all the instances in the batch.\n",
            "\n",
            "For the second attention layer, there’s nothing special. The only thing to note is that\n",
            "we are using encoder_pad_mask, not decoder_pad_mask, because this attention layer\n",
            "uses the encoder’s final outputs as its values.\n",
            "\n",
            "We’re  almost  done.  We  just  need  to  add  the  final  output  layer,  create  the  model,\n",
            "compile it, and train it:\n",
            "\n",
            "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
            "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
            "                       outputs=[Y_proba])\n",
            "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
            "              metrics=[\"accuracy\"])\n",
            "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
            "          validation_data=((X_valid, X_valid_dec), Y_valid))\n",
            "\n",
            "Congratulations!  You’ve  built  a  full  transformer  from  scratch,  and  trained  it  for\n",
            "automatic translation. This is getting quite advanced!\n",
            "\n",
            "The Keras team has created a new Keras NLP project, including an\n",
            "API to build a transformer more easily. You may also be interested\n",
            "in the new Keras CV project for computer vision.\n",
            "\n",
            "Attention Mechanisms \n",
            "\n",
            "| \n",
            "\n",
            "619\n",
            "\n",
            "\fBut the field didn’t stop there. Let’s now explore some of the recent advances.\n",
            "\n",
            "An Avalanche of Transformer Models\n",
            "The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress\n",
            "has been astounding, with larger and larger transformer-based architectures trained\n",
            "on immense datasets.\n",
            "\n",
            "First,  the  GPT  paper27  by  Alec  Radford  and  other  OpenAI  researchers  once  again\n",
            "demonstrated  the  effectiveness  of  unsupervised  pretraining,  like  the  ELMo  and\n",
            "ULMFiT  papers  before  it,  but  this  time  using  a  transformer-like  architecture.  The\n",
            "authors  pretrained  a  large  but  fairly  simple  architecture  composed  of  a  stack  of\n",
            "12  transformer  modules  using  only  masked  multi-head  attention  layers,  like  in  the\n",
            "original  transformer’s  decoder.  They  trained  it  on  a  very  large  dataset,  using  the\n",
            "same autoregressive technique we used for our Shakespearean char-RNN: just predict\n",
            "the  next  token.  This  is  a  form  of  self-supervised  learning.  Then  they  fine-tuned  it\n",
            "on  various  language  tasks,  using  only  minor  adaptations  for  each  task.  The  tasks\n",
            "were quite diverse: they included text classification, entailment (whether sentence A\n",
            "imposes,  involves,  or  implies  sentence  B  as  a  necessary  consequence),28  similarity\n",
            "(e.g., “Nice weather today” is very similar to “It is sunny”), and question answering\n",
            "(given  a  few  paragraphs  of  text  giving  some  context,  the  model  must  answer  some\n",
            "multiple-choice questions).\n",
            "\n",
            "Then  Google’s  BERT  paper29  came  out:  it  also  demonstrated  the  effectiveness  of\n",
            "self-supervised  pretraining  on  a  large  corpus,  using  a  similar  architecture  to  GPT\n",
            "but with nonmasked multi-head attention layers only, like in the original transform‐\n",
            "er’s  encoder.  This  means  that  the  model  is  naturally  bidirectional;  hence  the  B  in\n",
            "BERT  (Bidirectional  Encoder  Representations  from  Transformers).  Most  importantly,\n",
            "the authors proposed two pretraining tasks that explain most of the model’s strength:\n",
            "\n",
            "Masked language model (MLM)\n",
            "\n",
            "Each word in a sentence has a 15% probability of being masked, and the model\n",
            "is  trained  to  predict  the  masked  words.  For  example,  if  the  original  sentence  is\n",
            "“She  had  fun  at  the  birthday  party”,  then  the  model  may  be  given  the  sentence\n",
            "“She <mask> fun at the <mask> party” and it must predict the words “had” and\n",
            "“birthday” (the other outputs will be ignored). To be more precise, each selected\n",
            "word has an 80% chance of being masked, a 10% chance of being replaced by a\n",
            "\n",
            "27 Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018).\n",
            "\n",
            "28 For example, the sentence “Jane had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”,\n",
            "\n",
            "but it is contradicted by “Everyone hated the party” and it is unrelated to “The Earth is flat”.\n",
            "\n",
            "29 Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”,\n",
            "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\n",
            "Linguistics: Human Language Technologies 1 (2019).\n",
            "\n",
            "620 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\frandom  word  (to  reduce  the  discrepancy  between  pretraining  and  fine-tuning,\n",
            "since  the  model  will  not  see  <mask>  tokens  during  fine-tuning),  and  a  10%\n",
            "chance of being left alone (to bias the model toward the correct answer).\n",
            "\n",
            "Next sentence prediction (NSP)\n",
            "\n",
            "The  model  is  trained  to  predict  whether  two  sentences  are  consecutive  or  not.\n",
            "For  example,  it  should  predict  that  “The  dog  sleeps”  and  “It  snores  loudly”  are\n",
            "consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are\n",
            "not  consecutive.  Later  research  showed  that  NSP  was  not  as  important  as  was\n",
            "initially thought, so it was dropped in most later architectures.\n",
            "\n",
            "The  model  is  trained  on  these  two  tasks  simultaneously  (see  Figure  16-11).  For  the\n",
            "NSP task, the authors inserted a class token (<CLS>) at the start of every input, and\n",
            "the corresponding output token represents the model’s prediction: sentence B follows\n",
            "sentence A, or it does not. The two input sentences are concatenated, separated only\n",
            "by a special separation token (<SEP>), and they are fed as input to the model. To help\n",
            "the  model  know  which  sentence  each  input  token  belongs  to,  a  segment  embedding\n",
            "is  added  on  top  of  each  token’s  positional  embeddings:  there  are  just  two  possible\n",
            "segment embeddings, one for sentence A and one for sentence B. For the MLM task,\n",
            "some  input  words  are  masked  (as  we  just  saw)  and  the  model  tries  to  predict  what\n",
            "those words were. The loss is only computed on the NSP prediction and the masked\n",
            "tokens, not on the unmasked ones.\n",
            "\n",
            "Figure 16-11. BERT training and fine-tuning process30\n",
            "\n",
            "After  this  unsupervised  pretraining  phase  on  a  very  large  corpus  of  text,  the  model\n",
            "is  then  fine-tuned  on  many  different  tasks,  changing  very  little  for  each  task.  For\n",
            "example,  for  text  classification  such  as  sentiment  analysis,  all  output  tokens  are\n",
            "\n",
            "30 This is figure 1 from the paper, reproduced with the kind authorization of the authors.\n",
            "\n",
            "An Avalanche of Transformer Models \n",
            "\n",
            "| \n",
            "\n",
            "621\n",
            "\n",
            "\fignored except for the first one, corresponding to the class token, and a new output\n",
            "layer replaces the previous one, which was just a binary classification layer for NSP.\n",
            "\n",
            "In February 2019, just a few months after BERT was published, Alec Radford, Jeffrey\n",
            "Wu,  and  other  OpenAI  researchers  published  the  GPT-2  paper,31  which  proposed  a\n",
            "very  similar  architecture  to  GPT,  but  larger  still  (with  over  1.5  billion  parameters!).\n",
            "The  researchers  showed  that  the  new  and  improved  GPT  model  could  perform\n",
            "zero-shot learning (ZSL), meaning it could achieve good performance on many tasks\n",
            "without  any  fine-tuning.  This  was  just  the  start  of  a  race  toward  larger  and  larger\n",
            "models: Google’s Switch Transformers32 (introduced in January 2021) used 1 trillion\n",
            "parameters, and soon much larger models came out, such as the Wu Dao 2.0 model\n",
            "by the Beijing Academy of Artificial Intelligence (BAII), announced in June 2021.\n",
            "\n",
            "An unfortunate consequence of this trend toward gigantic models is that only well-\n",
            "funded  organizations  can  afford  to  train  such  models:  it  can  easily  cost  hundreds\n",
            "of  thousands  of  dollars  or  more.  And  the  energy  required  to  train  a  single  model\n",
            "corresponds  to  an  American  household’s  electricity  consumption  for  several  years;\n",
            "it’s not eco-friendly at all. Many of these models are just too big to even be used on\n",
            "regular hardware: they wouldn’t fit in RAM, and they would be horribly slow. Lastly,\n",
            "some are so costly that they are not released publicly.\n",
            "\n",
            "Luckily,  ingenious  researchers  are  finding  new  ways  to  downsize  transformers  and\n",
            "make them more data-efficient. For example, the DistilBERT model,33 introduced in\n",
            "October 2019 by Victor Sanh et al. from Hugging Face, is a small and fast transformer\n",
            "model  based  on  BERT.  It  is  available  on  Hugging  Face’s  excellent  model  hub,  along\n",
            "with thousands of others—you’ll see an example later in this chapter.\n",
            "\n",
            "DistilBERT  was  trained  using  distillation  (hence  the  name):  this  means  transferring\n",
            "knowledge  from  a  teacher  model  to  a  student  one,  which  is  usually  much  smaller\n",
            "than the teacher model. This is typically done by using the teacher’s predicted proba‐\n",
            "bilities  for  each  training  instance  as  targets  for  the  student.  Surprisingly,  distillation\n",
            "often works better than training the student from scratch on the same dataset as the\n",
            "teacher! Indeed, the student benefits from the teacher’s more nuanced labels.\n",
            "\n",
            "Many  more  transformer  architectures  came  out  after  BERT,  almost  on  a  monthly\n",
            "basis, often improving on the state of the art across all NLP tasks: XLNet (June 2019),\n",
            "RoBERTa  (July  2019),  StructBERT  (August  2019),  ALBERT  (September  2019),  T5\n",
            "(October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020),\n",
            "\n",
            "31 Alec Radford et al., “Language Models Are Unsupervised Multitask Learners” (2019).\n",
            "\n",
            "32 William Fedus et al., “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient\n",
            "\n",
            "Sparsity” (2021).\n",
            "\n",
            "33 Victor Sanh et al., “DistilBERT, A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”, arXiv\n",
            "\n",
            "preprint arXiv:1910.01108 (2019).\n",
            "\n",
            "622 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fSwitch  Transformers  (January  2021),  Wu  Dao  2.0  (June  2021),  Gopher  (December\n",
            "2021), GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022),\n",
            "and the list goes on and on. Each of these models brought new ideas and techniques,34\n",
            "but  I  particularly  like  the  T5  paper35  by  Google  researchers:  it  frames  all  NLP  tasks\n",
            "as  text-to-text,  using  an  encoder–decoder  transformer.  For  example,  to  translate  “I\n",
            "like soccer” to Spanish, you can just call the model with the input sentence “translate\n",
            "English to Spanish: I like soccer” and it outputs “me gusta el fútbol”. To summarize\n",
            "a paragraph, you just enter “summarize:” followed by the paragraph, and it outputs\n",
            "the summary. For classification, you only need to change the prefix to “classify:” and\n",
            "the model outputs the class name, as text. This simplifies using the model, and it also\n",
            "makes it possible to pretrain it on even more tasks.\n",
            "\n",
            "Last but not least, in April 2022, Google researchers used a new large-scale training\n",
            "platform  named  Pathways  (which  we  will  briefly  discuss  in  Chapter  19)  to  train  a\n",
            "humongous  language  model  named  the  Pathways  Language  Model  (PaLM),36  with  a\n",
            "whopping  540  billion  parameters,  using  over  6,000  TPUs.  Other  than  its  incredible\n",
            "size,  this  model  is  a  standard  transformer,  using  decoders  only  (i.e.,  with  masked\n",
            "multi-head  attention  layers),  with  just  a  few  tweaks  (see  the  paper  for  details).\n",
            "This  model  achieved  incredible  performance  on  all  sorts  of  NLP  tasks,  particularly\n",
            "in  natural  language  understanding  (NLU).  It’s  capable  of  impressive  feats,  such  as\n",
            "explaining jokes, giving detailed step-by-step answers to questions, and even coding.\n",
            "This is in part due to the model’s size, but also thanks to a technique called Chain of\n",
            "thought prompting,37 which was introduced a couple months earlier by another team\n",
            "of Google researchers.\n",
            "\n",
            "In question answering tasks, regular prompting typically includes a few examples of\n",
            "questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
            "tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A:\n",
            "11.” The prompt then continues with the actual question, such as “Q: John takes care\n",
            "of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How\n",
            "many hours a week does he spend taking care of dogs? A:”, and the model’s job is to\n",
            "append the answer: in this case, “35.”\n",
            "\n",
            "34 Mariya Yao summarized many of these models in this post: https://homl.info/yaopost.\n",
            "\n",
            "35 Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv\n",
            "\n",
            "preprint arXiv:1910.10683 (2019).\n",
            "\n",
            "36 Aakanksha Chowdhery et al., “PaLM: Scaling Language Modeling with Pathways”, arXiv preprint\n",
            "\n",
            "arXiv:2204.02311 (2022).\n",
            "\n",
            "37 Jason Wei et al., “Chain of Thought Prompting Elicits Reasoning in Large Language Models”, arXiv preprint\n",
            "\n",
            "arXiv:2201.11903 (2022).\n",
            "\n",
            "An Avalanche of Transformer Models \n",
            "\n",
            "| \n",
            "\n",
            "623\n",
            "\n",
            "\fBut with chain of thought prompting, the example answers include all the reasoning\n",
            "steps that lead to the conclusion. For example, instead of “A: 11”, the prompt contains\n",
            "“A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 =\n",
            "11.” This encourages the model to give a detailed answer to the actual question, such\n",
            "as “John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care\n",
            "of their business. So that is 10 × .5 = 5 hours a day. 5 hours a day × 7 days a week =\n",
            "35 hours a week. The answer is 35 hours a week.” This is an actual example from the\n",
            "paper!\n",
            "\n",
            "Not  only  does  the  model  give  the  right  answer  much  more  frequently  than  using\n",
            "regular  prompting—we’re  encouraging  the  model  to  think  things  through—but  it\n",
            "also  provides  all  the  reasoning  steps,  which  can  be  useful  to  better  understand  the\n",
            "rationale behind a model’s answer.\n",
            "\n",
            "Transformers have taken over NLP, but they didn’t stop there: they soon expanded to\n",
            "computer vision as well.\n",
            "\n",
            "Vision Transformers\n",
            "One of the first applications of attention mechanisms beyond NMT was in generating\n",
            "image captions using visual attention:38 a convolutional neural network first processes\n",
            "the  image  and  outputs  some  feature  maps,  then  a  decoder  RNN  equipped  with  an\n",
            "attention mechanism generates the caption, one word at a time. \n",
            "\n",
            "At each decoder time step (i.e., each word), the decoder uses the attention model to\n",
            "focus  on  just  the  right  part  of  the  image.  For  example,  in  Figure  16-12,  the  model\n",
            "generated the caption “A woman is throwing a frisbee in a park”, and you can see what\n",
            "part  of  the  input  image  the  decoder  focused  its  attention  on  when  it  was  about  to\n",
            "output the word “frisbee”: clearly, most of its attention was focused on the frisbee.\n",
            "\n",
            "38 Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, Proceedings\n",
            "\n",
            "of the 32nd International Conference on Machine Learning (2015): 2048–2057.\n",
            "\n",
            "624 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fFigure 16-12. Visual attention: an input image (left) and the model’s focus before\n",
            "producing the word “frisbee” (right)39\n",
            "\n",
            "Explainability\n",
            "One extra benefit of attention mechanisms is that they make it easier to understand\n",
            "what  led  the  model  to  produce  its  output.  This  is  called  explainability.  It  can  be\n",
            "especially useful when the model makes a mistake: for example, if an image of a dog\n",
            "walking in the snow is labeled as “a wolf walking in the snow”, then you can go back\n",
            "and check what the model focused on when it output the word “wolf ”. You may find\n",
            "that  it  was  paying  attention  not  only  to  the  dog,  but  also  to  the  snow,  hinting  at  a\n",
            "possible  explanation:  perhaps  the  way  the  model  learned  to  distinguish  dogs  from\n",
            "wolves  is  by  checking  whether  or  not  there’s  a  lot  of  snow  around.  You  can  then\n",
            "fix  this  by  training  the  model  with  more  images  of  wolves  without  snow,  and  dogs\n",
            "with snow. This example comes from a great 2016 paper40 by Marco Tulio Ribeiro et\n",
            "al.  that  uses  a  different  approach  to  explainability:  learning  an  interpretable  model\n",
            "locally around a classifier’s prediction.\n",
            "\n",
            "In  some  applications,  explainability  is  not  just  a  tool  to  debug  a  model;  it  can  be  a\n",
            "legal requirement—think of a system deciding whether or not it should grant you a\n",
            "loan.\n",
            "\n",
            "39 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.\n",
            "\n",
            "40 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier”, Proceed‐\n",
            "ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n",
            "1135–1144.\n",
            "\n",
            "Vision Transformers \n",
            "\n",
            "| \n",
            "\n",
            "625\n",
            "\n",
            "\fWhen  transformers  came  out  in  2017  and  people  started  to  experiment  with  them\n",
            "beyond NLP, they were first used alongside CNNs, without replacing them. Instead,\n",
            "transformers were generally used to replace RNNs, for example, in image captioning\n",
            "models.  Transformers  became  slightly  more  visual  in  a  2020  paper41  by  Facebook\n",
            "researchers,  which  proposed  a  hybrid  CNN–transformer  architecture  for  object\n",
            "detection.  Once  again,  the  CNN  first  processes  the  input  images  and  outputs  a  set\n",
            "of  feature  maps,  then  these  feature  maps  are  converted  to  sequences  and  fed  to  a\n",
            "transformer, which outputs bounding box predictions. But again, most of the visual\n",
            "work is still done by the CNN.\n",
            "\n",
            "Then,  in  October  2020,  a  team  of  Google  researchers  released  a  paper42  that  intro‐\n",
            "duced a fully transformer-based vision model, called a vision transformer (ViT). The\n",
            "idea is surprisingly simple: just chop the image into little 16 × 16 squares, and treat\n",
            "the  sequence  of  squares  as  if  it  were  a  sequence  of  word  representations.  To  be\n",
            "more  precise,  the  squares  are  first  flattened  into  16  ×  16  ×  3  =  768-dimensional\n",
            "vectors—the 3 is for the RGB color channels—then these vectors go through a linear\n",
            "layer  that  transforms  them  but  retains  their  dimensionality.  The  resulting  sequence\n",
            "of  vectors  can  then  be  treated  just  like  a  sequence  of  word  embeddings:  this  means\n",
            "adding  positional  embeddings,  and  passing  the  result  to  the  transformer.  That’s  it!\n",
            "This model beat the state of the art on ImageNet image classification, but to be fair\n",
            "the  authors  had  to  use  over  300  million  additional  images  for  training.  This  makes\n",
            "sense  since  transformers  don’t  have  as  many  inductive  biases  as  convolution  neural\n",
            "nets, so they need extra data just to learn things that CNNs implicitly assume.\n",
            "\n",
            "An  inductive  bias  is  an  implicit  assumption  made  by  the  model,\n",
            "due  to  its  architecture.  For  example,  linear  models  implicitly\n",
            "assume  that  the  data  is,  well,  linear.  CNNs  implicitly  assume  that\n",
            "patterns learned in one location will likely be useful in other loca‐\n",
            "tions as well. RNNs implicitly assume that the inputs are ordered,\n",
            "and  that  recent  tokens  are  more  important  than  older  ones.  The\n",
            "more  inductive  biases  a  model  has,  assuming  they  are  correct,\n",
            "the  less  training  data  the  model  will  require.  But  if  the  implicit\n",
            "assumptions are wrong, then the model may perform poorly even\n",
            "if it is trained on a large dataset.\n",
            "\n",
            "41 Nicolas Carion et al., “End-to-End Object Detection with Transformers”, arXiv preprint arxiv:2005.12872\n",
            "\n",
            "(2020).\n",
            "\n",
            "42 Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”,\n",
            "\n",
            "arXiv preprint arxiv:2010.11929 (2020).\n",
            "\n",
            "626 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fJust  two  months  later,  a  team  of  Facebook  researchers  released  a  paper43  that  intro‐\n",
            "duced  data-efficient  image  transformers  (DeiTs).  Their  model  achieved  competitive\n",
            "results on ImageNet without requiring any additional data for training. The model’s\n",
            "architecture  is  virtually  the  same  as  the  original  ViT,  but  the  authors  used  a  distil‐\n",
            "lation  technique  to  transfer  knowledge  from  state-of-the-art  CNN  models  to  their\n",
            "model.\n",
            "\n",
            "Then,  in  March  2021,  DeepMind  released  an  important  paper44  that  introduced  the\n",
            "Perceiver  architecture.  It  is  a  multimodal  transformer,  meaning  you  can  feed  it  text,\n",
            "images,  audio,  or  virtually  any  other  modality.  Until  then,  transformers  had  been\n",
            "restricted to fairly short sequences because of the performance and RAM bottleneck\n",
            "in the attention layers. This excluded modalities such as audio or video, and it forced\n",
            "researchers to treat images as sequences of patches, rather than sequences of pixels.\n",
            "The bottleneck is due to self-attention, where every token must attend to every other\n",
            "token:  if  the  input  sequence  has  M  tokens,  then  the  attention  layer  must  compute\n",
            "an  M  ×  M  matrix,  which  can  be  huge  if  M  is  very  large.  The  Perceiver  solves  this\n",
            "problem  by  gradually  improving  a  fairly  short  latent  representation  of  the  inputs,\n",
            "composed of N tokens—typically just a few hundred. (The word latent means hidden,\n",
            "or  internal.)  The  model  uses  cross-attention  layers  only,  feeding  them  the  latent\n",
            "representation as the queries, and the (possibly large) inputs as the values. This only\n",
            "requires computing an M × N matrix, so the computational complexity is linear with\n",
            "regard to M, instead of quadratic. After going through several cross-attention layers,\n",
            "if  everything  goes  well,  the  latent  representation  ends  up  capturing  everything  that\n",
            "matters  in  the  inputs.  The  authors  also  suggested  sharing  the  weights  between  con‐\n",
            "secutive cross-attention layers: if you do that, then the Perceiver effectively becomes\n",
            "an RNN. Indeed, the shared cross-attention layers can be seen as the same memory\n",
            "cell  at  different  time  steps,  and  the  latent  representation  corresponds  to  the  cell’s\n",
            "context vector. The same inputs are repeatedly fed to the memory cell at every time\n",
            "step. It looks like RNNs are not dead after all!\n",
            "\n",
            "Just  a  month  later,  Mathilde  Caron  et  al.  introduced  DINO,45  an  impressive  vision\n",
            "transformer  trained  entirely  without  labels,  using  self-supervision,  and  capable  of\n",
            "high-accuracy semantic segmentation. The model is duplicated during training, with\n",
            "one network acting as a teacher and the other acting as a student. Gradient descent\n",
            "only  affects  the  student,  while  the  teacher’s  weights  are  just  an  exponential  moving\n",
            "average of the student’s weights. The student is trained to match the teacher’s predic‐\n",
            "\n",
            "43 Hugo Touvron et al., “Training Data-Efficient Image Transformers & Distillation Through Attention”, arXiv\n",
            "\n",
            "preprint arxiv:2012.12877 (2020).\n",
            "\n",
            "44 Andrew Jaegle et al., “Perceiver: General Perception with Iterative Attention”, arXiv preprint arxiv:2103.03206\n",
            "\n",
            "(2021).\n",
            "\n",
            "45 Mathilde Caron et al., “Emerging Properties in Self-Supervised Vision Transformers”, arXiv preprint\n",
            "\n",
            "arxiv:2104.14294 (2021).\n",
            "\n",
            "Vision Transformers \n",
            "\n",
            "| \n",
            "\n",
            "627\n",
            "\n",
            "\ftions:  since  they’re  almost  the  same  model,  this  is  called  self-distillation.  At  each\n",
            "training  step,  the  input  images  are  augmented  in  different  ways  for  the  teacher  and\n",
            "the  student,  so  they  don’t  see  the  exact  same  image,  but  their  predictions  must\n",
            "match.  This  forces  them  to  come  up  with  high-level  representations.  To  prevent\n",
            "mode collapse, where both the student and the teacher would always output the same\n",
            "thing, completely ignoring the inputs, DINO keeps track of a moving average of the\n",
            "teacher’s outputs, and it tweaks the teacher’s predictions to ensure that they remain\n",
            "centered on zero, on average. DINO also forces the teacher to have high confidence in\n",
            "its predictions: this is called sharpening. Together, these techniques preserve diversity\n",
            "in the teacher’s outputs.\n",
            "\n",
            "In a 2021 paper,46 Google researchers showed how to scale ViTs up or down, depend‐\n",
            "ing on the amount of data. They managed to create a huge 2 billion parameter model\n",
            "that reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a\n",
            "scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only\n",
            "10,000 images: that’s just 10 images per class!\n",
            "\n",
            "And progress in visual transformers has continued steadily to this day. For example,\n",
            "in March 2022, a paper47 by Mitchell Wortsman et al. demonstrated that it’s possible\n",
            "to  first  train  multiple  transformers,  then  average  their  weights  to  create  a  new  and\n",
            "improved  model.  This  is  similar  to  an  ensemble  (see  Chapter  7),  except  there’s  just\n",
            "one model in the end, which means there’s no inference time penalty.\n",
            "\n",
            "The latest trend in transformers consists in building large multimodal models, often\n",
            "capable of zero-shot or few-shot learning. For example, OpenAI’s 2021 CLIP paper48\n",
            "proposed  a  large  transformer  model  pretrained  to  match  captions  with  images:  this\n",
            "task  allows  it  to  learn  excellent  image  representations,  and  the  model  can  then  be\n",
            "used directly for tasks such as image classification using simple text prompts such as\n",
            "“a  photo  of  a  cat”.  Soon  after,  OpenAI  announced  DALL·E,49  capable  of  generating\n",
            "amazing images based on text prompts. The DALL·E 2,50 which generates even higher\n",
            "quality images using a diffusion model (see Chapter 17).\n",
            "\n",
            "In  April  2022,  DeepMind  released  the  Flamingo  paper,51  which  introduced  a  family\n",
            "of models pretrained on a wide variety of tasks across multiple modalities, including\n",
            "text,  images,  and  videos.  A  single  model  can  be  used  across  very  different  tasks,\n",
            "\n",
            "46 Xiaohua Zhai et al., “Scaling Vision Transformers”, arXiv preprint arxiv:2106.04560v1 (2021).\n",
            "\n",
            "47 Mitchell Wortsman et al., “Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accu‐\n",
            "\n",
            "racy Without Increasing Inference Time”, arXiv preprint arxiv:2203.05482v1 (2022).\n",
            "\n",
            "48 Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision”, arXiv pre‐\n",
            "\n",
            "print arxiv:2103.00020 (2021).\n",
            "\n",
            "49 Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation”, arXiv preprint arxiv:2102.12092 (2021).\n",
            "\n",
            "50 Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP Latents”, arXiv preprint\n",
            "\n",
            "arxiv:2204.06125 (2022).\n",
            "\n",
            "628 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fsuch  as  question  answering,  image  captioning,  and  more.  Soon  after,  in  May  2022,\n",
            "DeepMind  introduced  GATO,52  a  multimodal  model  that  can  be  used  as  a  policy\n",
            "for a reinforcement learning agent (RL will be introduced in Chapter 18). The same\n",
            "transformer can chat with you, caption images, play Atari games, control (simulated)\n",
            "robotic  arms,  and  more,  all  with  “only”  1.2  billion  parameters.  And  the  adventure\n",
            "continues!\n",
            "\n",
            "These astounding advances have led some researchers to claim that\n",
            "human-level AI is near, that “scale is all you need”, and that some\n",
            "of these models may be “slightly conscious”. Others point out that\n",
            "despite the amazing progress, these models still lack the reliability\n",
            "and adaptability of human intelligence, our ability to reason sym‐\n",
            "bolically, to generalize based on a single example, and more.\n",
            "\n",
            "As you can see, transformers are everywhere! And the good news is that you gener‐\n",
            "ally  won’t  have  to  implement  transformers  yourself  since  many  excellent  pretrained\n",
            "models  are  readily  available  for  download  via  TensorFlow  Hub  or  Hugging  Face’s\n",
            "model hub. You’ve already seen how to use a model from TF Hub, so let’s close this\n",
            "chapter by taking a quick look at Hugging Face’s ecosystem.\n",
            "\n",
            "Hugging Face’s Transformers Library\n",
            "It’s  impossible  to  talk  about  transformers  today  without  mentioning  Hugging  Face,\n",
            "an AI company that has built a whole ecosystem of easy-to-use open source tools for\n",
            "NLP, vision, and beyond. The central component of their ecosystem is the Transform‐\n",
            "ers  library,  which  allows  you  to  easily  download  a  pretrained  model,  including  its\n",
            "corresponding tokenizer, and then fine-tune it on your own dataset, if needed. Plus,\n",
            "the library supports TensorFlow, PyTorch, and JAX (with the Flax library).\n",
            "\n",
            "The  simplest  way  to  use  the  Transformers  library  is  to  use  the  transformers.\n",
            "pipeline() function: you just specify which task you want, such as sentiment analy‐\n",
            "sis, and it downloads a default pretrained model, ready to be used—it really couldn’t\n",
            "be any simpler:\n",
            "\n",
            "51 Jean-Baptiste Alayrac et al., “Flamingo: a Visual Language Model for Few-Shot Learning”, arXiv preprint\n",
            "\n",
            "arxiv:2204.14198 (2022).\n",
            "\n",
            "52 Scott Reed et al., “A Generalist Agent”, arXiv preprint arxiv:2205.06175 (2022).\n",
            "\n",
            "Hugging Face’s Transformers Library \n",
            "\n",
            "| \n",
            "\n",
            "629\n",
            "\n",
            "\ffrom transformers import pipeline\n",
            "\n",
            "classifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\n",
            "result = classifier(\"The actors were very convincing\".)\n",
            "\n",
            "The result is a Python list containing one dictionary per input text:\n",
            "\n",
            ">>> result\n",
            "[{'label': 'POSITIVE', 'score': 0.9998071789741516}]\n",
            "\n",
            "In this example, the model correctly found that the sentence is positive, with around\n",
            "99.98% confidence. Of course, you can also pass a batch of sentences to the model:\n",
            "\n",
            ">>> classifier([\"I am from India.\", \"I am from Iraq.\"])\n",
            "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
            " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]\n",
            "\n",
            "Bias and Fairness\n",
            "As  the  output  suggests,  this  specific  classifier  loves  Indians,  but  is  severely  biased\n",
            "against Iraqis. You can try this code with your own country or city. Such an undesira‐\n",
            "ble bias generally comes in large part from the training data itself: in this case, there\n",
            "were plenty of negative sentences related to the wars in Iraq in the training data. This\n",
            "bias was then amplified during the fine-tuning process since the model was forced to\n",
            "choose between just two classes: positive or negative. If you add a neutral class when\n",
            "fine-tuning, then the country bias mostly disappears. But the training data is not the\n",
            "only source of bias: the model’s architecture, the type of loss or regularization used for\n",
            "training, the optimizer; all of these can affect what the model ends up learning. Even\n",
            "a mostly unbiased model can be used in a biased way, much like survey questions can\n",
            "be biased.\n",
            "\n",
            "Understanding bias in AI and mitigating its negative effects is still an area of active\n",
            "research,  but  one  thing  is  certain:  you  should  pause  and  think  before  you  rush  to\n",
            "deploy  a  model  to  production.  Ask  yourself  how  the  model  could  do  harm,  even\n",
            "indirectly. For example, if the model’s predictions are used to decide whether or not\n",
            "to  give  someone  a  loan,  the  process  should  be  fair.  So,  make  sure  you  evaluate  the\n",
            "model’s  performance  not  just  on  average  over  the  whole  test  set,  but  across  various\n",
            "subsets as well: for example, you may find that although the model works very well\n",
            "on average, its performance is abysmal for some categories of people. You may also\n",
            "want to run counterfactual tests: for example, you may want to check that the model’s\n",
            "predictions do not change when you simply switch someone’s gender.\n",
            "\n",
            "If  the  model  works  well  on  average,  it’s  tempting  to  push  it  to  production  and\n",
            "move  on  to  something  else,  especially  if  it’s  just  one  component  of  a  much  larger\n",
            "system. But in general, if you don’t fix such issues, no one else will, and your model\n",
            "may  end  up  doing  more  harm  than  good.  The  solution  depends  on  the  problem:  it\n",
            "may require rebalancing the dataset, fine-tuning on a different dataset, switching to\n",
            "another pretrained model, tweaking the model’s architecture or hyperparameters, etc.\n",
            "\n",
            "630 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fThe pipeline() function uses the default model for the given task. For example, for\n",
            "text classification tasks such as sentiment analysis, at the time of writing, it defaults to\n",
            "distilbert-base-uncased-finetuned-sst-2-english—a  DistilBERT  model  with\n",
            "an uncased tokenizer, trained on English Wikipedia and a corpus of English books,\n",
            "and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task. It’s also possible\n",
            "to  manually  specify  a  different  model.  For  example,  you  could  use  a  DistilBERT\n",
            "model  fine-tuned  on  the  Multi-Genre  Natural  Language  Inference  (MultiNLI)  task,\n",
            "which classifies two sentences into three classes: contradiction, neutral, or entailment.\n",
            "Here is how:\n",
            "\n",
            ">>> model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
            ">>> classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
            ">>> classifier_mnli(\"She loves me. [SEP] She loves me not.\")\n",
            "[{'label': 'contradiction', 'score': 0.9790192246437073}]\n",
            "\n",
            "You can find the available models at https://huggingface.co/models,\n",
            "and the list of tasks at https://huggingface.co/tasks.\n",
            "\n",
            "The pipeline API is very simple and convenient, but sometimes you will need more\n",
            "control. For such cases, the Transformers library provides many classes, including all\n",
            "sorts of tokenizers, models, configurations, callbacks, and much more. For example,\n",
            "let’s load the same DistilBERT model, along with its corresponding tokenizer, using\n",
            "the TFAutoModelForSequenceClassification and AutoTokenizer classes:\n",
            "\n",
            "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "\n",
            "Next,  let’s  tokenize  a  couple  of  pairs  of  sentences.  In  this  code,  we  activate  padding\n",
            "and specify that we want TensorFlow tensors instead of Python lists:\n",
            "\n",
            "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
            "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
            "                      padding=True, return_tensors=\"tf\")\n",
            "\n",
            "Instead  of  passing  \"Sentence  1  [SEP]  Sentence  2\"  to  the\n",
            "tokenizer,  you  can  equivalently  pass  it  a  tuple:  (\"Sentence  1\",\n",
            "\"Sentence 2\").\n",
            "\n",
            "Hugging Face’s Transformers Library \n",
            "\n",
            "| \n",
            "\n",
            "631\n",
            "\n",
            "\fThe output is a dictionary-like instance of the BatchEncoding class, which contains\n",
            "the sequences of token IDs, as well as a mask containing 0s for the padding tokens:\n",
            "\n",
            ">>> token_ids\n",
            "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
            "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
            "         102,    0,    0,    0],\n",
            "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
            "        2003, 2214, 1012,  102]], dtype=int32)>,\n",
            " 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
            "\n",
            "If you set return_token_type_ids=True when calling the tokenizer, you will also get\n",
            "an extra tensor that indicates which sentence each token belongs to. This is needed by\n",
            "some models, but not DistilBERT.\n",
            "\n",
            "Next,  we  can  directly  pass  this  BatchEncoding  object  to  the  model;  it  returns  a\n",
            "TFSequenceClassifierOutput object containing its predicted class logits:\n",
            "\n",
            ">>> outputs = model(token_ids)\n",
            ">>> outputs\n",
            "TFSequenceClassifierOutput(loss=None, logits=[<tf.Tensor: [...] numpy=\n",
            "array([[-2.1123817 ,  1.1786783 ,  1.4101017 ],\n",
            "       [-0.01478387,  1.0962474 , -0.9919954 ]], dtype=float32)>], [...])\n",
            "\n",
            "Lastly,  we  can  apply  the  softmax  activation  function  to  convert  these  logits  to  class\n",
            "probabilities,  and  use  the  argmax()  function  to  predict  the  class  with  the  highest\n",
            "probability for each input sentence pair:\n",
            "\n",
            ">>> Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
            ">>> Y_probas\n",
            "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "array([[0.01619702, 0.43523544, 0.5485676 ],\n",
            "       [0.08672056, 0.85204804, 0.06123142]], dtype=float32)>\n",
            ">>> Y_pred = tf.argmax(Y_probas, axis=1)\n",
            ">>> Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral\n",
            "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])>\n",
            "\n",
            "In  this  example,  the  model  correctly  classifies  the  first  sentence  pair  as  neutral  (the\n",
            "fact that I like soccer does not imply that everyone else does) and the second pair as\n",
            "an entailment (Joe must indeed be quite old).\n",
            "\n",
            "If  you  wish  to  fine-tune  this  model  on  your  own  dataset,  you  can  train  the  model\n",
            "as  usual  with  Keras  since  it’s  just  a  regular  Keras  model  with  a  few  extra  methods.\n",
            "However,  because  the  model  outputs  logits  instead  of  probabilities,  you  must  use\n",
            "the  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
            "loss\n",
            "instead of the usual \"sparse_categorical_crossentropy\" loss. Moreover, the model\n",
            "does  not  support  BatchEncoding  inputs  during  training,  so  you  must  use  its  data\n",
            "attribute to get a regular dictionary instead:\n",
            "\n",
            "632 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fsentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
            "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
            "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
            "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
            "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
            "history = model.fit(X_train, y_train, epochs=2)\n",
            "\n",
            "Hugging Face has also built a Datasets library that you can use to easily download a\n",
            "standard dataset (such as IMDb) or a custom one, and use it to fine-tune your model.\n",
            "It’s  similar  to  TensorFlow  Datasets,  but  it  also  provides  tools  to  perform  common\n",
            "preprocessing  tasks  on  the  fly,  such  as  masking.  The  list  of  datasets  is  available  at\n",
            "https://huggingface.co/datasets.\n",
            "\n",
            "This should get you started with Hugging Face’s ecosystem. To learn more, you can\n",
            "head over to https://huggingface.co/docs for the documentation, which includes many\n",
            "tutorial notebooks, videos, the full API, and more. I also recommend you check out\n",
            "the O’Reilly book Natural Language Processing with Transformers: Building Language\n",
            "Applications with Hugging Face by Lewis Tunstall, Leandro von Werra, and Thomas\n",
            "Wolf—all from the Hugging Face team.\n",
            "\n",
            "In the next chapter we will discuss how to learn deep representations in an unsuper‐\n",
            "vised  way  using  autoencoders,  and  we  will  use  generative  adversarial  networks  to\n",
            "produce images and more!\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
            "\n",
            "2.\n",
            "2. Why  do  people  use  encoder–decoder  RNNs  rather  than  plain  sequence-to-\n",
            "\n",
            "sequence RNNs for automatic translation?\n",
            "\n",
            "3.\n",
            "3. How  can  you  deal  with  variable-length  input  sequences?  What  about  variable-\n",
            "\n",
            "length output sequences?\n",
            "\n",
            "4. What  is  beam  search,  and  why  would  you  use  it?  What  tool  can  you  use  to\n",
            "4.\n",
            "\n",
            "implement it?\n",
            "\n",
            "5.\n",
            "5. What is an attention mechanism? How does it help?\n",
            "\n",
            "6.\n",
            "6. What  is  the  most  important  layer  in  the  transformer  architecture?  What  is  its\n",
            "\n",
            "purpose?\n",
            "\n",
            "7. When would you need to use sampled softmax?\n",
            "7.\n",
            "\n",
            "8. Embedded  Reber  grammars  were  used  by  Hochreiter  and  Schmidhuber  in  their\n",
            "8.\n",
            "paper  about  LSTMs.  They  are  artificial  grammars  that  produce  strings  such  as\n",
            "“BPBTSXXVPSEPE”. Check out Jenny Orr’s nice introduction to this topic, then\n",
            "choose  a  particular  embedded  Reber  grammar  (such  as  the  one  represented\n",
            "on  Orr’s  page),  then  train  an  RNN  to  identify  whether  a  string  respects  that\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "633\n",
            "\n",
            "\fgrammar  or  not.  You  will  first  need  to  write  a  function  capable  of  generating  a\n",
            "training batch containing about 50% strings that respect the grammar, and 50%\n",
            "that don’t.\n",
            "\n",
            "9.\n",
            "9. Train an encoder–decoder model that can convert a date string from one format\n",
            "\n",
            "to another (e.g., from “April 22, 2019” to “2019-04-22”).\n",
            "\n",
            "10. Go  through  the  example  on  the  Keras  website  for  “Natural  language  image\n",
            "10.\n",
            "search  with  a  Dual  Encoder”.  You  will  learn  how  to  build  a  model  capable  of\n",
            "representing both images and text within the same embedding space. This makes\n",
            "it  possible  to  search  for  images  using  a  text  prompt,  like  in  the  CLIP  model  by\n",
            "OpenAI.\n",
            "\n",
            "11.\n",
            "11. Use  the  Hugging  Face  Transformers  library  to  download  a  pretrained  language\n",
            "model capable of generating text (e.g., GPT), and try generating more convincing\n",
            "Shakespearean  text.  You  will  need  to  use  the  model’s  generate()  method—see\n",
            "Hugging Face’s documentation for more details.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "634 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 16: Natural Language Processing with RNNs and Attention\n",
            "\n",
            "\fCHAPTER 17\n",
            "Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "Autoencoders are artificial neural networks capable of learning dense representations\n",
            "of the input data, called latent representations or codings, without any supervision (i.e.,\n",
            "the training set is unlabeled). These codings typically have a much lower dimension‐\n",
            "ality  than  the  input  data,  making  autoencoders  useful  for  dimensionality  reduction\n",
            "(see Chapter 8), especially for visualization purposes. Autoencoders also act as feature\n",
            "detectors, and they can be used for unsupervised pretraining of deep neural networks\n",
            "(as we discussed in Chapter 11). Lastly, some autoencoders are generative models: they\n",
            "are capable of randomly generating new data that looks very similar to the training\n",
            "data. For example, you could train an autoencoder on pictures of faces, and it would\n",
            "then be able to generate new faces.\n",
            "\n",
            "Generative  adversarial  networks  (GANs)  are  also  neural  nets  capable  of  generating\n",
            "data.  In  fact,  they  can  generate  pictures  of  faces  so  convincing  that  it  is  hard  to\n",
            "believe  the  people  they  represent  do  not  exist.  You  can  judge  so  for  yourself  by\n",
            "visiting  https://thispersondoesnotexist.com,  a  website  that  shows  faces  generated  by\n",
            "a GAN architecture called StyleGAN. You can also check out https://thisrentaldoesno\n",
            "texist.com  to  see  some  generated  Airbnb  listings.  GANs  are  now  widely  used  for\n",
            "super resolution (increasing the resolution of an image), colorization, powerful image\n",
            "editing  (e.g.,  replacing  photo  bombers  with  realistic  background),  turning  simple\n",
            "sketches into photorealistic images, predicting the next frames in a video, augmenting\n",
            "a dataset (to train other models), generating other types of data (such as text, audio,\n",
            "and time series), identifying the weaknesses in other models to strengthen them, and\n",
            "more.\n",
            "\n",
            "A more recent addition to the generative learning party is diffusion models. In 2021,\n",
            "they managed to generate more diverse and higher-quality images than GANs, while\n",
            "also being much easier to train. However, diffusion models are much slower to run.\n",
            "\n",
            "635\n",
            "\n",
            "\fAutoencoders, GANs, and diffusion models are all unsupervised, they all learn latent\n",
            "representations,  they  can  all  be  used  as  generative  models,  and  they  have  many\n",
            "similar applications. However, they work very differently:\n",
            "\n",
            "• Autoencoders simply learn to copy their inputs to their outputs. This may sound\n",
            "•\n",
            "like a trivial task, but as you will see, constraining the network in various ways\n",
            "can make it rather difficult. For example, you can limit the size of the latent rep‐\n",
            "resentations, or you can add noise to the inputs and train the network to recover\n",
            "the  original  inputs.  These  constraints  prevent  the  autoencoder  from  trivially\n",
            "copying the inputs directly to the outputs, which forces it to learn efficient ways\n",
            "of representing the data. In short, the codings are byproducts of the autoencoder\n",
            "learning the identity function under some constraints.\n",
            "\n",
            "• GANs  are  composed  of  two  neural  networks:  a  generator  that  tries  to  generate\n",
            "•\n",
            "data  that  looks  similar  to  the  training  data,  and  a  discriminator  that  tries  to  tell\n",
            "real data from fake data. This architecture is very original in deep learning in that\n",
            "the generator and the discriminator compete against each other during training:\n",
            "the generator is often compared to a criminal trying to make realistic counterfeit\n",
            "money,  while  the  discriminator  is  like  the  police  investigator  trying  to  tell  real\n",
            "money  from  fake.  Adversarial  training  (training  competing  neural  networks)  is\n",
            "widely considered one of the most important innovations of the 2010s. In 2016,\n",
            "Yann LeCun even said that it was “the most interesting idea in the last 10 years in\n",
            "machine learning”.\n",
            "\n",
            "•\n",
            "• A denoising diffusion probabilistic model (DDPM) is trained to remove a tiny bit\n",
            "of noise from an image. If you then take an image entirely full of Gaussian noise\n",
            "and repeatedly run the diffusion model on that image, a high-quality image will\n",
            "gradually emerge, similar to the training images (but not identical).\n",
            "\n",
            "In this chapter we will start by exploring in more depth how autoencoders work and\n",
            "how to use them for dimensionality reduction, feature extraction, unsupervised pre‐\n",
            "training, or as generative models. This will naturally lead us to GANs. We will build\n",
            "a  simple  GAN  to  generate  fake  images,  but  we  will  see  that  training  is  often  quite\n",
            "difficult.  We  will  discuss  the  main  difficulties  you  will  encounter  with  adversarial\n",
            "training,  as  well  as  some  of  the  main  techniques  to  work  around  these  difficulties.\n",
            "And lastly, we will build and train a DDPM and use it to generate images. Let’s start\n",
            "with autoencoders!\n",
            "\n",
            "636 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fEfficient Data Representations\n",
            "Which of the following number sequences do you find the easiest to memorize?\n",
            "\n",
            "•\n",
            "• 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\n",
            "\n",
            "•\n",
            "• 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\n",
            "\n",
            "At first glance, it would seem that the first sequence should be easier, since it is much\n",
            "shorter. However, if you look carefully at the second sequence, you will notice that it\n",
            "is just the list of even numbers from 50 down to 14. Once you notice this pattern, the\n",
            "second sequence becomes much easier to memorize than the first because you only\n",
            "need  to  remember  the  pattern  (i.e.,  decreasing  even  numbers)  and  the  starting  and\n",
            "ending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize\n",
            "very  long  sequences,  you  would  not  care  much  about  the  existence  of  a  pattern  in\n",
            "the  second  sequence.  You  would  just  learn  every  number  by  heart,  and  that  would\n",
            "be that. The fact that it is hard to memorize long sequences is what makes it useful\n",
            "to  recognize  patterns,  and  hopefully  this  clarifies  why  constraining  an  autoencoder\n",
            "during training pushes it to discover and exploit patterns in the data.\n",
            "\n",
            "The  relationship  between  memory,  perception,  and  pattern  matching  was  famously\n",
            "studied by William Chase and Herbert Simon1 in the early 1970s. They observed that\n",
            "expert chess players were able to memorize the positions of all the pieces in a game\n",
            "by  looking  at  the  board  for  just  five  seconds,  a  task  that  most  people  would  find\n",
            "impossible. However, this was only the case when the pieces were placed in realistic\n",
            "positions  (from  actual  games),  not  when  the  pieces  were  placed  randomly.  Chess\n",
            "experts don’t have a much better memory than you and I; they just see chess patterns\n",
            "more easily, thanks to their experience with the game. Noticing patterns helps them\n",
            "store information efficiently.\n",
            "\n",
            "Just  like  the  chess  players  in  this  memory  experiment,  an  autoencoder  looks  at\n",
            "the  inputs,  converts  them  to  an  efficient  latent  representation,  and  then  spits  out\n",
            "something that (hopefully) looks very close to the inputs. An autoencoder is always\n",
            "composed of two parts: an encoder (or recognition network) that converts the inputs to\n",
            "a latent representation, followed by a decoder (or generative network) that converts the\n",
            "internal representation to the outputs (see Figure 17-1).\n",
            "\n",
            "1 William G. Chase and Herbert A. Simon, “Perception in Chess”, Cognitive Psychology 4, no. 1 (1973): 55–81.\n",
            "\n",
            "Efficient Data Representations \n",
            "\n",
            "| \n",
            "\n",
            "637\n",
            "\n",
            "\fFigure 17-1. The chess memory experiment (left) and a simple autoencoder (right)\n",
            "\n",
            "As  you  can  see,  an  autoencoder  typically  has  the  same  architecture  as  a  multilayer\n",
            "perceptron (MLP; see Chapter 10), except that the number of neurons in the output\n",
            "layer must be equal to the number of inputs. In this example, there is just one hidden\n",
            "layer  composed  of  two  neurons  (the  encoder),  and  one  output  layer  composed  of\n",
            "three neurons (the decoder). The outputs are often called the reconstructions because\n",
            "the  autoencoder  tries  to  reconstruct  the  inputs.  The  cost  function  contains  a  recon‐\n",
            "struction loss that penalizes the model when the reconstructions are different from the\n",
            "inputs.\n",
            "\n",
            "Because the internal representation has a lower dimensionality than the input data (it\n",
            "is 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete\n",
            "autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to\n",
            "output  a  copy  of  its  inputs.  It  is  forced  to  learn  the  most  important  features  in  the\n",
            "input data (and drop the unimportant ones).\n",
            "\n",
            "Let’s see how to implement a very simple undercomplete autoencoder for dimension‐\n",
            "ality reduction.\n",
            "\n",
            "638 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fPerforming PCA with an Undercomplete Linear\n",
            "Autoencoder\n",
            "If  the  autoencoder  uses  only  linear  activations  and  the  cost  function  is  the  mean\n",
            "squared  error  (MSE),  then  it  ends  up  performing  principal  component  analysis\n",
            "(PCA; see Chapter 8).\n",
            "\n",
            "The  following  code  builds  a  simple  linear  autoencoder  to  perform  PCA  on  a  3D\n",
            "dataset, projecting it to 2D:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "encoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\n",
            "decoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\n",
            "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
            "\n",
            "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
            "autoencoder.compile(loss=\"mse\", optimizer=optimizer)\n",
            "\n",
            "This code is really not very different from all the MLPs we built in past chapters, but\n",
            "there are a few things to note:\n",
            "\n",
            "•\n",
            "• We  organized  the  autoencoder  into  two  subcomponents:  the  encoder  and  the\n",
            "decoder. Both are regular Sequential models with a single Dense layer each, and\n",
            "the autoencoder is a Sequential model containing the encoder followed by the\n",
            "decoder (remember that a model can be used as a layer in another model).\n",
            "\n",
            "•\n",
            "• The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).\n",
            "\n",
            "• To  perform  PCA,  we  do  not  use  any  activation  function  (i.e.,  all  neurons  are\n",
            "•\n",
            "linear), and the cost function is the MSE. That’s because PCA is a linear transfor‐\n",
            "mation. We will see more complex and nonlinear autoencoders shortly.\n",
            "\n",
            "Now  let’s  train  the  model  on  the  same  simple  generated  3D  dataset  we  used  in\n",
            "Chapter 8 and use it to encode that dataset (i.e., project it to 2D):\n",
            "\n",
            "X_train = [...]  # generate a 3D dataset, like in Chapter 8\n",
            "history = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\n",
            "codings = encoder.predict(X_train)\n",
            "\n",
            "Note that X_train is used as both the inputs and the targets. Figure 17-2 shows the\n",
            "original 3D dataset (on the left) and the output of the autoencoder’s hidden layer (i.e.,\n",
            "the  coding  layer,  on  the  right).  As  you  can  see,  the  autoencoder  found  the  best  2D\n",
            "plane  to  project  the  data  onto,  preserving  as  much  variance  in  the  data  as  it  could\n",
            "(just like PCA).\n",
            "\n",
            "Performing PCA with an Undercomplete Linear Autoencoder \n",
            "\n",
            "| \n",
            "\n",
            "639\n",
            "\n",
            "\fFigure 17-2. Approximate PCA performed by an undercomplete linear autoencoder\n",
            "\n",
            "You  can  think  of  an  autoencoder  as  performing  a  form  of  self-\n",
            "supervised learning, since it is based on a supervised learning tech‐\n",
            "nique with automatically generated labels (in this case simply equal\n",
            "to the inputs).\n",
            "\n",
            "Stacked Autoencoders\n",
            "Just  like  other  neural  networks  we  have  discussed,  autoencoders  can  have  multiple\n",
            "hidden layers. In this case they are called stacked autoencoders (or deep autoencoders).\n",
            "Adding  more  layers  helps  the  autoencoder  learn  more  complex  codings.  That  said,\n",
            "one must be careful not to make the autoencoder too powerful. Imagine an encoder\n",
            "so powerful that it just learns to map each input to a single arbitrary number (and the\n",
            "decoder learns the reverse mapping). Obviously such an autoencoder will reconstruct\n",
            "the training data perfectly, but it will not have learned any useful data representation\n",
            "in the process, and it is unlikely to generalize well to new instances.\n",
            "\n",
            "The architecture of a stacked autoencoder is typically symmetrical with regard to the\n",
            "central hidden layer (the coding layer). To put it simply, it looks like a sandwich. For\n",
            "example,  an  autoencoder  for  Fashion  MNIST  (introduced  in  Chapter  10)  may  have\n",
            "784 inputs, followed by a hidden layer with 100 neurons, then a central hidden layer\n",
            "of 30 neurons, then another hidden layer with 100 neurons, and an output layer with\n",
            "784 neurons. This stacked autoencoder is represented in Figure 17-3.\n",
            "\n",
            "640 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fFigure 17-3. Stacked autoencoder\n",
            "\n",
            "Implementing a Stacked Autoencoder Using Keras\n",
            "You can implement a stacked autoencoder very much like a regular deep MLP:\n",
            "\n",
            "stacked_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
            "])\n",
            "stacked_decoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(28 * 28),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
            "\n",
            "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
            "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
            "                         validation_data=(X_valid, X_valid))\n",
            "\n",
            "Stacked Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "641\n",
            "\n",
            "\fLet’s go through this code:\n",
            "\n",
            "•\n",
            "• Just like earlier, we split the autoencoder model into two submodels: the encoder\n",
            "\n",
            "and the decoder.\n",
            "\n",
            "• The  encoder  takes  28  ×  28–pixel  grayscale  images,  flattens  them  so  that  each\n",
            "•\n",
            "image is represented as a vector of size 784, then processes these vectors through\n",
            "two  Dense  layers  of  diminishing  sizes  (100  units  then  30  units),  both  using  the\n",
            "ReLU activation function. For each input image, the encoder outputs a vector of\n",
            "size 30.\n",
            "\n",
            "• The decoder takes codings of size 30 (output by the encoder) and processes them\n",
            "•\n",
            "through  two  Dense  layers  of  increasing  sizes  (100  units  then  784  units),  and  it\n",
            "reshapes  the  final  vectors  into  28  ×  28  arrays  so  the  decoder’s  outputs  have  the\n",
            "same shape as the encoder’s inputs.\n",
            "\n",
            "•\n",
            "• When  compiling  the  stacked  autoencoder,  we  use  the  MSE  loss  and  Nadam\n",
            "\n",
            "optimization.\n",
            "\n",
            "• Finally,  we  train  the  model  using  X_train  as  both  the  inputs  and  the  targets.\n",
            "•\n",
            "\n",
            "Similarly, we use X_valid as both the validation inputs and targets.\n",
            "\n",
            "Visualizing the Reconstructions\n",
            "One way to ensure that an autoencoder is properly trained is to compare the inputs\n",
            "and the outputs: the differences should not be too significant. Let’s plot a few images\n",
            "from the validation set, as well as their reconstructions:\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
            "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
            "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
            "    for image_index in range(n_images):\n",
            "        plt.subplot(2, n_images, 1 + image_index)\n",
            "        plt.imshow(images[image_index], cmap=\"binary\")\n",
            "        plt.axis(\"off\")\n",
            "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
            "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
            "        plt.axis(\"off\")\n",
            "\n",
            "plot_reconstructions(stacked_ae)\n",
            "plt.show()\n",
            "\n",
            "Figure 17-4 shows the resulting images.\n",
            "\n",
            "642 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fFigure 17-4. Original images (top) and their reconstructions (bottom)\n",
            "\n",
            "The  reconstructions  are  recognizable,  but  a  bit  too  lossy.  We  may  need  to  train  the\n",
            "model  for  longer,  or  make  the  encoder  and  decoder  deeper,  or  make  the  codings\n",
            "larger.  But  if  we  make  the  network  too  powerful,  it  will  manage  to  make  perfect\n",
            "reconstructions without having learned any useful patterns in the data. For now, let’s\n",
            "go with this model.\n",
            "\n",
            "Visualizing the Fashion MNIST Dataset\n",
            "Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s\n",
            "dimensionality. For visualization, this does not give great results compared to other\n",
            "dimensionality reduction algorithms (such as those we discussed in Chapter 8), but\n",
            "one big advantage of autoencoders is that they can handle large datasets with many\n",
            "instances  and  many  features.  So,  one  strategy  is  to  use  an  autoencoder  to  reduce\n",
            "the  dimensionality  down  to  a  reasonable  level,  then  use  another  dimensionality\n",
            "reduction  algorithm  for  visualization.  Let’s  use  this  strategy  to  visualize  Fashion\n",
            "MNIST.  First  we’ll  use  the  encoder  from  our  stacked  autoencoder  to  reduce  the\n",
            "dimensionality down to 30, then we’ll use Scikit-Learn’s implementation of the t-SNE\n",
            "algorithm to reduce the dimensionality down to 2 for visualization:\n",
            "\n",
            "from sklearn.manifold import TSNE\n",
            "\n",
            "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
            "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
            "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n",
            "\n",
            "Now we can plot the dataset:\n",
            "\n",
            "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
            "plt.show()\n",
            "\n",
            "Figure  17-5  shows  the  resulting  scatterplot,  beautified  a  bit  by  displaying  some  of\n",
            "the  images.  The  t-SNE  algorithm  identified  several  clusters  that  match  the  classes\n",
            "reasonably well (each class is represented by a different color).\n",
            "\n",
            "Stacked Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "643\n",
            "\n",
            "\fFigure 17-5. Fashion MNIST visualization using an autoencoder followed by t-SNE\n",
            "\n",
            "So, autoencoders can be used for dimensionality reduction. Another application is for\n",
            "unsupervised pretraining.\n",
            "\n",
            "Unsupervised Pretraining Using Stacked Autoencoders\n",
            "As we discussed in Chapter 11, if you are tackling a complex supervised task but you\n",
            "do not have a lot of labeled training data, one solution is to find a neural network that\n",
            "performs  a  similar  task  and  reuse  its  lower  layers.  This  makes  it  possible  to  train  a\n",
            "high-performance model using little training data because your neural network won’t\n",
            "have to learn all the low-level features; it will just reuse the feature detectors learned\n",
            "by the existing network.\n",
            "\n",
            "Similarly,  if  you  have  a  large  dataset  but  most  of  it  is  unlabeled,  you  can  first  train\n",
            "a  stacked  autoencoder  using  all  the  data,  then  reuse  the  lower  layers  to  create  a\n",
            "neural network for your actual task and train it using the labeled data. For example,\n",
            "Figure  17-6  shows  how  to  use  a  stacked  autoencoder  to  perform  unsupervised  pre‐\n",
            "training for a classification neural network. When training the classifier, if you really\n",
            "\n",
            "644 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fdon’t have much labeled training data, you may want to freeze the pretrained layers\n",
            "(at least the lower ones).\n",
            "\n",
            "Figure 17-6. Unsupervised pretraining using autoencoders\n",
            "\n",
            "Having  plenty  of  unlabeled  data  and  little  labeled  data  is  com‐\n",
            "mon.  Building  a  large  unlabeled  dataset  is  often  cheap  (e.g.,  a\n",
            "simple  script  can  download  millions  of  images  off  the  internet),\n",
            "but  labeling  those  images  (e.g.,  classifying  them  as  cute  or  not)\n",
            "can  usually  be  done  reliably  only  by  humans.  Labeling  instances\n",
            "is  time-consuming  and  costly,  so  it’s  normal  to  have  only  a  few\n",
            "thousand human-labeled instances, or even less.\n",
            "\n",
            "There  is  nothing  special  about  the  implementation:  just  train  an  autoencoder  using\n",
            "all the training data (labeled plus unlabeled), then reuse its encoder layers to create a\n",
            "new neural network (see the exercises at the end of this chapter for an example).\n",
            "\n",
            "Next, let’s look at a few techniques for training stacked autoencoders.\n",
            "\n",
            "Tying Weights\n",
            "When  an  autoencoder  is  neatly  symmetrical,  like  the  one  we  just  built,  a  common\n",
            "technique  is  to  tie  the  weights  of  the  decoder  layers  to  the  weights  of  the  encoder\n",
            "layers.  This  halves  the  number  of  weights  in  the  model,  speeding  up  training  and\n",
            "limiting the risk of overfitting. Specifically, if the autoencoder has a total of N layers\n",
            "(not counting the input layer), and WL represents the connection weights of the Lth\n",
            "layer (e.g., layer 1 is the first hidden layer, layer N/2 is the coding layer, and layer N\n",
            "⊺\n",
            "is the output layer), then the decoder layer weights can be defined as WL = WN–L+1\n",
            "(with L = N / 2 + 1, …, N).\n",
            "\n",
            "Stacked Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "645\n",
            "\n",
            "\fTo tie weights between layers using Keras, let’s define a custom layer:\n",
            "\n",
            "class DenseTranspose(tf.keras.layers.Layer):\n",
            "    def __init__(self, dense, activation=None, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.dense = dense\n",
            "        self.activation = tf.keras.activations.get(activation)\n",
            "\n",
            "    def build(self, batch_input_shape):\n",
            "        self.biases = self.add_weight(name=\"bias\",\n",
            "                                      shape=self.dense.input_shape[-1],\n",
            "                                      initializer=\"zeros\")\n",
            "        super().build(batch_input_shape)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
            "        return self.activation(Z + self.biases)\n",
            "\n",
            "This  custom  layer  acts  like  a  regular  Dense  layer,  but  it  uses  another  Dense  layer’s\n",
            "weights, transposed (setting transpose_b=True is equivalent to transposing the sec‐\n",
            "ond argument, but it’s more efficient as it performs the transposition on the fly within\n",
            "the  matmul()  operation).  However,  it  uses  its  own  bias  vector.  Now  we  can  build  a\n",
            "new  stacked  autoencoder,  much  like  the  previous  one  but  with  the  decoder’s  Dense\n",
            "layers tied to the encoder’s Dense layers:\n",
            "\n",
            "dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
            "dense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
            "\n",
            "tied_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    dense_1,\n",
            "    dense_2\n",
            "])\n",
            "\n",
            "tied_decoder = tf.keras.Sequential([\n",
            "    DenseTranspose(dense_2, activation=\"relu\"),\n",
            "    DenseTranspose(dense_1),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "\n",
            "tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])\n",
            "\n",
            "This  model  achieves  roughly  the  same  reconstruction  error  as  the  previous  model,\n",
            "using almost half the number of parameters.\n",
            "\n",
            "Training One Autoencoder at a Time\n",
            "Rather  than  training  the  whole  stacked  autoencoder  in  one  go  like  we  just  did,\n",
            "it  is  possible  to  train  one  shallow  autoencoder  at  a  time,  then  stack  all  of  them\n",
            "into  a  single  stacked  autoencoder  (hence  the  name),  as  shown  in  Figure  17-7.  This\n",
            "\n",
            "646 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\ftechnique is not used so much these days, but you may still run into papers that talk\n",
            "about “greedy layerwise training”, so it’s good to know what it means.\n",
            "\n",
            "Figure 17-7. Training one autoencoder at a time\n",
            "\n",
            "During  the  first  phase  of  training,  the  first  autoencoder  learns  to  reconstruct  the\n",
            "inputs.  Then  we  encode  the  whole  training  set  using  this  first  autoencoder,  and\n",
            "this  gives  us  a  new  (compressed)  training  set.  We  then  train  a  second  autoencoder\n",
            "on  this  new  dataset.  This  is  the  second  phase  of  training.  Finally,  we  build  a  big\n",
            "sandwich  using  all  these  autoencoders,  as  shown  in  Figure  17-7  (i.e.,  we  first  stack\n",
            "the hidden layers of each autoencoder, then the output layers in reverse order). This\n",
            "gives us the final stacked autoencoder (see the “Training One Autoencoder at a Time”\n",
            "section in the chapter’s notebook for an implementation). We could easily train more\n",
            "autoencoders this way, building a very deep stacked autoencoder.\n",
            "\n",
            "As  I  mentioned  earlier,  one  of  the  triggers  of  the  deep  learning  tsunami  was  the\n",
            "discovery  in  2006  by  Geoffrey  Hinton  et  al.  that  deep  neural  networks  can  be\n",
            "pretrained  in  an  unsupervised  fashion,  using  this  greedy  layerwise  approach.  They\n",
            "used restricted Boltzmann machines (RBMs; see https://homl.info/extra-anns) for this\n",
            "purpose, but in 2007 Yoshua Bengio et al.2 showed that autoencoders worked just as\n",
            "well. For several years this was the only efficient way to train deep nets, until many of\n",
            "the techniques introduced in Chapter 11 made it possible to just train a deep net in\n",
            "one shot.\n",
            "\n",
            "2 Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks”, Proceedings of the 19th International\n",
            "\n",
            "Conference on Neural Information Processing Systems (2006): 153–160.\n",
            "\n",
            "Stacked Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "647\n",
            "\n",
            "\fAutoencoders  are  not  limited  to  dense  networks:  you  can  also  build  convolutional\n",
            "autoencoders. Let’s look at these now.\n",
            "\n",
            "Convolutional Autoencoders\n",
            "If  you  are  dealing  with  images,  then  the  autoencoders  we  have  seen  so  far  will  not\n",
            "work well (unless the images are very small): as you saw in Chapter 14, convolutional\n",
            "neural  networks  are  far  better  suited  than  dense  networks  to  working  with  images.\n",
            "So if you want to build an autoencoder for images (e.g., for unsupervised pretraining\n",
            "or  dimensionality  reduction),  you  will  need  to  build  a  convolutional  autoencoder.3\n",
            "The encoder is a regular CNN composed of convolutional layers and pooling layers.\n",
            "It  typically  reduces  the  spatial  dimensionality  of  the  inputs  (i.e.,  height  and  width)\n",
            "while increasing the depth (i.e., the number of feature maps). The decoder must do\n",
            "the reverse (upscale the image and reduce its depth back to the original dimensions),\n",
            "and  for  this  you  can  use  transpose  convolutional  layers  (alternatively,  you  could\n",
            "combine upsampling layers with convolutional layers). Here is a basic convolutional\n",
            "autoencoder for Fashion MNIST:\n",
            "\n",
            "conv_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Reshape([28, 28, 1]),\n",
            "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
            "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
            "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
            "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\n",
            "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
            "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
            "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
            "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
            "])\n",
            "conv_decoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(3 * 3 * 16),\n",
            "    tf.keras.layers.Reshape((3, 3, 16)),\n",
            "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
            "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
            "                                    activation=\"relu\"),\n",
            "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
            "\n",
            "3 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction”, Proceedings\n",
            "\n",
            "of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59.\n",
            "\n",
            "648 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fIt’s also possible to create autoencoders with other architecture types, such as RNNs\n",
            "(see the notebook for an example).\n",
            "\n",
            "OK, let’s step back for a second. So far we have looked at various kinds of autoencod‐\n",
            "ers  (basic,  stacked,  and  convolutional),  and  how  to  train  them  (either  in  one  shot\n",
            "or layer by layer). We also looked at a couple of applications: data visualization and\n",
            "unsupervised pretraining.\n",
            "\n",
            "Up  to  now,  in  order  to  force  the  autoencoder  to  learn  interesting  features,  we  have\n",
            "limited  the  size  of  the  coding  layer,  making  it  undercomplete.  There  are  actually\n",
            "many other kinds of constraints that can be used, including ones that allow the cod‐\n",
            "ing layer to be just as large as the inputs, or even larger, resulting in an overcomplete\n",
            "autoencoder. Then, in the following sections we’ll look at a few more kinds of autoen‐\n",
            "coders: denoising autoencoders, sparse autoencoders, and variational autoencoders.\n",
            "\n",
            "Denoising Autoencoders\n",
            "Another  way  to  force  the  autoencoder  to  learn  useful  features  is  to  add  noise  to  its\n",
            "inputs, training it to recover the original, noise-free inputs. This idea has been around\n",
            "since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008\n",
            "paper,4 Pascal Vincent et al. showed that autoencoders could also be used for feature\n",
            "extraction. In a 2010 paper,5 Vincent et al. introduced stacked denoising autoencoders.\n",
            "\n",
            "The  noise  can  be  pure  Gaussian  noise  added  to  the  inputs,  or  it  can  be  randomly\n",
            "switched-off  inputs,  just  like  in  dropout  (introduced  in  Chapter  11).  Figure  17-8\n",
            "shows both options.\n",
            "\n",
            "The  implementation  is  straightforward:  it  is  a  regular  stacked  autoencoder  with\n",
            "an  additional  Dropout  layer  applied  to  the  encoder’s  inputs  (or  you  could  use  a\n",
            "GaussianNoise  layer  instead).  Recall  that  the  Dropout  layer  is  only  active  during\n",
            "training (and so is the GaussianNoise layer):\n",
            "\n",
            "4 Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders”, Proceedings\n",
            "\n",
            "of the 25th International Conference on Machine Learning (2008): 1096–1103.\n",
            "\n",
            "5 Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network\n",
            "\n",
            "with a Local Denoising Criterion”, Journal of Machine Learning Research 11 (2010): 3371–3408.\n",
            "\n",
            "Denoising Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "649\n",
            "\n",
            "\fdropout_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dropout(0.5),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
            "])\n",
            "dropout_decoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(28 * 28),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n",
            "\n",
            "Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right)\n",
            "\n",
            "Figure  17-9  shows  a  few  noisy  images  (with  half  the  pixels  turned  off),  and  the\n",
            "images  reconstructed  by  the  dropout-based  denoising  autoencoder.  Notice  how  the\n",
            "autoencoder  guesses  details  that  are  actually  not  in  the  input,  such  as  the  top  of\n",
            "the white shirt (bottom row, fourth image). As you can see, not only can denoising\n",
            "autoencoders  be  used  for  data  visualization  or  unsupervised  pretraining,  like  the\n",
            "other autoencoders we’ve discussed so far, but they can also be used quite simply and\n",
            "efficiently to remove noise from images.\n",
            "\n",
            "650 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fFigure 17-9. Noisy images (top) and their reconstructions (bottom)\n",
            "\n",
            "Sparse Autoencoders\n",
            "Another  kind  of  constraint  that  often  leads  to  good  feature  extraction  is  sparsity:\n",
            "by  adding  an  appropriate  term  to  the  cost  function,  the  autoencoder  is  pushed  to\n",
            "reduce  the  number  of  active  neurons  in  the  coding  layer.  For  example,  it  may  be\n",
            "pushed to have on average only 5% significantly active neurons in the coding layer.\n",
            "This  forces  the  autoencoder  to  represent  each  input  as  a  combination  of  a  small\n",
            "number of activations. As a result, each neuron in the coding layer typically ends up\n",
            "representing  a  useful  feature  (if  you  could  speak  only  a  few  words  per  month,  you\n",
            "would probably try to make them worth listening to).\n",
            "\n",
            "A  simple  approach  is  to  use  the  sigmoid  activation  function  in  the  coding  layer  (to\n",
            "constrain  the  codings  to  values  between  0  and  1),  use  a  large  coding  layer  (e.g.,\n",
            "with 300 units), and add some ℓ1 regularization to the coding layer’s activations. The\n",
            "decoder is just a regular decoder:\n",
            "\n",
            "sparse_l1_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
            "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
            "])\n",
            "sparse_l1_decoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(28 * 28),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
            "\n",
            "Sparse Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "651\n",
            "\n",
            "\fThis  ActivityRegularization  layer  just  returns  its  inputs,  but  as  a  side  effect  it\n",
            "adds  a  training  loss  equal  to  the  sum  of  the  absolute  values  of  its  inputs.  This  only\n",
            "affects  training.  Equivalently,  you  could  remove  the  ActivityRegularization  layer\n",
            "and  set  activity_regularizer=tf.keras.regularizers.l1(1e-4)  in  the  previous\n",
            "layer.  This  penalty  will  encourage  the  neural  network  to  produce  codings  close  to\n",
            "0,  but  since  it  will  also  be  penalized  if  it  does  not  reconstruct  the  inputs  correctly,\n",
            "it  will  have  to  output  at  least  a  few  nonzero  values.  Using  the  ℓ1  norm  rather  than\n",
            "the  ℓ2  norm  will  push  the  neural  network  to  preserve  the  most  important  codings\n",
            "while eliminating the ones that are not needed for the input image (rather than just\n",
            "reducing all codings).\n",
            "\n",
            "Another approach, which often yields better results, is to measure the actual sparsity\n",
            "of  the  coding  layer  at  each  training  iteration,  and  penalize  the  model  when  the\n",
            "measured sparsity differs from a target sparsity. We do so by computing the average\n",
            "activation  of  each  neuron  in  the  coding  layer,  over  the  whole  training  batch.  The\n",
            "batch size must not be too small, or else the mean will not be accurate.\n",
            "\n",
            "Once we have the mean activation per neuron, we want to penalize the neurons that\n",
            "are  too  active,  or  not  active  enough,  by  adding  a  sparsity  loss  to  the  cost  function.\n",
            "For  example,  if  we  measure  that  a  neuron  has  an  average  activation  of  0.3,  but  the\n",
            "target  sparsity  is  0.1,  it  must  be  penalized  to  activate  less.  One  approach  could  be\n",
            "simply  adding  the  squared  error  (0.3  –  0.1)2  to  the  cost  function,  but  in  practice  a\n",
            "better approach is to use the Kullback–Leibler (KL) divergence (briefly discussed in\n",
            "Chapter 4), which has much stronger gradients than the mean squared error, as you\n",
            "can see in Figure 17-10.\n",
            "\n",
            "Figure 17-10. Sparsity loss\n",
            "\n",
            "652 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fGiven  two  discrete  probability  distributions  P  and  Q,  the  KL  divergence  between\n",
            "these distributions, noted DKL(P ∥ Q), can be computed using Equation 17-1.\n",
            "\n",
            "Equation 17-1. Kullback–Leibler divergence\n",
            "\n",
            "DKL P ∥ Q = ∑\n",
            "i\n",
            "\n",
            "P i log\n",
            "\n",
            "P i\n",
            "Q i\n",
            "\n",
            "In  our  case,  we  want  to  measure  the  divergence  between  the  target  probability  p\n",
            "that a neuron in the coding layer will activate and the actual probability q, estimated\n",
            "by  measuring  the  mean  activation  over  the  training  batch.  So,  the  KL  divergence\n",
            "simplifies to Equation 17-2.\n",
            "\n",
            "Equation 17-2. KL divergence between the target sparsity p and the actual sparsity q\n",
            "\n",
            "DKL p ∥ q = p log\n",
            "\n",
            "p\n",
            "q + 1 − p log\n",
            "\n",
            "1 − p\n",
            "1 − q\n",
            "\n",
            "Once  we  have  computed  the  sparsity  loss  for  each  neuron  in  the  coding  layer,  we\n",
            "sum  up  these  losses  and  add  the  result  to  the  cost  function.  In  order  to  control  the\n",
            "relative importance of the sparsity loss and the reconstruction loss, we can multiply\n",
            "the  sparsity  loss  by  a  sparsity  weight  hyperparameter.  If  this  weight  is  too  high,  the\n",
            "model  will  stick  closely  to  the  target  sparsity,  but  it  may  not  reconstruct  the  inputs\n",
            "properly, making the model useless. Conversely, if it is too low, the model will mostly\n",
            "ignore the sparsity objective and will not learn any interesting features.\n",
            "\n",
            "We now have all we need to implement a sparse autoencoder based on the KL diver‐\n",
            "gence. First, let’s create a custom regularizer to apply KL divergence regularization:\n",
            "\n",
            "kl_divergence = tf.keras.losses.kullback_leibler_divergence\n",
            "\n",
            "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
            "    def __init__(self, weight, target):\n",
            "        self.weight = weight\n",
            "        self.target = target\n",
            "\n",
            "    def __call__(self, inputs):\n",
            "        mean_activities = tf.reduce_mean(inputs, axis=0)\n",
            "        return self.weight * (\n",
            "            kl_divergence(self.target, mean_activities) +\n",
            "            kl_divergence(1. - self.target, 1. - mean_activities))\n",
            "\n",
            "Now  we  can  build  the  sparse  autoencoder,  using  the  KLDivergenceRegularizer  for\n",
            "the coding layer’s activations:\n",
            "\n",
            "Sparse Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "653\n",
            "\n",
            "\fkld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\n",
            "sparse_kl_encoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(300, activation=\"sigmoid\",\n",
            "                          activity_regularizer=kld_reg)\n",
            "])\n",
            "sparse_kl_decoder = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(28 * 28),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "sparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n",
            "\n",
            "After training this sparse autoencoder on Fashion MNIST, the coding layer will have\n",
            "roughly 10% sparsity.\n",
            "\n",
            "Variational Autoencoders\n",
            "An important category of autoencoders was introduced in 2013 by Diederik Kingma\n",
            "and Max Welling6 and quickly became one of the most popular variants: variational\n",
            "autoencoders (VAEs).\n",
            "\n",
            "VAEs are quite different from all the autoencoders we have discussed so far, in these\n",
            "particular ways:\n",
            "\n",
            "• They are probabilistic autoencoders, meaning that their outputs are partly deter‐\n",
            "•\n",
            "mined  by  chance,  even  after  training  (as  opposed  to  denoising  autoencoders,\n",
            "which use randomness only during training).\n",
            "\n",
            "•\n",
            "• Most importantly, they are generative autoencoders, meaning that they can gener‐\n",
            "\n",
            "ate new instances that look like they were sampled from the training set.\n",
            "\n",
            "Both these properties make VAEs rather similar to RBMs, but they are easier to train,\n",
            "and the sampling process is much faster (with RBMs you need to wait for the network\n",
            "to  stabilize  into  a  “thermal  equilibrium”  before  you  can  sample  a  new  instance).  As\n",
            "their name suggests, variational autoencoders perform variational Bayesian inference,\n",
            "which is an efficient way of carrying out approximate Bayesian inference. Recall that\n",
            "Bayesian  inference  means  updating  a  probability  distribution  based  on  new  data,\n",
            "using equations derived from Bayes’ theorem. The original distribution is called the\n",
            "prior,  while  the  updated  distribution  is  called  the  posterior.  In  our  case,  we  want  to\n",
            "find a good approximation of the data distribution. Once we have that, we can sample\n",
            "from it.\n",
            "\n",
            "6 Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes”, arXiv preprint arXiv:1312.6114\n",
            "\n",
            "(2013).\n",
            "\n",
            "654 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fLet’s  take  a  look  at  how  VAEs  work.  Figure  17-11  (left)  shows  a  variational  autoen‐\n",
            "coder.  You  can  recognize  the  basic  structure  of  all  autoencoders,  with  an  encoder\n",
            "followed by a decoder (in this example, they both have two hidden layers), but there\n",
            "is  a  twist:  instead  of  directly  producing  a  coding  for  a  given  input,  the  encoder\n",
            "produces  a  mean  coding  μ  and  a  standard  deviation  σ.  The  actual  coding  is  then\n",
            "sampled randomly from a Gaussian distribution with mean μ and standard deviation\n",
            "σ.  After  that  the  decoder  decodes  the  sampled  coding  normally.  The  right  part  of\n",
            "the  diagram  shows  a  training  instance  going  through  this  autoencoder.  First,  the\n",
            "encoder produces μ and σ, then a coding is sampled randomly (notice that it is not\n",
            "exactly located at μ), and finally this coding is decoded; the final output resembles the\n",
            "training instance.\n",
            "\n",
            "Figure 17-11. A variational autoencoder (left) and an instance going through it (right)\n",
            "\n",
            "As  you  can  see  in  the  diagram,  although  the  inputs  may  have  a  very  convoluted\n",
            "distribution, a variational autoencoder tends to produce codings that look as though\n",
            "they  were  sampled  from  a  simple  Gaussian  distribution:7  during  training,  the  cost\n",
            "\n",
            "7 Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions.\n",
            "\n",
            "Variational Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "655\n",
            "\n",
            "\ffunction (discussed next) pushes the codings to gradually migrate within the coding\n",
            "space (also called the latent space) to end up looking like a cloud of Gaussian points.\n",
            "One great consequence is that after training a variational autoencoder, you can very\n",
            "easily  generate  a  new  instance:  just  sample  a  random  coding  from  the  Gaussian\n",
            "distribution, decode it, and voilà!\n",
            "\n",
            "Now, let’s look at the cost function. It is composed of two parts. The first is the usual\n",
            "reconstruction loss that pushes the autoencoder to reproduce its inputs. We can use\n",
            "the  MSE  for  this,  as  we  did  earlier.  The  second  is  the  latent  loss  that  pushes  the\n",
            "autoencoder  to  have  codings  that  look  as  though  they  were  sampled  from  a  simple\n",
            "Gaussian  distribution:  it  is  the  KL  divergence  between  the  target  distribution  (i.e.,\n",
            "the  Gaussian  distribution)  and  the  actual  distribution  of  the  codings.  The  math  is\n",
            "a  bit  more  complex  than  with  the  sparse  autoencoder,  in  particular  because  of  the\n",
            "Gaussian  noise,  which  limits  the  amount  of  information  that  can  be  transmitted  to\n",
            "the  coding  layer.  This  pushes  the  autoencoder  to  learn  useful  features.  Luckily,  the\n",
            "equations simplify, so the latent loss can be computed using Equation 17-3.8\n",
            "\n",
            "Equation 17-3. Variational autoencoder’s latent loss\n",
            "\n",
            "ℒ = −\n",
            "\n",
            "n\n",
            "\n",
            "1\n",
            "2 ∑\n",
            "i = 1\n",
            "\n",
            "1 + log σi\n",
            "\n",
            "2 − σi\n",
            "\n",
            "2 − μi\n",
            "\n",
            "2\n",
            "\n",
            "In  this  equation,  ℒ  is  the  latent  loss,  n  is  the  codings’  dimensionality,  and  μi  and  σi\n",
            "are the mean and standard deviation of the ith component of the codings. The vectors\n",
            "μ  and  σ  (which  contain  all  the  μi  and  σi)  are  output  by  the  encoder,  as  shown  in\n",
            "Figure 17-11 (left).\n",
            "\n",
            "A common tweak to the variational autoencoder’s architecture is to make the encoder\n",
            "output γ = log(σ2) rather than σ. The latent loss can then be computed as shown in\n",
            "Equation 17-4. This approach is more numerically stable and speeds up training.\n",
            "\n",
            "Equation 17-4. Variational autoencoder’s latent loss, rewritten using γ = log(σ²)\n",
            "\n",
            "ℒ = −\n",
            "\n",
            "n\n",
            "\n",
            "1\n",
            "2 ∑\n",
            "i = 1\n",
            "\n",
            "1 + γi − exp γi − μi\n",
            "\n",
            "2\n",
            "\n",
            "Let’s  start  building  a  variational  autoencoder  for  Fashion  MNIST  (as  shown  in  Fig‐\n",
            "ure  17-11,  but  using  the  γ  tweak).  First,  we  will  need  a  custom  layer  to  sample  the\n",
            "codings, given μ and γ:\n",
            "\n",
            "8 For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s\n",
            "\n",
            "great tutorial (2016).\n",
            "\n",
            "656 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fclass Sampling(tf.keras.layers.Layer):\n",
            "    def call(self, inputs):\n",
            "        mean, log_var = inputs\n",
            "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean\n",
            "\n",
            "This Sampling layer takes two inputs: mean (μ) and log_var (γ). It uses the function\n",
            "tf.random.normal()  to  sample  a  random  vector  (of  the  same  shape  as  γ)  from  the\n",
            "Gaussian distribution, with mean 0 and standard deviation 1. Then it multiplies it by\n",
            "exp(γ / 2) (which is equal to σ, as you can verify mathematically), and finally it adds μ\n",
            "and returns the result. This samples a codings vector from the Gaussian distribution\n",
            "with mean μ and standard deviation σ.\n",
            "\n",
            "Next, we can create the encoder, using the functional API because the model is not\n",
            "entirely sequential:\n",
            "\n",
            "codings_size = 10\n",
            "\n",
            "inputs = tf.keras.layers.Input(shape=[28, 28])\n",
            "Z = tf.keras.layers.Flatten()(inputs)\n",
            "Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\n",
            "Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\n",
            "codings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μ\n",
            "codings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γ\n",
            "codings = Sampling()([codings_mean, codings_log_var])\n",
            "variational_encoder = tf.keras.Model(\n",
            "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
            "\n",
            "Note  that  the  Dense  layers  that  output  codings_mean  (μ)  and  codings_log_var\n",
            "(γ)  have  the  same  inputs  (i.e.,  the  outputs  of  the  second  Dense  layer).  We  then\n",
            "pass  both  codings_mean  and  codings_log_var  to  the  Sampling  layer.  Finally,  the\n",
            "variational_encoder model has three outputs. Only the codings are required, but\n",
            "we add codings_mean and codings_log_var as well, in case we want to inspect their\n",
            "values. Now let’s build the decoder:\n",
            "\n",
            "decoder_inputs = tf.keras.layers.Input(shape=[codings_size])\n",
            "x = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\n",
            "x = tf.keras.layers.Dense(150, activation=\"relu\")(x)\n",
            "x = tf.keras.layers.Dense(28 * 28)(x)\n",
            "outputs = tf.keras.layers.Reshape([28, 28])(x)\n",
            "variational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
            "\n",
            "For  this  decoder,  we  could  have  used  the  sequential  API  instead  of  the  functional\n",
            "API, since it is really just a simple stack of layers, virtually identical to many of the\n",
            "decoders we have built so far. Finally, let’s build the variational autoencoder model:\n",
            "\n",
            "_, _, codings = variational_encoder(inputs)\n",
            "reconstructions = variational_decoder(codings)\n",
            "variational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])\n",
            "\n",
            "We ignore the first two outputs of the encoder (we only want to feed the codings to\n",
            "the decoder). Lastly, we must add the latent loss and the reconstruction loss:\n",
            "\n",
            "Variational Autoencoders \n",
            "\n",
            "| \n",
            "\n",
            "657\n",
            "\n",
            "\flatent_loss = -0.5 * tf.reduce_sum(\n",
            "    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\n",
            "    axis=-1)\n",
            "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)\n",
            "\n",
            "We first apply Equation 17-4 to compute the latent loss for each instance in the batch,\n",
            "summing over the last axis. Then we compute the mean loss over all the instances in\n",
            "the batch, and we divide the result by 784 to ensure it has the appropriate scale com‐\n",
            "pared to the reconstruction loss. Indeed, the variational autoencoder’s reconstruction\n",
            "loss  is  supposed  to  be  the  sum  of  the  pixel  reconstruction  errors,  but  when  Keras\n",
            "computes  the  \"mse\"  loss  it  computes  the  mean  over  all  784  pixels,  rather  than  the\n",
            "sum. So, the reconstruction loss is 784 times smaller than we need it to be. We could\n",
            "define  a  custom  loss  to  compute  the  sum  rather  than  the  mean,  but  it  is  simpler  to\n",
            "divide the latent loss by 784 (the final loss will be 784 times smaller than it should be,\n",
            "but this just means that we should use a larger learning rate).\n",
            "\n",
            "And finally, we can compile and fit the autoencoder!\n",
            "\n",
            "variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
            "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
            "                             validation_data=(X_valid, X_valid))\n",
            "\n",
            "Generating Fashion MNIST Images\n",
            "Now  let’s  use  this  variational  autoencoder  to  generate  images  that  look  like  fashion\n",
            "items. All we need to do is sample random codings from a Gaussian distribution and\n",
            "decode them:\n",
            "\n",
            "codings = tf.random.normal(shape=[3 * 7, codings_size])\n",
            "images = variational_decoder(codings).numpy()\n",
            "\n",
            "Figure 17-12 shows the 12 generated images.\n",
            "\n",
            "Figure 17-12. Fashion MNIST images generated by the variational autoencoder\n",
            "\n",
            "658 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fThe majority of these images look fairly convincing, if a bit too fuzzy. The rest are not\n",
            "great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!\n",
            "\n",
            "Variational autoencoders make it possible to perform semantic interpolation: instead\n",
            "of interpolating between two images at the pixel level, which would look as if the two\n",
            "images were just overlaid, we can interpolate at the codings level. For example, let’s\n",
            "take a few codings along an arbitrary line in latent space and decode them. We get a\n",
            "sequence of images that gradually go from pants to sweaters (see Figure 17-13):\n",
            "\n",
            "codings = np.zeros([7, codings_size])\n",
            "codings[:, 3] = np.linspace(-0.8, 0.8, 7)  # axis 3 looks best in this case\n",
            "images = variational_decoder(codings).numpy()\n",
            "\n",
            "Figure 17-13. Semantic interpolation\n",
            "\n",
            "Let’s now turn our attention to GANs: they are harder to train, but when you manage\n",
            "to get them to work, they produce pretty amazing images.\n",
            "\n",
            "Generative Adversarial Networks\n",
            "Generative adversarial networks were proposed in a 2014 paper9 by Ian Goodfellow et\n",
            "al., and although the idea got researchers excited almost instantly, it took a few years\n",
            "to overcome some of the difficulties of training GANs. Like many great ideas, it seems\n",
            "simple  in  hindsight:  make  neural  networks  compete  against  each  other  in  the  hope\n",
            "that  this  competition  will  push  them  to  excel.  As  shown  in  Figure  17-14,  a  GAN  is\n",
            "composed of two neural networks:\n",
            "\n",
            "Generator\n",
            "\n",
            "Takes  a  random  distribution  as  input  (typically  Gaussian)  and  outputs  some\n",
            "data—typically,  an  image.  You  can  think  of  the  random  inputs  as  the  latent\n",
            "representations (i.e., codings) of the image to be generated. So, as you can see, the\n",
            "generator offers the same functionality as a decoder in a variational autoencoder,\n",
            "and  it  can  be  used  in  the  same  way  to  generate  new  images:  just  feed  it  some\n",
            "Gaussian  noise,  and  it  outputs  a  brand-new  image.  However,  it  is  trained  very\n",
            "differently, as you will soon see.\n",
            "\n",
            "9 Ian Goodfellow et al., “Generative Adversarial Nets”, Proceedings of the 27th International Conference on\n",
            "\n",
            "Neural Information Processing Systems 2 (2014): 2672–2680.\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "659\n",
            "\n",
            "\fDiscriminator\n",
            "\n",
            "Takes either a fake image from the generator or a real image from the training set\n",
            "as input, and must guess whether the input image is fake or real.\n",
            "\n",
            "Figure 17-14. A generative adversarial network\n",
            "\n",
            "During  training,  the  generator  and  the  discriminator  have  opposite  goals:  the  dis‐\n",
            "criminator  tries  to  tell  fake  images  from  real  images,  while  the  generator  tries  to\n",
            "produce  images  that  look  real  enough  to  trick  the  discriminator.  Because  the  GAN\n",
            "is  composed  of  two  networks  with  different  objectives,  it  cannot  be  trained  like  a\n",
            "regular neural network. Each training iteration is divided into two phases:\n",
            "\n",
            "• In the first phase, we train the discriminator. A batch of real images is sampled\n",
            "•\n",
            "from  the  training  set  and  is  completed  with  an  equal  number  of  fake  images\n",
            "produced by the generator. The labels are set to 0 for fake images and 1 for real\n",
            "images, and the discriminator is trained on this labeled batch for one step, using\n",
            "the  binary  cross-entropy  loss.  Importantly,  backpropagation  only  optimizes  the\n",
            "weights of the discriminator during this phase.\n",
            "\n",
            "•\n",
            "• In  the  second  phase,  we  train  the  generator.  We  first  use  it  to  produce  another\n",
            "batch  of  fake  images,  and  once  again  the  discriminator  is  used  to  tell  whether\n",
            "the  images  are  fake  or  real.  This  time  we  do  not  add  real  images  in  the  batch,\n",
            "and  all  the  labels  are  set  to  1  (real):  in  other  words,  we  want  the  generator  to\n",
            "\n",
            "660 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fproduce images that the discriminator will (wrongly) believe to be real! Crucially,\n",
            "the weights of the discriminator are frozen during this step, so backpropagation\n",
            "only affects the weights of the generator.\n",
            "\n",
            "The generator never actually sees any real images, yet it gradually\n",
            "learns  to  produce  convincing  fake  images!  All  it  gets  is  the  gradi‐\n",
            "ents flowing back through the discriminator. Fortunately, the better\n",
            "the discriminator gets, the more information about the real images\n",
            "is  contained  in  these  secondhand  gradients,  so  the  generator  can\n",
            "make significant progress.\n",
            "\n",
            "Let’s go ahead and build a simple GAN for Fashion MNIST.\n",
            "\n",
            "First, we need to build the generator and the discriminator. The generator is similar\n",
            "to  an  autoencoder’s  decoder,  and  the  discriminator  is  a  regular  binary  classifier:\n",
            "it  takes  an  image  as  input  and  ends  with  a  Dense  layer  containing  a  single  unit\n",
            "and  using  the  sigmoid  activation  function.  For  the  second  phase  of  each  training\n",
            "iteration, we also need the full GAN model containing the generator followed by the\n",
            "discriminator:\n",
            "\n",
            "codings_size = 30\n",
            "\n",
            "Dense = tf.keras.layers.Dense\n",
            "generator = tf.keras.Sequential([\n",
            "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
            "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
            "    Dense(28 * 28, activation=\"sigmoid\"),\n",
            "    tf.keras.layers.Reshape([28, 28])\n",
            "])\n",
            "discriminator = tf.keras.Sequential([\n",
            "    tf.keras.layers.Flatten(),\n",
            "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
            "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
            "    Dense(1, activation=\"sigmoid\")\n",
            "])\n",
            "gan = tf.keras.Sequential([generator, discriminator])\n",
            "\n",
            "Next,  we  need  to  compile  these  models.  As  the  discriminator  is  a  binary  classifier,\n",
            "we  can  naturally  use  the  binary  cross-entropy  loss.  The  gan  model  is  also  a  binary\n",
            "classifier, so it can use the binary cross-entropy loss as well. However, the generator\n",
            "will  only  be  trained  through  the  gan  model,  so  we  do  not  need  to  compile  it  at  all.\n",
            "Importantly, the discriminator should not be trained during the second phase, so we\n",
            "make it non-trainable before compiling the gan model:\n",
            "\n",
            "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
            "discriminator.trainable = False\n",
            "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "661\n",
            "\n",
            "\fThe trainable attribute is taken into account by Keras only when\n",
            "compiling a model, so after running this code, the discriminator\n",
            "is  trainable  if  we  call  its  fit()  method  or  its  train_on_batch()\n",
            "method (which we will be using), while it is not trainable when we\n",
            "call these methods on the gan model.\n",
            "\n",
            "Since the training loop is unusual, we cannot use the regular fit() method. Instead,\n",
            "we  will  write  a  custom  training  loop.  For  this,  we  first  need  to  create  a  Dataset  to\n",
            "iterate through the images:\n",
            "\n",
            "batch_size = 32\n",
            "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=1000)\n",
            "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
            "\n",
            "We are now ready to write the training loop. Let’s wrap it in a train_gan() function:\n",
            "\n",
            "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
            "    generator, discriminator = gan.layers\n",
            "    for epoch in range(n_epochs):\n",
            "        for X_batch in dataset:\n",
            "            # phase 1 - training the discriminator\n",
            "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
            "            generated_images = generator(noise)\n",
            "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
            "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
            "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
            "            # phase 2 - training the generator\n",
            "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
            "            y2 = tf.constant([[1.]] * batch_size)\n",
            "            gan.train_on_batch(noise, y2)\n",
            "\n",
            "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)\n",
            "\n",
            "As discussed earlier, you can see the two phases at each iteration:\n",
            "\n",
            "• In  phase  one  we  feed  Gaussian  noise  to  the  generator  to  produce  fake  images,\n",
            "•\n",
            "and  we  complete  this  batch  by  concatenating  an  equal  number  of  real  images.\n",
            "The targets y1 are set to 0 for fake images and 1 for real images. Then we train\n",
            "the discriminator on this batch. Remember that the discriminator is trainable in\n",
            "this phase, but we are not touching the generator.\n",
            "\n",
            "• In phase two, we feed the GAN some Gaussian noise. Its generator will start by\n",
            "•\n",
            "producing  fake  images,  then  the  discriminator  will  try  to  guess  whether  these\n",
            "images  are  fake  or  real.  In  this  phase,  we  are  trying  to  improve  the  generator,\n",
            "which means that we want the discriminator to fail: this is why the targets y2 are\n",
            "all set to 1, although the images are fake. In this phase, the discriminator is not\n",
            "trainable, so the only part of the gan model that will improve is the generator.\n",
            "\n",
            "662 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fThat’s  it!  After  training,  you  can  randomly  sample  some  codings  from  a  Gaussian\n",
            "distribution, and feed them to the generator to produce new images:\n",
            "\n",
            "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
            "generated_images = generator.predict(codings)\n",
            "\n",
            "If you display the generated images (see Figure 17-15), you will see that at the end of\n",
            "the first epoch, they already start to look like (very noisy) Fashion MNIST images.\n",
            "\n",
            "Figure 17-15. Images generated by the GAN after one epoch of training\n",
            "\n",
            "Unfortunately, the images never really get much better than that, and you may even\n",
            "find epochs where the GAN seems to be forgetting what it learned. Why is that? Well,\n",
            "it turns out that training a GAN can be challenging. Let’s see why.\n",
            "\n",
            "The Difficulties of Training GANs\n",
            "During training, the generator and the discriminator constantly try to outsmart each\n",
            "other, in a zero-sum game. As training advances, the game may end up in a state that\n",
            "game  theorists  call  a  Nash  equilibrium,  named  after  the  mathematician  John  Nash:\n",
            "this is when no player would be better off changing their own strategy, assuming the\n",
            "other players do not change theirs. For example, a Nash equilibrium is reached when\n",
            "everyone drives on the left side of the road: no driver would be better off being the\n",
            "only one to switch sides. Of course, there is a second possible Nash equilibrium: when\n",
            "everyone  drives  on  the  right  side  of  the  road.  Different  initial  states  and  dynamics\n",
            "may  lead  to  one  equilibrium  or  the  other.  In  this  example,  there  is  a  single  optimal\n",
            "strategy  once  an  equilibrium  is  reached  (i.e.,  driving  on  the  same  side  as  everyone\n",
            "else), but a Nash equilibrium can involve multiple competing strategies (e.g., a preda‐\n",
            "tor chases its prey, the prey tries to escape, and neither would be better off changing\n",
            "their strategy).\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "663\n",
            "\n",
            "\fSo how does this apply to GANs? Well, the authors of the GAN paper demonstrated\n",
            "that  a  GAN  can  only  reach  a  single  Nash  equilibrium:  that’s  when  the  generator\n",
            "produces perfectly realistic images, and the discriminator is forced to guess (50% real,\n",
            "50%  fake).  This  fact  is  very  encouraging:  it  would  seem  that  you  just  need  to  train\n",
            "the  GAN  for  long  enough,  and  it  will  eventually  reach  this  equilibrium,  giving  you\n",
            "a  perfect  generator.  Unfortunately,  it’s  not  that  simple:  nothing  guarantees  that  the\n",
            "equilibrium will ever be reached.\n",
            "\n",
            "The  biggest  difficulty  is  called  mode  collapse:  this  is  when  the  generator’s  outputs\n",
            "gradually become less diverse. How can this happen? Suppose that the generator gets\n",
            "better at producing convincing shoes than any other class. It will fool the discrimina‐\n",
            "tor  a  bit  more  with  shoes,  and  this  will  encourage  it  to  produce  even  more  images\n",
            "of  shoes.  Gradually,  it  will  forget  how  to  produce  anything  else.  Meanwhile,  the\n",
            "only  fake  images  that  the  discriminator  will  see  will  be  shoes,  so  it  will  also  forget\n",
            "how to discriminate fake images of other classes. Eventually, when the discriminator\n",
            "manages  to  discriminate  the  fake  shoes  from  the  real  ones,  the  generator  will  be\n",
            "forced to move to another class. It may then become good at shirts, forgetting about\n",
            "shoes, and the discriminator will follow. The GAN may gradually cycle across a few\n",
            "classes, never really becoming very good at any of them.\n",
            "\n",
            "Moreover, because the generator and the discriminator are constantly pushing against\n",
            "each other, their parameters may end up oscillating and becoming unstable. Training\n",
            "may  begin  properly,  then  suddenly  diverge  for  no  apparent  reason,  due  to  these\n",
            "instabilities. And since many factors affect these complex dynamics, GANs are very\n",
            "sensitive  to  the  hyperparameters:  you  may  have  to  spend  a  lot  of  effort  fine-tuning\n",
            "them.  In  fact,  that’s  why  I  used  RMSProp  rather  than  Nadam  when  compiling  the\n",
            "models: when using Nadam, I ran into a severe mode collapse.\n",
            "\n",
            "These problems have kept researchers very busy since 2014: many papers have been\n",
            "published on this topic, some proposing new cost functions10 (though a 2018 paper11\n",
            "by Google researchers questions their efficiency) or techniques to stabilize training or\n",
            "to avoid the mode collapse issue. For example, a popular technique called experience\n",
            "replay  consists  of  storing  the  images  produced  by  the  generator  at  each  iteration  in\n",
            "a replay buffer (gradually dropping older generated images) and training the discrim‐\n",
            "inator  using  real  images  plus  fake  images  drawn  from  this  buffer  (rather  than  just\n",
            "fake  images  produced  by  the  current  generator).  This  reduces  the  chances  that  the\n",
            "discriminator will overfit the latest generator’s outputs. Another common technique\n",
            "is  called  mini-batch  discrimination:  it  measures  how  similar  images  are  across  the\n",
            "batch  and  provides  this  statistic  to  the  discriminator,  so  it  can  easily  reject  a  whole\n",
            "\n",
            "10 For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee.\n",
            "\n",
            "11 Mario Lucic et al., “Are GANs Created Equal? A Large-Scale Study”, Proceedings of the 32nd International\n",
            "\n",
            "Conference on Neural Information Processing Systems (2018): 698–707.\n",
            "\n",
            "664 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fbatch of fake images that lack diversity. This encourages the generator to produce a\n",
            "greater variety of images, reducing the chance of mode collapse. Other papers simply\n",
            "propose specific architectures that happen to perform well.\n",
            "\n",
            "In  short,  this  is  still  a  very  active  field  of  research,  and  the  dynamics  of  GANs  are\n",
            "still  not  perfectly  understood.  But  the  good  news  is  that  great  progress  has  been\n",
            "made, and some of the results are truly astounding! So let’s look at some of the most\n",
            "successful architectures, starting with deep convolutional GANs, which were the state\n",
            "of  the  art  just  a  few  years  ago.  Then  we  will  look  at  two  more  recent  (and  more\n",
            "complex) architectures.\n",
            "\n",
            "Deep Convolutional GANs\n",
            "The authors of the original GAN paper experimented with convolutional layers, but\n",
            "only tried to generate small images. Soon after, many researchers tried to build GANs\n",
            "based  on  deeper  convolutional  nets  for  larger  images.  This  proved  to  be  tricky,  as\n",
            "training  was  very  unstable,  but  Alec  Radford  et  al.  finally  succeeded  in  late  2015,\n",
            "after  experimenting  with  many  different  architectures  and  hyperparameters.  They\n",
            "called  their  architecture  deep  convolutional  GANs  (DCGANs).12  Here  are  the  main\n",
            "guidelines they proposed for building stable convolutional GANs:\n",
            "\n",
            "•\n",
            "• Replace any pooling layers with strided convolutions (in the discriminator) and\n",
            "\n",
            "transposed convolutions (in the generator).\n",
            "\n",
            "•\n",
            "• Use  batch  normalization  in  both  the  generator  and  the  discriminator,  except  in\n",
            "\n",
            "the generator’s output layer and the discriminator’s input layer.\n",
            "\n",
            "•\n",
            "• Remove fully connected hidden layers for deeper architectures.\n",
            "\n",
            "•\n",
            "• Use ReLU activation in the generator for all layers except the output layer, which\n",
            "\n",
            "should use tanh.\n",
            "\n",
            "•\n",
            "• Use leaky ReLU activation in the discriminator for all layers.\n",
            "\n",
            "These  guidelines  will  work  in  many  cases,  but  not  always,  so  you  may  still  need\n",
            "to  experiment  with  different  hyperparameters.  In  fact,  just  changing  the  random\n",
            "seed and training the exact same model again will sometimes work. Here is a small\n",
            "DCGAN that works reasonably well with Fashion MNIST:\n",
            "\n",
            "12 Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial\n",
            "\n",
            "Networks”, arXiv preprint arXiv:1511.06434 (2015).\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "665\n",
            "\n",
            "\fcodings_size = 100\n",
            "\n",
            "generator = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(7 * 7 * 128),\n",
            "    tf.keras.layers.Reshape([7, 7, 128]),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
            "                                    padding=\"same\", activation=\"relu\"),\n",
            "    tf.keras.layers.BatchNormalization(),\n",
            "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
            "                                    padding=\"same\", activation=\"tanh\"),\n",
            "])\n",
            "discriminator = tf.keras.Sequential([\n",
            "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
            "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
            "    tf.keras.layers.Dropout(0.4),\n",
            "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
            "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
            "    tf.keras.layers.Dropout(0.4),\n",
            "    tf.keras.layers.Flatten(),\n",
            "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
            "])\n",
            "gan = tf.keras.Sequential([generator, discriminator])\n",
            "\n",
            "The  generator  takes  codings  of  size  100,  projects  them  to  6,272  dimensions  (7  *  7\n",
            "*  128),  and  reshapes  the  result  to  get  a  7  ×  7  ×  128  tensor.  This  tensor  is  batch\n",
            "normalized  and  fed  to  a  transposed  convolutional  layer  with  a  stride  of  2,  which\n",
            "upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result\n",
            "is  batch  normalized  again  and  fed  to  another  transposed  convolutional  layer  with  a\n",
            "stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64\n",
            "to 1. This layer uses the tanh activation function, so the outputs will range from –1 to\n",
            "1. For this reason, before training the GAN, we need to rescale the training set to that\n",
            "same range. We also need to reshape it to add the channel dimension:\n",
            "\n",
            "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale\n",
            "\n",
            "The  discriminator  looks  much  like  a  regular  CNN  for  binary  classification,  except\n",
            "instead of using max pooling layers to downsample the image, we use strided convo‐\n",
            "lutions  (strides=2).  Note  that  we  use  the  leaky  ReLU  activation  function.  Overall,\n",
            "we  respected  the  DCGAN  guidelines,  except  we  replaced  the  BatchNormalization\n",
            "layers  in  the  discriminator  with  Dropout  layers;  otherwise,  training  was  unstable  in\n",
            "this  case.  Feel  free  to  tweak  this  architecture:  you  will  see  how  sensitive  it  is  to  the\n",
            "hyperparameters, especially the relative learning rates of the two networks.\n",
            "\n",
            "Lastly,  to  build  the  dataset  and  then  compile  and  train  this  model,  we  can  use  the\n",
            "same code as earlier. After 50 epochs of training, the generator produces images like\n",
            "those shown in Figure 17-16. It’s still not perfect, but many of these images are pretty\n",
            "convincing.\n",
            "\n",
            "666 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fFigure 17-16. Images generated by the DCGAN after 50 epochs of training\n",
            "\n",
            "If you scale up this architecture and train it on a large dataset of faces, you can get\n",
            "fairly realistic images. In fact, DCGANs can learn quite meaningful latent representa‐\n",
            "tions, as you can see in Figure 17-17: many images were generated, and nine of them\n",
            "were picked manually (top left), including three representing men with glasses, three\n",
            "men without glasses, and three women without glasses. For each of these categories,\n",
            "the codings that were used to generate the images were averaged, and an image was\n",
            "generated  based  on  the  resulting  mean  codings  (lower  left).  In  short,  each  of  the\n",
            "three lower-left images represents the mean of the three images located above it. But\n",
            "this  is  not  a  simple  mean  computed  at  the  pixel  level  (this  would  result  in  three\n",
            "overlapping faces), it is a mean computed in the latent space, so the images still look\n",
            "like normal faces. Amazingly, if you compute men with glasses, minus men without\n",
            "glasses,  plus  women  without  glasses—where  each  term  corresponds  to  one  of  the\n",
            "mean codings—and you generate the image that corresponds to this coding, you get\n",
            "the image at the center of the 3 × 3 grid of faces on the right: a woman with glasses!\n",
            "The eight other images around it were generated based on the same vector plus a bit\n",
            "of noise, to illustrate the semantic interpolation capabilities of DCGANs. Being able\n",
            "to do arithmetic on faces feels like science fiction!\n",
            "\n",
            "DCGANs  aren’t  perfect,  though.  For  example,  when  you  try  to  generate  very  large\n",
            "images using DCGANs, you often end up with locally convincing features but overall\n",
            "inconsistencies, such as shirts with one sleeve much longer than the other, different\n",
            "earrings, or eyes looking in opposite directions. How can you fix this?\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "667\n",
            "\n",
            "\fFigure 17-17. Vector arithmetic for visual concepts (part of figure 7 from the DCGAN\n",
            "paper)13\n",
            "\n",
            "If  you  add  each  image’s  class  as  an  extra  input  to  both  the  gener‐\n",
            "ator  and  the  discriminator,  they  will  both  learn  what  each  class\n",
            "looks  like,  and  thus  you  will  be  able  to  control  the  class  of  each\n",
            "image  produced  by  the  generator.  This  is  called  a  conditional\n",
            "GAN(CGAN).14\n",
            "\n",
            "Progressive Growing of GANs\n",
            "In  a  2018  paper,15  Nvidia  researchers  Tero  Kerras  et  al.  proposed  an  important\n",
            "technique: they suggested generating small images at the beginning of training, then\n",
            "gradually  adding  convolutional  layers  to  both  the  generator  and  the  discriminator\n",
            "to  produce  larger  and  larger  images  (4  ×  4,  8  ×  8,  16  ×  16,  …,  512  ×  512,  1,024  ×\n",
            "1,024). This approach resembles greedy layer-wise training of stacked autoencoders.\n",
            "The  extra  layers  get  added  at  the  end  of  the  generator  and  at  the  beginning  of  the\n",
            "discriminator, and previously trained layers remain trainable.\n",
            "\n",
            "For  example,  when  growing  the  generator’s  outputs  from  4  ×  4  to  8  ×  8  (see  Fig‐\n",
            "ure  17-18),  an  upsampling  layer  (using  nearest  neighbor  filtering)  is  added  to  the\n",
            "\n",
            "13 Reproduced with the kind authorization of the authors.\n",
            "\n",
            "14 Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets”, arXiv preprint\n",
            "\n",
            "arXiv:1411.1784 (2014).\n",
            "\n",
            "15 Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability, and Variation”, Proceedings\n",
            "\n",
            "of the International Conference on Learning Representations (2018).\n",
            "\n",
            "668 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fexisting convolutional layer (“Conv 1”) to produce 8 × 8 feature maps. These are fed\n",
            "to the new convolutional layer (“Conv 2”), which in turn feeds into a new output con‐\n",
            "volutional layer. To avoid breaking the trained weights of Conv 1, we gradually fade\n",
            "in the two new convolutional layers (represented with dashed lines in Figure 17-18)\n",
            "and  fade  out  the  original  output  layer.  The  final  outputs  are  a  weighted  sum  of  the\n",
            "new  outputs  (with  weight  α)  and  the  original  outputs  (with  weight  1  –  α),  slowly\n",
            "increasing  α  from  0  to  1.  A  similar  fade-in/fade-out  technique  is  used  when  a  new\n",
            "convolutional  layer  is  added  to  the  discriminator  (followed  by  an  average  pooling\n",
            "layer for downsampling). Note that all convolutional layers use \"same\" padding and\n",
            "strides of 1, so they preserve the height and width of their inputs. This includes the\n",
            "original  convolutional  layer,  so  it  now  produces  8  ×  8  outputs  (since  its  inputs  are\n",
            "now 8 × 8). Lastly, the output layers use kernel size 1. They just project their inputs\n",
            "down to the desired number of color channels (typically 3).\n",
            "\n",
            "Figure 17-18. A progressively growing GAN: a GAN generator outputs 4 × 4 color\n",
            "images (left); we extend it to output 8 × 8 images (right)\n",
            "\n",
            "The paper also introduced several other techniques aimed at increasing the diversity\n",
            "of the outputs (to avoid mode collapse) and making training more stable:\n",
            "\n",
            "Mini-batch standard deviation layer\n",
            "\n",
            "Added  near  the  end  of  the  discriminator.  For  each  position  in  the  inputs,  it\n",
            "computes the standard deviation across all channels and all instances in the batch\n",
            "(S = tf.math.reduce_std(inputs, axis=[0, -1])). These standard deviations\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "669\n",
            "\n",
            "\fare  then  averaged  across  all  points  to  get  a  single  value  (v  =  tf.reduce_\n",
            "mean(S)). Finally, an extra feature map is added to each instance in the batch and\n",
            "filled  with  the  computed  value  (tf.concat([inputs,  tf.fill([batch_size,\n",
            "height, width, 1], v)], axis=-1)). How does this help? Well, if the gener‐\n",
            "ator  produces  images  with  little  variety,  then  there  will  be  a  small  standard\n",
            "deviation  across  feature  maps  in  the  discriminator.  Thanks  to  this  layer,  the\n",
            "discriminator  will  have  easy  access  to  this  statistic,  making  it  less  likely  to  be\n",
            "fooled  by  a  generator  that  produces  too  little  diversity.  This  will  encourage  the\n",
            "generator to produce more diverse outputs, reducing the risk of mode collapse.\n",
            "\n",
            "Equalized learning rate\n",
            "\n",
            "2\n",
            "\n",
            "Initializes  all  weights  using  a  Gaussian  distribution  with  mean  0  and  standard\n",
            "deviation  1  rather  than  using  He  initialization.  However,  the  weights  are  scaled\n",
            "down  at  runtime  (i.e.,  every  time  the  layer  is  executed)  by  the  same  factor  as\n",
            "ninputs,  where  ninputs  is  the  number\n",
            "in  He  initialization:  they  are  divided  by \n",
            "of  inputs  to  the  layer.  The  paper  demonstrated  that  this  technique  significantly\n",
            "improved the GAN’s performance when using RMSProp, Adam, or other adap‐\n",
            "tive gradient optimizers. Indeed, these optimizers normalize the gradient updates\n",
            "by their estimated standard deviation (see Chapter 11), so parameters that have\n",
            "a larger dynamic range16 will take longer to train, while parameters with a small\n",
            "dynamic range may be updated too quickly, leading to instabilities. By rescaling\n",
            "the  weights  as  part  of  the  model  itself  rather  than  just  rescaling  them  upon\n",
            "initialization,  this  approach  ensures  that  the  dynamic  range  is  the  same  for  all\n",
            "parameters  throughout  training,  so  they  all  learn  at  the  same  speed.  This  both\n",
            "speeds up and stabilizes training.\n",
            "\n",
            "Pixelwise normalization layer\n",
            "\n",
            "Added after each convolutional layer in the generator. It normalizes each activa‐\n",
            "tion based on all the activations in the same image and at the same location, but\n",
            "across all channels (dividing by the square root of the mean squared activation).\n",
            "In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),\n",
            "axis=-1,  keepdims=True)  +  1e-8)  (the  smoothing  term  1e-8  is  needed  to\n",
            "avoid division by zero). This technique avoids explosions in the activations due\n",
            "to excessive competition between the generator and the discriminator.\n",
            "\n",
            "The  combination  of  all  these  techniques  allowed  the  authors  to  generate  extremely\n",
            "convincing high-definition images of faces. But what exactly do we call “convincing”?\n",
            "Evaluation  is  one  of  the  big  challenges  when  working  with  GANs:  although  it  is\n",
            "possible to automatically evaluate the diversity of the generated images, judging their\n",
            "quality is a much trickier and subjective task. One technique is to use human raters,\n",
            "\n",
            "16 The dynamic range of a variable is the ratio between the highest and the lowest value it may take.\n",
            "\n",
            "670 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fbut  this  is  costly  and  time-consuming.  So,  the  authors  proposed  to  measure  the\n",
            "similarity between the local image structure of the generated images and the training\n",
            "images, considering every scale. This idea led them to another groundbreaking inno‐\n",
            "vation: StyleGANs.\n",
            "\n",
            "StyleGANs\n",
            "The  state  of  the  art  in  high-resolution  image  generation  was  advanced  once  again\n",
            "by  the  same  Nvidia  team  in  a  2018  paper17  that  introduced  the  popular  StyleGAN\n",
            "architecture. The authors used style transfer techniques in the generator to ensure that\n",
            "the  generated  images  have  the  same  local  structure  as  the  training  images,  at  every\n",
            "scale,  greatly  improving  the  quality  of  the  generated  images.  The  discriminator  and\n",
            "the  loss  function  were  not  modified,  only  the  generator.  A  StyleGAN  generator  is\n",
            "composed of two networks (see Figure 17-19):\n",
            "\n",
            "Mapping network\n",
            "\n",
            "An  eight-layer  MLP  that  maps  the  latent  representations  z  (i.e.,  the  codings)\n",
            "to  a  vector  w.  This  vector  is  then  sent  through  multiple  affine  transformations\n",
            "(i.e.,  Dense  layers  with  no  activation  functions,  represented  by  the  “A”  boxes  in\n",
            "Figure 17-19), which produces multiple vectors. These vectors control the style of\n",
            "the generated image at different levels, from fine-grained texture (e.g., hair color)\n",
            "to high-level features (e.g., adult or child). In short, the mapping network maps\n",
            "the codings to multiple style vectors.\n",
            "\n",
            "Synthesis network\n",
            "\n",
            "Responsible  for  generating  the  images.  It  has  a  constant  learned  input  (to  be\n",
            "clear, this input will be constant after training, but during training it keeps getting\n",
            "tweaked by backpropagation). It processes this input through multiple convolu‐\n",
            "tional and upsampling layers, as earlier, but there are two twists. First, some noise\n",
            "is added to the input and to all the outputs of the convolutional layers (before the\n",
            "activation function). Second, each noise layer is followed by an adaptive instance\n",
            "normalization (AdaIN) layer: it standardizes each feature map independently (by\n",
            "subtracting the feature map’s mean and dividing by its standard deviation), then\n",
            "it uses the style vector to determine the scale and offset of each feature map (the\n",
            "style vector contains one scale and one bias term for each feature map).\n",
            "\n",
            "17 Tero Karras et al., “A Style-Based Generator Architecture for Generative Adversarial Networks”, arXiv preprint\n",
            "\n",
            "arXiv:1812.04948 (2018).\n",
            "\n",
            "Generative Adversarial Networks \n",
            "\n",
            "| \n",
            "\n",
            "671\n",
            "\n",
            "\fFigure 17-19. StyleGAN’s generator architecture (part of Figure 1 from the StyleGAN\n",
            "paper)18\n",
            "\n",
            "The  idea  of  adding  noise  independently  from  the  codings  is  very  important.  Some\n",
            "parts of an image are quite random, such as the exact position of each freckle or hair.\n",
            "In  earlier  GANs,  this  randomness  had  to  either  come  from  the  codings  or  be  some\n",
            "pseudorandom  noise  produced  by  the  generator  itself.  If  it  came  from  the  codings,\n",
            "it meant that the generator had to dedicate a significant portion of the codings’ repre‐\n",
            "sentational power to storing noise, which this is quite wasteful. Moreover, the noise\n",
            "had to be able to flow through the network and reach the final layers of the generator:\n",
            "this  seems  like  an  unnecessary  constraint  that  probably  slowed  down  training.  And\n",
            "finally, some visual artifacts may appear because the same noise was used at different\n",
            "levels.  If  instead  the  generator  tried  to  produce  its  own  pseudorandom  noise,  this\n",
            "noise might not look very convincing, leading to more visual artifacts. Plus, part of\n",
            "the generator’s weights would be dedicated to generating pseudorandom noise, which\n",
            "again  seems  wasteful.  By  adding  extra  noise  inputs,  all  these  issues  are  avoided;  the\n",
            "\n",
            "18 Reproduced with the kind authorization of the authors.\n",
            "\n",
            "672 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fGAN is able to use the provided noise to add the right amount of stochasticity to each\n",
            "part of the image.\n",
            "\n",
            "The  added  noise  is  different  for  each  level.  Each  noise  input  consists  of  a  single\n",
            "feature  map  full  of  Gaussian  noise,  which  is  broadcast  to  all  feature  maps  (of  the\n",
            "given level) and scaled using learned per-feature scaling factors (this is represented by\n",
            "the “B” boxes in Figure 17-19) before it is added.\n",
            "\n",
            "Finally,  StyleGAN  uses  a  technique  called  mixing  regularization  (or  style  mixing),\n",
            "where a percentage of the generated images are produced using two different codings.\n",
            "Specifically, the codings c1 and c2 are sent through the mapping network, giving two\n",
            "style vectors w1 and w2. Then the synthesis network generates an image based on the\n",
            "styles w1 for the first levels and the styles w2 for the remaining levels. The cutoff level\n",
            "is picked randomly. This prevents the network from assuming that styles at adjacent\n",
            "levels  are  correlated,  which  in  turn  encourages  locality  in  the  GAN,  meaning  that\n",
            "each style vector only affects a limited number of traits in the generated image.\n",
            "\n",
            "There is such a wide variety of GANs out there that it would require a whole book to\n",
            "cover them all. Hopefully this introduction has given you the main ideas, and most\n",
            "importantly the desire to learn more. Go ahead and implement your own GAN, and\n",
            "do not get discouraged if it has trouble learning at first: unfortunately, this is normal,\n",
            "and  it  will  require  quite  a  bit  of  patience  to  get  it  working,  but  the  result  is  worth\n",
            "it.  If  you’re  struggling  with  an  implementation  detail,  there  are  plenty  of  Keras  or\n",
            "TensorFlow  implementations  that  you  can  look  at.  In  fact,  if  all  you  want  is  to  get\n",
            "some  amazing  results  quickly,  then  you  can  just  use  a  pretrained  model  (e.g.,  there\n",
            "are pretrained StyleGAN models available for Keras).\n",
            "\n",
            "Now  that  we’ve  examined  autoencoders  and  GANs,  let’s  look  at  one  last  type  of\n",
            "architecture: diffusion models.\n",
            "\n",
            "Diffusion Models\n",
            "The ideas behind diffusion models have been around for many years, but they were\n",
            "first  formalized  in  their  modern  form  in  a  2015  paper19  by  Jascha  Sohl-Dickstein\n",
            "et  al.  from  Stanford  University  and  UC  Berkeley.  The  authors  applied  tools  from\n",
            "thermodynamics to model a diffusion process, similar to a drop of milk diffusing in\n",
            "a cup of tea. The core idea is to train a model to learn the reverse process: start from\n",
            "the completely mixed state, and gradually “unmix” the milk from the tea. Using this\n",
            "idea, they obtained promising results in image generation, but since GANs produced\n",
            "more convincing images back then, diffusion models did not get as much attention.\n",
            "\n",
            "19 Jascha Sohl-Dickstein et al., “Deep Unsupervised Learning using Nonequilibrium Thermodynamics”, arXiv\n",
            "\n",
            "preprint arXiv:1503.03585 (2015).\n",
            "\n",
            "Diffusion Models \n",
            "\n",
            "| \n",
            "\n",
            "673\n",
            "\n",
            "\fThen, in 2020, Jonathan Ho et al., also from UC Berkeley, managed to build a diffu‐\n",
            "sion model capable of generating highly realistic images, which they called a denoising\n",
            "diffusion probabilistic model (DDPM).20 A few months later, a 2021 paper21 by OpenAI\n",
            "researchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM architecture and\n",
            "proposed several improvements that allowed DDPMs to finally beat GANs: not only\n",
            "are  DDPMs  much  easier  to  train  than  GANs,  but  the  generated  images  are  more\n",
            "diverse and of even higher quality. The main downside of DDPMs, as you will see, is\n",
            "that they take a very long time to generate images, compared to GANs or VAEs.\n",
            "\n",
            "So how exactly does a DDPM work? Well, suppose you start with a picture of a cat\n",
            "(like the one you’ll see in Figure 17-20), noted x0, and at each time step t you add a\n",
            "little  bit  of  Gaussian  noise  to  the  image,  with  mean  0  and  variance  βt.  This  noise  is\n",
            "independent for each pixel: we call it isotropic. You first obtain the image x1, then x2,\n",
            "and so on, until the cat is completely hidden by the noise, impossible to see. The last\n",
            "time step is noted T. In the original DDPM paper, the authors used T = 1,000, and\n",
            "they scheduled the variance βt in such a way that the cat signal fades linearly between\n",
            "time steps 0 and T. In the improved DDPM paper, T was bumped up to 4,000, and\n",
            "the  variance  schedule  was  tweaked  to  change  more  slowly  at  the  beginning  and  at\n",
            "the end. In short, we’re gradually drowning the cat in noise: this is called the forward\n",
            "process.\n",
            "\n",
            "As we add more and more Gaussian noise in the forward process, the distribution of\n",
            "pixel values becomes more and more Gaussian. One important detail I left out is that\n",
            "the pixel values get rescaled slightly at each step, by a factor of  1 − βt. This ensures\n",
            "that the mean of the pixel values gradually approaches 0, since the scaling factor is a\n",
            "bit smaller than 1 (imagine repeatedly multiplying a number by 0.99). It also ensures\n",
            "that the variance will gradually converge to 1. This is because the standard deviation\n",
            "of  the  pixel  values  also  gets  scaled  by  1 − βt,  so  the  variance  gets  scaled  by  1  –  βt\n",
            "(i.e., the square of the scaling factor). But the variance cannot shrink to 0 since we’re\n",
            "adding Gaussian noise with variance βt at each step. And since variances add up when\n",
            "you sum Gaussian distributions, you can see that the variance can only converge to\n",
            "1 – βt + βt = 1.\n",
            "\n",
            "The forward diffusion process is summarized in Equation 17-5. This equation won’t\n",
            "teach you anything new about the forward process, but it’s useful to understand this\n",
            "type of mathematical notation, as it’s often used in ML papers. This equation defines\n",
            "the probability distribution q of xt given xt–1 as a Gaussian distribution with mean xt–1\n",
            "times the scaling factor, and with a covariance matrix equal to βtI. This is the identity\n",
            "matrix I multiplied by βt, which means that the noise is isotropic with variance βt.\n",
            "\n",
            "20 Jonathan Ho et al., “Denoising Diffusion Probabilistic Models” (2020).\n",
            "\n",
            "21 Alex Nichol and Prafulla Dhariwal, “Improved Denoising Diffusion Probabilistic Models” (2021).\n",
            "\n",
            "674 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fEquation 17-5. Probability distribution q of the forward diffusion process\n",
            "\n",
            "q xt xt − 1 = N 1 − βtxt − 1 , βtI\n",
            "\n",
            "Interestingly,  there’s  a  shortcut  for  the  forward  process:  it’s  possible  to  sample  an\n",
            "image xt given x0 without having to first compute x1, x2, …, xt–1. Indeed, since the sum\n",
            "of multiple Gaussian distributions is also a Gaussian distribution, all the noise can be\n",
            "added in just one shot using Equation 17-6. This is the equation we will be using, as it\n",
            "is much faster.\n",
            "\n",
            "Equation 17-6. Shortcut for the forward diffusion process\n",
            "\n",
            "q xt x0 = N αtx0 , 1 − αt I\n",
            "\n",
            "Our  goal,  of  course,  is  not  to  drown  cats  in  noise.  On  the  contrary,  we  want  to\n",
            "create many new cats! We can do so by training a model that can perform the reverse\n",
            "process: going from xt to xt–1. We can then use it to remove a tiny bit of noise from an\n",
            "image, and repeat the operation many times until all the noise is gone. If we train the\n",
            "model on a dataset containing many cat images, then we can give it a picture entirely\n",
            "full of Gaussian noise, and the model will gradually make a brand new cat appear (see\n",
            "Figure 17-20).\n",
            "\n",
            "Figure 17-20. The forward process q and reverse process p\n",
            "\n",
            "OK, so let’s start coding! The first thing we need to do is to code the forward process.\n",
            "For this, we will first need to implement the variance schedule. How can we control\n",
            "how  fast  the  cat  disappears?  Initially,  100%  of  the  variance  comes  from  the  original\n",
            "cat image. Then at each time step t, the variance gets multiplied by 1 – βt, as explained\n",
            "earlier, and noise gets added. So, the part of the variance that comes from the initial\n",
            "distribution shrinks by a factor of 1 – βt at each step. If we define αt = 1 – βt, then after\n",
            "t time steps, the cat signal will have been multiplied by a factor of α̅t = α1×α2×…×αt =\n",
            "αt. It’s this “cat signal” factor α̅t that we want to schedule so it shrinks down\n",
            "αt = ∏i = 1\n",
            "\n",
            "t\n",
            "\n",
            "Diffusion Models \n",
            "\n",
            "| \n",
            "\n",
            "675\n",
            "\n",
            "\ffrom  1  to  0  gradually  between  time  steps  0  and  T.  In  the  improved  DDPM  paper,\n",
            "the authors schedule α̅t according to Equation 17-7. This schedule is represented in\n",
            "Figure 17-21.\n",
            "\n",
            "Equation 17-7. Variance schedule equations for the forward diffusion process\n",
            "\n",
            "βt = 1 −\n",
            "\n",
            "αt\n",
            "αt − 1\n",
            "\n",
            ", with αt =\n",
            "\n",
            "f t\n",
            "f 0\n",
            "\n",
            " and  f t = cos\n",
            "\n",
            "2\n",
            "\n",
            "t/T + s\n",
            "1 + s ·\n",
            "\n",
            "π\n",
            "2\n",
            "\n",
            "In these equations:\n",
            "\n",
            "•\n",
            "• s is a tiny value which prevents βt from being too small near t = 0. In the paper,\n",
            "\n",
            "the authors used s = 0.008.\n",
            "\n",
            "•\n",
            "• βt is clipped to be no larger than 0.999, to avoid instabilities near t = T.\n",
            "\n",
            "Figure 17-21. Noise variance schedule βt, and the remaining signal variance α̅t\n",
            "\n",
            "Let’s create a small function to compute αt, βt, and α̅t, and call it with T = 4,000:\n",
            "\n",
            "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
            "    t = np.arange(T + 1)\n",
            "    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\n",
            "    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n",
            "    alpha = np.append(1, alpha).astype(np.float32)  # add α₀ = 1\n",
            "    beta = 1 - alpha\n",
            "    alpha_cumprod = np.cumprod(alpha)\n",
            "    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\n",
            "\n",
            "T = 4000\n",
            "alpha, alpha_cumprod, beta = variance_schedule(T)\n",
            "\n",
            "676 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fTo train our model to reverse the diffusion process, we will need noisy images from\n",
            "different  time  steps  of  the  forward  process.  For  this,  let’s  create  a  prepare_batch()\n",
            "function that will take a batch of clean images from the dataset and prepare them:\n",
            "\n",
            "def prepare_batch(X):\n",
            "    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # scale from –1 to +1\n",
            "    X_shape = tf.shape(X)\n",
            "    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\n",
            "    alpha_cm = tf.gather(alpha_cumprod, t)\n",
            "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
            "    noise = tf.random.normal(X_shape)\n",
            "    return {\n",
            "        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\n",
            "        \"time\": t,\n",
            "    }, noise\n",
            "\n",
            "Let’s go through this code:\n",
            "\n",
            "•\n",
            "• For  simplicity  we  will  use  Fashion  MNIST,  so  the  function  must  first  add  a\n",
            "channel axis. It will also help to scale the pixel values from –1 to 1, so it’s closer to\n",
            "the final Gaussian distribution with mean 0 and variance 1.\n",
            "\n",
            "• Next,  the  function  creates  t,  a  vector  containing  a  random  time  step  for  each\n",
            "•\n",
            "\n",
            "image in the batch, between 1 and T.\n",
            "\n",
            "• Then it uses tf.gather() to get the value of alpha_cumprod for each of the time\n",
            "•\n",
            "steps in the vector t. This gives us the vector alpha_cm, containing one value of α̅t\n",
            "for each image.\n",
            "\n",
            "• The next line reshapes the alpha_cm from [batch size] to [batch size, 1, 1, 1]. This\n",
            "•\n",
            "\n",
            "is needed to ensure alpha_cm can be broadcasted with the batch X.\n",
            "\n",
            "•\n",
            "• Then we generate some Gaussian noise with mean 0 and variance 1.\n",
            "\n",
            "• Lastly,  we  use  Equation  17-6  to  apply  the  diffusion  process  to  the  images.  Note\n",
            "•\n",
            "that  x  **  0.5  is  equal  to  the  square  root  of  x.  The  function  returns  a  tuple\n",
            "containing  the  inputs  and  the  targets.  The  inputs  are  represented  as  a  Python\n",
            "dict containing the noisy images and the time steps used to generate them. The\n",
            "targets are the Gaussian noise used to generate each image.\n",
            "\n",
            "With  this  setup,  the  model  will  predict  the  noise  that  should  be\n",
            "subtracted  from  the  input  image  to  get  the  original  image.  Why\n",
            "not  predict  the  original  image  directly?  Well,  the  authors  tried:  it\n",
            "simply doesn’t work as well.\n",
            "\n",
            "Diffusion Models \n",
            "\n",
            "| \n",
            "\n",
            "677\n",
            "\n",
            "\fNext,  we’ll  create  a  training  dataset  and  a  validation  set  that  will  apply  the\n",
            "prepare_batch()  function  to  every  batch.  As  earlier,  X_train  and  X_valid  contain\n",
            "the Fashion MNIST images with pixel values ranging from 0 to 1:\n",
            "\n",
            "def prepare_dataset(X, batch_size=32, shuffle=False):\n",
            "    ds = tf.data.Dataset.from_tensor_slices(X)\n",
            "    if shuffle:\n",
            "        ds = ds.shuffle(buffer_size=10_000)\n",
            "    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n",
            "\n",
            "train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\n",
            "valid_set = prepare_dataset(X_valid, batch_size=32)\n",
            "\n",
            "Now we’re ready to build the actual diffusion model itself. It can be any model you\n",
            "want, as long as it takes the noisy images and time steps as inputs, and predicts the\n",
            "noise to subtract from the input images:\n",
            "\n",
            "def build_diffusion_model():\n",
            "    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n",
            "    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n",
            "    [...]  # build the model based on the noisy images and the time steps\n",
            "    outputs = [...]  # predict the noise (same shape as the input images)\n",
            "    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])\n",
            "\n",
            "The DDPM authors used a modified U-Net architecture,22 which has many similari‐\n",
            "ties with the FCN architecture we discussed in Chapter 14 for semantic segmentation:\n",
            "it’s  a  convolutional  neural  network  that  gradually  downsamples  the  input  images,\n",
            "then gradually upsamples them again, with skip connections crossing over from each\n",
            "level of the downsampling part to the corresponding level in the upsampling part. To\n",
            "take into account the time steps, they encoded them using the same technique as the\n",
            "positional encodings in the transformer architecture (see Chapter 16). At every level\n",
            "in the U-Net architecture, they passed these time encodings through Dense layers and\n",
            "fed  them  to  the  U-Net.  Lastly,  they  also  used  multi-head  attention  layers  at  various\n",
            "levels.  See  this  chapter’s  notebook  for  a  basic  implementation,  or  https://homl.info/\n",
            "ddpmcode for the official implementation: it’s based on TF 1.x, which is deprecated,\n",
            "but it’s quite readable.\n",
            "\n",
            "WE can now train the model normally. The authors noted that using the MAE loss\n",
            "worked better than the MSE. You can also use the Huber loss:\n",
            "\n",
            "model = build_diffusion_model()\n",
            "model.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\n",
            "history = model.fit(train_set, validation_data=valid_set, epochs=100)\n",
            "\n",
            "22 Olaf Ronneberger et al., “U-Net: Convolutional Networks for Biomedical Image Segmentation”, arXiv preprint\n",
            "\n",
            "arXiv:1505.04597 (2015).\n",
            "\n",
            "678 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fOnce  the  model  is  trained,  you  can  use  it  to  generate  new  images.  Unfortunately,\n",
            "there’s  no  shortcut  in  the  reverse  diffusion  process,  so  you  have  to  sample  xT\n",
            "randomly  from  a  Gaussian  distribution  with  mean  0  and  variance  1,  then  pass  it\n",
            "to the model to predict the noise; subtract it from the image using Equation 17-8, and\n",
            "you get xT–1. Repeat the process 3,999 more times until you get x0: if all went well, it\n",
            "should look like a regular Fashion MNIST image!\n",
            "\n",
            "Equation 17-8. Going one step in reverse in the diffusion process\n",
            "\n",
            "xt − 1 =\n",
            "\n",
            "1\n",
            "αt\n",
            "\n",
            "xt −\n",
            "\n",
            "βt\n",
            "1 − αt\n",
            "\n",
            "ϵθ xt, t + βtz\n",
            "\n",
            "In this equation, ϵθ(xt, t) represents the noise predicted by the model given the input\n",
            "image  xt  and  the  time  step  t.  The  θ  represents  the  model  parameters.  Moreover,\n",
            "z  is  Gaussian  noise  with  mean  0  and  variance  1.  This  makes  the  reverse  process\n",
            "stochastic: if you run it multiple times, you will get different images.\n",
            "\n",
            "Let’s  write  a  function  that  implements  this  reverse  process,  and  call  it  to  generate  a\n",
            "few images:\n",
            "\n",
            "def generate(model, batch_size=32):\n",
            "    X = tf.random.normal([batch_size, 28, 28, 1])\n",
            "    for t in range(T, 0, -1):\n",
            "        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\n",
            "        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n",
            "        X = (\n",
            "            1 / alpha[t] ** 0.5\n",
            "            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\n",
            "            + (1 - alpha[t]) ** 0.5 * noise\n",
            "        )\n",
            "    return X\n",
            "\n",
            "X_gen = generate(model)  # generated images\n",
            "\n",
            "This  may  take  a  minute  or  two.  That’s  the  main  drawback  of  diffusion  models:\n",
            "generating images is slow since the model needs to be called many times. It’s possible\n",
            "to make this faster by using a smaller T value, or by using the same model prediction\n",
            "for several steps at a time, but the resulting images may not look as nice. That said,\n",
            "despite  this  speed  limitation,  diffusion  models  do  produce  high-quality  and  diverse\n",
            "images, as you can see in Figure 17-22.\n",
            "\n",
            "Diffusion Models \n",
            "\n",
            "| \n",
            "\n",
            "679\n",
            "\n",
            "\fFigure 17-22. Images generated by the DDPM\n",
            "\n",
            "Diffusion  models  have  made  tremendous  progress  recently.  In  particular,  a  paper\n",
            "published in December 2021 by Robin Rombach, Andreas Blattmann, et al.,23 intro‐\n",
            "duced latent diffusion models, where the diffusion process takes place in latent space,\n",
            "rather  than  in  pixel  space.  To  achieve  this,  a  powerful  autoencoder  is  used  to  com‐\n",
            "press  each  training  image  into  a  much  smaller  latent  space,  where  the  diffusion\n",
            "process  takes  place,  then  the  autoencoder  is  used  to  decompress  the  final  latent\n",
            "representation,  generating  the  output  image.  This  considerably  speeds  up  image\n",
            "generation, and reduces training time and cost dramatically. Importantly, the quality\n",
            "of the generated images is outstanding.\n",
            "\n",
            "Moreover,  the  researchers  also  adapted  various  conditioning  techniques  to  guide\n",
            "the  diffusion  process  using  text  prompts,  images,  or  any  other  inputs.  This  makes\n",
            "it  possible  to  quickly  produce  a  beautiful,  high-resolution  image  of  a  salamander\n",
            "reading a book, or anything else you might fancy. You can also condition the image\n",
            "generation  process  using  an  input  image.  This  enables  many  applications,  such  as\n",
            "outpainting—where an input image is extended beyond its borders—or inpainting—\n",
            "where holes in an image are filled in.\n",
            "\n",
            "Lastly,  a  powerful  pretrained  latent  diffusion  model  named  Stable  Diffusion  was\n",
            "open  sourced  in  August  2022  by  a  collaboration  between  LMU  Munich  and  a  few\n",
            "companies,  including  StabilityAI,  and  Runway,  with  support  from  EleutherAI  and\n",
            "LAION. In September 2022, it was ported to TensorFlow and included in KerasCV,\n",
            "a computer vision library built by the Keras team. Now anyone can generate mind‐\n",
            "\n",
            "23 Robin Rombach, Andreas Blattmann, et al., “High-Resolution Image Synthesis with Latent Diffusion Models”,\n",
            "\n",
            "arXiv preprint arXiv:2112.10752 (2021).\n",
            "\n",
            "680 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fblowing images in seconds, for free, even on a regular laptop (see the last exercise in\n",
            "this chapter). The possibilities are endless!\n",
            "\n",
            "In the next chapter we will move on to an entirely different branch of deep learning:\n",
            "deep reinforcement learning.\n",
            "\n",
            "Exercises\n",
            "\n",
            "1. What are the main tasks that autoencoders are used for?\n",
            "1.\n",
            "\n",
            "2. Suppose you want to train a classifier, and you have plenty of unlabeled training\n",
            "2.\n",
            "data  but  only  a  few  thousand  labeled  instances.  How  can  autoencoders  help?\n",
            "How would you proceed?\n",
            "\n",
            "3.\n",
            "3. If  an  autoencoder  perfectly  reconstructs  the  inputs,  is  it  necessarily  a  good\n",
            "\n",
            "autoencoder? How can you evaluate the performance of an autoencoder?\n",
            "\n",
            "4.\n",
            "4. What are undercomplete and overcomplete autoencoders? What is the main risk\n",
            "of  an  excessively  undercomplete  autoencoder?  What  about  the  main  risk  of  an\n",
            "overcomplete autoencoder?\n",
            "\n",
            "5.\n",
            "5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
            "\n",
            "6.\n",
            "6. What is a generative model? Can you name a type of generative autoencoder?\n",
            "\n",
            "7.\n",
            "7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
            "\n",
            "8.\n",
            "8. What are the main difficulties when training GANs?\n",
            "\n",
            "9.\n",
            "9. What are diffusion models good at? What is their main limitation?\n",
            "\n",
            "10. Try  using  a  denoising  autoencoder  to  pretrain  an  image  classifier.  You  can  use\n",
            "10.\n",
            "MNIST (the simplest option), or a more complex image dataset such as CIFAR10\n",
            "if you want a bigger challenge. Regardless of the dataset you’re using, follow these\n",
            "steps:\n",
            "\n",
            "a.\n",
            "a. Split  the  dataset  into  a  training  set  and  a  test  set.  Train  a  deep  denoising\n",
            "\n",
            "autoencoder on the full training set.\n",
            "\n",
            "b.\n",
            "b. Check that the images are fairly well reconstructed. Visualize the images that\n",
            "\n",
            "most activate each neuron in the coding layer.\n",
            "\n",
            "c. Build a classification DNN, reusing the lower layers of the autoencoder. Train\n",
            "c.\n",
            "it using only 500 images from the training set. Does it perform better with or\n",
            "without pretraining?\n",
            "\n",
            "11. Train a variational autoencoder on the image dataset of your choice, and use it to\n",
            "11.\n",
            "generate images. Alternatively, you can try to find an unlabeled dataset that you\n",
            "are interested in and see if you can generate new samples.\n",
            "\n",
            "Exercises \n",
            "\n",
            "| \n",
            "\n",
            "681\n",
            "\n",
            "\f12. Train a DCGAN to tackle the image dataset of your choice, and use it to generate\n",
            "12.\n",
            "images.  Add  experience  replay  and  see  if  this  helps.  Turn  it  into  a  conditional\n",
            "GAN where you can control the generated class.\n",
            "\n",
            "13.\n",
            "13. Go through KerasCV’s excellent Stable Diffusion tutorial, and generate a beauti‐\n",
            "ful  drawing  of  a  salamander  reading  a  book.  If  you  post  your  best  drawing  on\n",
            "Twitter, please tag me at @aureliengeron. I’d love to see your creations!\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "682 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 17: Autoencoders, GANs, and Diffusion Models\n",
            "\n",
            "\fCHAPTER 18\n",
            "Reinforcement Learning\n",
            "\n",
            "Reinforcement  learning  (RL)  is  one  of  the  most  exciting  fields  of  machine  learning\n",
            "today, and also one of the oldest. It has been around since the 1950s, producing many\n",
            "interesting  applications  over  the  years,1  particularly  in  games  (e.g.,  TD-Gammon,\n",
            "a  Backgammon-playing  program)  and  in  machine  control,  but  seldom  making  the\n",
            "headline  news.  However,  a  revolution  took  place  in  2013,  when  researchers  from  a\n",
            "British startup called DeepMind demonstrated a system that could learn to play just\n",
            "about  any  Atari  game  from  scratch,2  eventually  outperforming  humans3  in  most  of\n",
            "them, using only raw pixels as inputs and without any prior knowledge of the rules\n",
            "of  the  games.4  This  was  the  first  of  a  series  of  amazing  feats,  culminating  with  the\n",
            "victory of their system AlphaGo against Lee Sedol, a legendary professional player of\n",
            "the game of Go, in March 2016 and against Ke Jie, the world champion, in May 2017.\n",
            "No program had ever come close to beating a master of this game, let alone the world\n",
            "champion. Today the whole field of RL is boiling with new ideas, with a wide range of\n",
            "applications.\n",
            "\n",
            "So how did DeepMind (bought by Google for over $500 million in 2014) achieve all\n",
            "this? With hindsight it seems rather simple: they applied the power of deep learning\n",
            "to the field of reinforcement learning, and it worked beyond their wildest dreams. In\n",
            "this chapter I will first explain what reinforcement learning is and what it’s good at,\n",
            "\n",
            "1 For more details, be sure to check out Richard Sutton and Andrew Barto’s book on RL, Reinforcement\n",
            "\n",
            "Learning: An Introduction (MIT Press).\n",
            "\n",
            "2 Volodymyr Mnih et al., “Playing Atari with Deep Reinforcement Learning”, arXiv preprint arXiv:1312.5602\n",
            "\n",
            "(2013).\n",
            "\n",
            "3 Volodymyr Mnih et al., “Human-Level Control Through Deep Reinforcement Learning”, Nature 518 (2015):\n",
            "\n",
            "529–533.\n",
            "\n",
            "4 Check out the videos of DeepMind’s system learning to play Space Invaders, Breakout, and other video games\n",
            "\n",
            "at https://homl.info/dqn3.\n",
            "\n",
            "683\n",
            "\n",
            "\fthen present two of the most important techniques in deep reinforcement learning:\n",
            "policy  gradients  and  deep  Q-networks,  including  a  discussion  of  Markov  decision\n",
            "processes. Let’s get started!\n",
            "\n",
            "Learning to Optimize Rewards\n",
            "In  reinforcement  learning,  a  software  agent  makes  observations  and  takes  actions\n",
            "within  an  environment,  and  in  return  it  receives  rewards  from  the  environment.  Its\n",
            "objective is to learn to act in a way that will maximize its expected rewards over time.\n",
            "If you don’t mind a bit of anthropomorphism, you can think of positive rewards as\n",
            "pleasure, and negative rewards as pain (the term “reward” is a bit misleading in this\n",
            "case).  In  short,  the  agent  acts  in  the  environment  and  learns  by  trial  and  error  to\n",
            "maximize its pleasure and minimize its pain.\n",
            "\n",
            "This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few\n",
            "examples (see Figure 18-1):\n",
            "\n",
            "a. The agent can be the program controlling a robot. In this case, the environment\n",
            "a.\n",
            "is  the  real  world,  the  agent  observes  the  environment  through  a  set  of  sensors\n",
            "such  as  cameras  and  touch  sensors,  and  its  actions  consist  of  sending  signals\n",
            "to  activate  motors.  It  may  be  programmed  to  get  positive  rewards  whenever  it\n",
            "approaches the target destination, and negative rewards whenever it wastes time\n",
            "or goes in the wrong direction.\n",
            "\n",
            "b.\n",
            "b. The agent can be the program controlling Ms. Pac-Man. In this case, the environ‐\n",
            "ment is a simulation of the Atari game, the actions are the nine possible joystick\n",
            "positions (upper left, down, center, and so on), the observations are screenshots,\n",
            "and the rewards are just the game points.\n",
            "\n",
            "c.\n",
            "c. Similarly, the agent can be the program playing a board game such as Go. It only\n",
            "\n",
            "gets a reward if it wins.\n",
            "\n",
            "d. The agent does not have to control a physically (or virtually) moving thing. For\n",
            "d.\n",
            "example,  it  can  be  a  smart  thermostat,  getting  positive  rewards  whenever  it  is\n",
            "close  to  the  target  temperature  and  saves  energy,  and  negative  rewards  when\n",
            "humans  need  to  tweak  the  temperature,  so  the  agent  must  learn  to  anticipate\n",
            "human needs.\n",
            "\n",
            "e.\n",
            "e. The agent can observe stock market prices and decide how much to buy or sell\n",
            "\n",
            "every second. Rewards are obviously the monetary gains and losses.\n",
            "\n",
            "Note  that  there  may  not  be  any  positive  rewards  at  all;  for  example,  the  agent  may\n",
            "move  around  in  a  maze,  getting  a  negative  reward  at  every  time  step,  so  it  had\n",
            "better find the exit as quickly as possible! There are many other examples of tasks to\n",
            "which reinforcement learning is well suited, such as self-driving cars, recommender\n",
            "\n",
            "684 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fsystems,  placing  ads  on  a  web  page,  or  controlling  where  an  image  classification\n",
            "system should focus its attention.\n",
            "\n",
            "Figure 18-1. Reinforcement learning examples: (a) robotics, (b) Ms. Pac-Man, (c) Go\n",
            "player, (d) thermostat, (e) automatic trader5\n",
            "\n",
            "Policy Search\n",
            "The algorithm a software agent uses to determine its actions is called its policy. The\n",
            "policy  could  be  a  neural  network  taking  observations  as  inputs  and  outputting  the\n",
            "action to take (see Figure 18-2).\n",
            "\n",
            "5 Images (a), (d), and (e) are in the public domain. Image (b) is a screenshot from the Ms. Pac-Man game,\n",
            "copyright Atari (fair use in this chapter). Image (c) is reproduced from Wikipedia; it was created by user\n",
            "Stevertigo and released under Creative Commons BY-SA 2.0.\n",
            "\n",
            "Policy Search \n",
            "\n",
            "| \n",
            "\n",
            "685\n",
            "\n",
            "\fFigure 18-2. Reinforcement learning using a neural network policy\n",
            "\n",
            "The policy can be any algorithm you can think of, and it does not have to be deter‐\n",
            "ministic. In fact, in some cases it does not even have to observe the environment! For\n",
            "example,  consider  a  robotic  vacuum  cleaner  whose  reward  is  the  amount  of  dust  it\n",
            "picks  up  in  30  minutes.  Its  policy  could  be  to  move  forward  with  some  probability\n",
            "p every second, or randomly rotate left or right with probability 1 – p. The rotation\n",
            "angle  would  be  a  random  angle  between  –r  and  +r.  Since  this  policy  involves  some\n",
            "randomness, it is called a stochastic policy. The robot will have an erratic trajectory,\n",
            "which guarantees that it will eventually get to any place it can reach and pick up all\n",
            "the dust. The question is, how much dust will it pick up in 30 minutes?\n",
            "\n",
            "How  would  you  train  such  a  robot?  There  are  just  two  policy  parameters  you  can\n",
            "tweak: the probability p and the angle range r. One possible learning algorithm could\n",
            "be  to  try  out  many  different  values  for  these  parameters,  and  pick  the  combination\n",
            "that performs best (see Figure 18-3). This is an example of policy search, in this case\n",
            "using a brute-force approach. When the policy space is too large (which is generally\n",
            "the case), finding a good set of parameters this way is like searching for a needle in a\n",
            "gigantic haystack.\n",
            "\n",
            "Another  way  to  explore  the  policy  space  is  to  use  genetic  algorithms.  For  example,\n",
            "you could randomly create a first generation of 100 policies and try them out, then\n",
            "“kill” the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An\n",
            "offspring is a copy of its parent7 plus some random variation. The surviving policies\n",
            "plus  their  offspring  together  constitute  the  second  generation.  You  can  continue  to\n",
            "iterate through generations this way until you find a good policy.8\n",
            "\n",
            "6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene\n",
            "\n",
            "pool”.\n",
            "\n",
            "7 If there is a single parent, this is called asexual reproduction. With two (or more) parents, it is called sexual\n",
            "\n",
            "reproduction. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of\n",
            "its parents’ genomes.\n",
            "\n",
            "8 One interesting example of a genetic algorithm used for reinforcement learning is the NeuroEvolution of\n",
            "\n",
            "Augmenting Topologies (NEAT) algorithm.\n",
            "\n",
            "686 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fFigure 18-3. Four points in the policy space (left) and the agent’s corresponding behavior\n",
            "(right)\n",
            "\n",
            "Yet another approach is to use optimization techniques, by evaluating the gradients of\n",
            "the rewards with regard to the policy parameters, then tweaking these parameters by\n",
            "following the gradients toward higher rewards.9 We will discuss this approach, called\n",
            "policy gradients (PG), in more detail later in this chapter. Going back to the vacuum\n",
            "cleaner robot, you could slightly increase p and evaluate whether doing so increases\n",
            "the  amount  of  dust  picked  up  by  the  robot  in  30  minutes;  if  it  does,  then  increase\n",
            "p  some  more,  or  else  reduce  p.  We  will  implement  a  popular  PG  algorithm  using\n",
            "TensorFlow, but before we do, we need to create an environment for the agent to live\n",
            "in—so it’s time to introduce OpenAI Gym.\n",
            "\n",
            "Introduction to OpenAI Gym\n",
            "One  of  the  challenges  of  reinforcement  learning  is  that  in  order  to  train  an  agent,\n",
            "you first need to have a working environment. If you want to program an agent that\n",
            "will learn to play an Atari game, you will need an Atari game simulator. If you want\n",
            "to  program  a  walking  robot,  then  the  environment  is  the  real  world,  and  you  can\n",
            "directly train your robot in that environment. However, this has its limits: if the robot\n",
            "falls  off  a  cliff,  you  can’t  just  click  Undo.  You  can’t  speed  up  time  either—adding\n",
            "more  computing  power  won’t  make  the  robot  move  any  faster—and  it’s  generally\n",
            "too expensive to train 1,000 robots in parallel. In short, training is hard and slow in\n",
            "the  real  world,  so  you  generally  need  a  simulated  environment  at  least  for  bootstrap\n",
            "\n",
            "9 This is called gradient ascent. It’s just like gradient descent, but in the opposite direction: maximizing instead\n",
            "\n",
            "of minimizing.\n",
            "\n",
            "Introduction to OpenAI Gym \n",
            "\n",
            "| \n",
            "\n",
            "687\n",
            "\n",
            "\ftraining.  For  example,  you  might  use  a  library  like  PyBullet  or  MuJoCo  for  3D\n",
            "physics simulation.\n",
            "\n",
            "OpenAI  Gym10  is  a  toolkit  that  provides  a  wide  variety  of  simulated  environments\n",
            "(Atari games, board games, 2D and 3D physical simulations, and so on), that you can\n",
            "use to train agents, compare them, or develop new RL algorithms.\n",
            "\n",
            "OpenAI  Gym  is  preinstalled  on  Colab,  but  it’s  an  older  version,  so  you’ll  need  to\n",
            "replace it with the latest one. You also need to install a few of its dependencies. If you\n",
            "are coding on your own machine instead of Colab, and you followed the installation\n",
            "instructions  at  https://homl.info/install,  then  you  can  skip  this  step;  otherwise,  enter\n",
            "these commands:\n",
            "\n",
            "# Only run these commands on Colab or Kaggle!\n",
            "%pip install -q -U gym\n",
            "%pip install -q -U gym[classic_control,box2d,atari,accept-rom-license]\n",
            "\n",
            "The first %pip command upgrades Gym to the latest version. The -q option stands for\n",
            "quiet: it makes the output less verbose. The -U option stands for upgrade. The second\n",
            "%pip command installs the libraries required to run various kinds of environments.\n",
            "This  includes  classic  environments  from  control  theory–the  science  of  controlling\n",
            "dynamical systems–such as balancing a pole on a cart. It also includes environments\n",
            "based on the Box2D library—a 2D physics engine for games. Lastly, it includes envi‐\n",
            "ronments  based  on  the  Arcade  Learning  Environment  (ALE),  which  is  an  emulator\n",
            "for Atari 2600 games. Several Atari game ROMs are downloaded automatically, and\n",
            "by running this code you agree with Atari’s ROM licenses.\n",
            "\n",
            "With  that,  you’re  ready  to  use  OpenAI  Gym.  Let’s  import  it  and  make  an\n",
            "environment:\n",
            "\n",
            "import gym\n",
            "\n",
            "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
            "\n",
            "Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart\n",
            "can  be  accelerated  left  or  right  in  order  to  balance  a  pole  placed  on  top  of  it  (see\n",
            "Figure 18-4). This is a classic control task.\n",
            "\n",
            "The gym.envs.registry dictionary contains the names and speci‐\n",
            "fications of all the available environments.\n",
            "\n",
            "10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to\n",
            "\n",
            "promote and develop friendly AIs that will benefit humanity (rather than exterminate it).\n",
            "\n",
            "688 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fFigure 18-4. The CartPole environment\n",
            "\n",
            "After  the  environment  is  created,  you  must  initialize  it  using  the  reset()  method,\n",
            "optionally specifying a random seed. This returns the first observation. Observations\n",
            "depend on the type of environment. For the CartPole environment, each observation\n",
            "is a 1D NumPy array containing four floats representing the cart’s horizontal position\n",
            "(0.0  =  center),  its  velocity  (positive  means  right),  the  angle  of  the  pole  (0.0  =\n",
            "vertical),  and  its  angular  velocity  (positive  means  clockwise).  The  reset()  method\n",
            "also  returns  a  dictionary  that  may  contain  extra  environment-specific  information.\n",
            "This can be useful for debugging or for training. For example, in many Atari environ‐\n",
            "ments,  it  contains  the  number  of  lives  left.  However,  in  the  CartPole  environment,\n",
            "this dictionary is empty.\n",
            "\n",
            ">>> obs, info = env.reset(seed=42)\n",
            ">>> obs\n",
            "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)\n",
            ">>> info\n",
            "{}\n",
            "\n",
            "Let’s  call  the  render()  method  to  render  this  environment  as  an  image.  Since  we\n",
            "set  render_mode=\"rgb_array\"  when  creating  the  environment,  the  image  will  be\n",
            "returned as a NumPy array:\n",
            "\n",
            ">>> img = env.render()\n",
            ">>> img.shape  # height, width, channels (3 = Red, Green, Blue)\n",
            "(400, 600, 3)\n",
            "\n",
            "You can then use Matplotlib’s imshow() function to display this image, as usual.\n",
            "\n",
            "Now let’s ask the environment what actions are possible:\n",
            "\n",
            ">>> env.action_space\n",
            "Discrete(2)\n",
            "\n",
            "Discrete(2)  means  that  the  possible  actions  are  integers  0  and  1,  which  represent\n",
            "accelerating  left  or  right.  Other  environments  may  have  additional  discrete  actions,\n",
            "or other kinds of actions (e.g., continuous). Since the pole is leaning toward the right\n",
            "(obs[2] > 0), let’s accelerate the cart toward the right:\n",
            "\n",
            "Introduction to OpenAI Gym \n",
            "\n",
            "| \n",
            "\n",
            "689\n",
            "\n",
            "\f>>> action = 1  # accelerate right\n",
            ">>> obs, reward, done, truncated, info = env.step(action)\n",
            ">>> obs\n",
            "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)\n",
            ">>> reward\n",
            "1.0\n",
            ">>> done\n",
            "False\n",
            ">>> truncated\n",
            "False\n",
            ">>> info\n",
            "{}\n",
            "\n",
            "The step() method executes the desired action and returns five values:\n",
            "\n",
            "obs\n",
            "\n",
            "This is the new observation. The cart is now moving toward the right (obs[1] >\n",
            "0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is\n",
            "now negative (obs[3] < 0), so it will likely be tilted toward the left after the next\n",
            "step.\n",
            "\n",
            "reward\n",
            "\n",
            "In this environment, you get a reward of 1.0 at every step, no matter what you do,\n",
            "so the goal is to keep the episode running for as long as possible.\n",
            "\n",
            "done\n",
            "\n",
            "This value will be True when the episode is over. This will happen when the pole\n",
            "tilts too much, or goes off the screen, or after 200 steps (in this last case, you have\n",
            "won). After that, the environment must be reset before it can be used again.\n",
            "\n",
            "truncated\n",
            "\n",
            "This  value  will  be  True  when  an  episode  is  interrupted  early,  for  example  by\n",
            "an environment wrapper that imposes a maximum number of steps per episode\n",
            "(see Gym’s documentation for more details on environment wrappers). Some RL\n",
            "algorithms  treat  truncated  episodes  differently  from  episodes  finished  normally\n",
            "(i.e., when done is True), but in this chapter we will treat them identically.\n",
            "\n",
            "info\n",
            "\n",
            "This environment-specific dictionary may provide extra information, just like the\n",
            "one returned by the reset() method.\n",
            "\n",
            "Once you have finished using an environment, you should call its\n",
            "close() method to free resources.\n",
            "\n",
            "690 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fLet’s hardcode a simple policy that accelerates left when the pole is leaning toward the\n",
            "left and accelerates right when the pole is leaning toward the right. We will run this\n",
            "policy to see the average rewards it gets over 500 episodes:\n",
            "\n",
            "def basic_policy(obs):\n",
            "    angle = obs[2]\n",
            "    return 0 if angle < 0 else 1\n",
            "\n",
            "totals = []\n",
            "for episode in range(500):\n",
            "    episode_rewards = 0\n",
            "    obs, info = env.reset(seed=episode)\n",
            "    for step in range(200):\n",
            "        action = basic_policy(obs)\n",
            "        obs, reward, done, truncated, info = env.step(action)\n",
            "        episode_rewards += reward\n",
            "        if done or truncated:\n",
            "            break\n",
            "\n",
            "    totals.append(episode_rewards)\n",
            "\n",
            "This code is self-explanatory. Let’s look at the result:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> np.mean(totals), np.std(totals), min(totals), max(totals)\n",
            "(41.698, 8.389445512070509, 24.0, 63.0)\n",
            "\n",
            "Even  with  500  tries,  this  policy  never  managed  to  keep  the  pole  upright  for  more\n",
            "than  63  consecutive  steps.  Not  great.  If  you  look  at  the  simulation  in  this  chapter’s\n",
            "notebook, you will see that the cart oscillates left and right more and more strongly\n",
            "until the pole tilts too much. Let’s see if a neural network can come up with a better\n",
            "policy.\n",
            "\n",
            "Neural Network Policies\n",
            "Let’s create a neural network policy. This neural network will take an observation as\n",
            "input, and it will output the action to be executed, just like the policy we hardcoded\n",
            "earlier. More precisely, it will estimate a probability for each action, and then we will\n",
            "select an action randomly, according to the estimated probabilities (see Figure 18-5).\n",
            "In  the  case  of  the  CartPole  environment,  there  are  just  two  possible  actions  (left  or\n",
            "right), so we only need one output neuron. It will output the probability p of action\n",
            "0 (left), and of course the probability of action 1 (right) will be 1 – p. For example, if\n",
            "it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with 30%\n",
            "probability.\n",
            "\n",
            "Neural Network Policies \n",
            "\n",
            "| \n",
            "\n",
            "691\n",
            "\n",
            "\fFigure 18-5. Neural network policy\n",
            "\n",
            "You  may  wonder  why  we  are  picking  a  random  action  based  on  the  probabilities\n",
            "given  by  the  neural  network,  rather  than  just  picking  the  action  with  the  highest\n",
            "score.  This  approach  lets  the  agent  find  the  right  balance  between  exploring  new\n",
            "actions  and  exploiting  the  actions  that  are  known  to  work  well.  Here’s  an  analogy:\n",
            "suppose  you  go  to  a  restaurant  for  the  first  time,  and  all  the  dishes  look  equally\n",
            "appealing, so you randomly pick one. If it turns out to be good, you can increase the\n",
            "probability that you’ll order it next time, but you shouldn’t increase that probability\n",
            "up  to  100%,  or  else  you  will  never  try  out  the  other  dishes,  some  of  which  may  be\n",
            "even better than the one you tried. This exploration/exploitation dilemma is central in\n",
            "reinforcement learning.\n",
            "\n",
            "Also note that in this particular environment, the past actions and observations can\n",
            "safely be ignored, since each observation contains the environment’s full state. If there\n",
            "were some hidden state, then you might need to consider past actions and observa‐\n",
            "tions as well. For example, if the environment only revealed the position of the cart\n",
            "but  not  its  velocity,  you  would  have  to  consider  not  only  the  current  observation\n",
            "but also the previous observation in order to estimate the current velocity. Another\n",
            "example  is  when  the  observations  are  noisy;  in  that  case,  you  generally  want  to  use\n",
            "the  past  few  observations  to  estimate  the  most  likely  current  state.  The  CartPole\n",
            "\n",
            "692 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fproblem is thus as simple as can be; the observations are noise-free, and they contain\n",
            "the environment’s full state.\n",
            "\n",
            "Here is the code to build a basic neural network policy using Keras:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
            "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
            "])\n",
            "\n",
            "We  use  a  Sequential  model  to  define  the  policy  network.  The  number  of  inputs\n",
            "is  the  size  of  the  observation  space—which  in  the  case  of  CartPole  is  4—and  we\n",
            "have  just  five  hidden  units  because  it’s  a  fairly  simple  task.  Finally,  we  want  to\n",
            "output a single probability—the probability of going left—so we have a single output\n",
            "neuron using the sigmoid activation function. If there were more than two possible\n",
            "actions, there would be one output neuron per action, and we would use the softmax\n",
            "activation function instead.\n",
            "\n",
            "OK,  we  now  have  a  neural  network  policy  that  will  take  observations  and  output\n",
            "action probabilities. But how do we train it?\n",
            "\n",
            "Evaluating Actions: The Credit Assignment Problem\n",
            "If we knew what the best action was at each step, we could train the neural network\n",
            "as  usual,  by  minimizing  the  cross  entropy  between  the  estimated  probability  distri‐\n",
            "bution  and  the  target  probability  distribution.  It  would  just  be  regular  supervised\n",
            "learning.  However,  in  reinforcement  learning  the  only  guidance  the  agent  gets  is\n",
            "through  rewards,  and  rewards  are  typically  sparse  and  delayed.  For  example,  if  the\n",
            "agent manages to balance the pole for 100 steps, how can it know which of the 100\n",
            "actions it took were good, and which of them were bad? All it knows is that the pole\n",
            "fell after the last action, but surely this last action is not entirely responsible. This is\n",
            "called the credit assignment problem: when the agent gets a reward, it is hard for it to\n",
            "know which actions should get credited (or blamed) for it. Think of a dog that gets\n",
            "rewarded hours after it behaved well; will it understand what it is being rewarded for?\n",
            "\n",
            "To tackle this problem, a common strategy is to evaluate an action based on the sum\n",
            "of  all  the  rewards  that  come  after  it,  usually  applying  a  discount  factor,  γ  (gamma),\n",
            "at  each  step.  This  sum  of  discounted  rewards  is  called  the  action’s  return.  Consider\n",
            "the example in Figure 18-6. If an agent decides to go right three times in a row and\n",
            "gets +10 reward after the first step, 0 after the second step, and finally –50 after the\n",
            "third step, then assuming we use a discount factor γ = 0.8, the first action will have\n",
            "a  return  of  10  +  γ  ×  0  +  γ2  ×  (–50)  =  –22.  If  the  discount  factor  is  close  to  0,  then\n",
            "future rewards won’t count for much compared to immediate rewards. Conversely, if\n",
            "the  discount  factor  is  close  to  1,  then  rewards  far  into  the  future  will  count  almost\n",
            "\n",
            "Evaluating Actions: The Credit Assignment Problem \n",
            "\n",
            "| \n",
            "\n",
            "693\n",
            "\n",
            "\fas much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With\n",
            "a  discount  factor  of  0.95,  rewards  13  steps  into  the  future  count  roughly  for  half  as\n",
            "much as immediate rewards (since 0.9513 ≈ 0.5), while with a discount factor of 0.99,\n",
            "rewards 69 steps into the future count for half as much as immediate rewards. In the\n",
            "CartPole environment, actions have fairly short-term effects, so choosing a discount\n",
            "factor of 0.95 seems reasonable.\n",
            "\n",
            "Figure 18-6. Computing an action’s return: the sum of discounted future rewards\n",
            "\n",
            "Of course, a good action may be followed by several bad actions that cause the pole to\n",
            "fall quickly, resulting in the good action getting a low return. Similarly, a good actor\n",
            "may sometimes star in a terrible movie. However, if we play the game enough times,\n",
            "on average good actions will get a higher return than bad ones. We want to estimate\n",
            "how  much  better  or  worse  an  action  is,  compared  to  the  other  possible  actions,  on\n",
            "average.  This  is  called  the  action  advantage.  For  this,  we  must  run  many  episodes\n",
            "and  normalize  all  the  action  returns,  by  subtracting  the  mean  and  dividing  by  the\n",
            "standard deviation. After that, we can reasonably assume that actions with a negative\n",
            "advantage were bad while actions with a positive advantage were good. OK, now that\n",
            "we  have  a  way  to  evaluate  each  action,  we  are  ready  to  train  our  first  agent  using\n",
            "policy gradients. Let’s see how.\n",
            "\n",
            "Policy Gradients\n",
            "As discussed earlier, PG algorithms optimize the parameters of a policy by following\n",
            "the  gradients  toward  higher  rewards.  One  popular  class  of  PG  algorithms,  called\n",
            "\n",
            "694 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fREINFORCE algorithms, was introduced back in 199211 by Ronald Williams. Here is\n",
            "one common variant:\n",
            "\n",
            "1.\n",
            "1. First, let the neural network policy play the game several times, and at each step,\n",
            "compute the gradients that would make the chosen action even more likely—but\n",
            "don’t apply these gradients yet.\n",
            "\n",
            "2. Once you have run several episodes, compute each action’s advantage, using the\n",
            "2.\n",
            "\n",
            "method described in the previous section.\n",
            "\n",
            "3. If an action’s advantage is positive, it means that the action was probably good,\n",
            "3.\n",
            "and  you  want  to  apply  the  gradients  computed  earlier  to  make  the  action  even\n",
            "more likely to be chosen in the future. However, if the action’s advantage is nega‐\n",
            "tive, it means the action was probably bad, and you want to apply the opposite\n",
            "gradients to make this action slightly less likely in the future. The solution is to\n",
            "multiply each gradient vector by the corresponding action’s advantage.\n",
            "\n",
            "4.\n",
            "4. Finally,  compute  the  mean  of  all  the  resulting  gradient  vectors,  and  use  it  to\n",
            "\n",
            "perform a gradient descent step.\n",
            "\n",
            "Let’s use Keras to implement this algorithm. We will train the neural network policy\n",
            "we  built  earlier  so  that  it  learns  to  balance  the  pole  on  the  cart.  First,  we  need  a\n",
            "function that will play one step. We will pretend for now that whatever action it takes\n",
            "is the right one so that we can compute the loss and its gradients. These gradients will\n",
            "just be saved for a while, and we will modify them later depending on how good or\n",
            "bad the action turned out to be:\n",
            "\n",
            "def play_one_step(env, obs, model, loss_fn):\n",
            "    with tf.GradientTape() as tape:\n",
            "        left_proba = model(obs[np.newaxis])\n",
            "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
            "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
            "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
            "\n",
            "    grads = tape.gradient(loss, model.trainable_variables)\n",
            "    obs, reward, done, truncated, info = env.step(int(action))\n",
            "    return obs, reward, done, truncated, grads\n",
            "\n",
            "Let’s walk though this function:\n",
            "\n",
            "• Within the GradientTape block (see Chapter 12), we start by calling the model,\n",
            "•\n",
            "giving it a single observation. We reshape the observation so it becomes a batch\n",
            "containing  a  single  instance,  as  the  model  expects  a  batch.  This  outputs  the\n",
            "probability of going left.\n",
            "\n",
            "11 Ronald J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement\n",
            "\n",
            "Leaning”, Machine Learning 8 (1992) : 229–256.\n",
            "\n",
            "Policy Gradients \n",
            "\n",
            "| \n",
            "\n",
            "695\n",
            "\n",
            "\f• Next,  we  sample  a  random  float  between  0  and  1,  and  we  check  whether  it  is\n",
            "•\n",
            "greater than left_proba. The action will be False with probability left_proba,\n",
            "or  True  with  probability  1  –  left_proba.  Once  we  cast  this  Boolean  to  an\n",
            "integer, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\n",
            "\n",
            "• We now define the target probability of going left: it is 1 minus the action (cast to\n",
            "•\n",
            "a float). If the action is 0 (left), then the target probability of going left will be 1. If\n",
            "the action is 1 (right), then the target probability will be 0.\n",
            "\n",
            "• Then we compute the loss using the given loss function, and we use the tape to\n",
            "•\n",
            "compute  the  gradient  of  the  loss  with  regard  to  the  model’s  trainable  variables.\n",
            "Again, these gradients will be tweaked later, before we apply them, depending on\n",
            "how good or bad the action turned out to be.\n",
            "\n",
            "• Finally,  we  play  the  selected  action,  and  we  return  the  new  observation,  the\n",
            "•\n",
            "reward, whether the episode is ended or not, whether it is truncated or not, and\n",
            "of course the gradients that we just computed.\n",
            "\n",
            "Now let’s create another function that will rely on the play_one_step() function to\n",
            "play multiple episodes, returning all the rewards and gradients for each episode and\n",
            "each step:\n",
            "\n",
            "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
            "    all_rewards = []\n",
            "    all_grads = []\n",
            "    for episode in range(n_episodes):\n",
            "        current_rewards = []\n",
            "        current_grads = []\n",
            "        obs, info = env.reset()\n",
            "        for step in range(n_max_steps):\n",
            "            obs, reward, done, truncated, grads = play_one_step(\n",
            "                env, obs, model, loss_fn)\n",
            "            current_rewards.append(reward)\n",
            "            current_grads.append(grads)\n",
            "            if done or truncated:\n",
            "                break\n",
            "\n",
            "        all_rewards.append(current_rewards)\n",
            "        all_grads.append(current_grads)\n",
            "\n",
            "    return all_rewards, all_grads\n",
            "\n",
            "This  code  returns  a  list  of  reward  lists:  one  reward  list  per  episode,  containing  one\n",
            "reward  per  step.  It  also  returns  a  list  of  gradient  lists:  one  gradient  list  per  episode,\n",
            "each containing one tuple of gradients per step and each tuple containing one gradi‐\n",
            "ent tensor per trainable variable.\n",
            "\n",
            "The  algorithm  will  use  the  play_multiple_episodes()  function  to  play  the  game\n",
            "several times (e.g., 10 times), then it will go back and look at all the rewards, discount\n",
            "them,  and  normalize  them.  To  do  that,  we  need  a  couple  more  functions;  the  first\n",
            "\n",
            "696 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fwill compute the sum of future discounted rewards at each step, and the second will\n",
            "normalize  all  these  discounted  rewards  (i.e.,  the  returns)  across  many  episodes  by\n",
            "subtracting the mean and dividing by the standard deviation:\n",
            "\n",
            "def discount_rewards(rewards, discount_factor):\n",
            "    discounted = np.array(rewards)\n",
            "    for step in range(len(rewards) - 2, -1, -1):\n",
            "        discounted[step] += discounted[step + 1] * discount_factor\n",
            "    return discounted\n",
            "\n",
            "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
            "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
            "                              for rewards in all_rewards]\n",
            "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
            "    reward_mean = flat_rewards.mean()\n",
            "    reward_std = flat_rewards.std()\n",
            "    return [(discounted_rewards - reward_mean) / reward_std\n",
            "            for discounted_rewards in all_discounted_rewards]\n",
            "\n",
            "Let’s check that this works:\n",
            "\n",
            ">>> discount_rewards([10, 0, -50], discount_factor=0.8)\n",
            "array([-22, -40, -50])\n",
            ">>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
            "...                                discount_factor=0.8)\n",
            "...\n",
            "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
            " array([1.26665318, 1.0727777 ])]\n",
            "\n",
            "The  call  to  discount_rewards()  returns  exactly  what  we  expect  (see  Figure  18-6).\n",
            "You  can  verify  that  the  function  discount_and_normalize_rewards()  does  indeed\n",
            "return  the  normalized  action  advantages  for  each  action  in  both  episodes.  Notice\n",
            "that the first episode was much worse than the second, so its normalized advantages\n",
            "are  all  negative;  all  actions  from  the  first  episode  would  be  considered  bad,  and\n",
            "conversely all actions from the second episode would be considered good.\n",
            "\n",
            "We are almost ready to run the algorithm! Now let’s define the hyperparameters. We\n",
            "will  run  150  training  iterations,  playing  10  episodes  per  iteration,  and  each  episode\n",
            "will last at most 200 steps. We will use a discount factor of 0.95:\n",
            "\n",
            "n_iterations = 150\n",
            "n_episodes_per_update = 10\n",
            "n_max_steps = 200\n",
            "discount_factor = 0.95\n",
            "\n",
            "We  also  need  an  optimizer  and  the  loss  function.  A  regular  Nadam  optimizer  with\n",
            "learning  rate  0.01  will  do  just  fine,  and  we  will  use  the  binary  cross-entropy  loss\n",
            "function because we are training a binary classifier (there are two possible actions—\n",
            "left or right):\n",
            "\n",
            "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
            "loss_fn = tf.keras.losses.binary_crossentropy\n",
            "\n",
            "Policy Gradients \n",
            "\n",
            "| \n",
            "\n",
            "697\n",
            "\n",
            "\fWe are now ready to build and run the training loop!\n",
            "\n",
            "for iteration in range(n_iterations):\n",
            "    all_rewards, all_grads = play_multiple_episodes(\n",
            "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
            "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
            "                                                       discount_factor)\n",
            "    all_mean_grads = []\n",
            "    for var_index in range(len(model.trainable_variables)):\n",
            "        mean_grads = tf.reduce_mean(\n",
            "            [final_reward * all_grads[episode_index][step][var_index]\n",
            "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
            "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
            "        all_mean_grads.append(mean_grads)\n",
            "\n",
            "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
            "\n",
            "Let’s walk through this code:\n",
            "\n",
            "• At  each  training  iteration,  this  loop  calls  the  play_multiple_episodes()  func‐\n",
            "•\n",
            "tion, which plays 10 episodes and returns the rewards and gradients for each step\n",
            "in each episode.\n",
            "\n",
            "• Then we call the discount_and_normalize_rewards() function to compute each\n",
            "•\n",
            "action’s  normalized  advantage,  called  the  final_reward  in  this  code.  This  pro‐\n",
            "vides a measure of how good or bad each action actually was, in hindsight.\n",
            "\n",
            "• Next,  we  go  through  each  trainable  variable,  and  for  each  of  them  we  compute\n",
            "•\n",
            "the  weighted  mean  of  the  gradients  for  that  variable  over  all  episodes  and  all\n",
            "steps, weighted by the final_reward.\n",
            "\n",
            "•\n",
            "• Finally, we apply these mean gradients using the optimizer: the model’s trainable\n",
            "\n",
            "variables will be tweaked, and hopefully the policy will be a bit better.\n",
            "\n",
            "And  we’re  done!  This  code  will  train  the  neural  network  policy,  and  it  will  success‐\n",
            "fully learn to balance the pole on the cart. The mean reward per episode will get very\n",
            "close to 200. By default, that’s the maximum for this environment. Success!\n",
            "\n",
            "The  simple  policy  gradients  algorithm  we  just  trained  solved  the  CartPole  task,  but\n",
            "it would not scale well to larger and more complex tasks. Indeed, it is highly sample\n",
            "inefficient,  meaning  it  needs  to  explore  the  game  for  a  very  long  time  before  it  can\n",
            "make significant progress. This is due to the fact that it must run multiple episodes to\n",
            "estimate the advantage of each action, as we have seen. However, it is the foundation\n",
            "of  more  powerful  algorithms,  such  as  actor-critic  algorithms  (which  we  will  discuss\n",
            "briefly at the end of this chapter).\n",
            "\n",
            "698 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fResearchers  try  to  find  algorithms  that  work  well  even  when  the\n",
            "agent  initially  knows  nothing  about  the  environment.  However,\n",
            "unless  you  are  writing  a  paper,  you  should  not  hesitate  to  inject\n",
            "prior knowledge into the agent, as it will speed up training dramat‐\n",
            "ically.  For  example,  since  you  know  that  the  pole  should  be  as\n",
            "vertical  as  possible,  you  could  add  negative  rewards  proportional\n",
            "to  the  pole’s  angle.  This  will  make  the  rewards  much  less  sparse\n",
            "and speed up training. Also, if you already have a reasonably good\n",
            "policy (e.g., hardcoded), you may want to train the neural network\n",
            "to imitate it before using policy gradients to improve it.\n",
            "\n",
            "We  will  now  look  at  another  popular  family  of  algorithms.  Whereas  PG  algorithms\n",
            "directly try to optimize the policy to increase rewards, the algorithms we will explore\n",
            "now  are  less  direct:  the  agent  learns  to  estimate  the  expected  return  for  each  state,\n",
            "or  for  each  action  in  each  state,  then  it  uses  this  knowledge  to  decide  how  to  act.\n",
            "To  understand  these  algorithms,  we  must  first  consider  Markov  decision  processes\n",
            "(MDPs).\n",
            "\n",
            "Markov Decision Processes\n",
            "In the early 20th century, the mathematician Andrey Markov studied stochastic pro‐\n",
            "cesses with no memory, called Markov chains. Such a process has a fixed number of\n",
            "states, and it randomly evolves from one state to another at each step. The probability\n",
            "for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s,\n",
            "s′), not on past states. This is why we say that the system has no memory.\n",
            "\n",
            "Figure 18-7 shows an example of a Markov chain with four states.\n",
            "\n",
            "Figure 18-7. Example of a Markov chain\n",
            "\n",
            "Suppose  that  the  process  starts  in  state  s0,  and  there  is  a  70%  chance  that  it  will\n",
            "remain  in  that  state  at  the  next  step.  Eventually  it  is  bound  to  leave  that  state  and\n",
            "never come back, because no other state points back to s0. If it goes to state s1, it will\n",
            "then  most  likely  go  to  state  s2  (90%  probability),  then  immediately  back  to  state  s1\n",
            "(with 100% probability). It may alternate a number of times between these two states,\n",
            "\n",
            "Markov Decision Processes \n",
            "\n",
            "| \n",
            "\n",
            "699\n",
            "\n",
            "\fbut  eventually  it  will  fall  into  state  s3  and  remain  there  forever,  since  there’s  no  way\n",
            "out: this is called a terminal state. Markov chains can have very different dynamics,\n",
            "and they are heavily used in thermodynamics, chemistry, statistics, and much more.\n",
            "\n",
            "Markov  decision  processes  were  first  described  in  the  1950s  by  Richard  Bellman.12\n",
            "They  resemble  Markov  chains,  but  with  a  twist:  at  each  step,  an  agent  can  choose\n",
            "one of several possible actions, and the transition probabilities depend on the chosen\n",
            "action.  Moreover,  some  state  transitions  return  some  reward  (positive  or  negative),\n",
            "and the agent’s goal is to find a policy that will maximize reward over time.\n",
            "\n",
            "For  example,  the  MDP  represented  in  Figure  18-8  has  three  states  (represented\n",
            "by  circles)  and  up  to  three  possible  discrete  actions  at  each  step  (represented  by\n",
            "diamonds).\n",
            "\n",
            "Figure 18-8. Example of a Markov decision process\n",
            "\n",
            "If it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses\n",
            "action a1, it just remains in state s0 with certainty, and without any reward. It can thus\n",
            "decide  to  stay  there  forever  if  it  wants  to.  But  if  it  chooses  action  a0,  it  has  a  70%\n",
            "probability of gaining a reward of +10 and remaining in state s0. It can then try again\n",
            "and again to gain as much reward as possible, but at one point it is going to end up\n",
            "instead in state s1. In state s1 it has only two possible actions: a0 or a2. It can choose\n",
            "to stay put by repeatedly choosing action a0, or it can choose to move on to state s2\n",
            "and get a negative reward of –50 (ouch). In state s2 it has no choice but to take action\n",
            "a1, which will most likely lead it back to state s0, gaining a reward of +40 on the way.\n",
            "You get the picture. By looking at this MDP, can you guess which strategy will gain\n",
            "the most reward over time? In state s0 it is clear that action a0 is the best option, and\n",
            "in state s2 the agent has no choice but to take action a1, but in state s1 it is not obvious\n",
            "whether the agent should stay put (a0) or go through the fire (a2).\n",
            "\n",
            "12 Richard Bellman, “A Markovian Decision Process”, Journal of Mathematics and Mechanics 6, no. 5 (1957):\n",
            "\n",
            "679–684.\n",
            "\n",
            "700 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fBellman  found  a  way  to  estimate  the  optimal  state  value  of  any  state  s,  noted  V*(s),\n",
            "which  is  the  sum  of  all  discounted  future  rewards  the  agent  can  expect  on  average\n",
            "after  it  reaches  the  state,  assuming  it  acts  optimally.  He  showed  that  if  the  agent\n",
            "acts optimally, then the Bellman optimality equation applies (see Equation 18-1). This\n",
            "recursive equation says that if the agent acts optimally, then the optimal value of the\n",
            "current  state  is  equal  to  the  reward  it  will  get  on  average  after  taking  one  optimal\n",
            "action, plus the expected optimal value of all possible next states that this action can\n",
            "lead to.\n",
            "\n",
            "Equation 18-1. Bellman optimality equation\n",
            "\n",
            "V * s = max\n",
            "\n",
            "a ∑\n",
            "\n",
            "s′\n",
            "\n",
            "T s, a, s′ R s, a, s′ + γ · V * s′\n",
            "\n",
            "for all s\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• T(s, a, s′) is the transition probability from state s to state s′, given that the agent\n",
            "•\n",
            "\n",
            "chose action a. For example, in Figure 18-8, T(s2, a1, s0) = 0.8.\n",
            "\n",
            "• R(s,  a,  s′)  is  the  reward  that  the  agent  gets  when  it  goes  from  state  s  to  state\n",
            "•\n",
            "s′,  given  that  the  agent  chose  action  a.  For  example,  in  Figure  18-8,  R(s2,  a1,\n",
            "s0) = +40.\n",
            "\n",
            "•\n",
            "• γ is the discount factor.\n",
            "\n",
            "This  equation  leads  directly  to  an  algorithm  that  can  precisely  estimate  the  optimal\n",
            "state value of every possible state: first initialize all the state value estimates to zero,\n",
            "and  then  iteratively  update  them  using  the  value  iteration  algorithm  (see  Equation\n",
            "18-2). A remarkable result is that, given enough time, these estimates are guaranteed\n",
            "to converge to the optimal state values, corresponding to the optimal policy.\n",
            "\n",
            "Equation 18-2. Value iteration algorithm\n",
            "\n",
            "V k + 1 s\n",
            "\n",
            "a ∑\n",
            "max\n",
            "\n",
            "s′\n",
            "\n",
            "T s, a, s′ R s, a, s′ + γ · V k s′\n",
            "\n",
            "for all s\n",
            "\n",
            "In  this  equation,  Vk(s)  is  the  estimated  value  of  state  s  at  the  kth  iteration  of  the\n",
            "algorithm.\n",
            "\n",
            "This  algorithm  is  an  example  of  dynamic  programming,  which\n",
            "breaks  down  a  complex  problem  into  tractable  subproblems  that\n",
            "can be tackled iteratively.\n",
            "\n",
            "Markov Decision Processes \n",
            "\n",
            "| \n",
            "\n",
            "701\n",
            "\n",
            "\fKnowing the optimal state values can be useful, in particular to evaluate a policy, but\n",
            "it  does  not  give  us  the  optimal  policy  for  the  agent.  Luckily,  Bellman  found  a  very\n",
            "similar algorithm to estimate the optimal state-action values, generally called Q-values\n",
            "(quality  values).  The  optimal  Q-value  of  the  state-action  pair  (s,  a),  noted  Q*(s,  a),\n",
            "is  the  sum  of  discounted  future  rewards  the  agent  can  expect  on  average  after  it\n",
            "reaches the state s and chooses action a, but before it sees the outcome of this action,\n",
            "assuming it acts optimally after that action.\n",
            "\n",
            "Let’s  look  at  how  it  works.  Once  again,  you  start  by  initializing  all  the  Q-value\n",
            "estimates  to  zero,  then  you  update  them  using  the  Q-value  iteration  algorithm  (see\n",
            "Equation 18-3).\n",
            "\n",
            "Equation 18-3. Q-value iteration algorithm\n",
            "\n",
            "Qk + 1 s, a\n",
            "\n",
            "∑\n",
            "s′\n",
            "\n",
            "T s, a, s′ R s, a, s′ + γ · max\n",
            "a′\n",
            "\n",
            "Qk s′, a′\n",
            "\n",
            "for all s, a\n",
            "\n",
            "Once  you  have  the  optimal  Q-values,  defining  the  optimal  policy,  noted  π*(s),  is\n",
            "trivial;  when  the  agent  is  in  state  s,  it  should  choose  the  action  with  the  highest\n",
            "\n",
            "Q-value for that state: π* s = argmax\n",
            "\n",
            "a\n",
            "\n",
            "Q* s, a .\n",
            "\n",
            "Let’s  apply  this  algorithm  to  the  MDP  represented  in  Figure  18-8.  First,  we  need  to\n",
            "define the MDP:\n",
            "\n",
            "transition_probabilities = [  # shape=[s, a, s']\n",
            "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
            "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
            "    [None, [0.8, 0.1, 0.1], None]\n",
            "]\n",
            "rewards = [  # shape=[s, a, s']\n",
            "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
            "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
            "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
            "]\n",
            "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
            "\n",
            "For  example,  to  know  the  transition  probability  of  going  from  s2  to  s0  after  play‐\n",
            "ing  action  a1,  we  will  look  up  transition_probabilities[2][1][0]  (which  is\n",
            "0.8).  Similarly,  to  get  the  corresponding  reward,  we  will  look  up  rewards[2][1]\n",
            "[0]  (which  is  +40).  And  to  get  the  list  of  possible  actions  in  s2,  we  will  look\n",
            "up  possible_actions[2]  (in  this  case,  only  action  a1  is  possible).  Next,  we  must\n",
            "initialize all the Q-values to zero (except for the impossible actions, for which we set\n",
            "the Q-values to –∞):\n",
            "\n",
            "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
            "for state, actions in enumerate(possible_actions):\n",
            "    Q_values[state, actions] = 0.0  # for all possible actions\n",
            "\n",
            "702 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fNow let’s run the Q-value iteration algorithm. It applies Equation 18-3 repeatedly, to\n",
            "all Q-values, for every state and every possible action:\n",
            "\n",
            "gamma = 0.90  # the discount factor\n",
            "\n",
            "for iteration in range(50):\n",
            "    Q_prev = Q_values.copy()\n",
            "    for s in range(3):\n",
            "        for a in possible_actions[s]:\n",
            "            Q_values[s, a] = np.sum([\n",
            "                    transition_probabilities[s][a][sp]\n",
            "                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
            "                for sp in range(3)])\n",
            "\n",
            "That’s it! The resulting Q-values look like this:\n",
            "\n",
            ">>> Q_values\n",
            "array([[18.91891892, 17.02702702, 13.62162162],\n",
            "       [ 0.        ,        -inf, -4.87971488],\n",
            "       [       -inf, 50.13365013,        -inf]])\n",
            "\n",
            "For example, when the agent is in state s0 and it chooses action a1, the expected sum\n",
            "of discounted future rewards is approximately 17.0.\n",
            "\n",
            "For each state, we can find the action that has the highest Q-value:\n",
            "\n",
            ">>> Q_values.argmax(axis=1)  # optimal action for each state\n",
            "array([0, 0, 1])\n",
            "\n",
            "This gives us the optimal policy for this MDP when using a discount factor of 0.90:\n",
            "in state s0 choose action a0, in state s1 choose action a0 (i.e., stay put), and in state s2\n",
            "choose action a1 (the only possible action). Interestingly, if we increase the discount\n",
            "factor to 0.95, the optimal policy changes: in state s1 the best action becomes a2 (go\n",
            "through the fire!). This makes sense because the more you value future rewards, the\n",
            "more you are willing to put up with some pain now for the promise of future bliss.\n",
            "\n",
            "Temporal Difference Learning\n",
            "Reinforcement  learning  problems  with  discrete  actions  can  often  be  modeled  as\n",
            "Markov  decision  processes,  but  the  agent  initially  has  no  idea  what  the  transition\n",
            "probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards\n",
            "are  going  to  be  either  (it  does  not  know  R(s,  a,  s′)).  It  must  experience  each  state\n",
            "and each transition at least once to know the rewards, and it must experience them\n",
            "multiple times if it is to have a reasonable estimate of the transition probabilities.\n",
            "\n",
            "The temporal difference (TD) learning algorithm is very similar to the Q-value itera‐\n",
            "tion  algorithm,  but  tweaked  to  take  into  account  the  fact  that  the  agent  has  only\n",
            "partial  knowledge  of  the  MDP.  In  general  we  assume  that  the  agent  initially  knows\n",
            "only  the  possible  states  and  actions,  and  nothing  more.  The  agent  uses  an  explora‐\n",
            "tion  policy—for  example,  a  purely  random  policy—to  explore  the  MDP,  and  as  it\n",
            "\n",
            "Temporal Difference Learning \n",
            "\n",
            "| \n",
            "\n",
            "703\n",
            "\n",
            "\fprogresses, the TD learning algorithm updates the estimates of the state values based\n",
            "on the transitions and rewards that are actually observed (see Equation 18-4).\n",
            "\n",
            "Equation 18-4. TD learning algorithm\n",
            "\n",
            "1 − α V k s + α r + γ · V k s′\n",
            "\n",
            "V k + 1 s\n",
            "or, equivalently: \n",
            "V k + 1 s\n",
            "with δk s, r, s′ = r + γ · V k s′ − Vk s\n",
            "\n",
            "V k s + α · δk s, r, s′\n",
            "\n",
            "In this equation:\n",
            "\n",
            "•\n",
            "• α is the learning rate (e.g., 0.01).\n",
            "• r + γ · Vk(s′) is called the TD target.\n",
            "•\n",
            "• δk(s, r, s′) is called the TD error.\n",
            "•\n",
            "\n",
            "A more concise way of writing the first form of this equation is to use the notation\n",
            "b, which means ak+1 ← (1 – α) · ak + α ·bk. So, the first line of Equation 18-4 can\n",
            "a\n",
            "\n",
            "α\n",
            "\n",
            "be rewritten like this: V s\n",
            "\n",
            "r + γ · V s′ .\n",
            "\n",
            "α\n",
            "\n",
            "TD learning has many similarities with stochastic gradient descent,\n",
            "including  the  fact  that  it  handles  one  sample  at  a  time.  Moreover,\n",
            "just like SGD, it can only truly converge if you gradually reduce the\n",
            "learning rate; otherwise, it will keep bouncing around the optimum\n",
            "Q-values.\n",
            "\n",
            "For  each  state  s,  this  algorithm  keeps  track  of  a  running  average  of  the  immediate\n",
            "rewards the agent gets upon leaving that state, plus the rewards it expects to get later,\n",
            "assuming it acts optimally.\n",
            "\n",
            "Q-Learning\n",
            "Similarly,  the  Q-learning  algorithm  is  an  adaptation  of  the  Q-value  iteration  algo‐\n",
            "rithm to the situation where the transition probabilities and the rewards are initially\n",
            "unknown (see Equation 18-5). Q-learning works by watching an agent play (e.g., ran‐\n",
            "domly)  and  gradually  improving  its  estimates  of  the  Q-values.  Once  it  has  accurate\n",
            "Q-value  estimates  (or  close  enough),  then  the  optimal  policy  is  just  choosing  the\n",
            "action that has the highest Q-value (i.e., the greedy policy).\n",
            "\n",
            "704 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fEquation 18-5. Q-learning algorithm\n",
            "\n",
            "Q s, a\n",
            "\n",
            "r + γ · max\n",
            "a′\n",
            "\n",
            "α\n",
            "\n",
            "  Q s′, a′\n",
            "\n",
            "For  each  state-action  pair  (s,  a),  this  algorithm  keeps  track  of  a  running  average  of\n",
            "the  rewards  r  the  agent  gets  upon  leaving  the  state  s  with  action  a,  plus  the  sum\n",
            "of  discounted  future  rewards  it  expects  to  get.  To  estimate  this  sum,  we  take  the\n",
            "maximum  of  the  Q-value  estimates  for  the  next  state  s′,  since  we  assume  that  the\n",
            "target policy will act optimally from then on.\n",
            "\n",
            "Let’s  implement  the  Q-learning  algorithm.  First,  we  will  need  to  make  an  agent\n",
            "explore  the  environment.  For  this,  we  need  a  step  function  so  that  the  agent  can\n",
            "execute one action and get the resulting state and reward:\n",
            "\n",
            "def step(state, action):\n",
            "    probas = transition_probabilities[state][action]\n",
            "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
            "    reward = rewards[state][action][next_state]\n",
            "    return next_state, reward\n",
            "\n",
            "Now  let’s  implement  the  agent’s  exploration  policy.  Since  the  state  space  is  pretty\n",
            "small,  a  simple  random  policy  will  be  sufficient.  If  we  run  the  algorithm  for  long\n",
            "enough, the agent will visit every state many times, and it will also try every possible\n",
            "action many times:\n",
            "\n",
            "def exploration_policy(state):\n",
            "    return np.random.choice(possible_actions[state])\n",
            "\n",
            "Next,  after  we  initialize  the  Q-values  just  like  earlier,  we  are  ready  to  run  the  Q-\n",
            "learning algorithm with learning rate decay (using power scheduling, introduced in\n",
            "Chapter 11):\n",
            "\n",
            "alpha0 = 0.05  # initial learning rate\n",
            "decay = 0.005  # learning rate decay\n",
            "gamma = 0.90  # discount factor\n",
            "state = 0  # initial state\n",
            "\n",
            "for iteration in range(10_000):\n",
            "    action = exploration_policy(state)\n",
            "    next_state, reward = step(state, action)\n",
            "    next_value = Q_values[next_state].max()  # greedy policy at the next step\n",
            "    alpha = alpha0 / (1 + iteration * decay)\n",
            "    Q_values[state, action] *= 1 - alpha\n",
            "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
            "    state = next_state\n",
            "\n",
            "Q-Learning \n",
            "\n",
            "| \n",
            "\n",
            "705\n",
            "\n",
            "\fThis algorithm will converge to the optimal Q-values, but it will take many iterations,\n",
            "and possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the\n",
            "Q-value iteration algorithm (left) converges very quickly, in fewer than 20 iterations,\n",
            "while  the  Q-learning  algorithm  (right)  takes  about  8,000  iterations  to  converge.\n",
            "Obviously, not knowing the transition probabilities or the rewards makes finding the\n",
            "optimal policy significantly harder!\n",
            "\n",
            "Figure 18-9. Learning curve of the Q-value iteration algorithm versus the Q-learning\n",
            "algorithm\n",
            "\n",
            "The  Q-learning  algorithm  is  called  an  off-policy  algorithm  because  the  policy  being\n",
            "trained is not necessarily the one used during training. For example, in the code we\n",
            "just ran, the policy being executed (the exploration policy) was completely random,\n",
            "while the policy being trained was never used. After training, the optimal policy cor‐\n",
            "responds to systematically choosing the action with the highest Q-value. Conversely,\n",
            "the policy gradients algorithm is an on-policy algorithm: it explores the world using\n",
            "the  policy  being  trained.  It  is  somewhat  surprising  that  Q-learning  is  capable  of\n",
            "learning the optimal policy by just watching an agent act randomly. Imagine learning\n",
            "to play golf when your teacher is a blindfolded monkey. Can we do better?\n",
            "\n",
            "Exploration Policies\n",
            "Of  course,  Q-learning  can  work  only  if  the  exploration  policy  explores  the  MDP\n",
            "thoroughly  enough.  Although  a  purely  random  policy  is  guaranteed  to  eventually\n",
            "visit every state and every transition many times, it may take an extremely long time\n",
            "to  do  so.  Therefore,  a  better  option  is  to  use  the  ε-greedy  policy  (ε  is  epsilon):  at\n",
            "each  step  it  acts  randomly  with  probability  ε,  or  greedily  with  probability  1–ε  (i.e.,\n",
            "choosing the action with the highest Q-value). The advantage of the ε-greedy policy\n",
            "(compared to a completely random policy) is that it will spend more and more time\n",
            "exploring the interesting parts of the environment, as the Q-value estimates get better\n",
            "and better, while still spending some time visiting unknown regions of the MDP. It is\n",
            "\n",
            "706 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fquite common to start with a high value for ε (e.g., 1.0) and then gradually reduce it\n",
            "(e.g., down to 0.05).\n",
            "\n",
            "Alternatively, rather than relying only on chance for exploration, another approach is\n",
            "to encourage the exploration policy to try actions that it has not tried much before.\n",
            "This  can  be  implemented  as  a  bonus  added  to  the  Q-value  estimates,  as  shown  in\n",
            "Equation 18-6.\n",
            "\n",
            "Equation 18-6. Q-learning using an exploration function\n",
            "\n",
            "Q s, a\n",
            "\n",
            "r + γ · max\n",
            "a′\n",
            "\n",
            "α\n",
            "\n",
            "  f Q s′, a′ , N s′, a′\n",
            "\n",
            "In this equation:\n",
            "\n",
            "• N(s′, a′) counts the number of times the action a′ was chosen in state s′.\n",
            "•\n",
            "\n",
            "•\n",
            "• f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a\n",
            "curiosity  hyperparameter  that  measures  how  much  the  agent  is  attracted  to  the\n",
            "unknown.\n",
            "\n",
            "Approximate Q-Learning and Deep Q-Learning\n",
            "The  main  problem  with  Q-learning  is  that  it  does  not  scale  well  to  large  (or  even\n",
            "medium)  MDPs  with  many  states  and  actions.  For  example,  suppose  you  wanted\n",
            "to  use  Q-learning  to  train  an  agent  to  play  Ms.  Pac-Man  (see  Figure  18-1).  There\n",
            "are  about  150  pellets  that  Ms.  Pac-Man  can  eat,  each  of  which  can  be  present  or\n",
            "absent (i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045.\n",
            "And if you add all the possible combinations of positions for all the ghosts and Ms.\n",
            "Pac-Man, the number of possible states becomes larger than the number of atoms in\n",
            "our planet, so there’s absolutely no way you can keep track of an estimate for every\n",
            "single Q-value.\n",
            "\n",
            "The solution is to find a function Qθ(s, a) that approximates the Q-value of any state-\n",
            "action pair (s, a) using a manageable number of parameters (given by the parameter\n",
            "vector  θ).  This  is  called  approximate  Q-learning.  For  years  it  was  recommended  to\n",
            "use  linear  combinations  of  handcrafted  features  extracted  from  the  state  (e.g.,  the\n",
            "distances of the closest ghosts, their directions, and so on) to estimate Q-values, but\n",
            "in 2013, DeepMind showed that using deep neural networks can work much better,\n",
            "especially for complex problems, and it does not require any feature engineering. A\n",
            "DNN used to estimate Q-values is called a deep Q-network (DQN), and using a DQN\n",
            "for approximate Q-learning is called deep Q-learning.\n",
            "\n",
            "Now,  how  can  we  train  a  DQN?  Well,  consider  the  approximate  Q-value  computed\n",
            "by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want\n",
            "\n",
            "Q-Learning \n",
            "\n",
            "| \n",
            "\n",
            "707\n",
            "\n",
            "\fthis  approximate  Q-value  to  be  as  close  as  possible  to  the  reward  r  that  we  actually\n",
            "observe after playing action a in state s, plus the discounted value of playing optimally\n",
            "from then on. To estimate this sum of future discounted rewards, we can just execute\n",
            "the DQN on the next state s′, for all possible actions a′. We get an approximate future\n",
            "Q-value for each possible action. We then pick the highest (since we assume we will\n",
            "be  playing  optimally)  and  discount  it,  and  this  gives  us  an  estimate  of  the  sum  of\n",
            "future discounted rewards. By summing the reward r and the future discounted value\n",
            "estimate,  we  get  a  target  Q-value  y(s,  a)  for  the  state-action  pair  (s,  a),  as  shown  in\n",
            "Equation 18-7.\n",
            "\n",
            "Equation 18-7. Target Q-value\n",
            "\n",
            "y s, a = r + γ · max\n",
            "a′\n",
            "\n",
            "  Qθ s′, a′\n",
            "\n",
            "With  this  target  Q-value,  we  can  run  a  training  step  using  any  gradient  descent\n",
            "algorithm.  Specifically,  we  generally  try  to  minimize  the  squared  error  between  the\n",
            "estimated Q-value Qθ(s, a) and the target Q-value y(s, a), or the Huber loss to reduce\n",
            "the algorithm’s sensitivity to large errors. And that’s the deep Q-learning algorithm!\n",
            "Let’s see how to implement it to solve the CartPole environment.\n",
            "\n",
            "Implementing Deep Q-Learning\n",
            "The  first  thing  we  need  is  a  deep  Q-network.  In  theory,  we  need  a  neural  net  that\n",
            "takes a state-action pair as input, and outputs an approximate Q-value. However, in\n",
            "practice it’s much more efficient to use a neural net that takes only a state as input,\n",
            "and outputs one approximate Q-value for each possible action. To solve the CartPole\n",
            "environment, we do not need a very complicated neural net; a couple of hidden layers\n",
            "will do:\n",
            "\n",
            "input_shape = [4]  # == env.observation_space.shape\n",
            "n_outputs = 2  # == env.action_space.n\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
            "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
            "    tf.keras.layers.Dense(n_outputs)\n",
            "])\n",
            "\n",
            "To  select  an  action  using  this  DQN,  we  pick  the  action  with  the  largest  predicted\n",
            "Q-value. To ensure that the agent explores the environment, we will use an ε-greedy\n",
            "policy (i.e., we will choose a random action with probability ε):\n",
            "\n",
            "708 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fdef epsilon_greedy_policy(state, epsilon=0):\n",
            "    if np.random.rand() < epsilon:\n",
            "        return np.random.randint(n_outputs)  # random action\n",
            "    else:\n",
            "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
            "        return Q_values.argmax()  # optimal action according to the DQN\n",
            "\n",
            "Instead  of  training  the  DQN  based  only  on  the  latest  experiences,  we  will  store  all\n",
            "experiences  in  a  replay  buffer  (or  replay  memory),  and  we  will  sample  a  random\n",
            "training  batch  from  it  at  each  training  iteration.  This  helps  reduce  the  correlations\n",
            "between the experiences in a training batch, which tremendously helps training. For\n",
            "this, we will just use a double-ended queue (deque):\n",
            "\n",
            "from collections import deque\n",
            "\n",
            "replay_buffer = deque(maxlen=2000)\n",
            "\n",
            "A deque is a queue elements can be efficiently added to or removed\n",
            "from on both ends. Inserting and deleting items from the ends of\n",
            "the  queue  is  very  fast,  but  random  access  can  be  slow  when  the\n",
            "queue gets long. If you need a very large replay buffer, you should\n",
            "use a circular buffer instead (see the notebook for an implementa‐\n",
            "tion), or check out DeepMind’s Reverb library.\n",
            "\n",
            "Each  experience  will  be  composed  of  six  elements:  a  state  s,  the  action  a  that  the\n",
            "agent  took,  the  resulting  reward  r,  the  next  state  s′  it  reached,  a  Boolean  indicating\n",
            "whether the episode ended at that point (done), and finally another Boolean indicat‐\n",
            "ing  whether  the  episode  was  truncated  at  that  point.  We  will  need  a  small  function\n",
            "to  sample  a  random  batch  of  experiences  from  the  replay  buffer.  It  will  return  six\n",
            "NumPy arrays corresponding to the six experience elements:\n",
            "\n",
            "def sample_experiences(batch_size):\n",
            "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
            "    batch = [replay_buffer[index] for index in indices]\n",
            "    return [\n",
            "        np.array([experience[field_index] for experience in batch])\n",
            "        for field_index in range(6)\n",
            "    ]  # [states, actions, rewards, next_states, dones, truncateds]\n",
            "\n",
            "Let’s also create a function that will play a single step using the ε-greedy policy, then\n",
            "store the resulting experience in the replay buffer:\n",
            "\n",
            "def play_one_step(env, state, epsilon):\n",
            "    action = epsilon_greedy_policy(state, epsilon)\n",
            "    next_state, reward, done, truncated, info = env.step(action)\n",
            "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
            "    return next_state, reward, done, truncated, info\n",
            "\n",
            "Implementing Deep Q-Learning \n",
            "\n",
            "| \n",
            "\n",
            "709\n",
            "\n",
            "\fFinally, let’s create one last function that will sample a batch of experiences from the\n",
            "replay buffer and train the DQN by performing a single gradient descent step on this\n",
            "batch:\n",
            "\n",
            "batch_size = 32\n",
            "discount_factor = 0.95\n",
            "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
            "loss_fn = tf.keras.losses.mean_squared_error\n",
            "\n",
            "def training_step(batch_size):\n",
            "    experiences = sample_experiences(batch_size)\n",
            "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
            "    next_Q_values = model.predict(next_states, verbose=0)\n",
            "    max_next_Q_values = next_Q_values.max(axis=1)\n",
            "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
            "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
            "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
            "    mask = tf.one_hot(actions, n_outputs)\n",
            "    with tf.GradientTape() as tape:\n",
            "        all_Q_values = model(states)\n",
            "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
            "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
            "\n",
            "    grads = tape.gradient(loss, model.trainable_variables)\n",
            "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
            "\n",
            "Here’s what’s happening in this code:\n",
            "\n",
            "•\n",
            "• First we define some hyperparameters, and we create the optimizer and the loss\n",
            "\n",
            "function.\n",
            "\n",
            "• Then  we  create  the  training_step()  function.  It  starts  by  sampling  a  batch  of\n",
            "•\n",
            "experiences, then it uses the DQN to predict the Q-value for each possible action\n",
            "in  each  experience’s  next  state.  Since  we  assume  that  the  agent  will  be  playing\n",
            "optimally, we only keep the maximum Q-value for each next state. Next, we use\n",
            "Equation  18-7  to  compute  the  target  Q-value  for  each  experience’s  state-action\n",
            "pair.\n",
            "\n",
            "• We  want  to  use  the  DQN  to  compute  the  Q-value  for  each  experienced  state-\n",
            "•\n",
            "action  pair,  but  the  DQN  will  also  output  the  Q-values  for  the  other  possible\n",
            "actions, not just for the action that was actually chosen by the agent. So, we need\n",
            "to mask out all the Q-values we do not need. The tf.one_hot() function makes\n",
            "it possible to convert an array of action indices into such a mask. For example,\n",
            "if  the  first  three  experiences  contain  actions  1,  1,  0,  respectively,  then  the  mask\n",
            "will start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply the DQN’s\n",
            "output with this mask, and this will zero out all the Q-values we do not want. We\n",
            "then sum over axis 1 to get rid of all the zeros, keeping only the Q-values of the\n",
            "experienced state-action pairs. This gives us the Q_values tensor, containing one\n",
            "predicted Q-value for each experience in the batch.\n",
            "\n",
            "710 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\f•\n",
            "• Next, we compute the loss: it is the mean squared error between the target and\n",
            "\n",
            "predicted Q-values for the experienced state-action pairs.\n",
            "\n",
            "•\n",
            "• Finally, we perform a gradient descent step to minimize the loss with regard to\n",
            "\n",
            "the model’s trainable variables.\n",
            "\n",
            "This was the hardest part. Now training the model is straightforward:\n",
            "\n",
            "for episode in range(600):\n",
            "    obs, info = env.reset()\n",
            "    for step in range(200):\n",
            "        epsilon = max(1 - episode / 500, 0.01)\n",
            "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
            "        if done or truncated:\n",
            "            break\n",
            "\n",
            "    if episode > 50:\n",
            "        training_step(batch_size)\n",
            "\n",
            "We  run  600  episodes,  each  for  a  maximum  of  200  steps.  At  each  step,  we  first\n",
            "compute  the  epsilon  value  for  the  ε-greedy  policy:  it  will  go  from  1  down  to  0.01,\n",
            "linearly,  in  a  bit  under  500  episodes.  Then  we  call  the  play_one_step()  function,\n",
            "which  will  use  the  ε-greedy  policy  to  pick  an  action,  then  execute  it  and  record  the\n",
            "experience in the replay buffer. If the episode is done or truncated, we exit the loop.\n",
            "Finally,  if  we  are  past  episode  50,  we  call  the  training_step()  function  to  train\n",
            "the  model  on  one  batch  sampled  from  the  replay  buffer.  The  reason  we  play  many\n",
            "episodes without training is to give the replay buffer some time to fill up (if we don’t\n",
            "wait enough, then there will not be enough diversity in the replay buffer). And that’s\n",
            "it: we just implemented the Deep Q-learning algorithm!\n",
            "\n",
            "Figure 18-10 shows the total rewards the agent got during each episode.\n",
            "\n",
            "Figure 18-10. Learning curve of the deep Q-learning algorithm\n",
            "\n",
            "Implementing Deep Q-Learning \n",
            "\n",
            "| \n",
            "\n",
            "711\n",
            "\n",
            "\fAs you can see, the algorithm took a while to start learning anything, in part because\n",
            "ε  was  very  high  at  the  beginning.  Then  its  progress  was  erratic:  it  first  reached  the\n",
            "max reward around episode 220, but it immediately dropped, then bounced up and\n",
            "down  a  few  times,  and  soon  after  it  looked  like  it  had  finally  stabilized  near  the\n",
            "max reward, at around episode 320, its score again dropped down dramatically. This\n",
            "is  called  catastrophic  forgetting,  and  it  is  one  of  the  big  problems  facing  virtually\n",
            "all  RL  algorithms:  as  the  agent  explores  the  environment,  it  updates  its  policy,  but\n",
            "what  it  learns  in  one  part  of  the  environment  may  break  what  it  learned  earlier  in\n",
            "other parts of the environment. The experiences are quite correlated, and the learning\n",
            "environment  keeps  changing—this  is  not  ideal  for  gradient  descent!  If  you  increase\n",
            "the size of the replay buffer, the algorithm will be less subject to this problem. Tuning\n",
            "the  learning  rate  may  also  help.  But  the  truth  is,  reinforcement  learning  is  hard:\n",
            "training is often unstable, and you may need to try many hyperparameter values and\n",
            "random  seeds  before  you  find  a  combination  that  works  well.  For  example,  if  you\n",
            "try changing the activation function from \"elu\" to \"relu\", the performance will be\n",
            "much lower.\n",
            "\n",
            "Reinforcement  learning  is  notoriously  difficult,  largely  because  of\n",
            "the  training  instabilities  and  the  huge  sensitivity  to  the  choice\n",
            "of  hyperparameter  values  and  random  seeds.13  As  the  researcher\n",
            "Andrej Karpathy put it, “[Supervised learning] wants to work. […]\n",
            "RL must be forced to work”. You will need time, patience, persever‐\n",
            "ance,  and  perhaps  a  bit  of  luck  too.  This  is  a  major  reason  RL  is\n",
            "not as widely adopted as regular deep learning (e.g., convolutional\n",
            "nets). But there are a few real-world applications, beyond AlphaGo\n",
            "and  Atari  games:  for  example,  Google  uses  RL  to  optimize  its\n",
            "datacenter  costs,  and  it  is  used  in  some  robotics  applications,  for\n",
            "hyperparameter tuning, and in recommender systems.\n",
            "\n",
            "You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator\n",
            "of  the  model’s  performance.  The  loss  might  go  down,  yet  the  agent  might  perform\n",
            "worse  (e.g.,  this  can  happen  when  the  agent  gets  stuck  in  one  small  region  of  the\n",
            "environment, and the DQN starts overfitting this region). Conversely, the loss could\n",
            "go up, yet the agent might perform better (e.g., if the DQN was underestimating the\n",
            "Q-values and it starts correctly increasing its predictions, the agent will likely perform\n",
            "better, getting more rewards, but the loss might increase because the DQN also sets\n",
            "the targets, which will be larger too). So, it’s preferable to plot the rewards.\n",
            "\n",
            "The basic deep Q-learning algorithm we’ve been using so far would be too unstable\n",
            "to  learn  to  play  Atari  games.  So  how  did  DeepMind  do  it?  Well,  they  tweaked  the\n",
            "algorithm!\n",
            "\n",
            "13 A great 2018 post by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.\n",
            "\n",
            "712 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fDeep Q-Learning Variants\n",
            "Let’s  look  at  a  few  variants  of  the  deep  Q-learning  algorithm  that  can  stabilize  and\n",
            "speed up training.\n",
            "\n",
            "Fixed Q-value Targets\n",
            "In the basic deep Q-learning algorithm, the model is used both to make predictions\n",
            "and  to  set  its  own  targets.  This  can  lead  to  a  situation  analogous  to  a  dog  chasing\n",
            "its  own  tail.  This  feedback  loop  can  make  the  network  unstable:  it  can  diverge,\n",
            "oscillate, freeze, and so on. To solve this problem, in their 2013 paper the DeepMind\n",
            "researchers used two DQNs instead of one: the first is the online model, which learns\n",
            "at each step and is used to move the agent around, and the other is the target model\n",
            "used only to define the targets. The target model is just a clone of the online model:\n",
            "\n",
            "target = tf.keras.models.clone_model(model)  # clone the model's architecture\n",
            "target.set_weights(model.get_weights())  # copy the weights\n",
            "\n",
            "Then, in the  training_step() function, we just need to change one line to use the\n",
            "target model instead of the online model when computing the Q-values of the next\n",
            "states:\n",
            "\n",
            "next_Q_values = target.predict(next_states, verbose=0)\n",
            "\n",
            "Finally,  in  the  training  loop,  we  must  copy  the  weights  of  the  online  model  to  the\n",
            "target model, at regular intervals (e.g., every 50 episodes):\n",
            "\n",
            "if episode % 50 == 0:\n",
            "    target.set_weights(model.get_weights())\n",
            "\n",
            "Since the target model is updated much less often than the online model, the Q-value\n",
            "targets are more stable, the feedback loop we discussed earlier is dampened, and its\n",
            "effects  are  less  severe.  This  approach  was  one  of  the  DeepMind  researchers’  main\n",
            "contributions in their 2013 paper, allowing agents to learn to play Atari games from\n",
            "raw  pixels.  To  stabilize  training,  they  used  a  tiny  learning  rate  of  0.00025,  they\n",
            "updated  the  target  model  only  every  10,000  steps  (instead  of  50),  and  they  used  a\n",
            "very large replay buffer of 1 million experiences. They decreased epsilon very slowly,\n",
            "from 1 to 0.1 in 1 million steps, and they let the algorithm run for 50 million steps.\n",
            "Moreover, their DQN was a deep convolutional net.\n",
            "\n",
            "Deep Q-Learning Variants \n",
            "\n",
            "| \n",
            "\n",
            "713\n",
            "\n",
            "\fNow let’s take a look at another DQN variant that managed to beat the state of the art\n",
            "once more.\n",
            "\n",
            "Double DQN\n",
            "In  a  2015  paper,14  DeepMind  researchers  tweaked  their  DQN  algorithm,  increasing\n",
            "its  performance  and  somewhat  stabilizing  training.  They  called  this  variant  double\n",
            "DQN. The update was based on the observation that the target network is prone to\n",
            "overestimating Q-values. Indeed, suppose all actions are equally good: the Q-values\n",
            "estimated  by  the  target  model  should  be  identical,  but  since  they  are  approxima‐\n",
            "tions,  some  may  be  slightly  greater  than  others,  by  pure  chance.  The  target  model\n",
            "will  always  select  the  largest  Q-value,  which  will  be  slightly  greater  than  the  mean\n",
            "Q-value,  most  likely  overestimating  the  true  Q-value  (a  bit  like  counting  the  height\n",
            "of  the  tallest  random  wave  when  measuring  the  depth  of  a  pool).  To  fix  this,  the\n",
            "researchers proposed using the online model instead of the target model when select‐\n",
            "ing the best actions for the next states, and using the target model only to estimate the\n",
            "Q-values for these best actions. Here is the updated training_step() function:\n",
            "\n",
            "def training_step(batch_size):\n",
            "    experiences = sample_experiences(batch_size)\n",
            "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
            "    next_Q_values = model.predict(next_states, verbose=0)  # ≠ target.predict()\n",
            "    best_next_actions = next_Q_values.argmax(axis=1)\n",
            "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
            "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n",
            "                        ).sum(axis=1)\n",
            "    [...]  # the rest is the same as earlier\n",
            "\n",
            "Just  a  few  months  later,  another  improvement  to  the  DQN  algorithm  was  propose;\n",
            "we’ll look at that next.\n",
            "\n",
            "Prioritized Experience Replay\n",
            "Instead  of  sampling  experiences  uniformly  from  the  replay  buffer,  why  not  sample\n",
            "important experiences more frequently? This idea is called importance sampling (IS)\n",
            "or  prioritized  experience  replay  (PER),  and  it  was  introduced  in  a  2015  paper15  by\n",
            "DeepMind researchers (once again!).\n",
            "\n",
            "14 Hado van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning”, Proceedings of the 30th\n",
            "\n",
            "AAAI Conference on Artificial Intelligence (2015): 2094–2100.\n",
            "\n",
            "15 Tom Schaul et al., “Prioritized Experience Replay”, arXiv preprint arXiv:1511.05952 (2015).\n",
            "\n",
            "714 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fMore  specifically,  experiences  are  considered  “important”  if  they  are  likely  to  lead\n",
            "to  fast  learning  progress.  But  how  can  we  estimate  this?  One  reasonable  approach\n",
            "is  to  measure  the  magnitude  of  the  TD  error  δ  =  r  +  γ·V(s′)  –  V(s).  A  large  TD\n",
            "error indicates that a transition (s, a, s′) is very surprising, and thus probably worth\n",
            "learning  from.16  When  an  experience  is  recorded  in  the  replay  buffer,  its  priority  is\n",
            "set to a very large value, to ensure that it gets sampled at least once. However, once\n",
            "it  is  sampled  (and  every  time  it  is  sampled),  the  TD  error  δ  is  computed,  and  this\n",
            "experience’s priority is set to p = |δ| (plus a small constant to ensure that every expe‐\n",
            "rience  has  a  nonzero  probability  of  being  sampled).  The  probability  P  of  sampling\n",
            "an experience with priority p is proportional to pζ, where ζ is a hyperparameter that\n",
            "controls  how  greedy  we  want  importance  sampling  to  be:  when  ζ  =  0,  we  just  get\n",
            "uniform  sampling,  and  when  ζ  =  1,  we  get  full-blown  importance  sampling.  In  the\n",
            "paper, the authors used ζ = 0.6, but the optimal value will depend on the task.\n",
            "\n",
            "There’s  one  catch,  though:  since  the  samples  will  be  biased  toward  important  expe‐\n",
            "riences,  we  must  compensate  for  this  bias  during  training  by  downweighting  the\n",
            "experiences  according  to  their  importance,  or  else  the  model  will  just  overfit  the\n",
            "important  experiences.  To  be  clear,  we  want  important  experiences  to  be  sampled\n",
            "more often, but this also means we must give them a lower weight during training.\n",
            "To do this, we define each experience’s training weight as w = (n P)–β, where n is the\n",
            "number of experiences in the replay buffer, and β is a hyperparameter that controls\n",
            "how much we want to compensate for the importance sampling bias (0 means not at\n",
            "all, while 1 means entirely). In the paper, the authors used β = 0.4 at the beginning of\n",
            "training and linearly increased it to β = 1 by the end of training. Again, the optimal\n",
            "value  will  depend  on  the  task,  but  if  you  increase  one,  you  will  usually  want  to\n",
            "increase the other as well.\n",
            "\n",
            "Now let’s look at one last important variant of the DQN algorithm.\n",
            "\n",
            "Dueling DQN\n",
            "The dueling DQN algorithm (DDQN, not to be confused with double DQN, although\n",
            "both techniques can easily be combined) was introduced in yet another 2015 paper17\n",
            "by  DeepMind  researchers.  To  understand  how  it  works,  we  must  first  note  that  the\n",
            "Q-value of a state-action pair (s, a) can be expressed as Q(s, a) = V(s) + A(s, a), where\n",
            "V(s) is the value of state s and A(s, a) is the advantage of taking the action a in state\n",
            "s, compared to all other possible actions in that state. Moreover, the value of a state is\n",
            "equal to the Q-value of the best action a* for that state (since we assume the optimal\n",
            "\n",
            "16 It could also just be that the rewards are noisy, in which case there are better methods for estimating an\n",
            "\n",
            "experience’s importance (see the paper for some examples).\n",
            "\n",
            "17 Ziyu Wang et al., “Dueling Network Architectures for Deep Reinforcement Learning”, arXiv preprint\n",
            "\n",
            "arXiv:1511.06581 (2015).\n",
            "\n",
            "Deep Q-Learning Variants \n",
            "\n",
            "| \n",
            "\n",
            "715\n",
            "\n",
            "\fpolicy will pick the best action), so V(s) = Q(s, a*), which implies that A(s, a*) = 0. In\n",
            "a dueling DQN, the model estimates both the value of the state and the advantage of\n",
            "each possible action. Since the best action should have an advantage of 0, the model\n",
            "subtracts the maximum predicted advantage from all predicted advantages. Here is a\n",
            "simple DDQN model, implemented using the functional API:\n",
            "\n",
            "input_states = tf.keras.layers.Input(shape=[4])\n",
            "hidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
            "hidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
            "state_values = tf.keras.layers.Dense(1)(hidden2)\n",
            "raw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\n",
            "advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\n",
            "                                            keepdims=True)\n",
            "Q_values = state_values + advantages\n",
            "model = tf.keras.Model(inputs=[input_states], outputs=[Q_values])\n",
            "\n",
            "The rest of the algorithm is just the same as earlier. In fact, you can build a double\n",
            "dueling  DQN  and  combine  it  with  prioritized  experience  replay!  More  generally,\n",
            "many RL techniques can be combined, as DeepMind demonstrated in a 2017 paper:18\n",
            "the paper’s authors combined six different techniques into an agent called Rainbow,\n",
            "which largely outperformed the state of the art.\n",
            "\n",
            "As you can see, deep reinforcement learning is a fast-growing field and there’s much\n",
            "more to discover!\n",
            "\n",
            "Overview of Some Popular RL Algorithms\n",
            "Before we close this chapter, let’s take a brief look at a few other popular algorithms:\n",
            "\n",
            "AlphaGo19\n",
            "\n",
            "AlphaGo uses a variant of Monte Carlo tree search (MCTS) based on deep neural\n",
            "networks  to  beat  human  champions  at  the  game  of  Go.  MCTS  was  invented  in\n",
            "1949  by  Nicholas  Metropolis  and  Stanislaw  Ulam.  It  selects  the  best  move  after\n",
            "running  many  simulations,  repeatedly  exploring  the  search  tree  starting  from\n",
            "the current position, and spending more time on the most promising branches.\n",
            "When  it  reaches  a  node  that  it  hasn’t  visited  before,  it  plays  randomly  until\n",
            "the  game  ends,  and  updates  its  estimates  for  each  visited  node  (excluding  the\n",
            "random  moves),  increasing  or  decreasing  each  estimate  depending  on  the  final\n",
            "outcome. AlphaGo is based on the same principle, but it uses a policy network\n",
            "to  select  moves,  rather  than  playing  randomly.  This  policy  net  is  trained  using\n",
            "policy  gradients.  The  original  algorithm  involved  three  more  neural  networks,\n",
            "\n",
            "18 Matteo Hessel et al., “Rainbow: Combining Improvements in Deep Reinforcement Learning”, arXiv preprint\n",
            "\n",
            "arXiv:1710.02298 (2017): 3215–3222.\n",
            "\n",
            "19 David Silver et al., “Mastering the Game of Go with Deep Neural Networks and Tree Search”, Nature 529\n",
            "\n",
            "(2016): 484–489.\n",
            "\n",
            "716 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fand  was  more  complicated,  but  it  was  simplified  in  the  AlphaGo  Zero  paper,20\n",
            "which uses a single neural network to both select moves and evaluate game states.\n",
            "The AlphaZero paper21 generalized this algorithm, making it capable of tackling\n",
            "not  only  the  game  of  Go,  but  also  chess  and  shogi  (Japanese  chess).  Lastly,  the\n",
            "MuZero  paper22  continued  to  improve  upon  this  algorithm,  outperforming  the\n",
            "previous  iterations  even  though  the  agent  starts  out  without  even  knowing  the\n",
            "rules of the game!\n",
            "\n",
            "Actor-critic algorithms\n",
            "\n",
            "Actor-critics  are  a  family  of  RL  algorithms  that  combine  policy  gradients  with\n",
            "deep  Q-networks.  An  actor-critic  agent  contains  two  neural  networks:  a  policy\n",
            "net  and  a  DQN.  The  DQN  is  trained  normally,  by  learning  from  the  agent’s\n",
            "experiences.  The  policy  net  learns  differently  (and  much  faster)  than  in  regular\n",
            "PG:  instead  of  estimating  the  value  of  each  action  by  going  through  multiple\n",
            "episodes,  then  summing  the  future  discounted  rewards  for  each  action,  and\n",
            "finally normalizing them, the agent (actor) relies on the action values estimated\n",
            "by the DQN (critic). It’s a bit like an athlete (the agent) learning with the help of a\n",
            "coach (the DQN).\n",
            "\n",
            "Asynchronous advantage actor-critic (A3C)23\n",
            "\n",
            "This  is  an  important  actor-critic  variant  introduced  by  DeepMind  researchers\n",
            "in 2016 where multiple agents learn in parallel, exploring different copies of the\n",
            "environment.  At  regular  intervals,  but  asynchronously  (hence  the  name),  each\n",
            "agent  pushes  some  weight  updates  to  a  master  network,  then  it  pulls  the  latest\n",
            "weights from that network. Each agent thus contributes to improving the master\n",
            "network and benefits from what the other agents have learned. Moreover, instead\n",
            "of  estimating  the  Q-values,  the  DQN  estimates  the  advantage  of  each  action\n",
            "(hence the second A in the name), which stabilizes training.\n",
            "\n",
            "Advantage actor-critic (A2C)\n",
            "\n",
            "A2C is a variant of the A3C algorithm that removes the asynchronicity. All model\n",
            "updates are synchronous, so gradient updates are performed over larger batches,\n",
            "which allows the model to better utilize the power of the GPU.\n",
            "\n",
            "20 David Silver et al., “Mastering the Game of Go Without Human Knowledge”, Nature 550 (2017): 354–359.\n",
            "\n",
            "21 David Silver et al., “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algo‐\n",
            "\n",
            "rithm”, arXiv preprint arXiv:1712.01815.\n",
            "\n",
            "22 Julian Schrittwieser et al., “Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model”, arXiv\n",
            "\n",
            "preprint arXiv:1911.08265 (2019).\n",
            "\n",
            "23 Volodymyr Mnih et al., “Asynchronous Methods for Deep Reinforcement Learning”, Proceedings of the 33rd\n",
            "\n",
            "International Conference on Machine Learning (2016): 1928–1937.\n",
            "\n",
            "Overview of Some Popular RL Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "717\n",
            "\n",
            "\fSoft actor-critic (SAC)24\n",
            "\n",
            "SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other\n",
            "UC  Berkeley  researchers.  It  learns  not  only  rewards,  but  also  to  maximize  the\n",
            "entropy of its actions. In other words, it tries to be as unpredictable as possible\n",
            "while  still  getting  as  many  rewards  as  possible.  This  encourages  the  agent  to\n",
            "explore  the  environment,  which  speeds  up  training,  and  makes  it  less  likely  to\n",
            "repeatedly execute the same action when the DQN produces imperfect estimates.\n",
            "This  algorithm  has  demonstrated  an  amazing  sample  efficiency  (contrary  to  all\n",
            "the previous algorithms, which learn very slowly).\n",
            "\n",
            "Proximal policy optimization (PPO)25\n",
            "\n",
            "This  algorithm  by  John  Schulman  and  other  OpenAI  researchers  is  based  on\n",
            "A2C,  but  it  clips  the  loss  function  to  avoid  excessively  large  weight  updates\n",
            "(which often lead to training instabilities). PPO is a simplification of the previous\n",
            "trust  region  policy  optimization26  (TRPO)  algorithm,  also  by  OpenAI.  OpenAI\n",
            "made the news in April 2019 with its AI called OpenAI Five, based on the PPO\n",
            "algorithm, which defeated the world champions at the multiplayer game Dota 2.\n",
            "\n",
            "Curiosity-based exploration27\n",
            "\n",
            "A recurring problem in RL is the sparsity of the rewards, which makes learning\n",
            "very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have\n",
            "proposed  an  exciting  way  to  tackle  this  issue:  why  not  ignore  the  rewards,  and\n",
            "just make the agent extremely curious to explore the environment? The rewards\n",
            "thus  become  intrinsic  to  the  agent,  rather  than  coming  from  the  environment.\n",
            "Similarly,  stimulating  curiosity  in  a  child  is  more  likely  to  give  good  results\n",
            "than  purely  rewarding  the  child  for  getting  good  grades.  How  does  this  work?\n",
            "The  agent  continuously  tries  to  predict  the  outcome  of  its  actions,  and  it  seeks\n",
            "situations where the outcome does not match its predictions. In other words, it\n",
            "wants to be surprised. If the outcome is predictable (boring), it goes elsewhere.\n",
            "However,  if  the  outcome  is  unpredictable  but  the  agent  notices  that  it  has  no\n",
            "control  over  it,  it  also  gets  bored  after  a  while.  With  only  curiosity,  the  authors\n",
            "succeeded in training an agent at many video games: even though the agent gets\n",
            "no penalty for losing, the game starts over, which is boring so it learns to avoid it.\n",
            "\n",
            "24 Tuomas Haarnoja et al., “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with\n",
            "a Stochastic Actor”, Proceedings of the 35th International Conference on Machine Learning (2018): 1856–1865.\n",
            "\n",
            "25 John Schulman et al., “Proximal Policy Optimization Algorithms”, arXiv preprint arXiv:1707.06347 (2017).\n",
            "\n",
            "26 John Schulman et al., “Trust Region Policy Optimization”, Proceedings of the 32nd International Conference on\n",
            "\n",
            "Machine Learning (2015): 1889–1897.\n",
            "\n",
            "27 Deepak Pathak et al., “Curiosity-Driven Exploration by Self-Supervised Prediction”, Proceedings of the 34th\n",
            "\n",
            "International Conference on Machine Learning (2017): 2778–2787.\n",
            "\n",
            "718 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fOpen-ended learning (OEL)\n",
            "\n",
            "The  objective  of  OEL  is  to  train  agents  capable  of  endlessly  learning  new  and\n",
            "interesting tasks, typically generated procedurally. We’re not there yet, but there\n",
            "has  been  some  amazing  progress  over  the  last  few  years.  For  example,  a  2019\n",
            "paper28 by a team of researchers from Uber AI introduced the POET algorithm,\n",
            "which  generates  multiple  simulated  2D  environments  with  bumps  and  holes\n",
            "and  trains  one  agent  per  environment:  the  agent’s  goal  is  to  walk  as  fast  as\n",
            "possible while avoiding the obstacles. The algorithm starts out with simple envi‐\n",
            "ronments,  but  they  gradually  get  harder  over  time:  this  is  called  curriculum\n",
            "learning. Moreover, although each agent is only trained within one environment,\n",
            "it must regularly compete against other agents, across all environments. In each\n",
            "environment, the winner is copied over and it replaces the agent that was there\n",
            "before.  This  way,  knowledge  is  regularly  transferred  across  environments,  and\n",
            "the  most  adaptable  agents  are  selected.  In  the  end,  the  agents  are  much  better\n",
            "walkers  than  agents  trained  on  a  single  task,  and  they  can  tackle  much  harder\n",
            "environments. Of course, this principle can be applied to other environments and\n",
            "tasks as well. If you’re interested in OEL, make sure to check out the Enhanced\n",
            "POET paper,29 as well as DeepMind’s 2021 paper30 on this topic.\n",
            "\n",
            "If you’d like to learn more about reinforcement learning, check out\n",
            "the book Reinforcement Learning by Phil Winder (O’Reilly).\n",
            "\n",
            "We  covered  many  topics  in  this  chapter:  policy  gradients,  Markov  chains,  Markov\n",
            "decision  processes,  Q-learning,  approximate  Q-learning,  and  deep  Q-learning  and\n",
            "its main variants (fixed Q-value targets, double DQN, dueling DQN, and prioritized\n",
            "experience  replay),  and  finally  we  took  a  quick  look  at  a  few  other  popular  algo‐\n",
            "rithms.  Reinforcement  learning  is  a  huge  and  exciting  field,  with  new  ideas  and\n",
            "algorithms popping out every day, so I hope this chapter sparked your curiosity: there\n",
            "is a whole world to explore!\n",
            "\n",
            "28 Rui Wang et al., “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and\n",
            "\n",
            "Diverse Learning Environments and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).\n",
            "\n",
            "29 Rui Wang et al., “Enhanced POET: Open-Ended Reinforcement Learning Through Unbounded Invention of\n",
            "\n",
            "Learning Challenges and Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).\n",
            "\n",
            "30 Open-Ended Learning Team et al., “Open-Ended Learning Leads to Generally Capable Agents”, arXiv preprint\n",
            "\n",
            "arXiv:2107.12808 (2021).\n",
            "\n",
            "Overview of Some Popular RL Algorithms \n",
            "\n",
            "| \n",
            "\n",
            "719\n",
            "\n",
            "\fExercises\n",
            "\n",
            "1.\n",
            "1. How would you define reinforcement learning? How is it different from regular\n",
            "\n",
            "supervised or unsupervised learning?\n",
            "\n",
            "2. Can  you  think  of  three  possible  applications  of  RL  that  were  not  mentioned  in\n",
            "2.\n",
            "this  chapter?  For  each  of  them,  what  is  the  environment?  What  is  the  agent?\n",
            "What are some possible actions? What are the rewards?\n",
            "\n",
            "3. What  is  the  discount  factor?  Can  the  optimal  policy  change  if  you  modify  the\n",
            "3.\n",
            "\n",
            "discount factor?\n",
            "\n",
            "4.\n",
            "4. How do you measure the performance of a reinforcement learning agent?\n",
            "\n",
            "5.\n",
            "5. What  is  the  credit  assignment  problem?  When  does  it  occur?  How  can  you\n",
            "\n",
            "alleviate it?\n",
            "\n",
            "6.\n",
            "6. What is the point of using a replay buffer?\n",
            "\n",
            "7.\n",
            "7. What is an off-policy RL algorithm?\n",
            "\n",
            "8.\n",
            "8. Use policy gradients to solve OpenAI Gym’s LunarLander-v2 environment.\n",
            "\n",
            "9. Use  a  double  dueling  DQN  to  train  an  agent  that  can  achieve  a  superhuman\n",
            "9.\n",
            "level at the famous Atari Breakout game (\"ALE/Breakout-v5\"). The observations\n",
            "are  images.  To  simplify  the  task,  you  should  convert  them  to  grayscale  (i.e.,\n",
            "average over the channels axis) then crop them and downsample them, so they’re\n",
            "just  large  enough  to  play,  but  not  more.  An  individual  image  does  not  tell  you\n",
            "which way the ball and the paddles are going, so you should merge two or three\n",
            "consecutive  images  to  form  each  state.  Lastly,  the  DQN  should  be  composed\n",
            "mostly of convolutional layers.\n",
            "\n",
            "10. If  you  have  about  $100  to  spare,  you  can  purchase  a  Raspberry  Pi  3  plus  some\n",
            "10.\n",
            "cheap  robotics  components,  install  TensorFlow  on  the  Pi,  and  go  wild!  For  an\n",
            "example, check out this fun post by Lukas Biewald, or take a look at GoPiGo or\n",
            "BrickPi.  Start  with  simple  goals,  like  making  the  robot  turn  around  to  find  the\n",
            "brightest  angle  (if  it  has  a  light  sensor)  or  the  closest  object  (if  it  has  a  sonar\n",
            "sensor), and move in that direction. Then you can start using deep learning: for\n",
            "example, if the robot has a camera, you can try to implement an object detection\n",
            "algorithm so it detects people and moves toward them. You can also try to use RL\n",
            "to make the agent learn on its own how to use the motors to achieve that goal.\n",
            "Have fun!\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "720 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 18: Reinforcement Learning\n",
            "\n",
            "\fCHAPTER 19\n",
            "Training and Deploying\n",
            "TensorFlow Models at Scale\n",
            "\n",
            "Once  you  have  a  beautiful  model  that  makes  amazing  predictions,  what  do  you  do\n",
            "with  it?  Well,  you  need  to  put  it  in  production!  This  could  be  as  simple  as  running\n",
            "the model on a batch of data, and perhaps writing a script that runs this model every\n",
            "night. However, it is often much more involved. Various parts of your infrastructure\n",
            "may need to use this model on live data, in which case you will probably want to wrap\n",
            "your model in a web service: this way, any part of your infrastructure can query the\n",
            "model at any time using a simple REST API (or some other protocol), as we discussed\n",
            "in Chapter 2. But as time passes, you’ll need to regularly retrain your model on fresh\n",
            "data and push the updated version to production. You must handle model versioning,\n",
            "gracefully  transition  from  one  model  to  the  next,  possibly  roll  back  to  the  previous\n",
            "model in case of problems, and perhaps run multiple different models in parallel to\n",
            "perform A/B experiments.1 If your product becomes successful, your service may start\n",
            "to get a large number of of queries per second (QPS), and it must scale up to support\n",
            "the load. A great solution to scale up your service, as you will see in this chapter, is\n",
            "to use TF Serving, either on your own hardware infrastructure or via a cloud service\n",
            "such as Google Vertex AI.2 It will take care of efficiently serving your model, handle\n",
            "graceful model transitions, and more. If you use the cloud platform you will also get\n",
            "many extra features, such as powerful monitoring tools.\n",
            "\n",
            "1 An A/B experiment consists in testing two different versions of your product on different subsets of users in\n",
            "\n",
            "order to check which version works best and get other insights.\n",
            "\n",
            "2 Google AI Platform (formerly known as Google ML Engine) and Google AutoML merged in 2021 to form\n",
            "\n",
            "Google Vertex AI.\n",
            "\n",
            "721\n",
            "\n",
            "\fMoreover,  if  you  have  a  lot  of  training  data  and  compute-intensive  models,  then\n",
            "training  time  may  be  prohibitively  long.  If  your  product  needs  to  adapt  to  changes\n",
            "quickly,  then  a  long  training  time  can  be  a  showstopper  (e.g.,  think  of  a  news\n",
            "recommendation system promoting news from last week). Perhaps even more impor‐\n",
            "tantly, a long training time will prevent you from experimenting with new ideas. In\n",
            "machine learning (as in many other fields), it is hard to know in advance which ideas\n",
            "will work, so you should try out as many as possible, as fast as possible. One way to\n",
            "speed up training is to use hardware accelerators such as GPUs or TPUs. To go even\n",
            "faster, you can train a model across multiple machines, each equipped with multiple\n",
            "hardware  accelerators.  TensorFlow’s  simple  yet  powerful  distribution  strategies  API\n",
            "makes this easy, as you will see.\n",
            "\n",
            "In  this  chapter  we  will  look  at  how  to  deploy  models,  first  using  TF  Serving,  then\n",
            "using  Vertex  AI.  We  will  also  take  a  quick  look  at  deploying  models  to  mobile\n",
            "apps, embedded devices, and web apps. Then we will discuss how to speed up com‐\n",
            "putations  using  GPUs  and  how  to  train  models  across  multiple  devices  and  servers\n",
            "using the distribution strategies API. Lastly, we will explore how to train models and\n",
            "fine-tune  their  hyperparameters  at  scale  using  Vertex  AI.  That’s  a  lot  of  topics  to\n",
            "discuss, so let’s dive in!\n",
            "\n",
            "Serving a TensorFlow Model\n",
            "Once you have trained a TensorFlow model, you can easily use it in any Python code:\n",
            "if it’s a Keras model, just call its predict() method! But as your infrastructure grows,\n",
            "there  comes  a  point  where  it  is  preferable  to  wrap  your  model  in  a  small  service\n",
            "whose sole role is to make predictions and have the rest of the infrastructure query\n",
            "it  (e.g.,  via  a  REST  or  gRPC  API).3  This  decouples  your  model  from  the  rest  of  the\n",
            "infrastructure, making it possible to easily switch model versions or scale the service\n",
            "up  as  needed  (independently  from  the  rest  of  your  infrastructure),  perform  A/B\n",
            "experiments, and ensure that all your software components rely on the same model\n",
            "versions. It also simplifies testing and development, and more. You could create your\n",
            "own microservice using any technology you want (e.g., using the Flask library), but\n",
            "why reinvent the wheel when you can just use TF Serving?\n",
            "\n",
            "Using TensorFlow Serving\n",
            "TF  Serving  is  a  very  efficient,  battle-tested  model  server,  written  in  C++.  It  can\n",
            "sustain  a  high  load,  serve  multiple  versions  of  your  models  and  watch  a  model\n",
            "repository to automatically deploy the latest versions, and more (see Figure 19-1).\n",
            "\n",
            "3 A REST (or RESTful) API is an API that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE,\n",
            "and uses JSON inputs and outputs. The gRPC protocol is more complex but more efficient; data is exchanged\n",
            "using protocol buffers (see Chapter 13).\n",
            "\n",
            "722 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fFigure 19-1. TF Serving can serve multiple models and automatically deploy the latest\n",
            "version of each model\n",
            "\n",
            "So  let’s  suppose  you  have  trained  an  MNIST  model  using  Keras,  and  you  want  to\n",
            "deploy  it  to  TF  Serving.  The  first  thing  you  have  to  do  is  export  this  model  to  the\n",
            "SavedModel format, introduced in Chapter 10.\n",
            "\n",
            "Exporting SavedModels\n",
            "\n",
            "You already know how to save the model: just call model.save(). Now to version the\n",
            "model, you just need to create a subdirectory for each model version. Easy!\n",
            "\n",
            "from pathlib import Path\n",
            "import tensorflow as tf\n",
            "\n",
            "X_train, X_valid, X_test = [...]  # load and split the MNIST dataset\n",
            "model = [...]  # build & train an MNIST model (also handles image preprocessing)\n",
            "\n",
            "model_name = \"my_mnist_model\"\n",
            "model_version = \"0001\"\n",
            "model_path = Path(model_name) / model_version\n",
            "model.save(model_path, save_format=\"tf\")\n",
            "\n",
            "It’s usually a good idea to include all the preprocessing layers in the final model you\n",
            "export so that it can ingest data in its natural form once it is deployed to production.\n",
            "This  avoids  having  to  take  care  of  preprocessing  separately  within  the  application\n",
            "that uses the model. Bundling the preprocessing steps within the model also makes it\n",
            "simpler to update them later on and limits the risk of mismatch between a model and\n",
            "the preprocessing steps it requires.\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "723\n",
            "\n",
            "\fSince  a  SavedModel  saves  the  computation  graph,  it  can  only  be\n",
            "used  with  models  that  are  based  exclusively  on  TensorFlow  oper‐\n",
            "ations,  excluding  the  tf.py_function()  operation,  which  wraps\n",
            "arbitrary Python code.\n",
            "\n",
            "TensorFlow comes with a small saved_model_cli command-line interface to inspect\n",
            "SavedModels. Let use it to inspect our exported model:\n",
            "\n",
            "$ saved_model_cli show --dir my_mnist_model/0001\n",
            "The given SavedModel contains the following tag-sets:\n",
            "'serve'\n",
            "\n",
            "What does this output mean? Well, a SavedModel contains one or more metagraphs.\n",
            "A  metagraph  is  a  computation  graph  plus  some  function  signature  definitions,\n",
            "including their input and output names, types, and shapes. Each metagraph is identi‐\n",
            "fied by a set of tags. For example, you may want to have a metagraph containing the\n",
            "full  computation  graph,  including  the  training  operations:  you  would  typically  tag\n",
            "this  one  as  \"train\".  And  you  might  have  another  metagraph  containing  a  pruned\n",
            "computation graph with only the prediction operations, including some GPU-specific\n",
            "operations:  this  one  might  be  tagged  as  \"serve\",  \"gpu\".  You  might  want  to  have\n",
            "other metagraphs as well. This can be done using TensorFlow’s low-level SavedModel\n",
            "API.  However,  when  you  save  a  Keras  model  using  its  save()  method,  it  saves  a\n",
            "single metagraph tagged as \"serve\". Let’s inspect this \"serve\" tag set:\n",
            "\n",
            "$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve\n",
            "The given SavedModel MetaGraphDef contains SignatureDefs with these keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serving_default\"\n",
            "\n",
            "This  metagraph  contains  two  signature  definitions:  an  initialization  function  called\n",
            "\"__saved_model_init_op\",  which  you  do  not  need  to  worry  about,  and  a  default\n",
            "serving function called \"serving_default\". When saving a Keras model, the default\n",
            "serving  function  is  the  model’s  call()  method,  which  makes  predictions,  as  you\n",
            "already know. Let’s get more details about this serving function:\n",
            "\n",
            "724 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve \\\n",
            "                       --signature_def serving_default\n",
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['flatten_input'] tensor_info:\n",
            "      dtype: DT_UINT8\n",
            "      shape: (-1, 28, 28)\n",
            "      name: serving_default_flatten_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['dense_1'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n",
            "\n",
            "Note that the function’s input is named  \"flatten_input\", and the output is named\n",
            "\"dense_1\". These correspond to the Keras model’s input and output layer names. You\n",
            "can also see the type and shape of the input and output data. Looks good!\n",
            "\n",
            "Now that you have a SavedModel, the next step is to install TF Serving.\n",
            "\n",
            "Installing and starting TensorFlow Serving\n",
            "\n",
            "There are many ways to install TF Serving: using the system’s package manager, using\n",
            "a Docker image,4 installing from source, and more. Since Colab runs on Ubuntu, we\n",
            "can use Ubuntu’s apt package manager like this:\n",
            "\n",
            "url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n",
            "src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n",
            "!echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n",
            "!curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n",
            "!apt update -q && apt-get install -y tensorflow-model-server\n",
            "%pip install -q -U tensorflow-serving-api\n",
            "\n",
            "This  code  starts  by  adding  TensorFlow’s  package  repository  to  Ubuntu’s  list  of\n",
            "package  sources.  Then  it  downloads  TensorFlow’s  public  GPG  key  and  adds  it  to\n",
            "the  package  manager’s  key  list  so  it  can  verify  TensorFlow’s  package  signatures.\n",
            "Next,  it  uses  apt  to  install  the  tensorflow-model-server  package.  Lastly,  it  installs\n",
            "the  tensorflow-serving-api  library,  which  we  will  need  to  communicate  with  the\n",
            "server.\n",
            "\n",
            "4 If you are not familiar with Docker, it allows you to easily download a set of applications packaged in a Docker\n",
            "image (including all their dependencies and usually some good default configuration) and then run them on\n",
            "your system using a Docker engine. When you run an image, the engine creates a Docker container that keeps\n",
            "the applications well isolated from your own system—but you can give it some limited access if you want. It\n",
            "is similar to a virtual machine, but much faster and lighter, as the container relies directly on the host’s kernel.\n",
            "This means that the image does not need to include or run its own kernel.\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "725\n",
            "\n",
            "\fNow we want to start the server. The command will require the absolute path of the\n",
            "base model directory (i.e., the path to my_mnist_model, not 0001), so let’s save that to\n",
            "the MODEL_DIR environment variable:\n",
            "\n",
            "import os\n",
            "\n",
            "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())\n",
            "\n",
            "We can then start the server:\n",
            "\n",
            "%%bash --bg\n",
            "tensorflow_model_server \\\n",
            "     --port=8500 \\\n",
            "     --rest_api_port=8501 \\\n",
            "     --model_name=my_mnist_model \\\n",
            "     --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1\n",
            "\n",
            "In  Jupyter  or  Colab,  the  %%bash  --bg  magic  command  executes  the  cell  as  a  bash\n",
            "script,  running  it  in  the  background.  The  >my_server.log  2>&1  part  redirects  the\n",
            "standard output and standard error to the my_server.log file. And that’s it! TF Serving\n",
            "is now running in the background, and its logs are saved to my_server.log. It loaded\n",
            "our  MNIST  model  (version  1),  and  it  is  now  waiting  for  gRPC  and  REST  requests,\n",
            "respectively, on ports 8500 and 8501.\n",
            "\n",
            "Running TF Serving in a Docker Container\n",
            "If  you  are  running  the  notebook  on  your  own  machine  and  you  have  installed\n",
            "Docker, you can run docker pull tensorflow/serving in a terminal to download\n",
            "the  TF  Serving  image.  The  TensorFlow  team  highly  recommends  this  installation\n",
            "method  because  it  is  simple,  it  will  not  mess  with  your  system,  and  it  offers  high\n",
            "performance.5 To start the server inside a Docker container, you can run the following\n",
            "command in a terminal:\n",
            "\n",
            "$ docker run -it --rm -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\n",
            "    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving\n",
            "\n",
            "Here is what all these command-line options mean:\n",
            "\n",
            "-it\n",
            "\n",
            "Makes the container interactive (so you can press Ctrl-C to stop it) and displays\n",
            "the server’s output.\n",
            "\n",
            "--rm\n",
            "\n",
            "Deletes  the  container  when  you  stop  it:  no  need  to  clutter  your  machine  with\n",
            "interrupted containers. However, it does not delete the image.\n",
            "\n",
            "5 There are also GPU images available, and other installation options. For more details, please check out the\n",
            "\n",
            "official installation instructions.\n",
            "\n",
            "726 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f-v \"/path/to/my_mnist_model:/models/my_mnist_model\"\n",
            "\n",
            "Makes  the  host’s  my_mnist_model  directory  available  to  the  container  at  the\n",
            "path  /models/mnist_model.  You  must  replace  /path/to/my_mnist_model  with  the\n",
            "absolute path of this directory. On Windows, remember to use \\ instead of / in\n",
            "the host path, but not in the container path (since the container runs on Linux).\n",
            "\n",
            "-p 8500:8500\n",
            "\n",
            "Makes  the  Docker  engine  forward  the  host’s  TCP  port  8500  to  the  container’s\n",
            "TCP port 8500. By default, TF Serving uses this port to serve the gRPC API.\n",
            "\n",
            "-p 8501:8501\n",
            "\n",
            "Forwards the host’s TCP port 8501 to the container’s TCP port 8501. The Docker\n",
            "image is configured to use this port by default to serve the REST API.\n",
            "\n",
            "-e MODEL_NAME=my_mnist_model\n",
            "\n",
            "Sets  the  container’s  MODEL_NAME  environment  variable,  so  TF  Serving  knows\n",
            "which model to serve. By default, it will look for models in the /models directory,\n",
            "and it will automatically serve the latest version it finds.\n",
            "\n",
            "tensorflow/serving\n",
            "\n",
            "This is the name of the image to run.\n",
            "\n",
            "Now that the server is up and running, let’s query it, first using the REST API, then\n",
            "the gRPC API.\n",
            "\n",
            "Querying TF Serving through the REST API\n",
            "\n",
            "Let’s start by creating the query. It must contain the name of the function signature\n",
            "you want to call, and of course the input data. Since the request must use the JSON\n",
            "format, we have to convert the input images from a NumPy array to a Python list:\n",
            "\n",
            "import json\n",
            "\n",
            "X_new = X_test[:3]  # pretend we have 3 new digit images to classify\n",
            "request_json = json.dumps({\n",
            "    \"signature_name\": \"serving_default\",\n",
            "    \"instances\": X_new.tolist(),\n",
            "})\n",
            "\n",
            "Note that the JSON format is 100% text-based. The request string looks like this:\n",
            "\n",
            ">>> request_json\n",
            "'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, ... ]]]}'\n",
            "\n",
            "Now  let’s  send  this  request  to  TF  Serving  via  an  HTTP  POST  request.  This  can  be\n",
            "done using the requests library (it is not part of Python’s standard library, but it is\n",
            "preinstalled on Colab):\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "727\n",
            "\n",
            "\fimport requests\n",
            "\n",
            "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
            "response = requests.post(server_url, data=request_json)\n",
            "response.raise_for_status()  # raise an exception in case of error\n",
            "response = response.json()\n",
            "\n",
            "If  all  goes  well,  the  response  should  be  a  dictionary  containing  a  single\n",
            "\"predictions\"  key.  The  corresponding  value  is  the  list  of  predictions.  This  list  is\n",
            "a Python list, so let’s convert it to a NumPy array and round the floats it contains to\n",
            "the second decimal:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> y_proba = np.array(response[\"predictions\"])\n",
            ">>> y_proba.round(2)\n",
            "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
            "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
            "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\n",
            "\n",
            "Hurray, we have the predictions! The model is close to 100% confident that the first\n",
            "image is a 7, 99% confident that the second image is a 2, and 97% confident that the\n",
            "third image is a 1. That’s correct.\n",
            "\n",
            "The  REST  API  is  nice  and  simple,  and  it  works  well  when  the  input  and  output\n",
            "data  are  not  too  large.  Moreover,  just  about  any  client  application  can  make  REST\n",
            "queries  without  additional  dependencies,  whereas  other  protocols  are  not  always\n",
            "so  readily  available.  However,  it  is  based  on  JSON,  which  is  text-based  and  fairly\n",
            "verbose.  For  example,  we  had  to  convert  the  NumPy  array  to  a  Python  list,  and\n",
            "every  float  ended  up  represented  as  a  string.  This  is  very  inefficient,  both  in  terms\n",
            "of serialization/deserialization time—we have to convert all the floats to strings and\n",
            "back—and in terms of payload size: many floats end up being represented using over\n",
            "15  characters,  which  translates  to  over  120  bits  for  32-bit  floats!  This  will  result  in\n",
            "high latency and bandwidth usage when transferring large NumPy arrays.6 So, let’s see\n",
            "how to use gRPC instead.\n",
            "\n",
            "When  transferring  large  amounts  of  data,  or  when  latency  is\n",
            "important,  it  is  much  better  to  use  the  gRPC  API,  if  the  client\n",
            "supports  it,  as  it  uses  a  compact  binary  format  and  an  efficient\n",
            "communication protocol based on HTTP/2 framing.\n",
            "\n",
            "6 To be fair, this can be mitigated by serializing the data first and encoding it to Base64 before creating the REST\n",
            "request. Moreover, REST requests can be compressed using gzip, which reduces the payload size significantly.\n",
            "\n",
            "728 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fQuerying TF Serving through the gRPC API\n",
            "\n",
            "The gRPC API expects a serialized  PredictRequest protocol buffer as input, and it\n",
            "outputs  a  serialized  PredictResponse  protocol  buffer.  These  protobufs  are  part  of\n",
            "the tensorflow-serving-api library, which we installed earlier. First, let’s create the\n",
            "request:\n",
            "\n",
            "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
            "\n",
            "request = PredictRequest()\n",
            "request.model_spec.name = model_name\n",
            "request.model_spec.signature_name = \"serving_default\"\n",
            "input_name = model.input_names[0]  # == \"flatten_input\"\n",
            "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n",
            "\n",
            "This  code  creates  a  PredictRequest  protocol  buffer  and  fills  in  the  required  fields,\n",
            "including  the  model  name  (defined  earlier),  the  signature  name  of  the  function  we\n",
            "want  to  call,  and  finally  the  input  data,  in  the  form  of  a  Tensor  protocol  buffer.\n",
            "The tf.make_tensor_proto() function creates a Tensor protocol buffer based on the\n",
            "given tensor or NumPy array, in this case X_new.\n",
            "\n",
            "Next, we’ll send the request to the server and get its response. For this, we will need\n",
            "the grpcio library, which is preinstalled in Colab:\n",
            "\n",
            "import grpc\n",
            "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
            "\n",
            "channel = grpc.insecure_channel('localhost:8500')\n",
            "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
            "response = predict_service.Predict(request, timeout=10.0)\n",
            "\n",
            "The code is quite straightforward: after the imports, we create a gRPC communica‐\n",
            "tion channel to localhost on TCP port 8500, then we create a gRPC service over this\n",
            "channel and use it to send a request, with a 10-second timeout. Note that the call is\n",
            "synchronous: it will block until it receives the response or when the timeout period\n",
            "expires.  In  this  example  the  channel  is  insecure  (no  encryption,  no  authentication),\n",
            "but gRPC and TF Serving also support secure channels over SSL/TLS.\n",
            "\n",
            "Next, let’s convert the PredictResponse protocol buffer to a tensor:\n",
            "\n",
            "output_name = model.output_names[0]  # == \"dense_1\"\n",
            "outputs_proto = response.outputs[output_name]\n",
            "y_proba = tf.make_ndarray(outputs_proto)\n",
            "\n",
            "If you run this code and print y_proba.round(2), you will get the exact same estima‐\n",
            "ted class probabilities as earlier. And that’s all there is to it: in just a few lines of code,\n",
            "you can now access your TensorFlow model remotely, using either REST or gRPC.\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "729\n",
            "\n",
            "\fDeploying a new model version\n",
            "\n",
            "Now  let’s  create  a  new  model  version  and  export  a  SavedModel,  this  time  to  the\n",
            "my_mnist_model/0002 directory:\n",
            "\n",
            "model = [...]  # build and train a new MNIST model version\n",
            "\n",
            "model_version = \"0002\"\n",
            "model_path = Path(model_name) / model_version\n",
            "model.save(model_path, save_format=\"tf\")\n",
            "\n",
            "At regular intervals (the delay is configurable), TF Serving checks the model directory\n",
            "for new model versions. If it finds one, it automatically handles the transition grace‐\n",
            "fully: by default, it answers pending requests (if any) with the previous model version,\n",
            "while handling new requests with the new version. As soon as every pending request\n",
            "has been answered, the previous model version is unloaded. You can see this at work\n",
            "in the TF Serving logs (in my_server.log):\n",
            "\n",
            "[...]\n",
            "Reading SavedModel from: /models/my_mnist_model/0002\n",
            "Reading meta graph with tags { serve }\n",
            "[...]\n",
            "Successfully loaded servable version {name: my_mnist_model version: 2}\n",
            "Quiescing servable version {name: my_mnist_model version: 1}\n",
            "Done quiescing servable version {name: my_mnist_model version: 1}\n",
            "Unloading servable version {name: my_mnist_model version: 1}\n",
            "\n",
            "If  the  SavedModel  contains  some  example  instances  in  the  assets/\n",
            "extra  directory,  you  can  configure  TF  Serving  to  run  the  new\n",
            "model on these instances before starting to use it to serve requests.\n",
            "This is called model warmup: it will ensure that everything is prop‐\n",
            "erly loaded, avoiding long response times for the first requests.\n",
            "\n",
            "This approach offers a smooth transition, but it may use too much RAM—especially\n",
            "GPU  RAM,  which  is  generally  the  most  limited.  In  this  case,  you  can  configure  TF\n",
            "Serving so that it handles all pending requests with the previous model version and\n",
            "unloads  it  before  loading  and  using  the  new  model  version.  This  configuration  will\n",
            "avoid  having  two  model  versions  loaded  at  the  same  time,  but  the  service  will  be\n",
            "unavailable for a short period.\n",
            "\n",
            "As  you  can  see,  TF  Serving  makes  it  straightforward  to  deploy  new  models.  More‐\n",
            "over, if you discover that version 2 does not work as well as you expected, then rolling\n",
            "back to version 1 is as simple as removing the my_mnist_model/0002 directory.\n",
            "\n",
            "730 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fAnother great feature of TF Serving is its automatic batching capa‐\n",
            "bility, which you can activate using the --enable_batching option\n",
            "upon  startup.  When  TF  Serving  receives  multiple  requests  within\n",
            "a short period of time (the delay is configurable), it will automat‐\n",
            "ically  batch  them  together  before  using  the  model.  This  offers  a\n",
            "significant performance boost by leveraging the power of the GPU.\n",
            "Once  the  model  returns  the  predictions,  TF  Serving  dispatches\n",
            "each  prediction  to  the  right  client.  You  can  trade  a  bit  of  latency\n",
            "for a greater throughput by increasing the batching delay (see the\n",
            "--batching_parameters_file option).\n",
            "\n",
            "If  you  expect  to  get  many  queries  per  second,  you  will  want  to  deploy  TF  Serving\n",
            "on multiple servers and load-balance the queries (see Figure 19-2). This will require\n",
            "deploying and managing many TF Serving containers across these servers. One way\n",
            "to  handle  that  is  to  use  a  tool  such  as  Kubernetes,  which  is  an  open  source  system\n",
            "for  simplifying  container  orchestration  across  many  servers.  If  you  do  not  want  to\n",
            "purchase,  maintain,  and  upgrade  all  the  hardware  infrastructure,  you  will  want  to\n",
            "use  virtual  machines  on  a  cloud  platform  such  as  Amazon  AWS,  Microsoft  Azure,\n",
            "Google  Cloud  Platform,  IBM  Cloud,  Alibaba  Cloud,  Oracle  Cloud,  or  some  other\n",
            "platform  as  a  service  (PaaS)  offering.  Managing  all  the  virtual  machines,  handling\n",
            "container orchestration (even with the help of Kubernetes), taking care of TF Serving\n",
            "configuration, tuning and monitoring—all of this can be a full-time job. Fortunately,\n",
            "some  service  providers  can  take  care  of  all  this  for  you.  In  this  chapter  we  will\n",
            "use  Vertex  AI:  it’s  the  only  platform  with  TPUs  today;  it  supports  TensorFlow  2,\n",
            "Scikit-Learn, and XGBoost; and it offers a nice suite of AI services. There are several\n",
            "other providers in this space that are capable of serving TensorFlow models as well,\n",
            "though, such as Amazon AWS SageMaker and Microsoft AI Platform, so make sure\n",
            "to check them out too.\n",
            "\n",
            "Figure 19-2. Scaling up TF Serving with load balancing\n",
            "\n",
            "Now let’s see how to serve our wonderful MNIST model on the cloud!\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "731\n",
            "\n",
            "\fCreating a Prediction Service on Vertex AI\n",
            "Vertex  AI  is  a  platform  within  Google  Cloud  Platform  (GCP)  that  offers  a  wide\n",
            "range of AI-related tools and services. You can upload datasets, get humans to label\n",
            "them, store commonly used features in a feature store and use them for training or\n",
            "in  production,  and  train  models  across  many  GPU  or  TPU  servers  with  automatic\n",
            "hyperparameter tuning or model architecture search (AutoML). You can also manage\n",
            "your trained models, use them to make batch predictions on large amounts of data,\n",
            "schedule  multiple  jobs  for  your  data  workflows,  serve  your  models  via  REST  or\n",
            "gRPC  at  scale,  and  experiment  with  your  data  and  models  within  a  hosted  Jupyter\n",
            "environment  called  the  Workbench.  There’s  even  a  Matching  Engine  service  that  lets\n",
            "you compare vectors very efficiently (i.e., approximate nearest neighbors). GCP also\n",
            "includes other AI services, such as APIs for computer vision, translation, speech-to-\n",
            "text, and more.\n",
            "\n",
            "Before we start, there’s a little bit of setup to take care of:\n",
            "\n",
            "1. Log  in  to  your  Google  account,  and  then  go  to  the  Google  Cloud  Platform\n",
            "1.\n",
            "console  (see  Figure  19-3).  If  you  don’t  have  a  Google  account,  you’ll  have  to\n",
            "create one.\n",
            "\n",
            "2.\n",
            "2. If  it’s  your  first  time  using  GCP,  you’ll  have  to  read  and  accept  the  terms  and\n",
            "conditions. New users are offered a free trial, including $300 worth of GCP credit\n",
            "that  you  can  use  over  the  course  of  90  days  (as  of  May  2022).  You’ll  only  need\n",
            "a  small  portion  of  that  to  pay  for  the  services  you’ll  use  in  this  chapter.  Upon\n",
            "signing up for a free trial, you’ll still need to create a payment profile and enter\n",
            "your  credit  card  number:  it’s  used  for  verification  purposes—probably  to  avoid\n",
            "people  using  the  free  trial  multiple  times—but  you  won’t  be  billed  for  the  first\n",
            "$300, and after that you’ll only be charged if you opt in by upgrading to a paid\n",
            "account.\n",
            "\n",
            "Figure 19-3. Google Cloud Platform console\n",
            "\n",
            "732 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f3. If  you  have  used  GCP  before  and  your  free  trial  has  expired,  then  the  services\n",
            "3.\n",
            "you will use in this chapter will cost you some money. It shouldn’t be too much,\n",
            "especially  if  you  remember  to  turn  off  the  services  when  you  don’t  need  them\n",
            "anymore. Make sure you understand and agree to the pricing conditions before\n",
            "you run any service. I hereby decline any responsibility if services end up costing\n",
            "more than you expected! Also make sure your billing account is active. To check,\n",
            "open the ☰ navigation menu at the top left and click Billing, then make sure you\n",
            "have set up a payment method and that the billing account is active.\n",
            "\n",
            "4. Every resource in GCP belongs to a project. This includes all the virtual machines\n",
            "4.\n",
            "you may use, the files you store, and the training jobs you run. When you create\n",
            "an account, GCP automatically creates a project for you, called “My First Project”.\n",
            "If you want, you can change its display name by going to the project settings: in\n",
            "the ☰ navigation menu, select “IAM and admin → Settings”, change the project’s\n",
            "display  name,  and  click  SAVE.  Note  that  the  project  also  has  a  unique  ID  and\n",
            "number. You can choose the project ID when you create a project, but you cannot\n",
            "change  it  later.  The  project  number  is  automatically  generated  and  cannot  be\n",
            "changed.  If  you  want  to  create  a  new  project,  click  the  project  name  at  the  top\n",
            "of the page, then click NEW PROJECT and enter the project name. You can also\n",
            "click EDIT to set the project ID. Make sure billing is active for this new project so\n",
            "that service fees can be billed (to your free credits, if any).\n",
            "\n",
            "Always  set  an  alarm  to  remind  yourself  to  turn  services  off\n",
            "when  you  know  you  will  only  need  them  for  a  few  hours,\n",
            "or  else  you  might  leave  them  running  for  days  or  months,\n",
            "incurring potentially significant costs.\n",
            "\n",
            "5.\n",
            "5. Now  that  you  have  a  GCP  account  and  a  project,  and  billing  is  activated,  you\n",
            "must  activate  the  APIs  you  need.  In  the  ☰  navigation  menu,  select  “APIs  and\n",
            "services”,  and  make  sure  the  Cloud  Storage  API  is  enabled.  If  needed,  click  +\n",
            "ENABLE APIS AND SERVICES, find Cloud Storage, and enable it. Also enable\n",
            "the Vertex AI API.\n",
            "\n",
            "You  could  continue  to  do  everything  via  the  GCP  console,  but  I  recommend  using\n",
            "Python  instead:  this  way  you  can  write  scripts  to  automate  just  about  anything  you\n",
            "want with GCP, and it’s often more convenient than clicking your way through menus\n",
            "and forms, especially for common tasks.\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "733\n",
            "\n",
            "\fGoogle Cloud CLI and Shell\n",
            "Google Cloud’s command-line interface (CLI) includes the gcloud command, which\n",
            "lets  you  control  almost  everything  in  GCP,  and  gsutil,  which  lets  you  interact\n",
            "with  Google  Cloud  Storage.  This  CLI  is  preinstalled  in  Colab:  all  you  need  to  do  is\n",
            "authenticate  using  google.auth.authenticate_user(),  and  you’re  good  to  go.  For\n",
            "example, !gcloud config list will display the configuration.\n",
            "\n",
            "GCP  also  offers  a  preconfigured  shell  environment  called  the  Google  Cloud  Shell,\n",
            "which you can use directly in your web browser; it runs on a free Linux VM (Debian)\n",
            "with  the  Google  Cloud  SDK  already  preinstalled  and  configured  for  you,  so  there’s\n",
            "no need to authenticate. The Cloud Shell is available anywhere in GCP: just click the\n",
            "Activate Cloud Shell icon at the top right of the page (see Figure 19-4).\n",
            "\n",
            "Figure 19-4. Activating the Google Cloud Shell\n",
            "\n",
            "If you prefer to install the CLI on your machine, then after installation you need to\n",
            "initialize  it  by  running  gcloud  init:  follow  the  instructions  to  log  in  to  GCP  and\n",
            "grant access to your GCP resources, then select the default GCP project you want to\n",
            "use (if you have more than one) and the default region where you want your jobs to\n",
            "run.\n",
            "\n",
            "The first thing you need to do before you can use any GCP service is to authenticate.\n",
            "The simplest solution when using Colab is to execute the following code:\n",
            "\n",
            "from google.colab import auth\n",
            "\n",
            "auth.authenticate_user()\n",
            "\n",
            "The authentication process is based on OAuth 2.0: a pop-up window will ask you to\n",
            "confirm that you want the Colab notebook to access your Google credentials. If you\n",
            "accept, you must select the same Google account you used for GCP. Then you will be\n",
            "asked to confirm that you agree to give Colab full access to all your data on Google\n",
            "Drive and in GCP. If you allow access, only the current notebook will have access, and\n",
            "only  until  the  Colab  runtime  expires.  Obviously,  you  should  only  accept  this  if  you\n",
            "trust the code in the notebook.\n",
            "\n",
            "734 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fIf  you  are  not  working  with  the  official  notebooks  from  https://\n",
            "github.com/ageron/handson-ml3,  then  you  should  be  extra  careful:\n",
            "if the notebook’s author is mischievous, they could include code to\n",
            "do whatever they want with your data.\n",
            "\n",
            "Authentication and Authorization on GCP\n",
            "In general, using OAuth 2.0 authentication is only recommended when an application\n",
            "must  access  the  user’s  personal  data  or  resources  from  another  application,  on  the\n",
            "user’s  behalf.  For  example,  some  applications  allow  the  user  to  save  data  to  their\n",
            "Google  Drive,  but  for  that  the  application  first  needs  the  user  to  authenticate  with\n",
            "Google  and  allow  access  to  Google  Drive.  In  general,  the  application  will  only  ask\n",
            "for  the  level  of  access  it  needs;  it  won’t  be  an  unlimited  access:  for  example,  the\n",
            "application will only request access to Google Drive, not Gmail or any other Google\n",
            "service. Moreover, the authorization usually expires after a while, and it can always be\n",
            "revoked.\n",
            "\n",
            "When  an  application  needs  to  access  a  service  on  GCP  on  its  own  behalf,  not  on\n",
            "behalf  of  the  user,  then  it  should  generally  use  a  service  account.  For  example,  if\n",
            "you build  a website that needs to send prediction requests  to a Vertex AI endpoint,\n",
            "then  the  website  will  be  accessing  the  service  on  its  own  behalf.  There’s  no  data  or\n",
            "resource  that  it  needs  to  access  in  the  user’s  Google  account.  In  fact,  many  users  of\n",
            "the website will not even have a Google account. For this scenario, you first need to\n",
            "create  a  service  account.  Select  “IAM  and  admin  →  Service  accounts”  in  the  GCP\n",
            "console’s ☰ navigation menu (or use the search box), then click + CREATE SERVICE\n",
            "ACCOUNT, fill in the first page of the form (service account name, ID, description),\n",
            "and click CREATE AND CONTINUE. Next, you must give this account some access\n",
            "rights.  Select  the  “Vertex  AI  user”  role:  this  will  allow  the  service  account  to  make\n",
            "predictions  and  use  other  Vertex  AI  services,  but  nothing  else.  Click  CONTINUE.\n",
            "You can now optionally grant some users access to the service account: this is useful\n",
            "when  your  GCP  user  account  is  part  of  an  organization  and  you  wish  to  authorize\n",
            "other  users  in  the  organization  to  deploy  applications  that  will  be  based  on  this\n",
            "service account, or to manage the service account itself. Next, click DONE.\n",
            "\n",
            "Once you have created a service account, your application must authenticate as that\n",
            "service  account.  There  are  several  ways  to  do  that.  If  your  application  is  hosted\n",
            "on  GCP—for  example,  if  you  are  coding  a  website  hosted  on  Google  Compute\n",
            "Engine—then  the  simplest  and  safest  solution  is  to  attach  the  service  account  to\n",
            "the  GCP  resource  that  hosts  your  website,  such  as  a  VM  instance  or  a  Google  App\n",
            "Engine  service.  This  can  be  done  when  creating  the  GCP  resource,  by  selecting  the\n",
            "service account in the “Identity and API access” section. Some resources, such as VM\n",
            "instances, also let you attach the service account after the VM instance is created: you\n",
            "must  stop  it  and  edit  its  settings.  In  any  case,  once  a  service  account  is  attached  to\n",
            "a VM instance, or any other GCP resource running your code, GCP’s client libraries\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "735\n",
            "\n",
            "\f(discussed shortly) will automatically authenticate as the chosen service account, with\n",
            "no extra step needed.\n",
            "\n",
            "If your application is hosted using Kubernetes, then you should use Google’s Work‐\n",
            "load  Identity  service  to  map  the  right  service  account  to  each  Kubernetes  service\n",
            "account.  If  your  application  is  not  hosted  on  GCP—for  example,  if  you  are  just\n",
            "running  the  Jupyter  notebook  on  your  own  machine—then  you  can  either  use  the\n",
            "Workload  Identity  Federation  service  (that’s  the  safest  but  hardest  option),  or  just\n",
            "generate an access key for your service account, save it to a JSON file, and point the\n",
            "GOOGLE_APPLICATION_CREDENTIALS environment variable to it so your client applica‐\n",
            "tion can access it. You can manage access keys by clicking the service account you just\n",
            "created, and then opening the KEYS tab. Make sure to keep the key file secret: it’s like\n",
            "a password for the service account.\n",
            "\n",
            "For more details on setting up authentication and authorization so your application\n",
            "can access GCP services, check out the documentation.\n",
            "\n",
            "Now  let’s  create  a  Google  Cloud  Storage  bucket  to  store  our  SavedModels  (a  GCS\n",
            "bucket is a container for your data). For this we will use the google-cloud-storage\n",
            "library,  which  is  preinstalled  in  Colab.  We  first  create  a  Client  object,  which  will\n",
            "serve as the interface with GCS, then we use it to create the bucket:\n",
            "\n",
            "from google.cloud import storage\n",
            "\n",
            "project_id = \"my_project\"  # change this to your project ID\n",
            "bucket_name = \"my_bucket\"  # change this to a unique bucket name\n",
            "location = \"us-central1\"\n",
            "\n",
            "storage_client = storage.Client(project=project_id)\n",
            "bucket = storage_client.create_bucket(bucket_name, location=location)\n",
            "\n",
            "If  you  want  to  reuse  an  existing  bucket,  replace  the  last  line\n",
            "with  bucket  =  storage_client.bucket(bucket_name).  Make\n",
            "sure location is set to the bucket’s region.\n",
            "\n",
            "GCS uses a single worldwide namespace for buckets, so simple names like “machine-\n",
            "learning”  will  most  likely  not  be  available.  Make  sure  the  bucket  name  conforms\n",
            "to  DNS  naming  conventions,  as  it  may  be  used  in  DNS  records.  Moreover,  bucket\n",
            "names are public, so do not put anything private in the name. It is common to use\n",
            "your  domain  name,  your  company  name,  or  your  project  ID  as  a  prefix  to  ensure\n",
            "uniqueness, or simply use a random number as part of the name.\n",
            "\n",
            "You can change the region if you want, but be sure to choose one that supports GPUs.\n",
            "Also,  you  may  want  to  consider  the  fact  that  prices  vary  greatly  between  regions,\n",
            "\n",
            "736 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fsome regions produce much more CO₂ than others, some regions do not support all\n",
            "services, and using a single-region bucket improves performance. See Google Cloud’s\n",
            "list of regions and Vertex AI’s documentation on locations for more details. If you are\n",
            "unsure, it might be best to stick with \"us-central1\".\n",
            "\n",
            "Next, let’s upload the my_mnist_model directory to the new bucket. Files in GCS are\n",
            "called  blobs  (or  objects),  and  under  the  hood  they  are  all  just  placed  in  the  bucket\n",
            "without  any  directory  structure.  Blob  names  can  be  arbitrary  Unicode  strings,  and\n",
            "they  can  even  contain  forward  slashes  (/).  The  GCP  console  and  other  tools  use\n",
            "these  slashes  to  give  the  illusion  that  there  are  directories.  So,  when  we  upload  the\n",
            "my_mnist_model directory, we only care about the files, not the directories:\n",
            "\n",
            "def upload_directory(bucket, dirpath):\n",
            "    dirpath = Path(dirpath)\n",
            "    for filepath in dirpath.glob(\"**/*\"):\n",
            "        if filepath.is_file():\n",
            "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
            "            blob.upload_from_filename(filepath)\n",
            "\n",
            "upload_directory(bucket, \"my_mnist_model\")\n",
            "\n",
            "This function works fine now, but it would be very slow if there were many files to\n",
            "upload.  It’s  not  too  hard  to  speed  it  up  tremendously  by  multithreading  it  (see  the\n",
            "notebook  for  an  implementation).  Alternatively,  if  you  have  the  Google  Cloud  CLI,\n",
            "then you can use following command instead:\n",
            "\n",
            "!gsutil -m cp -r my_mnist_model gs://{bucket_name}/\n",
            "\n",
            "Next, let’s tell Vertex AI about our MNIST model. To communicate with Vertex AI,\n",
            "we  can  use  the  google-cloud-aiplatform  library  (it  still  uses  the  old  AI  Platform\n",
            "name  instead  of  Vertex  AI).  It’s  not  preinstalled  in  Colab,  so  we  need  to  install  it.\n",
            "After  that,  we  can  import  the  library  and  initialize  it—just  to  specify  some  default\n",
            "values  for  the  project  ID  and  the  location—then  we  can  create  a  new  Vertex  AI\n",
            "model: we specify a display name, the GCS path to our model (in this case the version\n",
            "0001),  and  the  URL  of  the  Docker  container  we  want  Vertex  AI  to  use  to  run  this\n",
            "model. If you visit that URL and navigate up one level, you will find other containers\n",
            "you can use. This one supports TensorFlow 2.8 with a GPU:\n",
            "\n",
            "from google.cloud import aiplatform\n",
            "\n",
            "server_image = \"gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\"\n",
            "\n",
            "aiplatform.init(project=project_id, location=location)\n",
            "mnist_model = aiplatform.Model.upload(\n",
            "    display_name=\"mnist\",\n",
            "    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\n",
            "    serving_container_image_uri=server_image,\n",
            ")\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "737\n",
            "\n",
            "\fNow let’s deploy this model so we can query it via a gRPC or REST API to make pre‐\n",
            "dictions. For this we first need to create an endpoint. This is what client applications\n",
            "connect to when they want to access a service. Then we need to deploy our model to\n",
            "this endpoint:\n",
            "\n",
            "endpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\n",
            "\n",
            "endpoint.deploy(\n",
            "    mnist_model,\n",
            "    min_replica_count=1,\n",
            "    max_replica_count=5,\n",
            "    machine_type=\"n1-standard-4\",\n",
            "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
            "    accelerator_count=1\n",
            ")\n",
            "\n",
            "This code may take a few minutes to run, because Vertex AI needs to set up a virtual\n",
            "machine. In this example, we use a fairly basic machine of type n1-standard-4 (see\n",
            "https://homl.info/machinetypes  for  other  types).  We  also  use  a  basic  GPU  of  type\n",
            "NVIDIA_TESLA_K80 (see https://homl.info/accelerators for other types). If you selected\n",
            "another region than \"us-central1\", then you may need to change the machine type\n",
            "or the accelerator type to values that are supported in that region (e.g., not all regions\n",
            "have Nvidia Tesla K80 GPUs).\n",
            "\n",
            "Google Cloud Platform enforces various GPU quotas, both world‐\n",
            "wide  and  per  region:  you  cannot  create  thousands  of  GPU  nodes\n",
            "without  prior  authorization  from  Google.  To  check  your  quotas,\n",
            "open  “IAM  and  admin  →  Quotas”  in  the  GCP  console.  If  some\n",
            "quotas  are  too  low  (e.g.,  if  you  need  more  GPUs  in  a  particular\n",
            "region), you can ask for them to be increased; it often takes about\n",
            "48 hours.\n",
            "\n",
            "Vertex  AI  will  initially  spawn  the  minimum  number  of  compute  nodes  (just  one  in\n",
            "this case), and whenever the number of queries per second becomes too high, it will\n",
            "spawn more nodes (up to the maximum number you defined, five in this case) and\n",
            "will  load-balance  the  queries  between  them.  If  the  QPS  rate  goes  down  for  a  while,\n",
            "Vertex  AI  will  stop  the  extra  compute  nodes  automatically.  The  cost  is  therefore\n",
            "directly linked to the load, as well as the machine and accelerator types you selected\n",
            "and the amount of data you store on GCS. This pricing model is great for occasional\n",
            "users and for services with important usage spikes. It’s also ideal for startups: the price\n",
            "remains low until the startup actually starts up.\n",
            "\n",
            "738 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fCongratulations,  you  have  deployed  your  first  model  to  the  cloud!  Now  let’s  query\n",
            "this prediction service:\n",
            "\n",
            "response = endpoint.predict(instances=X_new.tolist())\n",
            "\n",
            "We  first  need  to  convert  the  images  we  want  to  classify  to  a  Python  list,  as  we  did\n",
            "earlier when we sent requests to TF Serving using the REST API. The response object\n",
            "contains  the  predictions,  represented  as  a  Python  list  of  lists  of  floats.  Let’s  round\n",
            "them to two decimal places and convert them to a NumPy array:\n",
            "\n",
            ">>> import numpy as np\n",
            ">>> np.round(response.predictions, 2)\n",
            "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
            "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
            "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\n",
            "\n",
            "Yes!  We  get  the  exact  same  predictions  as  earlier.  We  now  have  a  nice  prediction\n",
            "service running on the cloud that we can query from anywhere securely, and which\n",
            "can automatically scale up or down depending on the number of QPS. When you are\n",
            "done using the endpoint, don’t forget to delete it, to avoid paying for nothing:\n",
            "\n",
            "endpoint.undeploy_all()  # undeploy all models from the endpoint\n",
            "endpoint.delete()\n",
            "\n",
            "Now let’s see how to run a job on Vertex AI to make predictions on a potentially very\n",
            "large batch of data.\n",
            "\n",
            "Running Batch Prediction Jobs on Vertex AI\n",
            "If we have a large number of predictions to make, then instead of calling our predic‐\n",
            "tion  service  repeatedly,  we  can  ask  Vertex  AI  to  run  a  prediction  job  for  us.  This\n",
            "does  not  require  an  endpoint,  only  a  model.  For  example,  let’s  run  a  prediction  job\n",
            "on  the  first  100  images  of  the  test  set,  using  our  MNIST  model.  For  this,  we  first\n",
            "need  to  prepare  the  batch  and  upload  it  to  GCS.  One  way  to  do  this  is  to  create  a\n",
            "file containing one instance per line, each formatted as a JSON value—this format is\n",
            "called JSON Lines—then pass this file to Vertex AI. So let’s create a JSON Lines file in\n",
            "a new directory, then upload this directory to GCS:\n",
            "\n",
            "batch_path = Path(\"my_mnist_batch\")\n",
            "batch_path.mkdir(exist_ok=True)\n",
            "with open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\n",
            "    for image in X_test[:100].tolist():\n",
            "        jsonl_file.write(json.dumps(image))\n",
            "        jsonl_file.write(\"\\n\")\n",
            "\n",
            "upload_directory(bucket, batch_path)\n",
            "\n",
            "Serving a TensorFlow Model \n",
            "\n",
            "| \n",
            "\n",
            "739\n",
            "\n",
            "\fNow we’re ready to launch the prediction job, specifying the job’s name, the type and\n",
            "number of machines and accelerators to use, the GCS path to the JSON Lines file we\n",
            "just created, and the path to the GCS directory where Vertex AI will save the model’s\n",
            "predictions:\n",
            "\n",
            "batch_prediction_job = mnist_model.batch_predict(\n",
            "    job_display_name=\"my_batch_prediction_job\",\n",
            "    machine_type=\"n1-standard-4\",\n",
            "    starting_replica_count=1,\n",
            "    max_replica_count=5,\n",
            "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
            "    accelerator_count=1,\n",
            "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
            "    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
            "    sync=True  # set to False if you don't want to wait for completion\n",
            ")\n",
            "\n",
            "For large batches, you can split the inputs into multiple JSON Lines\n",
            "files and list them all via the gcs_source argument.\n",
            "\n",
            "This  will  take  a  few  minutes,  mostly  to  spawn  the  compute  nodes  on  Vertex  AI.\n",
            "Once  this  command  completes,  the  predictions  will  be  available  in  a  set  of  files\n",
            "named  something  like  prediction.results-00001-of-00002.  These  files  use  the  JSON\n",
            "Lines format by default, and each value is a dictionary containing an instance and its\n",
            "corresponding prediction (i.e., 10 probabilities). The instances are listed in the same\n",
            "order as the inputs. The job also outputs prediction-errors* files, which can be useful\n",
            "for debugging if something goes wrong. We can iterate through all these output files\n",
            "using batch_prediction_job.iter_outputs(), so let’s go through all the predictions\n",
            "and store them in a y_probas array:\n",
            "\n",
            "y_probas = []\n",
            "for blob in batch_prediction_job.iter_outputs():\n",
            "    if \"prediction.results\" in blob.name:\n",
            "        for line in blob.download_as_text().splitlines():\n",
            "            y_proba = json.loads(line)[\"prediction\"]\n",
            "            y_probas.append(y_proba)\n",
            "\n",
            "Now let’s see how good these predictions are:\n",
            "\n",
            ">>> y_pred = np.argmax(y_probas, axis=1)\n",
            ">>> accuracy = np.sum(y_pred == y_test[:100]) / 100\n",
            "0.98\n",
            "\n",
            "Nice, 98% accuracy!\n",
            "\n",
            "740 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fThe  JSON  Lines  format  is  the  default,  but  when  dealing  with  large  instances\n",
            "such  as  images,  it  is  too  verbose.  Luckily,  the  batch_predict()  method  accepts\n",
            "an  instances_format  argument  that  lets  you  choose  another  format  if  you  want.\n",
            "It  defaults  to  \"jsonl\",  but  you  can  change  it  to  \"csv\",  \"tf-record\",  \"tf-record-\n",
            "gzip\",  \"bigquery\",  or  \"file-list\".  If  you  set  it  to  \"file-list\",  then  the\n",
            "gcs_source  argument  should  point  to  a  text  file  containing  one  input  filepath  per\n",
            "line;  for  instance,  pointing  to  PNG  image  files.  Vertex  AI  will  read  these  files\n",
            "as  binary,  encode  them  using  Base64,  and  pass  the  resulting  byte  strings  to  the\n",
            "model.  This  means  that  you  must  add  a  preprocessing  layer  in  your  model  to\n",
            "parse  the  Base64  strings,  using  tf.io.decode_base64().  If  the  files  are  images,\n",
            "you  must  then  parse  the  result  using  a  function  like  tf.io.decode_image()  or\n",
            "tf.io.decode_png(), as discussed in Chapter 13.\n",
            "\n",
            "When  you’re  finished  using  the  model,  you  can  delete  it  if  you  want,  by  running\n",
            "mnist_model.delete(). You can also delete the directories you created in your GCS\n",
            "bucket, optionally the bucket itself (if it’s empty), and the batch prediction job:\n",
            "\n",
            "for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n",
            "    blobs = bucket.list_blobs(prefix=prefix)\n",
            "    for blob in blobs:\n",
            "        blob.delete()\n",
            "\n",
            "bucket.delete()  # if the bucket is empty\n",
            "batch_prediction_job.delete()\n",
            "\n",
            "You now know how to deploy a model to Vertex AI, create a prediction service, and\n",
            "run batch prediction jobs. But what if you want to deploy your model to a mobile app\n",
            "instead? Or to an embedded device, such as a heating control system, a fitness tracker,\n",
            "or a self-driving car?\n",
            "\n",
            "Deploying a Model to a Mobile or Embedded Device\n",
            "Machine learning models are not limited to running on big centralized servers with\n",
            "multiple GPUs: they can run closer to the source of data (this is called edge comput‐\n",
            "ing),  for  example  in  the  user’s  mobile  device  or  in  an  embedded  device.  There  are\n",
            "many benefits to decentralizing the computations and moving them toward the edge:\n",
            "it allows the device to be smart even when it’s not connected to the internet, it reduces\n",
            "latency  by  not  having  to  send  data  to  a  remote  server  and  reduces  the  load  on  the\n",
            "servers, and it may improve privacy, since the user’s data can stay on the device.\n",
            "\n",
            "However, deploying models to the edge has its downsides too. The device’s computing\n",
            "resources  are  generally  tiny  compared  to  a  beefy  multi-GPU  server.  A  large  model\n",
            "may not fit in the device, it may use too much RAM and CPU, and it may take too\n",
            "long  to  download.  As  a  result,  the  application  may  become  unresponsive,  and  the\n",
            "device may heat up and quickly run out of battery. To avoid all this, you need to make\n",
            "\n",
            "Deploying a Model to a Mobile or Embedded Device \n",
            "\n",
            "| \n",
            "\n",
            "741\n",
            "\n",
            "\fa  lightweight  and  efficient  model,  without  sacrificing  too  much  of  its  accuracy.  The\n",
            "TFLite  library  provides  several  tools7  to  help  you  deploy  your  models  to  the  edge,\n",
            "with three main objectives:\n",
            "\n",
            "•\n",
            "• Reduce the model size, to shorten download time and reduce RAM usage.\n",
            "\n",
            "•\n",
            "• Reduce  the  amount  of  computations  needed  for  each  prediction,  to  reduce\n",
            "\n",
            "latency, battery usage, and heating.\n",
            "\n",
            "•\n",
            "• Adapt the model to device-specific constraints.\n",
            "\n",
            "To  reduce  the  model  size,  TFLite’s  model  converter  can  take  a  SavedModel  and\n",
            "compress it to a much lighter format based on FlatBuffers. This is an efficient cross-\n",
            "platform serialization library (a bit like protocol buffers) initially created by Google\n",
            "for gaming. It is designed so you can load FlatBuffers straight to RAM without any\n",
            "preprocessing: this reduces the loading time and memory footprint. Once the model\n",
            "is loaded into a mobile or embedded device, the TFLite interpreter will execute it to\n",
            "make predictions. Here is how you can convert a SavedModel to a FlatBuffer and save\n",
            "it to a .tflite file:\n",
            "\n",
            "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
            "tflite_model = converter.convert()\n",
            "with open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n",
            "    f.write(tflite_model)\n",
            "\n",
            "You  can  also  save  a  Keras  model  directly  to  a  FlatBuffer  using\n",
            "tf.lite.TFLiteConverter.from_keras_model(model).\n",
            "\n",
            "The converter also optimizes the model, both to shrink it and to reduce its latency. It\n",
            "prunes  all  the  operations  that  are  not  needed  to  make  predictions  (such  as  training\n",
            "operations), and it optimizes computations whenever possible; for example, 3 × a +\n",
            "4  ×_  a_  +  5  ×  a  will  be  converted  to  12  ×  a.  Addtionally,  it  tries  to  fuse  operations\n",
            "whenever possible. For example, if possible, batch normalization layers end up folded\n",
            "into  the  previous  layer’s  addition  and  multiplication  operations.  To  get  a  good  idea\n",
            "of how much TFLite can optimize a model, download one of the pretrained TFLite\n",
            "models,  such  as  Inception_V1_quant  (click  tflite&pb),  unzip  the  archive,  then  open\n",
            "the  excellent  Netron  graph  visualization  tool  and  upload  the  .pb  file  to  view  the\n",
            "original model. It’s a big, complex graph, right? Next, open the optimized .tflite model\n",
            "and marvel at its beauty!\n",
            "\n",
            "7 Also check out TensorFlow’s Graph Transform Tool for modifying and optimizing computational graphs.\n",
            "\n",
            "742 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fAnother way you can reduce the model size—other than simply using smaller neural\n",
            "network  architectures—is  by  using  smaller  bit-widths:  for  example,  if  you  use  half-\n",
            "floats  (16  bits)  rather  than  regular  floats  (32  bits),  the  model  size  will  shrink  by  a\n",
            "factor of 2, at the cost of a (generally small) accuracy drop. Moreover, training will be\n",
            "faster, and you will use roughly half the amount of GPU RAM.\n",
            "\n",
            "TFLite’s  converter  can  go  further  than  that,  by  quantizing  the  model  weights  down\n",
            "to  fixed-point,  8-bit  integers!  This  leads  to  a  fourfold  size  reduction  compared  to\n",
            "using 32-bit floats. The simplest approach is called post-training quantization: it just\n",
            "quantizes  the  weights  after  training,  using  a  fairly  basic  but  efficient  symmetrical\n",
            "quantization technique. It finds the maximum absolute weight value, m, then it maps\n",
            "the  floating-point  range  –m  to  +m  to  the  fixed-point  (integer)  range  –127  to  +127.\n",
            "For example, if the weights range from –1.5 to +0.8, then the bytes –127, 0, and +127\n",
            "will correspond to the floats –1.5, 0.0, and +1.5, respectively (see Figure 19-5). Note\n",
            "that  0.0  always  maps  to  0  when  using  symmetrical  quantization.  Also  note  that  the\n",
            "byte  values  +68  to  +127  will  not  be  used  in  this  example,  since  they  map  to  floats\n",
            "greater than +0.8.\n",
            "\n",
            "Figure 19-5. From 32-bit floats to 8-bit integers, using symmetrical quantization\n",
            "\n",
            "To  perform  this  post-training  quantization,  simply  add  DEFAULT  to  the  list  of  con‐\n",
            "verter optimizations before calling the convert() method:\n",
            "\n",
            "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
            "\n",
            "This  technique  dramatically  reduces  the  model’s  size,  which  makes  it  much  faster\n",
            "to  download,  and  uses  less  storage  space.  At  runtime  the  quantized  weights  get\n",
            "converted back to floats before they are used. These recovered floats are not perfectly\n",
            "identical  to  the  original  floats,  but  they’re  not  too  far  off,  so  the  accuracy  loss  is\n",
            "usually  acceptable.  To  avoid  recomputing  the  float  values  all  the  time,  which  would\n",
            "severely  slow  down  the  model,  TFLite  caches  them:  unfortunately,  this  means  that\n",
            "this technique does not reduce RAM usage, and it doesn’t speed up the model either.\n",
            "It’s mostly useful to reduce the application’s size.\n",
            "\n",
            "Deploying a Model to a Mobile or Embedded Device \n",
            "\n",
            "| \n",
            "\n",
            "743\n",
            "\n",
            "\fThe most effective way to reduce latency and power consumption is to also quantize\n",
            "the activations so that the computations can be done entirely with integers, without\n",
            "the need for any floating-point operations. Even when using the same bit-width (e.g.,\n",
            "32-bit  integers  instead  of  32-bit  floats),  integer  computations  use  less  CPU  cycles,\n",
            "consume  less  energy,  and  produce  less  heat.  And  if  you  also  reduce  the  bit-width\n",
            "(e.g.,  down  to  8-bit  integers),  you  can  get  huge  speedups.  Moreover,  some  neural\n",
            "network accelerator devices—such as Google’s Edge TPU—can only process integers,\n",
            "so full quantization of both weights and activations is compulsory. This can be done\n",
            "post-training; it requires a calibration step to find the maximum absolute value of the\n",
            "activations, so you need to provide a representative sample of training data to TFLite\n",
            "(it  does  not  need  to  be  huge),  and  it  will  process  the  data  through  the  model  and\n",
            "measure the activation statistics required for quantization. This step is typically fast.\n",
            "\n",
            "The main problem with quantization is that it loses a bit of accuracy: it is similar to\n",
            "adding noise to the weights and activations. If the accuracy drop is too severe, then\n",
            "you may need to use quantization-aware training. This means adding fake quantiza‐\n",
            "tion operations to the model so it can learn to ignore the quantization noise during\n",
            "training;  the  final  weights  will  then  be  more  robust  to  quantization.  Moreover,  the\n",
            "calibration step can be taken care of automatically during training, which simplifies\n",
            "the whole process.\n",
            "\n",
            "I  have  explained  the  core  concepts  of  TFLite,  but  going  all  the  way  to  coding  a\n",
            "mobile  or  embedded  application  woud  require  a  dedicated  book.  Fortunately,  some\n",
            "exist:  if  you  want  to  learn  more  about  building  TensorFlow  applications  for  mobile\n",
            "and  embedded  devices,  check  out  the  O’Reilly  books  TinyML:  Machine  Learning\n",
            "with TensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden\n",
            "(former lead of the TFLite team) and Daniel Situnayake and AI and Machine Learning\n",
            "for On-Device Development, by Laurence Moroney.\n",
            "\n",
            "Now what if you want to use your model in a website, running directly in the user’s\n",
            "browser?\n",
            "\n",
            "Running a Model in a Web Page\n",
            "Running your machine learning model on the client side, in the user’s browser, rather\n",
            "than on the server side can be useful in many scenarios, such as:\n",
            "\n",
            "• When your web application is often used in situations where the user’s connec‐\n",
            "•\n",
            "tivity  is  intermittent  or  slow  (e.g.,  a  website  for  hikers),  so  running  the  model\n",
            "directly on the client side is the only way to make your website reliable.\n",
            "\n",
            "•\n",
            "• When you need the model’s responses to be as fast as possible (e.g., for an online\n",
            "game). Removing the need to query the server to make predictions will definitely\n",
            "reduce the latency and make the website much more responsive.\n",
            "\n",
            "744 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f• When your web service makes predictions based on some private user data, and\n",
            "•\n",
            "you  want  to  protect  the  user’s  privacy  by  making  the  predictions  on  the  client\n",
            "side so that the private data never has to leave the user’s machine.\n",
            "\n",
            "For all these scenarios, you can use the TensorFlow.js (TFJS) JavaScript library. This\n",
            "library can load a TFLite model and make predictions directly in the user’s browser.\n",
            "For  example,  the  following  JavaScript  module  imports  the  TFJS  library,  downloads\n",
            "a pretrained MobileNet model, and uses this model to classify an image and log the\n",
            "predictions. You can play with the code at https://homl.info/tfjscode, using Glitch.com,\n",
            "a website that lets you build web apps in your browser for free; click the PREVIEW\n",
            "button in the lower-right corner of the page to see the code in action:\n",
            "\n",
            "import \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\n",
            "import \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\n",
            "\n",
            "const image = document.getElementById(\"image\");\n",
            "\n",
            "mobilenet.load().then(model => {\n",
            "    model.classify(image).then(predictions => {\n",
            "        for (var i = 0; i < predictions.length; i++) {\n",
            "            let className = predictions[i].className\n",
            "            let proba = (predictions[i].probability * 100).toFixed(1)\n",
            "            console.log(className + \" : \" + proba + \"%\");\n",
            "        }\n",
            "    });\n",
            "});\n",
            "\n",
            "It’s  even  possible  to  turn  this  website  into  a  progressive  web  app  (PWA):  this  is  a\n",
            "website that respects a number of criteria8 that allow it to be viewed in any browser,\n",
            "and even installed as a standalone app on a mobile device. For example, try visiting\n",
            "https://homl.info/tfjswpa  on  a  mobile  device:  most  modern  browsers  will  ask  you\n",
            "whether you would like to add TFJS Demo to your home screen. If you accept, you\n",
            "will see a new icon in your list of applications. Clicking this icon will load the TFJS\n",
            "Demo website inside its own window, just like a regular mobile app. A PWA can even\n",
            "be configured to work offline, by using a service worker: this is a JavaScript module\n",
            "that runs in its own separate thread in the browser and intercepts network requests,\n",
            "allowing it to cache resources so the PWA can run faster, or even entirely offline. It\n",
            "can also deliver push messages, run tasks in the background, and more. PWAs allow\n",
            "you to manage a single code base for the web and for mobile devices. They also make\n",
            "it easier to ensure that all users run the same version of your application. You can play\n",
            "with this TFJS Demo’s PWA code on Glitch.com at https://homl.info/wpacode.\n",
            "\n",
            "8 For example, a PWA must include icons of various sizes for different mobile devices, it must be served via\n",
            "\n",
            "HTTPS, it must include a manifest file containing metadata such as the name of the app and the background\n",
            "color.\n",
            "\n",
            "Running a Model in a Web Page \n",
            "\n",
            "| \n",
            "\n",
            "745\n",
            "\n",
            "\fCheck out many more demos of machine learning models running\n",
            "in your browser at https://tensorflow.org/js/demos.\n",
            "\n",
            "TFJS  also  supports  training  a  model  directly  in  your  web  browser!  And  it’s  actually\n",
            "pretty  fast.  If  your  computer  has  a  GPU  card,  then  TFJS  can  generally  use  it,  even\n",
            "if it’s not an Nvidia card. Indeed, TFJS will use WebGL when it’s available, and since\n",
            "modern  web  browsers  generally  support  a  wide  range  of  GPU  cards,  TFJS  actually\n",
            "supports  more  GPU  cards  than  regular  TensorFlow  (which  only  supports  Nvidia\n",
            "cards).\n",
            "\n",
            "Training  a  model  in  a  user’s  web  browser  can  be  especially  useful  to  guarantee  that\n",
            "this user’s data remains private. A model can be trained centrally, and then fine-tuned\n",
            "locally,  in  the  browser,  based  on  that  user’s  data.  If  you’re  interested  in  this  topic,\n",
            "check out federated learning.\n",
            "\n",
            "Once  again,  doing  justice  to  this  topic  would  require  a  whole  book.  If  you  want  to\n",
            "learn more about TensorFlow.js, check out the O’reilly books Practical Deep Learning\n",
            "for  Cloud,  Mobile,  and  Edge,  by  Anirudh  Koul  et  al.,  or  Learning  TensorFlow.js,  by\n",
            "Gant Laborde.\n",
            "\n",
            "Now  that  you’ve  seen  how  to  deploy  TensorFlow  models  to  TF  Serving,  or  to  the\n",
            "cloud with Vertex AI, or to mobile and embedded devices using TFLite, or to a web\n",
            "browser using TFJS, let’s discuss how to use GPUs to speed up computations.\n",
            "\n",
            "Using GPUs to Speed Up Computations\n",
            "In Chapter 11 we looked at several techniques that can considerably speed up train‐\n",
            "ing: better weight initialization, sophisticated optimizers, and so on. But even with all\n",
            "of these techniques, training a large neural network on a single machine with a single\n",
            "CPU can take hours, days, or even weeks, depending on the task. Thanks to GPUs,\n",
            "this training time can be reduced down to minutes or hours. Not only does this save\n",
            "an enormous amount of time, but it also means that you can experiment with various\n",
            "models much more easily, and frequently retrain your models on fresh data.\n",
            "\n",
            "In the previous chapters, we used GPU-enabled runtimes on Google Colab. All you\n",
            "have  to  do  for  this  is  select  “Change  runtime  type”  from  the  Runtime  menu,  and\n",
            "choose the GPU accelerator type; TensorFlow automatically detects the GPU and uses\n",
            "it  to  speed  up  computations,  and  the  code  is  exactly  the  same  as  without  a  GPU.\n",
            "Then, in this chapter you saw how to deploy your models to Vertex AI on multiple\n",
            "GPU-enabled  compute  nodes:  it’s  just  a  matter  of  selecting  the  right  GPU-enabled\n",
            "Docker  image  when  creating  the  Vertex  AI  model,  and  selecting  the  desired  GPU\n",
            "type when calling endpoint.deploy(). But what if you want to buy your own GPU?\n",
            "\n",
            "746 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fAnd  what  if  you  want  to  distribute  the  computations  across  the  CPU  and  multiple\n",
            "GPU  devices  on  a  single  machine  (see  Figure  19-6)?  This  is  what  we  will  discuss\n",
            "now, then later in this chapter we will discuss how to distribute computations across\n",
            "multiple servers.\n",
            "\n",
            "Figure 19-6. Executing a TensorFlow graph across multiple devices in parallel\n",
            "\n",
            "Getting Your Own GPU\n",
            "If you know that you’ll be using a GPU heavily and for a long period of time, then\n",
            "buying your own can make financial sense. You may also want to train your models\n",
            "locally because you do not want to upload your data to the cloud. Or perhaps you just\n",
            "want to buy a GPU card for gaming, and you’d like to use it for deep learning as well.\n",
            "\n",
            "If you decide to purchase a GPU card, then take some time to make the right choice.\n",
            "You  will  need  to  consider  the  amount  of  RAM  you  will  need  for  your  tasks  (e.g.,\n",
            "typically at least 10 GB for image processing or NLP), the bandwidth (i.e., how fast\n",
            "you can send data into and out of the GPU), the number of cores, the cooling system,\n",
            "etc. Tim Dettmers wrote an excellent blog post to help you choose: I encourage you\n",
            "to read it carefully. At the time of this writing, TensorFlow only supports Nvidia cards\n",
            "with  CUDA  Compute  Capability  3.5+  (as  well  as  Google’s  TPUs,  of  course),  but  it\n",
            "may extend its support to other manufacturers, so make sure to check TensorFlow’s\n",
            "documentation to see what devices are supported today.\n",
            "\n",
            "Using GPUs to Speed Up Computations \n",
            "\n",
            "| \n",
            "\n",
            "747\n",
            "\n",
            "\fIf  you  go  for  an  Nvidia  GPU  card,  you  will  need  to  install  the  appropriate  Nvidia\n",
            "drivers  and  several  Nvidia  libraries.9  These  include  the  Compute  Unified  Device\n",
            "Architecture library (CUDA) Toolkit, which allows developers to use CUDA-enabled\n",
            "GPUs  for  all  sorts  of  computations  (not  just  graphics  acceleration),  and  the  CUDA\n",
            "Deep Neural Network library (cuDNN), a GPU-accelerated library of common DNN\n",
            "computations such as activation layers, normalization, forward and backward convo‐\n",
            "lutions, and pooling (see Chapter 14). cuDNN is part of Nvidia’s Deep Learning SDK.\n",
            "Note that you will need to create an Nvidia developer account in order to download\n",
            "it.  TensorFlow  uses  CUDA  and  cuDNN  to  control  the  GPU  cards  and  accelerate\n",
            "computations (see Figure 19-7).\n",
            "\n",
            "Figure 19-7. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs\n",
            "\n",
            "Once  you  have  installed  the  GPU  card(s)  and  all  the  required  drivers  and  libraries,\n",
            "you can use the nvidia-smi command to check that everything is properly installed.\n",
            "This command lists the available GPU cards, as well as all the processes running on\n",
            "each  card.  In  this  example,  it’s  an  Nvidia  Tesla  T4  GPU  card  with  about  15  GB  of\n",
            "available RAM, and there are no processes currently running on it:\n",
            "\n",
            "9 Please check the TensorFlow docs for detailed and up-to-date installation instructions, as they change quite\n",
            "\n",
            "often.\n",
            "\n",
            "748 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f$ nvidia-smi\n",
            "Sun Apr 10 04:52:10 2022\n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "To check that TensorFlow actually sees your GPU, run the following commands and\n",
            "make sure the result is not empty:\n",
            "\n",
            ">>> physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
            ">>> physical_gpus\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "Managing the GPU RAM\n",
            "By default TensorFlow automatically grabs almost all the RAM in all available GPUs\n",
            "the first time you run a computation. It does this to limit GPU RAM fragmentation.\n",
            "This  means  that  if  you  try  to  start  a  second  TensorFlow  program  (or  any  program\n",
            "that requires the GPU), it will quickly run out of RAM. This does not happen as often\n",
            "as you might think, as you will most often have a single TensorFlow program running\n",
            "on  a  machine:  usually  a  training  script,  a  TF  Serving  node,  or  a  Jupyter  notebook.\n",
            "If  you  need  to  run  multiple  programs  for  some  reason  (e.g.,  to  train  two  different\n",
            "models in parallel on the same machine), then you will need to split the GPU RAM\n",
            "between these processes more evenly.\n",
            "\n",
            "Using GPUs to Speed Up Computations \n",
            "\n",
            "| \n",
            "\n",
            "749\n",
            "\n",
            "\fIf  you  have  multiple  GPU  cards  on  your  machine,  a  simple  solution  is  to  assign\n",
            "each of them to a single process. To do this, you can set the CUDA_VISIBLE_DEVICES\n",
            "environment  variable  so  that  each  process  only  sees  the  appropriate  GPU  card(s).\n",
            "Also set the CUDA_DEVICE_ORDER environment variable to PCI_BUS_ID to ensure that\n",
            "each  ID  always  refers  to  the  same  GPU  card.  For  example,  if  you  have  four  GPU\n",
            "cards, you could start two programs, assigning two GPUs to each of them, by execut‐\n",
            "ing commands like the following in two separate terminal windows:\n",
            "\n",
            "$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n",
            "# and in another terminal:\n",
            "$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\n",
            "\n",
            "Program  1  will  then  only  see  GPU  cards  0  and  1,  named  \"/gpu:0\"  and  \"/gpu:1\",\n",
            "respectively,  in  TensorFlow,  and  program  2  will  only  see  GPU  cards  2  and  3,\n",
            "named  \"/gpu:1\"  and  \"/gpu:0\",  respectively  (note  the  order).  Everything  will  work\n",
            "fine  (see  Figure  19-8).  Of  course,  you  can  also  define  these  environment  variables\n",
            "in  Python  by  setting  os.environ[\"CUDA_DEVICE_ORDER\"]  and  os.environ[\"CUDA_\n",
            "VISIBLE_DEVICES\"], as long as you do so before using TensorFlow.\n",
            "\n",
            "Figure 19-8. Each program gets two GPUs\n",
            "\n",
            "Another  option  is  to  tell  TensorFlow  to  grab  only  a  specific  amount  of  GPU  RAM.\n",
            "This  must  be  done  immediately  after  importing  TensorFlow.  For  example,  to  make\n",
            "TensorFlow  grab  only  2  GiB  of  RAM  on  each  GPU,  you  must  create  a  logical  GPU\n",
            "device (sometimes called a virtual GPU device) for each physical GPU device and set\n",
            "its memory limit to 2 GiB (i.e., 2,048 MiB):\n",
            "\n",
            "for gpu in physical_gpus:\n",
            "    tf.config.set_logical_device_configuration(\n",
            "        gpu,\n",
            "        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
            "    )\n",
            "\n",
            "Let’s  suppose  you  have  four  GPUs,  each  with  at  least  4  GiB  of  RAM:  in  this  case,\n",
            "two  programs  like  this  one  can  run  in  parallel,  each  using  all  four  GPU  cards  (see\n",
            "Figure 19-9). If you run the nvidia-smi command while both programs are running,\n",
            "you should see that each process holds 2 GiB of RAM on each card.\n",
            "\n",
            "750 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fFigure 19-9. Each program gets all four GPUs, but with only 2 GiB of RAM on each\n",
            "GPU\n",
            "\n",
            "Yet another option is to tell TensorFlow to grab memory only when it needs it. Again,\n",
            "this must be done immediately after importing TensorFlow:\n",
            "\n",
            "for gpu in physical_gpus:\n",
            "    tf.config.experimental.set_memory_growth(gpu, True)\n",
            "\n",
            "Another  way  to  do  this  is  to  set  the  TF_FORCE_GPU_ALLOW_GROWTH  environment\n",
            "variable  to  true.  With  this  option,  TensorFlow  will  never  release  memory  once\n",
            "it  has  grabbed  it  (again,  to  avoid  memory  fragmentation),  except  of  course  when\n",
            "the  program  ends.  It  can  be  harder  to  guarantee  deterministic  behavior  using  this\n",
            "option (e.g., one program may crash because another program’s memory usage went\n",
            "through  the  roof),  so  in  production  you’ll  probably  want  to  stick  with  one  of  the\n",
            "previous options. However, there are some cases where it is very useful: for example,\n",
            "when  you  use  a  machine  to  run  multiple  Jupyter  notebooks,  several  of  which  use\n",
            "TensorFlow. The TF_FORCE_GPU_ALLOW_GROWTH environment variable is set to true in\n",
            "Colab runtimes.\n",
            "\n",
            "Lastly, in some cases you may want to split a GPU into two or more logical devices.\n",
            "For  example,  this  is  useful  if  you  only  have  one  physical  GPU—like  in  a  Colab\n",
            "runtime—but  you  want  to  test  a  multi-GPU  algorithm.  The  following  code  splits\n",
            "GPU #0 into two logical devices, with 2 GiB of RAM each (again, this must be done\n",
            "immediately after importing TensorFlow):\n",
            "\n",
            "tf.config.set_logical_device_configuration(\n",
            "    physical_gpus[0],\n",
            "    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
            "     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
            ")\n",
            "\n",
            "These two logical devices are called \"/gpu:0\" and \"/gpu:1\", and you can use them as\n",
            "if they were two normal GPUs. You can list all logical devices like this:\n",
            "\n",
            "Using GPUs to Speed Up Computations \n",
            "\n",
            "| \n",
            "\n",
            "751\n",
            "\n",
            "\f>>> logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
            ">>> logical_gpus\n",
            "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
            " LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n",
            "\n",
            "Now let’s see how TensorFlow decides which devices it should use to place variables\n",
            "and execute operations.\n",
            "\n",
            "Placing Operations and Variables on Devices\n",
            "Keras and tf.data generally do a good job of placing operations and variables where\n",
            "they belong, but you can also place operations and variables manually on each device,\n",
            "if you want more control:\n",
            "\n",
            "•\n",
            "• You generally want to place the data preprocessing operations on the CPU, and\n",
            "\n",
            "place the neural network operations on the GPUs.\n",
            "\n",
            "•\n",
            "• GPUs usually have a fairly limited communication bandwidth, so it is important\n",
            "\n",
            "to avoid unnecessary data transfers into and out of the GPUs.\n",
            "\n",
            "• Adding  more  CPU  RAM  to  a  machine  is  simple  and  fairly  cheap,  so  there’s\n",
            "•\n",
            "usually  plenty  of  it,  whereas  the  GPU  RAM  is  baked  into  the  GPU:  it  is  an\n",
            "expensive and thus limited resource, so if a variable is not needed in the next few\n",
            "training steps, it should probably be placed on the CPU (e.g., datasets generally\n",
            "belong on the CPU).\n",
            "\n",
            "By  default,  all  variables  and  all  operations  will  be  placed  on  the  first  GPU  (the  one\n",
            "named \"/gpu:0\"), except for variables and operations that don’t have a GPU kernel:10\n",
            "these are placed on the CPU (always named \"/cpu:0\"). A tensor or variable’s device\n",
            "attribute tells you which device it was placed on:11\n",
            "\n",
            ">>> a = tf.Variable([1., 2., 3.])  # float32 variable goes to the GPU\n",
            ">>> a.device\n",
            "'/job:localhost/replica:0/task:0/device:GPU:0'\n",
            ">>> b = tf.Variable([1, 2, 3])  # int32 variable goes to the CPU\n",
            ">>> b.device\n",
            "'/job:localhost/replica:0/task:0/device:CPU:0'\n",
            "\n",
            "You can safely ignore the prefix /job:localhost/replica:0/task:0 for now; we will\n",
            "discuss jobs, replicas, and tasks later in this chapter. As you can see, the first variable\n",
            "was placed on GPU #0, which is the default device. However, the second variable was\n",
            "\n",
            "10 As we saw in Chapter 12, a kernel is an operation’s implementation for a specific data type and device type.\n",
            "For example, there is a GPU kernel for the float32 tf.matmul() operation, but there is no GPU kernel for\n",
            "int32 tf.matmul(), only a CPU kernel.\n",
            "\n",
            "11 You can also use tf.debugging.set_log_device_placement(True) to log all device placements.\n",
            "\n",
            "752 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fplaced on the CPU: this is because there are no GPU kernels for integer variables, or\n",
            "for operations involving integer tensors, so TensorFlow fell back to the CPU.\n",
            "\n",
            "If  you  want  to  place  an  operation  on  a  different  device  than  the  default  one,  use  a\n",
            "tf.device() context:\n",
            "\n",
            ">>> with tf.device(\"/cpu:0\"):\n",
            "...     c = tf.Variable([1., 2., 3.])\n",
            "...\n",
            ">>> c.device\n",
            "'/job:localhost/replica:0/task:0/device:CPU:0'\n",
            "\n",
            "The  CPU  is  always  treated  as  a  single  device  (\"/cpu:0\"),  even\n",
            "if  your  machine  has  multiple  CPU  cores.  Any  operation  placed\n",
            "on  the  CPU  may  run  in  parallel  across  multiple  cores  if  it  has  a\n",
            "multithreaded kernel.\n",
            "\n",
            "If you explicitly try to place an operation or variable on a device that does not exist\n",
            "or for which there is no kernel, then TensorFlow will silently fall back to the device\n",
            "it would have chosen by default. This is useful when you want to be able to run the\n",
            "same code on different machines that don’t have the same number of GPUs. However,\n",
            "you can run tf.config.set_soft_device_placement(False) if you prefer to get an\n",
            "exception.\n",
            "\n",
            "Now, how exactly does TensorFlow execute operations across multiple devices?\n",
            "\n",
            "Parallel Execution Across Multiple Devices\n",
            "As  we  saw  in  Chapter  12,  one  of  the  benefits  of  using  TF  functions  is  parallelism.\n",
            "Let’s  look  at  this  a  bit  more  closely.  When  TensorFlow  runs  a  TF  function,  it  starts\n",
            "by analyzing its graph to find the list of operations that need to be evaluated, and it\n",
            "counts how many dependencies each of them has. TensorFlow then adds each opera‐\n",
            "tion with zero dependencies (i.e., each source operation) to the evaluation queue of\n",
            "this operation’s device (see Figure 19-10). Once an operation has been evaluated, the\n",
            "dependency  counter  of  each  operation  that  depends  on  it  is  decremented.  Once  an\n",
            "operation’s dependency counter reaches zero, it is pushed to the evaluation queue of\n",
            "its device. And once all the outputs have been computed, they are returned.\n",
            "\n",
            "Using GPUs to Speed Up Computations \n",
            "\n",
            "| \n",
            "\n",
            "753\n",
            "\n",
            "\fFigure 19-10. Parallelized execution of a TensorFlow graph\n",
            "\n",
            "Operations  in  the  CPU’s  evaluation  queue  are  dispatched  to  a  thread  pool  called\n",
            "the  inter-op  thread  pool.  If  the  CPU  has  multiple  cores,  then  these  operations  will\n",
            "effectively  be  evaluated  in  parallel.  Some  operations  have  multithreaded  CPU  ker‐\n",
            "nels:  these  kernels  split  their  tasks  into  multiple  suboperations,  which  are  placed  in\n",
            "another evaluation queue and dispatched to a second thread pool called the intra-op\n",
            "thread pool (shared by all multithreaded CPU kernels). In short, multiple operations\n",
            "and suboperations may be evaluated in parallel on different CPU cores.\n",
            "\n",
            "For  the  GPU,  things  are  a  bit  simpler.  Operations  in  a  GPU’s  evaluation  queue  are\n",
            "evaluated  sequentially.  However,  most  operations  have  multithreaded  GPU  kernels,\n",
            "typically implemented by libraries that TensorFlow depends on, such as CUDA and\n",
            "cuDNN.  These  implementations  have  their  own  thread  pools,  and  they  typically\n",
            "exploit  as  many  GPU  threads  as  they  can  (which  is  the  reason  why  there  is  no\n",
            "need for an inter-op thread pool in GPUs: each operation already floods most GPU\n",
            "threads).\n",
            "\n",
            "For  example,  in  Figure  19-10,  operations  A,  B,  and  C  are  source  ops,  so  they  can\n",
            "immediately  be  evaluated.  Operations  A  and  B  are  placed  on  the  CPU,  so  they  are\n",
            "sent  to  the  CPU’s  evaluation  queue,  then  they  are  dispatched  to  the  inter-op  thread\n",
            "pool  and  immediately  evaluated  in  parallel.  Operation  A  happens  to  have  a  multi‐\n",
            "threaded  kernel;  its  computations  are  split  into  three  parts,  which  are  executed  in\n",
            "parallel by the intra-op thread pool. Operation C goes to GPU #0’s evaluation queue,\n",
            "and in this example its GPU kernel happens to use cuDNN, which manages its own\n",
            "\n",
            "754 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fintra-op  thread  pool  and  runs  the  operation  across  many  GPU  threads  in  parallel.\n",
            "Suppose C finishes first. The dependency counters of D and E are decremented and\n",
            "they reach 0, so both operations are pushed to GPU #0’s evaluation queue, and they\n",
            "are  executed  sequentially.  Note  that  C  only  gets  evaluated  once,  even  though  both\n",
            "D  and  E  depend  on  it.  Suppose  B  finishes  next.  Then  F’s  dependency  counter  is\n",
            "decremented from 4 to 3, and since that’s not 0, it does not run yet. Once A, D, and\n",
            "E are finished, then F’s dependency counter reaches 0, and it is pushed to the CPU’s\n",
            "evaluation queue and evaluated. Finally, TensorFlow returns the requested outputs.\n",
            "\n",
            "An extra bit of magic that TensorFlow performs is when the TF function modifies a\n",
            "stateful resource, such as a variable: it ensures that the order of execution matches the\n",
            "order in the code, even if there is no explicit dependency between the statements. For\n",
            "example,  if  your  TF  function  contains  v.assign_add(1)  followed  by  v.assign(v  *\n",
            "2), TensorFlow will ensure that these operations are executed in that order.\n",
            "\n",
            "You  can  control  the  number  of  threads  in  the  inter-op  thread\n",
            "pool  by  calling  tf.config.threading.set_inter_op_parallel\n",
            "ism_threads().  To  set  the  number  of  intra-op  threads,  use\n",
            "tf.config.threading.set_intra_op_parallelism_threads().\n",
            "This  is  useful  if  you  do  not  want  TensorFlow  to  use  all  the  CPU\n",
            "cores or if you want it to be single-threaded.12\n",
            "\n",
            "With that, you have all you need to run any operation on any device, and exploit the\n",
            "power of your GPUs! Here are some of the things you could do:\n",
            "\n",
            "• You  could  train  several  models  in  parallel,  each  on  its  own  GPU:  just\n",
            "•\n",
            "write  a  training  script  for  each  model  and  run  them  in  parallel,  setting\n",
            "CUDA_DEVICE_ORDER  and  CUDA_VISIBLE_DEVICES  so  that  each  script  only  sees\n",
            "a  single  GPU  device.  This  is  great  for  hyperparameter  tuning,  as  you  can  train\n",
            "in parallel multiple models with different hyperparameters. If you have a single\n",
            "machine with two GPUs, and it takes one hour to train one model on one GPU,\n",
            "then  training  two  models  in  parallel,  each  on  its  own  dedicated  GPU,  will  take\n",
            "just one hour. Simple!\n",
            "\n",
            "• You  could  train  a  model  on  a  single  GPU  and  perform  all  the  preprocessing  in\n",
            "•\n",
            "parallel on the CPU, using the dataset’s prefetch() method13 to prepare the next\n",
            "few  batches  in  advance  so  that  they  are  ready  when  the  GPU  needs  them  (see\n",
            "Chapter 13).\n",
            "\n",
            "12 This can be useful if you want to guarantee perfect reproducibility, as I explain in this video, based on TF 1.\n",
            "13 At the time of writing, it only prefetches the data to the CPU RAM, but use tf.data.experimental.prefetch\n",
            "_to_device() to make it prefetch the data and push it to the device of your choice so that the GPU does not\n",
            "waste time waiting for the data to be transferred.\n",
            "\n",
            "Using GPUs to Speed Up Computations \n",
            "\n",
            "| \n",
            "\n",
            "755\n",
            "\n",
            "\f• If  your  model  takes  two  images  as  input  and  processes  them  using  two  CNNs\n",
            "•\n",
            "before joining their outputs,14 then it will probably run much faster if you place\n",
            "each CNN on a different GPU.\n",
            "\n",
            "•\n",
            "• You can create an efficient ensemble: just place a different trained model on each\n",
            "GPU so that you can get all the predictions much faster to produce the ensemble’s\n",
            "final prediction.\n",
            "\n",
            "But what if you want to speed up training by using multiple GPUs?\n",
            "\n",
            "Training Models Across Multiple Devices\n",
            "There  are  two  main  approaches  to  training  a  single  model  across  multiple  devices:\n",
            "model  parallelism,  where  the  model  is  split  across  the  devices,  and  data  parallelism,\n",
            "where  the  model  is  replicated  across  every  device,  and  each  replica  is  trained  on  a\n",
            "different subset of the data. Let’s look at these two options.\n",
            "\n",
            "Model Parallelism\n",
            "So  far  we  have  trained  each  neural  network  on  a  single  device.  What  if  we  want\n",
            "to  train  a  single  neural  network  across  multiple  devices?  This  requires  chopping\n",
            "the  model  into  separate  chunks  and  running  each  chunk  on  a  different  device.\n",
            "Unfortunately, such model parallelism turns out to be pretty tricky, and its effective‐\n",
            "ness really depends on the architecture of your neural network. For fully connected\n",
            "networks,  there  is  generally  not  much  to  be  gained  from  this  approach  (see  Fig‐\n",
            "ure 19-11). Intuitively, it may seem that an easy way to split the model is to place each\n",
            "layer  on  a  different  device,  but  this  does  not  work  because  each  layer  needs  to  wait\n",
            "for  the  output  of  the  previous  layer  before  it  can  do  anything.  So  perhaps  you  can\n",
            "slice it vertically—for example, with the left half of each layer on one device, and the\n",
            "right part on another device? This is slightly better, since both halves of each layer can\n",
            "indeed  work  in  parallel,  but  the  problem  is  that  each  half  of  the  next  layer  requires\n",
            "the  output  of  both  halves,  so  there  will  be  a  lot  of  cross-device  communication\n",
            "(represented by the dashed arrows). This is likely to completely cancel out the benefit\n",
            "of  the  parallel  computation,  since  cross-device  communication  is  slow  (and  even\n",
            "more so when the devices are located on different machines).\n",
            "\n",
            "14 If the two CNNs are identical, then it is called a Siamese neural network.\n",
            "\n",
            "756 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fFigure 19-11. Splitting a fully connected neural network\n",
            "\n",
            "Some  neural  network  architectures,  such  as  convolutional  neural  networks  (see\n",
            "Chapter 14), contain layers that are only partially connected to the lower layers, so it\n",
            "is much easier to distribute chunks across devices in an efficient way (Figure 19-12).\n",
            "\n",
            "Figure 19-12. Splitting a partially connected neural network\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "757\n",
            "\n",
            "\fDeep  recurrent  neural  networks  (see  Chapter  15)  can  be  split  a  bit  more  efficiently\n",
            "across multiple GPUs. If you split the network horizontally by placing each layer on a\n",
            "different device, and feed the network with an input sequence to process, then at the\n",
            "first time step only one device will be active (working on the sequence’s first value), at\n",
            "the second step two will be active (the second layer will be handling the output of the\n",
            "first layer for the first value, while the first layer will be handling the second value),\n",
            "and  by  the  time  the  signal  propagates  to  the  output  layer,  all  devices  will  be  active\n",
            "simultaneously  (Figure  19-13).  There  is  still  a  lot  of  cross-device  communication\n",
            "going on, but since each cell may be fairly complex, the benefit of running multiple\n",
            "cells  in  parallel  may  (in  theory)  outweigh  the  communication  penalty.  However,  in\n",
            "practice a regular stack of LSTM layers running on a single GPU actually runs much\n",
            "faster.\n",
            "\n",
            "Figure 19-13. Splitting a deep recurrent neural network\n",
            "\n",
            "In short, model parallelism may speed up running or training some types of neural\n",
            "networks,  but  not  all,  and  it  requires  special  care  and  tuning,  such  as  making  sure\n",
            "that  devices  that  need  to  communicate  the  most  run  on  the  same  machine.15  Next\n",
            "we’ll look at a much simpler and generally more efficient option: data parallelism.\n",
            "\n",
            "15 If you are interested in going further with model parallelism, check out Mesh TensorFlow.\n",
            "\n",
            "758 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fData Parallelism\n",
            "Another way to parallelize the training of a neural network is to replicate it on every\n",
            "device  and  run  each  training  step  simultaneously  on  all  replicas,  using  a  different\n",
            "mini-batch for each. The gradients computed by each replica are then averaged, and\n",
            "the result is used to update the model parameters. This is called data parallelism, or\n",
            "sometimes  single  program,  multiple  data  (SPMD).  There  are  many  variants  of  this\n",
            "idea, so let’s look at the most important ones.\n",
            "\n",
            "Data parallelism using the mirrored strategy\n",
            "\n",
            "Arguably  the  simplest  approach  is  to  completely  mirror  all  the  model  parameters\n",
            "across  all  the  GPUs  and  always  apply  the  exact  same  parameter  updates  on  every\n",
            "GPU.  This  way,  all  replicas  always  remain  perfectly  identical.  This  is  called  the\n",
            "mirrored strategy, and it turns out to be quite efficient, especially when using a single\n",
            "machine (see Figure 19-14).\n",
            "\n",
            "Figure 19-14. Data parallelism using the mirrored strategy\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "759\n",
            "\n",
            "\fThe  tricky  part  when  using  this  approach  is  to  efficiently  compute  the  mean  of  all\n",
            "the  gradients  from  all  the  GPUs  and  distribute  the  result  across  all  the  GPUs.  This\n",
            "can be done using an AllReduce algorithm, a class of algorithms where multiple nodes\n",
            "collaborate  to  efficiently  perform  a  reduce  operation  (such  as  computing  the  mean,\n",
            "sum, and max), while ensuring that all nodes obtain the same final result. Fortunately,\n",
            "there are off-the-shelf implementations of such algorithms, as you will see.\n",
            "\n",
            "Data parallelism with centralized parameters\n",
            "\n",
            "Another  approach  is  to  store  the  model  parameters  outside  of  the  GPU  devices\n",
            "performing  the  computations  (called  workers);  for  example,  on  the  CPU  (see  Fig‐\n",
            "ure 19-15). In a distributed setup, you may place all the parameters on one or more\n",
            "CPU-only servers called parameter servers, whose only role is to host and update the\n",
            "parameters.\n",
            "\n",
            "Figure 19-15. Data parallelism with centralized parameters\n",
            "\n",
            "Whereas the mirrored strategy imposes synchronous weight updates across all GPUs,\n",
            "this  centralized  approach  allows  either  synchronous  or  asynchronous  updates.  Let’s\n",
            "take a look at the pros and cons of both options.\n",
            "\n",
            "760 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fSynchronous updates.    With synchronous updates, the aggregator waits until all gradi‐\n",
            "ents  are  available  before  it  computes  the  average  gradients  and  passes  them  to  the\n",
            "optimizer,  which  will  update  the  model  parameters.  Once  a  replica  has  finished\n",
            "computing its gradients, it must wait for the parameters to be updated before it can\n",
            "proceed  to  the  next  mini-batch.  The  downside  is  that  some  devices  may  be  slower\n",
            "than others, so the fast devices will have to wait for the slow ones at every step, mak‐\n",
            "ing the whole process as slow as the slowest device. Moreover, the parameters will be\n",
            "copied to every device almost at the same time (immediately after the gradients are\n",
            "applied), which may saturate the parameter servers’ bandwidth.\n",
            "\n",
            "To reduce the waiting time at each step, you could ignore the gradi‐\n",
            "ents  from  the  slowest  few  replicas  (typically  ~10%).  For  example,\n",
            "you  could  run  20  replicas,  but  only  aggregate  the  gradients  from\n",
            "the  fastest  18  replicas  at  each  step,  and  just  ignore  the  gradients\n",
            "from  the  last  2.  As  soon  as  the  parameters  are  updated,  the  first\n",
            "18 replicas can start working again immediately, without having to\n",
            "wait for the 2 slowest replicas. This setup is generally described as\n",
            "having 18 replicas plus 2 spare replicas.16\n",
            "\n",
            "Asynchronous updates.     With  asynchronous  updates,  whenever  a  replica  has  finished\n",
            "computing  the  gradients,  the  gradients  are  immediately  used  to  update  the  model\n",
            "parameters.  There  is  no  aggregation  (it  removes  the  “mean”  step  in  Figure  19-15)\n",
            "and  no  synchronization.  Replicas  work  independently  of  the  other  replicas.  Since\n",
            "there is no waiting for the other replicas, this approach runs more training steps per\n",
            "minute. Moreover, although the parameters still need to be copied to every device at\n",
            "every step, this happens at different times for each replica, so the risk of bandwidth\n",
            "saturation is reduced.\n",
            "\n",
            "Data  parallelism  with  asynchronous  updates  is  an  attractive  choice  because  of  its\n",
            "simplicity, the absence of synchronization delay, and its better use of the bandwidth.\n",
            "However,  although  it  works  reasonably  well  in  practice,  it  is  almost  surprising  that\n",
            "it  works  at  all!  Indeed,  by  the  time  a  replica  has  finished  computing  the  gradients\n",
            "based  on  some  parameter  values,  these  parameters  will  have  been  updated  several\n",
            "times by other replicas (on average N – 1 times, if there are N replicas), and there is\n",
            "no guarantee that the computed gradients will still be pointing in the right direction\n",
            "(see  Figure  19-16).  When  gradients  are  severely  out  of  date,  they  are  called  stale\n",
            "gradients:  they  can  slow  down  convergence,  introducing  noise  and  wobble  effects\n",
            "\n",
            "16 This name is slightly confusing because it sounds like some replicas are special, doing nothing. In reality, all\n",
            "replicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary\n",
            "at every step (unless some devices are really slower than others). However, it does mean that if one or two\n",
            "servers crash, training will continue just fine.\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "761\n",
            "\n",
            "\f(the learning curve may contain temporary oscillations), or they can even make the\n",
            "training algorithm diverge.\n",
            "\n",
            "Figure 19-16. Stale gradients when using asynchronous updates\n",
            "\n",
            "There are a few ways you can reduce the effect of stale gradients:\n",
            "\n",
            "•\n",
            "• Reduce the learning rate.\n",
            "\n",
            "•\n",
            "• Drop stale gradients or scale them down.\n",
            "\n",
            "•\n",
            "• Adjust the mini-batch size.\n",
            "\n",
            "• Start the first few epochs using just one replica (this is called the warmup phase).\n",
            "•\n",
            "Stale  gradients  tend  to  be  more  damaging  at  the  beginning  of  training,  when\n",
            "gradients  are  typically  large  and  the  parameters  have  not  settled  into  a  valley\n",
            "of  the  cost  function  yet,  so  different  replicas  may  push  the  parameters  in  quite\n",
            "different directions.\n",
            "\n",
            "A  paper  published  by  the  Google  Brain  team  in  201617  benchmarked  various\n",
            "approaches  and  found  that  using  synchronous  updates  with  a  few  spare  replicas\n",
            "was more efficient than using asynchronous updates, not only converging faster but\n",
            "also producing a better model. However, this is still an active area of research, so you\n",
            "should not rule out asynchronous updates just yet.\n",
            "\n",
            "17 Jianmin Chen et al., “Revisiting Distributed Synchronous SGD”, arXiv preprint arXiv:1604.00981 (2016).\n",
            "\n",
            "762 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fBandwidth saturation\n",
            "\n",
            "Whether  you  use  synchronous  or  asynchronous  updates,  data  parallelism  with  cen‐\n",
            "tralized  parameters  still  requires  communicating  the  model  parameters  from  the\n",
            "parameter  servers  to  every  replica  at  the  beginning  of  each  training  step,  and  the\n",
            "gradients in the other direction at the end of each training step. Similarly, when using\n",
            "the  mirrored  strategy,  the  gradients  produced  by  each  GPU  will  need  to  be  shared\n",
            "with  every  other  GPU.  Unfortunately,  there  often  comes  a  point  where  adding  an\n",
            "extra  GPU  will  not  improve  performance  at  all  because  the  time  spent  moving  the\n",
            "data  into  and  out  of  GPU  RAM  (and  across  the  network  in  a  distributed  setup)\n",
            "will outweigh the speedup obtained by splitting the computation load. At that point,\n",
            "adding more GPUs will just worsen the bandwidth saturation and actually slow down\n",
            "training.\n",
            "\n",
            "Saturation is more severe for large dense models, since they have a lot of parameters\n",
            "and gradients to transfer. It is less severe for small models (but the parallelization gain\n",
            "is limited) and for large sparse models, where the gradients are typically mostly zeros\n",
            "and  so  can  be  communicated  efficiently.  Jeff  Dean,  initiator  and  lead  of  the  Google\n",
            "Brain  project,  reported  typical  speedups  of  25–40×  when  distributing  computations\n",
            "across  50  GPUs  for  dense  models,  and  a  300×  speedup  for  sparser  models  trained\n",
            "across 500 GPUs. As you can see, sparse models really do scale better. Here are a few\n",
            "concrete examples:\n",
            "\n",
            "•\n",
            "• Neural machine translation: 6× speedup on 8 GPUs\n",
            "\n",
            "•\n",
            "• Inception/ImageNet: 32× speedup on 50 GPUs\n",
            "\n",
            "•\n",
            "• RankBrain: 300× speedup on 500 GPUs\n",
            "\n",
            "There is plenty of research going on to alleviate the bandwidth saturation issue, with\n",
            "the goal of allowing training to scale linearly with the number of GPUs available. For\n",
            "example,  a  2018  paper18  by  a  team  of  researchers  from  Carnegie  Mellon  University,\n",
            "Stanford  University,  and  Microsoft  Research  proposed  a  system  called  PipeDream\n",
            "that managed to reduce network communications by over 90%, making it possible to\n",
            "train large models across many machines. They achieved this using a new technique\n",
            "called  pipeline  parallelism,  which  combines  model  parallelism  and  data  parallelism:\n",
            "the model is chopped into consecutive parts, called stages, each of which is trained on\n",
            "a different machine. This results in an asynchronous pipeline in which all machines\n",
            "work in parallel with very little idle time. During training, each stage alternates one\n",
            "round of forward propagation and one round of backpropagation (see Figure 19-17):\n",
            "it pulls a mini-batch from its input queue, processes it, and sends the outputs to the\n",
            "next  stage’s  input  queue,  then  it  pulls  one  mini-batch  of  gradients  from  its  gradient\n",
            "\n",
            "18 Aaron Harlap et al., “PipeDream: Fast and Efficient Pipeline Parallel DNN Training”, arXiv preprint\n",
            "\n",
            "arXiv:1806.03377 (2018).\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "763\n",
            "\n",
            "\fqueue,  backpropagates  these  gradients  and  updates  its  own  model  parameters,  and\n",
            "pushes  the  backpropagated  gradients  to  the  previous  stage’s  gradient  queue.  It  then\n",
            "repeats  the  whole  process  again  and  again.  Each  stage  can  also  use  regular  data\n",
            "parallelism (e.g., using the mirrored strategy), independently from the other stages.\n",
            "\n",
            "Figure 19-17. PipeDream’s pipeline parallelism\n",
            "\n",
            "However,  as  it’s  presented  here,  PipeDream  would  not  work  so  well.  To  understand\n",
            "why,  consider  mini-batch  #5  in  Figure  19-17:  when  it  went  through  stage  1  during\n",
            "the forward pass, the gradients from mini-batch #4 had not yet been backpropagated\n",
            "through that stage, but by the time #5’s gradients flow back to stage 1, #4’s gradients\n",
            "will  have  been  used  to  update  the  model  parameters,  so  #5’s  gradients  will  be  a\n",
            "bit  stale.  As  we  have  seen,  this  can  degrade  training  speed  and  accuracy,  and  even\n",
            "make  it  diverge:  the  more  stages  there  are,  the  worse  this  problem  becomes.  The\n",
            "paper’s  authors  proposed  methods  to  mitigate  this  issue,  though:  for  example,  each\n",
            "stage  saves  weights  during  forward  propagation  and  restores  them  during  backpro‐\n",
            "pagation,  to  ensure  that  the  same  weights  are  used  for  both  the  forward  pass  and\n",
            "the backward pass. This is called weight stashing. Thanks to this, PipeDream demon‐\n",
            "strates impressive scaling capability, well beyond simple data parallelism.\n",
            "\n",
            "The latest breakthrough in this field of research was published in a 2022 paper19 by\n",
            "Google  researchers:  they  developed  a  system  called  Pathways  that  uses  automated\n",
            "model  parallelism,  asynchronous  gang  scheduling,  and  other  techniques  to  reach\n",
            "close  to  100%  hardware  utilization  across  thousands  of  TPUs!  Scheduling  means\n",
            "organizing when and where each task must run, and gang scheduling means running\n",
            "related tasks at the same time in parallel and close to each other to reduce the time\n",
            "tasks have to wait for the others’ outputs. As we saw in Chapter 16, this system was\n",
            "used to train a massive language model across over 6,000 TPUs, with close to 100%\n",
            "hardware utilization: that’s a mindblowing engineering feat.\n",
            "\n",
            "At  the  time  of  writing,  Pathways  is  not  public  yet,  but  it’s  likely  that  in  the  near\n",
            "future  you  will  be  able  to  train  huge  models  on  Vertex  AI  using  Pathways  or  a\n",
            "similar  system.  In  the  meantime,  to  reduce  the  saturation  problem,  you’ll  probably\n",
            "want to use a few powerful GPUs rather than plenty of weak GPUs, and if you need\n",
            "to  train  a  model  across  multiple  servers,  you  should  group  your  GPUs  on  few  and\n",
            "\n",
            "19 Paul Barham et al., “Pathways: Asynchronous Distributed Dataflow for ML”, arXiv preprint arXiv:2203.12533\n",
            "\n",
            "(2022).\n",
            "\n",
            "764 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fvery well interconnected servers. You can also try dropping the float precision from\n",
            "32  bits  (tf.float32)  to  16  bits  (tf.bfloat16).  This  will  cut  in  half  the  amount  of\n",
            "data to transfer, often without much impact on the convergence rate or the model’s\n",
            "performance. Lastly, if you are using centralized parameters, you can shard (split) the\n",
            "parameters  across  multiple  parameter  servers:  adding  more  parameter  servers  will\n",
            "reduce the network load on each server and limit the risk of bandwidth saturation.\n",
            "\n",
            "OK,  now  that  we’ve  gone  through  all  the  theory,  let’s  actually  train  a  model  across\n",
            "multiple GPUs!\n",
            "\n",
            "Training at Scale Using the Distribution Strategies API\n",
            "Luckily, TensorFlow comes with a very nice API that takes care of all the complexity\n",
            "of  distributing  your  model  across  multiple  devices  and  machines:  the  distribution\n",
            "strategies API. To train a Keras model across all available GPUs (on a single machine,\n",
            "for  now)  using  data  parallelism  with  the  mirrored  strategy,  just  create  a  Mirrored\n",
            "Strategy object, call its scope() method to get a distribution context, and wrap the\n",
            "creation  and  compilation  of  your  model  inside  that  context.  Then  call  the  model’s\n",
            "fit() method normally:\n",
            "\n",
            "strategy = tf.distribute.MirroredStrategy()\n",
            "\n",
            "with strategy.scope():\n",
            "    model = tf.keras.Sequential([...])  # create a Keras model normally\n",
            "    model.compile([...])  # compile the model normally\n",
            "\n",
            "batch_size = 100  # preferably divisible by the number of replicas\n",
            "model.fit(X_train, y_train, epochs=10,\n",
            "          validation_data=(X_valid, y_valid), batch_size=batch_size)\n",
            "\n",
            "Under  the  hood,  Keras  is  distribution-aware,  so  in  this  MirroredStrategy  context\n",
            "it  knows  that  it  must  replicate  all  variables  and  operations  across  all  available  GPU\n",
            "devices. If you look at the model’s weights, they are of type MirroredVariable:\n",
            "\n",
            ">>> type(model.weights[0])\n",
            "tensorflow.python.distribute.values.MirroredVariable\n",
            "\n",
            "Note that the fit() method will automatically split each training batch across all the\n",
            "replicas, so it’s preferable to ensure that the batch size is divisible by the number of\n",
            "replicas  (i.e.,  the  number  of  available  GPUs)  so  that  all  replicas  get  batches  of  the\n",
            "same  size.  And  that’s  all!  Training  will  generally  be  significantly  faster  than  using  a\n",
            "single device, and the code change was really minimal.\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "765\n",
            "\n",
            "\fOnce  you  have  finished  training  your  model,  you  can  use  it  to  make  predictions\n",
            "efficiently: call the predict() method, and it will automatically split the batch across\n",
            "all replicas, making predictions in parallel. Again, the batch size must be divisible by\n",
            "the  number  of  replicas.  If  you  call  the  model’s  save()  method,  it  will  be  saved  as  a\n",
            "regular model, not as a mirrored model with multiple replicas. So when you load it, it\n",
            "will run like a regular model, on a single device: by default on GPU #0, or on the CPU\n",
            "if there are no GPUs. If you want to load a model and run it on all available devices,\n",
            "you must call tf.keras.models.load_model() within a distribution context:\n",
            "\n",
            "with strategy.scope():\n",
            "    model = tf.keras.models.load_model(\"my_mirrored_model\")\n",
            "\n",
            "If you only want to use a subset of all the available GPU devices, you can pass the list\n",
            "to the MirroredStrategy’s constructor:\n",
            "\n",
            "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
            "\n",
            "By default, the MirroredStrategy class uses the NVIDIA Collective Communications\n",
            "Library (NCCL) for the AllReduce mean operation, but you can change it by setting\n",
            "the cross_device_ops argument to an instance of the tf.distribute.Hierarchical\n",
            "CopyAllReduce  class,  or  an  instance  of  the  tf.distribute.ReductionToOneDevice\n",
            "class. The default NCCL option is based on the tf.distribute.NcclAllReduce class,\n",
            "which  is  usually  faster,  but  this  depends  on  the  number  and  types  of  GPUs,  so  you\n",
            "may want to give the alternatives a try.20\n",
            "\n",
            "If  you  want  to  try  using  data  parallelism  with  centralized  parameters,  replace  the\n",
            "MirroredStrategy with the CentralStorageStrategy:\n",
            "\n",
            "strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
            "\n",
            "You  can  optionally  set  the  compute_devices  argument  to  specify  the  list  of  devices\n",
            "you want to use as workers—by default it will use all available GPUs—and you can\n",
            "optionally set the parameter_device argument to specify the device you want to store\n",
            "the parameters on. By default it will use the CPU, or the GPU if there is just one.\n",
            "\n",
            "Now let’s see how to train a model across a cluster of TensorFlow servers!\n",
            "\n",
            "Training a Model on a TensorFlow Cluster\n",
            "A TensorFlow cluster is a group of TensorFlow processes running in parallel, usually\n",
            "on different machines, and talking to each other to complete some work—for exam‐\n",
            "ple, training or executing a neural network model. Each TF process in the cluster is\n",
            "called  a  task,  or  a  TF  server.  It  has  an  IP  address,  a  port,  and  a  type  (also  called  its\n",
            "\n",
            "20 For more details on AllReduce algorithms, read Yuichiro Ueno’s post on the technologies behind deep\n",
            "\n",
            "learning and Sylvain Jeaugey’s post on massively scaling deep learning training with NCCL.\n",
            "\n",
            "766 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\frole or its job). The type can be either \"worker\", \"chief\", \"ps\" (parameter server), or\n",
            "\"evaluator\":\n",
            "\n",
            "• Each  worker  performs  computations,  usually  on  a  machine  with  one  or  more\n",
            "•\n",
            "\n",
            "GPUs.\n",
            "\n",
            "• The chief performs computations as well (it is a worker), but it also handles extra\n",
            "•\n",
            "work such as writing TensorBoard logs or saving checkpoints. There is a single\n",
            "chief  in  a  cluster.  If  no  chief  is  specified  explicitly,  then  by  convention  the  first\n",
            "worker is the chief.\n",
            "\n",
            "• A parameter server only keeps track of variable values, and it is usually on a CPU-\n",
            "•\n",
            "only machine. This type of task is only used with the ParameterServerStrategy.\n",
            "\n",
            "•\n",
            "• An evaluator obviously takes care of evaluation. This type is not used often, and\n",
            "\n",
            "when it’s used, there’s usually just one evaluator.\n",
            "\n",
            "To  start  a  TensorFlow  cluster,  you  must  first  define  its  specification.  This  means\n",
            "defining each task’s IP address, TCP port, and type. For example, the following cluster\n",
            "specification defines a cluster with three tasks (two workers and one parameter server;\n",
            "see Figure 19-18). The cluster spec is a dictionary with one key per job, and the values\n",
            "are lists of task addresses (IP:port):\n",
            "\n",
            "cluster_spec = {\n",
            "    \"worker\": [\n",
            "        \"machine-a.example.com:2222\",     # /job:worker/task:0\n",
            "        \"machine-b.example.com:2222\"      # /job:worker/task:1\n",
            "    ],\n",
            "    \"ps\": [\"machine-a.example.com:2221\"]  # /job:ps/task:0\n",
            "}\n",
            "\n",
            "In general there will be a single task per machine, but as this example shows, you can\n",
            "configure multiple tasks on the same machine if you want. In this case, if they share\n",
            "the same GPUs, make sure the RAM is split appropriately, as discussed earlier.\n",
            "\n",
            "By  default,  every  task  in  the  cluster  may  communicate  with  every\n",
            "other  task,  so  make  sure  to  configure  your  firewall  to  authorize\n",
            "all  communications  between  these  machines  on  these  ports  (it’s\n",
            "usually simpler if you use the same port on every machine).\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "767\n",
            "\n",
            "\fFigure 19-18. An example TensorFlow cluster\n",
            "\n",
            "When  you  start  a  task,  you  must  give  it  the  cluster  spec,  and  you  must  also  tell\n",
            "it  what  its  type  and  index  are  (e.g.,  worker  #0).  The  simplest  way  to  specify  every‐\n",
            "thing  at  once  (both  the  cluster  spec  and  the  current  task’s  type  and  index)  is  to\n",
            "set  the  TF_CONFIG  environment  variable  before  starting  TensorFlow.  It  must  be  a\n",
            "JSON-encoded  dictionary  containing  a  cluster  specification  (under  the  \"cluster\"\n",
            "key) and the type and index of the current task (under the \"task\" key). For example,\n",
            "the  following  TF_CONFIG  environment  variable  uses  the  cluster  we  just  defined  and\n",
            "specifies that the task to start is worker #0:\n",
            "\n",
            "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
            "    \"cluster\": cluster_spec,\n",
            "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
            "})\n",
            "\n",
            "In general you want to define the TF_CONFIG environment variable\n",
            "outside of Python, so the code does not need to include the current\n",
            "task’s  type  and  index  (this  makes  it  possible  to  use  the  same  code\n",
            "across all workers).\n",
            "\n",
            "Now  let’s  train  a  model  on  a  cluster!  We  will  start  with  the  mirrored  strategy.  First,\n",
            "you  need  to  set  the  TF_CONFIG  environment  variable  appropriately  for  each  task.\n",
            "There should be no parameter server (remove the \"ps\" key in the cluster spec), and\n",
            "in  general  you  will  want  a  single  worker  per  machine.  Make  extra  sure  you  set  a\n",
            "different task index for each task. Finally, run the following script on every worker:\n",
            "\n",
            "768 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fimport tempfile\n",
            "import tensorflow as tf\n",
            "\n",
            "strategy = tf.distribute.MultiWorkerMirroredStrategy()  # at the start!\n",
            "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
            "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
            "[...] # load and split the MNIST dataset\n",
            "\n",
            "with strategy.scope():\n",
            "    model = tf.keras.Sequential([...])  # build the Keras model\n",
            "    model.compile([...])  # compile the model\n",
            "\n",
            "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
            "\n",
            "if resolver.task_id == 0:  # the chief saves the model to the right location\n",
            "    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\n",
            "else:\n",
            "    tmpdir = tempfile.mkdtemp()  # other workers save to a temporary directory\n",
            "    model.save(tmpdir, save_format=\"tf\")\n",
            "    tf.io.gfile.rmtree(tmpdir)  # and we can delete this directory at the end!\n",
            "\n",
            "That’s  almost  the  same  code  you  used  earlier,  except  this  time  you  are  using  the\n",
            "MultiWorkerMirroredStrategy. When you start this script on the first workers, they\n",
            "will remain blocked at the AllReduce step, but training will begin as soon as the last\n",
            "worker starts up, and you will see them all advancing at exactly the same rate since\n",
            "they synchronize at each step.\n",
            "\n",
            "When  using  the  MultiWorkerMirroredStrategy,  it’s  important  to\n",
            "ensure that all workers do the same thing, including saving model\n",
            "checkpoints  or  writing  TensorBoard  logs,  even  though  you  will\n",
            "only  keep  what  the  chief  writes.  This  is  because  these  operations\n",
            "may need to run the AllReduce operations, so all workers must be\n",
            "in sync.\n",
            "\n",
            "There  are  two  AllReduce  implementations  for  this  distribution  strategy:  a  ring  All‐\n",
            "Reduce  algorithm  based  on  gRPC  for  the  network  communications,  and  NCCL’s\n",
            "implementation.  The  best  algorithm  to  use  depends  on  the  number  of  workers,  the\n",
            "number and types of GPUs, and the network. By default, TensorFlow will apply some\n",
            "heuristics to select the right algorithm for you, but you can force NCCL (or RING)\n",
            "like this:\n",
            "\n",
            "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
            "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
            "        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "769\n",
            "\n",
            "\fIf  you  prefer  to  implement  asynchronous  data  parallelism  with  parameter  servers,\n",
            "change  the  strategy  to  ParameterServerStrategy,  add  one  or  more  parameter\n",
            "servers,  and  configure  TF_CONFIG  appropriately  for  each  task.  Note  that  although\n",
            "the  workers  will  work  asynchronously,  the  replicas  on  each  worker  will  work\n",
            "synchronously.\n",
            "\n",
            "Lastly, if you have access to TPUs on Google Cloud—for example, if you use Colab\n",
            "and you set the accelerator type to TPU—then you can create a TPUStrategy like this:\n",
            "\n",
            "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
            "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
            "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
            "\n",
            "This needs to be run right after importing TensorFlow. You can then use this strategy\n",
            "normally.\n",
            "\n",
            "If you are a researcher, you may be eligible to use TPUs for free; see\n",
            "https://tensorflow.org/tfrc for more details.\n",
            "\n",
            "You can now train models across multiple GPUs and multiple servers: give yourself\n",
            "a  pat  on  the  back!  If  you  want  to  train  a  very  large  model,  however,  you  will  need\n",
            "many GPUs, across many servers, which will require either buying a lot of hardware\n",
            "or  managing  a  lot  of  cloud  virtual  machines.  In  many  cases,  it’s  less  hassle  and  less\n",
            "expensive to use a cloud service that takes care of provisioning and managing all this\n",
            "infrastructure for you, just when you need it. Let’s see how to do that using Vertex AI.\n",
            "\n",
            "Running Large Training Jobs on Vertex AI\n",
            "Vertex  AI  allows  you  to  create  custom  training  jobs  with  your  own  training  code.\n",
            "In  fact,  you  can  use  almost  the  same  training  code  as  you  would  use  on  your  own\n",
            "TF  cluster.  The  main  thing  you  must  change  is  where  the  chief  should  save  the\n",
            "model,  the  checkpoints,  and  the  TensorBoard  logs.  Instead  of  saving  the  model  to\n",
            "a  local  directory,  the  chief  must  save  it  to  GCS,  using  the  path  provided  by  Vertex\n",
            "AI  in  the  AIP_MODEL_DIR  environment  variable.  For  the  model  checkpoints  and\n",
            "TensorBoard  logs,  you  should  use  the  paths  contained  in  the  AIP_CHECKPOINT_DIR\n",
            "and  AIP_TENSORBOARD_LOG_DIR  environment  variables,  respectively.  Of  course,  you\n",
            "must also make sure that the training data can be accessed from the virtual machines,\n",
            "such  as  on  GCS,  or  another  GCP  service  like  BigQuery,  or  directly  from  the  web.\n",
            "Lastly, Vertex AI sets the \"chief\" task type explicitly, so you should identify the chief\n",
            "using resolved.task_type == \"chief\" instead of resolved.task_id == 0:\n",
            "\n",
            "770 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fimport os\n",
            "[...]  # other imports, create MultiWorkerMirroredStrategy, and resolver\n",
            "\n",
            "if resolver.task_type == \"chief\":\n",
            "    model_dir = os.getenv(\"AIP_MODEL_DIR\")  # paths provided by Vertex AI\n",
            "    tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
            "    checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
            "else:\n",
            "    tmp_dir = Path(tempfile.mkdtemp())  # other workers use temporary dirs\n",
            "    model_dir = tmp_dir / \"model\"\n",
            "    tensorboard_log_dir = tmp_dir / \"logs\"\n",
            "    checkpoint_dir = tmp_dir / \"ckpt\"\n",
            "\n",
            "callbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\n",
            "             tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\n",
            "[...]  # build and  compile using the strategy scope, just like earlier\n",
            "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10,\n",
            "          callbacks=callbacks)\n",
            "model.save(model_dir, save_format=\"tf\")\n",
            "\n",
            "If  you  place  the  training  data  on  GCS,  you  can  create  a\n",
            "tf.data.TextLineDataset or tf.data.TFRecordDataset to access\n",
            "it:  just  use  the  GCS  paths  as  the  filenames  (e.g.,  gs://my_bucket/\n",
            "data/001.csv).  These  datasets  rely  on  the  tf.io.gfile  package  to\n",
            "access files: it supports both local files and GCS files.\n",
            "\n",
            "Now you can create a custom training job on Vertex AI, based on this script. You’ll\n",
            "need to specify the job name, the path to your training script, the Docker image to\n",
            "use for training, the one to use for predictions (after training), any additional Python\n",
            "libraries you may need, and lastly the bucket that Vertex AI should use as a staging\n",
            "directory to store the training script. By default, that’s also where the training script\n",
            "will save the trained model, as well as the TensorBoard logs and model checkpoints (if\n",
            "any). Let’s create the job:\n",
            "\n",
            "custom_training_job = aiplatform.CustomTrainingJob(\n",
            "    display_name=\"my_custom_training_job\",\n",
            "    script_path=\"my_vertex_ai_training_task.py\",\n",
            "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
            "    model_serving_container_image_uri=server_image,\n",
            "    requirements=[\"gcsfs==2022.3.0\"],  # not needed, this is just an example\n",
            "    staging_bucket=f\"gs://{bucket_name}/staging\"\n",
            ")\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "771\n",
            "\n",
            "\fAnd now let’s run it on two workers, each with two GPUs:\n",
            "\n",
            "mnist_model2 = custom_training_job.run(\n",
            "    machine_type=\"n1-standard-4\",\n",
            "    replica_count=2,\n",
            "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
            "    accelerator_count=2,\n",
            ")\n",
            "\n",
            "And that’s it: Vertex AI will provision the compute nodes you requested (within your\n",
            "quotas), and it will run your training script across them. Once the job is complete, the\n",
            "run() method will return a trained model that you can use exactly like the one you\n",
            "created earlier: you can deploy it to an endpoint, or use it to make batch predictions.\n",
            "If  anything  goes  wrong  during  training,  you  can  view  the  logs  in  the  GCP  console:\n",
            "in  the  ☰  navigation  menu,  select  Vertex  AI  →  Training,  click  on  your  training  job,\n",
            "and click VIEW LOGS. Alternatively, you can click the CUSTOM JOBS tab and copy\n",
            "the job’s ID (e.g., 1234), then select Logging from the ☰ navigation menu and query\n",
            "resource.labels.job_id=1234.\n",
            "\n",
            "To  visualize  the  training  progress,  just  start  TensorBoard  and\n",
            "point its --logdir to the GCS path of the logs. It will use applica‐\n",
            "tion  default  credentials,  which  you  can  set  up  using  gcloud  auth\n",
            "application-default login. Vertex AI also offers hosted Tensor‐\n",
            "Board servers if you prefer.\n",
            "\n",
            "If you want to try out a few hyperparameter values, one option is to run multiple jobs.\n",
            "You can pass the hyperparameter values to your script as command-line arguments\n",
            "by setting the args parameter when calling the run() method, or you can pass them\n",
            "as environment variables using the environment_variables parameter.\n",
            "\n",
            "However, if you want to run a large hyperparameter tuning job on the cloud, a much\n",
            "better option is to use Vertex AI’s hyperparameter tuning service. Let’s see how.\n",
            "\n",
            "Hyperparameter Tuning on Vertex AI\n",
            "Vertex AI’s hyperparameter tuning service is based on a Bayesian optimization algo‐\n",
            "rithm,  capable  of  quickly  finding  optimal  combinations  of  hyperparameters.  To  use\n",
            "it,  you  first  need  to  create  a  training  script  that  accepts  hyperparameter  values  as\n",
            "command-line arguments. For example, your script could use the argparse standard\n",
            "library like this:\n",
            "\n",
            "772 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fimport argparse\n",
            "\n",
            "parser = argparse.ArgumentParser()\n",
            "parser.add_argument(\"--n_hidden\", type=int, default=2)\n",
            "parser.add_argument(\"--n_neurons\", type=int, default=256)\n",
            "parser.add_argument(\"--learning_rate\", type=float, default=1e-2)\n",
            "parser.add_argument(\"--optimizer\", default=\"adam\")\n",
            "args = parser.parse_args()\n",
            "\n",
            "The  hyperparameter  tuning  service  will  call  your  script  multiple  times,  each  time\n",
            "with different hyperparameter values: each run is called a trial, and the set of trials is\n",
            "called a study. Your training script must then use the given hyperparameter values to\n",
            "build and compile a model. You can use a mirrored distribution strategy if you want,\n",
            "in case each trial runs on a multi-GPU machine. Then the script can load the dataset\n",
            "and train the model. For example:\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "def build_model(args):\n",
            "    with tf.distribute.MirroredStrategy().scope():\n",
            "        model = tf.keras.Sequential()\n",
            "        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
            "        for _ in range(args.n_hidden):\n",
            "            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n",
            "        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
            "        opt = tf.keras.optimizers.get(args.optimizer)\n",
            "        opt.learning_rate = args.learning_rate\n",
            "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
            "                      metrics=[\"accuracy\"])\n",
            "        return model\n",
            "\n",
            "[...]  # load the dataset\n",
            "model = build_model(args)\n",
            "history = model.fit([...])\n",
            "\n",
            "You can use the AIP_* environment variables we mentioned earlier\n",
            "to determine where to save the checkpoints, the TensorBoard logs,\n",
            "and the final model.\n",
            "\n",
            "Lastly, the script must report the model’s performance back to Vertex AI’s hyperpara‐\n",
            "meter  tuning  service,  so  it  can  decide  which  hyperparameters  to  try  next.  For  this,\n",
            "you  must  use  the  hypertune  library,  which  is  automatically  installed  on  Vertex  AI\n",
            "training VMs:\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "773\n",
            "\n",
            "\fimport hypertune\n",
            "\n",
            "hypertune = hypertune.HyperTune()\n",
            "hypertune.report_hyperparameter_tuning_metric(\n",
            "    hyperparameter_metric_tag=\"accuracy\",  # name of the reported metric\n",
            "    metric_value=max(history.history[\"val_accuracy\"]),  # metric value\n",
            "    global_step=model.optimizer.iterations.numpy(),\n",
            ")\n",
            "\n",
            "Now  that  your  training  script  is  ready,  you  need  to  define  the  type  of  machine  you\n",
            "would like to run it on. For this, you must define a custom job, which Vertex AI will\n",
            "use as a template for each trial:\n",
            "\n",
            "trial_job = aiplatform.CustomJob.from_local_script(\n",
            "    display_name=\"my_search_trial_job\",\n",
            "    script_path=\"my_vertex_ai_trial.py\",  # path to your training script\n",
            "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
            "    staging_bucket=f\"gs://{bucket_name}/staging\",\n",
            "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
            "    accelerator_count=2,  # in this example, each trial will have 2 GPUs\n",
            ")\n",
            "\n",
            "Finally, you’re ready to create and run the hyperparameter tuning job:\n",
            "\n",
            "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
            "\n",
            "hp_job = aiplatform.HyperparameterTuningJob(\n",
            "    display_name=\"my_hp_search_job\",\n",
            "    custom_job=trial_job,\n",
            "    metric_spec={\"accuracy\": \"maximize\"},\n",
            "    parameter_spec={\n",
            "        \"learning_rate\": hpt.DoubleParameterSpec(min=1e-3, max=10, scale=\"log\"),\n",
            "        \"n_neurons\": hpt.IntegerParameterSpec(min=1, max=300, scale=\"linear\"),\n",
            "        \"n_hidden\": hpt.IntegerParameterSpec(min=1, max=10, scale=\"linear\"),\n",
            "        \"optimizer\": hpt.CategoricalParameterSpec([\"sgd\", \"adam\"]),\n",
            "    },\n",
            "    max_trial_count=100,\n",
            "    parallel_trial_count=20,\n",
            ")\n",
            "hp_job.run()\n",
            "\n",
            "Here, we tell Vertex AI to maximize the metric named \"accuracy\": this name must\n",
            "match  the  name  of  the  metric  reported  by  the  training  script.  We  also  define  the\n",
            "search  space,  using  a  log  scale  for  the  learning  rate  and  a  linear  (i.e.,  uniform)\n",
            "scale  for  the  other  hyperparameters.  The  hyperparameter  names  must  match  the\n",
            "command-line arguments of the training script. Then we set the maximum number\n",
            "of trials to 100, and the maximum number of trials running in parallel to 20. If you\n",
            "increase the number of parallel trials to (say) 60, the total search time will be reduced\n",
            "significantly, by a factor of up to 3. But the first 60 trials will be started in parallel, so\n",
            "they will not benefit from the other trials’ feedback. Therefore, you should increase\n",
            "the max number of trials to compensate—for example, up to about 140.\n",
            "\n",
            "774 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\fThis will take quite a while. Once the job is completed, you can fetch the trial results\n",
            "using hp_job.trials. Each trial result is represented as a protobuf object, containing\n",
            "the hyperparameter values and the resulting metrics. Let’s find the best trial:\n",
            "\n",
            "def get_final_metric(trial, metric_id):\n",
            "    for metric in trial.final_measurement.metrics:\n",
            "        if metric.metric_id == metric_id:\n",
            "            return metric.value\n",
            "\n",
            "trials = hp_job.trials\n",
            "trial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\n",
            "best_trial = trials[np.argmax(trial_accuracies)]\n",
            "\n",
            "Now let’s look at this trial’s accuracy, and its hyperparameter values:\n",
            "\n",
            ">>> max(trial_accuracies)\n",
            "0.977400004863739\n",
            ">>> best_trial.id\n",
            "'98'\n",
            ">>> best_trial.parameters\n",
            "[parameter_id: \"learning_rate\" value { number_value: 0.001 },\n",
            " parameter_id: \"n_hidden\" value { number_value: 8.0 },\n",
            " parameter_id: \"n_neurons\" value { number_value: 216.0 },\n",
            " parameter_id: \"optimizer\" value { string_value: \"adam\" }\n",
            "]\n",
            "\n",
            "That’s it! Now you can get this trial’s SavedModel, optionally train it a bit more, and\n",
            "deploy it to production.\n",
            "\n",
            "Vertex AI also includes an AutoML service, which completely takes\n",
            "care of finding the right model architecture and training it for you.\n",
            "All  you  need  to  do  is  upload  your  dataset  to  Vertex  AI  using  a\n",
            "special  format  that  depends  on  the  type  of  dataset  (images,  text,\n",
            "tabular,  video,  etc.),  then  create  an  AutoML  training  job,  pointing\n",
            "to  the  dataset  and  specifying  the  maximum  number  of  compute\n",
            "hours you’re willing to spend. See the notebook for an example.\n",
            "\n",
            "Hyperparameter Tuning Using Keras Tuner on Vertex AI\n",
            "Instead of using Vertex AI’s hyperparameter tuning service, you can use Keras Tuner\n",
            "(introduced in Chapter 10) and run it on Vertex AI VMs. Keras Tuner provides a sim‐\n",
            "ple  way  to  scale  hyperparameter  search  by  distributing  it  across  multiple  machines:\n",
            "it  only  requires  setting  three  environment  variables  on  each  machine,  then  running\n",
            "your regular Keras Tuner code on each machine. You can use the exact same script on\n",
            "all machines. One of the machines acts as the chief (i.e., the oracle), and the others act\n",
            "as workers. Each worker asks the chief which hyperparameter values to try, then the\n",
            "worker trains the model using these hyperparameter values, and finally it reports the\n",
            "\n",
            "Training Models Across Multiple Devices \n",
            "\n",
            "| \n",
            "\n",
            "775\n",
            "\n",
            "\fmodel’s performance back to the chief, which can then decide which hyperparameter\n",
            "values the worker should try next.\n",
            "\n",
            "The three environment variables you need to set on each machine are:\n",
            "\n",
            "KERASTUNER_TUNER_ID\n",
            "\n",
            "This  is  equal  to  \"chief\"  on  the  chief  machine,  or  a  unique  identifier  on  each\n",
            "worker machine, such as \"worker0\", \"worker1\", etc.\n",
            "\n",
            "KERASTUNER_ORACLE_IP\n",
            "\n",
            "This is the IP address or hostname of the chief machine. The chief itself should\n",
            "generally use \"0.0.0.0\" to listen on every IP address on the machine.\n",
            "\n",
            "KERASTUNER_ORACLE_PORT\n",
            "\n",
            "This is the TCP port that the chief will be listening on.\n",
            "\n",
            "You can distribute Keras Tuner across any set of machines. If you want to run it on\n",
            "Vertex AI machines, then you can spawn a regular training job, and just modify the\n",
            "training  script  to  set  the  environment  variables  properly  before  using  Keras  Tuner.\n",
            "See the notebook for an example.\n",
            "\n",
            "Now you have all the tools and knowledge you need to create state-of-the-art neural\n",
            "net architectures and train them at scale using various distribution strategies, on your\n",
            "own infrastructure or on the cloud, and then deploy them anywhere. In other words,\n",
            "you now have superpowers: use them well!\n",
            "\n",
            "Exercises\n",
            "\n",
            "1.\n",
            "1. What does a SavedModel contain? How do you inspect its content?\n",
            "\n",
            "2.\n",
            "2. When  should  you  use  TF  Serving?  What  are  its  main  features?  What  are  some\n",
            "\n",
            "tools you can use to deploy it?\n",
            "\n",
            "3. How do you deploy a model across multiple TF Serving instances?\n",
            "3.\n",
            "\n",
            "4.\n",
            "4. When should you use the gRPC API rather than the REST API to query a model\n",
            "\n",
            "served by TF Serving?\n",
            "\n",
            "5. What  are  the  different  ways  TFLite  reduces  a  model’s  size  to  make  it  run  on  a\n",
            "5.\n",
            "\n",
            "mobile or embedded device?\n",
            "\n",
            "6. What is quantization-aware training, and why would you need it?\n",
            "6.\n",
            "\n",
            "7.\n",
            "7. What  are  model  parallelism  and  data  parallelism?  Why  is  the  latter  generally\n",
            "\n",
            "recommended?\n",
            "\n",
            "8. When training a model across multiple servers, what distribution strategies can\n",
            "8.\n",
            "\n",
            "you use? How do you choose which one to use?\n",
            "\n",
            "776 \n",
            "\n",
            "| \n",
            "\n",
            "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
            "\n",
            "\f9. Train a model (any model you like) and deploy it to TF Serving or Google Vertex\n",
            "9.\n",
            "AI.  Write  the  client  code  to  query  it  using  the  REST  API  or  the  gRPC  API.\n",
            "Update the model and deploy the new version. Your client code will now query\n",
            "the new version. Roll back to the first version.\n",
            "\n",
            "10. Train any model across multiple GPUs on the same machine using the Mirrored\n",
            "10.\n",
            "Strategy  (if  you  do  not  have  access  to  GPUs,  you  can  use  Google  Colab  with\n",
            "a  GPU  runtime  and  create  two  logical  GPUs).  Train  the  model  again  using  the\n",
            "CentralStorageStrategy and compare the training time.\n",
            "\n",
            "11. Fine-tune  a  model  of  your  choice  on  Vertex  AI,  using  either  Keras  Tuner  or\n",
            "11.\n",
            "\n",
            "Vertex AI’s hyperparameter tuning service.\n",
            "\n",
            "Solutions  to  these  exercises  are  available  at  the  end  of  this  chapter’s  notebook,  at\n",
            "https://homl.info/colab3.\n",
            "\n",
            "Thank You!\n",
            "Before we close the last chapter of this book, I would like to thank you for reading it\n",
            "up to the last paragraph. I truly hope that you had as much fun reading this book as I\n",
            "had writing it, and that it will be useful for your projects, big or small.\n",
            "\n",
            "If you find errors, please send feedback. More generally, I would love to know what\n",
            "you  think,  so  please  don’t  hesitate  to  contact  me  via  O’Reilly,  through  the  ageron/\n",
            "handson-ml3 GitHub project, or on Twitter at @aureliengeron.\n",
            "\n",
            "Going forward, my best advice to you is to practice and practice: try going through all\n",
            "the exercises (if you have not done so already), play with the notebooks, join Kaggle\n",
            "or  some  other  ML  community,  watch  ML  courses,  read  papers,  attend  conferences,\n",
            "and  meet  experts.  Things  move  fast,  so  try  to  keep  up  to  date.  Several  YouTube\n",
            "channels regularly present deep learning papers in great detail, in a very approachable\n",
            "way. I particularly recommend the channels by Yannic Kilcher, Letitia Parcalabescu,\n",
            "and  Xander  Steenbrugge.  For  fascinating  ML  discussions  and  higher-level  insights,\n",
            "make  sure  to  check  out  ML  Street  Talk,  and  Lex  Fridman’s  channel.  It  also  helps\n",
            "tremendously to have a concrete project to work on, whether it is for work or for fun\n",
            "(ideally  for  both),  so  if  there’s  anything  you  have  always  dreamed  of  building,  give\n",
            "it a shot! Work incrementally; don’t shoot for the moon right away, but stay focused\n",
            "on your project and build it piece by piece. It will require patience and perseverance,\n",
            "but when you have a walking robot, or a working chatbot, or whatever else you fancy\n",
            "building, it will be immensely rewarding!\n",
            "\n",
            "My greatest hope is that this book will inspire you to build a wonderful ML applica‐\n",
            "tion that will benefit all of us. What will it be?\n",
            "\n",
            "—Aurélien Géron\n",
            "\n",
            "Thank You! \n",
            "\n",
            "| \n",
            "\n",
            "777\n",
            "\n",
            "\f\fAPPENDIX A\n",
            "Machine Learning Project Checklist\n",
            "\n",
            "This checklist can guide you through your machine learning projects. There are eight\n",
            "main steps:\n",
            "\n",
            "1.\n",
            "1. Frame the problem and look at the big picture.\n",
            "\n",
            "2.\n",
            "2. Get the data.\n",
            "\n",
            "3.\n",
            "3. Explore the data to gain insights.\n",
            "\n",
            "4.\n",
            "4. Prepare the data to better expose the underlying data patterns to machine learn‐\n",
            "\n",
            "ing algorithms.\n",
            "\n",
            "5.\n",
            "5. Explore many different models and shortlist the best ones.\n",
            "\n",
            "6.\n",
            "6. Fine-tune your models and combine them into a great solution.\n",
            "\n",
            "7.\n",
            "7. Present your solution.\n",
            "\n",
            "8.\n",
            "8. Launch, monitor, and maintain your system.\n",
            "\n",
            "Obviously, you should feel free to adapt this checklist to your needs.\n",
            "\n",
            "Frame the Problem and Look at the Big Picture\n",
            "\n",
            "1.\n",
            "1. Define the objective in business terms.\n",
            "\n",
            "2.\n",
            "2. How will your solution be used?\n",
            "\n",
            "3. What are the current solutions/workarounds (if any)?\n",
            "3.\n",
            "\n",
            "4. How  should  you  frame  this  problem  (supervised/unsupervised,  online/offline,\n",
            "4.\n",
            "\n",
            "etc.)?\n",
            "\n",
            "5. How should performance be measured?\n",
            "5.\n",
            "\n",
            "6.\n",
            "6. Is the performance measure aligned with the business objective?\n",
            "\n",
            "779\n",
            "\n",
            "\f7.\n",
            "7. What would be the minimum performance needed to reach the business objec‐\n",
            "\n",
            "tive?\n",
            "\n",
            "8.\n",
            "8. What are comparable problems? Can you reuse experience or tools?\n",
            "\n",
            "9.\n",
            "9. Is human expertise available?\n",
            "\n",
            "10.\n",
            "10. How would you solve the problem manually?\n",
            "\n",
            "11. List the assumptions you (or others) have made so far.\n",
            "11.\n",
            "\n",
            "12.\n",
            "12. Verify assumptions if possible.\n",
            "\n",
            "Get the Data\n",
            "Note: automate as much as possible so you can easily get fresh data.\n",
            "\n",
            "1.\n",
            "1. List the data you need and how much you need.\n",
            "\n",
            "2.\n",
            "2. Find and document where you can get that data.\n",
            "\n",
            "3.\n",
            "3. Check how much space it will take.\n",
            "\n",
            "4.\n",
            "4. Check legal obligations, and get authorization if necessary.\n",
            "\n",
            "5.\n",
            "5. Get access authorizations.\n",
            "\n",
            "6.\n",
            "6. Create a workspace (with enough storage space).\n",
            "\n",
            "7.\n",
            "7. Get the data.\n",
            "\n",
            "8.\n",
            "8. Convert  the  data  to  a  format  you  can  easily  manipulate  (without  changing  the\n",
            "\n",
            "data itself).\n",
            "\n",
            "9.\n",
            "9. Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
            "\n",
            "10.\n",
            "10. Check the size and type of data (time series, sample, geographical, etc.).\n",
            "\n",
            "11. Sample a test set, put it aside, and never look at it (no data snooping!).\n",
            "11.\n",
            "\n",
            "Explore the Data\n",
            "Note: try to get insights from a field expert for these steps.\n",
            "\n",
            "1.\n",
            "1. Create a copy of the data for exploration (sampling it down to a manageable size\n",
            "\n",
            "if necessary).\n",
            "\n",
            "2. Create a Jupyter notebook to keep a record of your data exploration.\n",
            "2.\n",
            "\n",
            "3.\n",
            "3. Study each attribute and its characteristics:\n",
            "\n",
            "•\n",
            "• Name\n",
            "\n",
            "• Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
            "•\n",
            "\n",
            "•\n",
            "• % of missing values\n",
            "\n",
            "780 \n",
            "\n",
            "|  Appendix A: Machine Learning Project Checklist\n",
            "\n",
            "\f•\n",
            "• Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
            "\n",
            "•\n",
            "• Usefulness for the task\n",
            "\n",
            "•\n",
            "• Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
            "\n",
            "4.\n",
            "4. For supervised learning tasks, identify the target attribute(s).\n",
            "\n",
            "5.\n",
            "5. Visualize the data.\n",
            "\n",
            "6. Study the correlations between attributes.\n",
            "6.\n",
            "\n",
            "7.\n",
            "7. Study how you would solve the problem manually.\n",
            "\n",
            "8.\n",
            "8. Identify the promising transformations you may want to apply.\n",
            "\n",
            "9.\n",
            "9. Identify extra data that would be useful (go back to “Get the Data” on page 780).\n",
            "\n",
            "10.\n",
            "10. Document what you have learned.\n",
            "\n",
            "Prepare the Data\n",
            "Notes:\n",
            "\n",
            "•\n",
            "• Work on copies of the data (keep the original dataset intact).\n",
            "\n",
            "•\n",
            "• Write functions for all data transformations you apply, for five reasons:\n",
            "\n",
            "—\n",
            "— So you can easily prepare the data the next time you get a fresh dataset\n",
            "\n",
            "—\n",
            "— So you can apply these transformations in future projects\n",
            "\n",
            "—\n",
            "— To clean and prepare the test set\n",
            "\n",
            "—\n",
            "— To clean and prepare new data instances once your solution is live\n",
            "\n",
            "—\n",
            "— To make it easy to treat your preparation choices as hyperparameters\n",
            "\n",
            "1. Clean the data:\n",
            "1.\n",
            "\n",
            "• Fix or remove outliers (optional).\n",
            "•\n",
            "\n",
            "• Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or\n",
            "•\n",
            "\n",
            "columns).\n",
            "\n",
            "2.\n",
            "2. Perform feature selection (optional):\n",
            "\n",
            "•\n",
            "• Drop the attributes that provide no useful information for the task.\n",
            "\n",
            "3.\n",
            "3. Perform feature engineering, where appropriate:\n",
            "\n",
            "• Discretize continuous features.\n",
            "•\n",
            "\n",
            "• Decompose features (e.g., categorical, date/time, etc.).\n",
            "•\n",
            "• Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
            "•\n",
            "\n",
            "•\n",
            "• Aggregate features into promising new features.\n",
            "\n",
            "Machine Learning Project Checklist \n",
            "\n",
            "| \n",
            "\n",
            "781\n",
            "\n",
            "\f4.\n",
            "4. Perform feature scaling:\n",
            "\n",
            "•\n",
            "• Standardize or normalize features.\n",
            "\n",
            "Shortlist Promising Models\n",
            "Notes:\n",
            "\n",
            "•\n",
            "• If the data is huge, you may want to sample smaller training sets so you can train\n",
            "many different models in a reasonable time (be aware that this penalizes complex\n",
            "models such as large neural nets or random forests).\n",
            "\n",
            "•\n",
            "• Once again, try to automate these steps as much as possible.\n",
            "\n",
            "1.\n",
            "1. Train many quick-and-dirty models from different categories (e.g., linear, naive\n",
            "\n",
            "Bayes, SVM, random forest, neural net, etc.) using standard parameters.\n",
            "\n",
            "2.\n",
            "2. Measure and compare their performance:\n",
            "\n",
            "•\n",
            "• For each model, use N-fold cross-validation and compute the mean and stan‐\n",
            "\n",
            "dard deviation of the performance measure on the N folds.\n",
            "\n",
            "3.\n",
            "3. Analyze the most significant variables for each algorithm.\n",
            "\n",
            "4.\n",
            "4. Analyze the types of errors the models make:\n",
            "\n",
            "•\n",
            "• What data would a human have used to avoid these errors?\n",
            "\n",
            "5.\n",
            "5. Perform a quick round of feature selection and engineering.\n",
            "\n",
            "6.\n",
            "6. Perform one or two more quick iterations of the five previous steps.\n",
            "\n",
            "7.\n",
            "7. Shortlist  the  top  three  to  five  most  promising  models,  preferring  models  that\n",
            "\n",
            "make different types of errors.\n",
            "\n",
            "Fine-Tune the System\n",
            "Notes:\n",
            "\n",
            "•\n",
            "• You will want to use as much data as possible for this step, especially as you move\n",
            "\n",
            "toward the end of fine-tuning.\n",
            "\n",
            "• As always, automate what you can.\n",
            "•\n",
            "\n",
            "1. Fine-tune the hyperparameters using cross-validation:\n",
            "1.\n",
            "\n",
            "• Treat  your  data  transformation  choices  as  hyperparameters,  especially  when\n",
            "•\n",
            "you are not sure about them (e.g., if you’re not sure whether to replace missing\n",
            "values with zeros or with the median value, or to just drop the rows).\n",
            "\n",
            "782 \n",
            "\n",
            "|  Appendix A: Machine Learning Project Checklist\n",
            "\n",
            "\f• Unless  there  are  very  few  hyperparameter  values  to  explore,  prefer  random\n",
            "•\n",
            "search  over  grid  search.  If  training  is  very  long,  you  may  prefer  a  Bayesian\n",
            "optimization  approach  (e.g.,  using  Gaussian  process  priors,  as  described  by\n",
            "Jasper Snoek et al.1).\n",
            "\n",
            "2.\n",
            "2. Try  ensemble  methods.  Combining  your  best  models  will  often  produce  better\n",
            "\n",
            "performance than running them individually.\n",
            "\n",
            "3.\n",
            "3. Once you are confident about your final model, measure its performance on the\n",
            "\n",
            "test set to estimate the generalization error.\n",
            "\n",
            "Don’t  tweak  your  model  after  measuring  the  generalization  error:\n",
            "you would just start overfitting the test set.\n",
            "\n",
            "Present Your Solution\n",
            "\n",
            "1.\n",
            "1. Document what you have done.\n",
            "\n",
            "2.\n",
            "2. Create a nice presentation:\n",
            "\n",
            "•\n",
            "• Make sure you highlight the big picture first.\n",
            "\n",
            "3.\n",
            "3. Explain why your solution achieves the business objective.\n",
            "\n",
            "4.\n",
            "4. Don’t forget to present interesting points you noticed along the way:\n",
            "\n",
            "•\n",
            "• Describe what worked and what did not.\n",
            "\n",
            "•\n",
            "• List your assumptions and your system’s limitations.\n",
            "\n",
            "5.\n",
            "5. Ensure  your  key  findings  are  communicated  through  beautiful  visualizations\n",
            "or  easy-to-remember  statements  (e.g.,  “the  median  income  is  the  number-one\n",
            "predictor of housing prices”).\n",
            "\n",
            "Launch!\n",
            "\n",
            "1. Get your solution ready for production (plug into production data inputs, write\n",
            "1.\n",
            "\n",
            "unit tests, etc.).\n",
            "\n",
            "2.\n",
            "2. Write monitoring code to check your system’s live performance at regular inter‐\n",
            "\n",
            "vals and trigger alerts when it drops:\n",
            "\n",
            "• Beware of slow degradation: models tend to “rot” as data evolves.\n",
            "•\n",
            "\n",
            "1 Jasper Snoek et al., “Practical Bayesian Optimization of Machine Learning Algorithms”, Proceedings of the 25th\n",
            "\n",
            "International Conference on Neural Information Processing Systems 2 (2012): 2951–2959.\n",
            "\n",
            "Machine Learning Project Checklist \n",
            "\n",
            "| \n",
            "\n",
            "783\n",
            "\n",
            "\f•\n",
            "• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐\n",
            "\n",
            "ing service).\n",
            "\n",
            "• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐\n",
            "•\n",
            "dom  values,  or  another  team’s  output  becoming  stale).  This  is  particularly\n",
            "important for online learning systems.\n",
            "\n",
            "3. Retrain  your  models  on  a  regular  basis  on  fresh  data  (automate  as  much  as\n",
            "3.\n",
            "\n",
            "possible).\n",
            "\n",
            "784 \n",
            "\n",
            "|  Appendix A: Machine Learning Project Checklist\n",
            "\n",
            "\fAPPENDIX B\n",
            "Autodiff\n",
            "\n",
            "This  appendix  explains  how  TensorFlow’s  autodifferentiation  (autodiff)  feature\n",
            "works, and how it compares to other solutions.\n",
            "\n",
            "Suppose you define a function f(x, y) = x2y + y + 2, and you need its partial derivatives\n",
            "∂f/∂x  and  ∂f/∂y,  typically  to  perform  gradient  descent  (or  some  other  optimization\n",
            "algorithm). Your main options are manual differentiation, finite difference approxi‐\n",
            "mation, forward-mode autodiff, and reverse-mode autodiff. TensorFlow implements\n",
            "reverse-mode  autodiff,  but  to  understand  it,  it’s  useful  to  look  at  the  other  options\n",
            "first. So let’s go through each of them, starting with manual differentiation.\n",
            "\n",
            "Manual Differentiation\n",
            "The first approach to compute derivatives is to pick up a pencil and a piece of paper\n",
            "and use your calculus knowledge to derive the appropriate equation. For the function\n",
            "f(x, y) just defined, it is not too hard; you just need to use five rules:\n",
            "\n",
            "•\n",
            "• The derivative of a constant is 0.\n",
            "\n",
            "• The derivative of λx is λ (where λ is a constant).\n",
            "•\n",
            "• The derivative of xλ is λxλ – 1, so the derivative of x2 is 2x.\n",
            "•\n",
            "\n",
            "• The derivative of a sum of functions is the sum of these functions’ derivatives.\n",
            "•\n",
            "\n",
            "• The derivative of λ times a function is λ times its derivative.\n",
            "•\n",
            "\n",
            "785\n",
            "\n",
            "\fFrom these rules, you can derive Equation B-1.\n",
            "\n",
            "Equation B-1. Partial derivatives of f(x, y)\n",
            "\n",
            "∂ f\n",
            "∂x =\n",
            "∂ f\n",
            "∂y =\n",
            "\n",
            "∂ x2y\n",
            "∂x +\n",
            "∂ x2y\n",
            "∂y +\n",
            "\n",
            "∂y\n",
            "∂x +\n",
            "∂y\n",
            "∂y +\n",
            "\n",
            "∂ x2\n",
            "∂x + 0 + 0 = 2xy\n",
            "\n",
            "∂2\n",
            "∂x = y\n",
            "∂2\n",
            "∂y = x2 + 1 + 0 = x2 + 1\n",
            "\n",
            "This  approach  can  become  very  tedious  for  more  complex  functions,  and  you  run\n",
            "the risk of making mistakes. Fortunately, there are other options. Let’s look at finite\n",
            "difference approximation now.\n",
            "\n",
            "Finite Difference Approximation\n",
            "Recall  that  the  derivative  h′(x0)  of  a  function  h(x)  at  a  point  x0  is  the  slope  of  the\n",
            "function  at  that  point.  More  precisely,  the  derivative  is  defined  as  the  limit  of  the\n",
            "slope  of  a  straight  line  going  through  this  point  x0  and  another  point  x  on  the\n",
            "function, as x gets infinitely close to x0 (see Equation B-2).\n",
            "\n",
            "Equation B-2. Definition of the derivative of a function h(x) at point x0\n",
            "\n",
            "ℎ′ x0 =\n",
            "\n",
            "x\n",
            "\n",
            "lim\n",
            "\n",
            "x0\n",
            "\n",
            "= lim\n",
            "ε\n",
            "\n",
            "0\n",
            "\n",
            "ℎ x − ℎ x0\n",
            "x − x0\n",
            "ℎ x0 + ε − ℎ x0\n",
            "ε\n",
            "\n",
            "So, if we wanted to calculate the partial derivative of f(x, y) with regard to x at x = 3\n",
            "and y = 4, we could compute f(3 + ε, 4) – f(3, 4) and divide the result by ε, using a\n",
            "very small value for ε. This type of numerical approximation of the derivative is called\n",
            "a finite difference approximation, and this specific equation is called Newton’s difference\n",
            "quotient. That’s exactly what the following code does:\n",
            "\n",
            "def f(x, y):\n",
            "    return x**2*y + y + 2\n",
            "\n",
            "def derivative(f, x, y, x_eps, y_eps):\n",
            "    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)\n",
            "\n",
            "df_dx = derivative(f, 3, 4, 0.00001, 0)\n",
            "df_dy = derivative(f, 3, 4, 0, 0.00001)\n",
            "\n",
            "Unfortunately, the result is imprecise (and it gets worse for more complicated func‐\n",
            "tions). The correct results are respectively 24 and 10, but instead we get:\n",
            "\n",
            "786 \n",
            "\n",
            "|  Appendix B: Autodiff\n",
            "\n",
            "\f>>> df_dx\n",
            "24.000039999805264\n",
            ">>> df_dy\n",
            "10.000000000331966\n",
            "\n",
            "Notice that to compute both partial derivatives, we have to call f() at least three times\n",
            "(we  called  it  four  times  in  the  preceding  code,  but  it  could  be  optimized).  If  there\n",
            "were 1,000 parameters, we would need to call f() at least 1,001 times. When you are\n",
            "dealing  with  large  neural  networks,  this  makes  finite  difference  approximation  way\n",
            "too inefficient.\n",
            "\n",
            "However, this method is so simple to implement that it is a great tool to check that\n",
            "the other methods are implemented correctly. For example, if it disagrees with your\n",
            "manually derived function, then your function probably contains a mistake.\n",
            "\n",
            "So far, we have considered two ways to compute gradients: using manual differentia‐\n",
            "tion and using finite difference approximation. Unfortunately, both are fatally flawed\n",
            "for  training  a  large-scale  neural  network.  So  let’s  turn  to  autodiff,  starting  with\n",
            "forward mode.\n",
            "\n",
            "Forward-Mode Autodiff\n",
            "Figure  B-1  shows  how  forward-mode  autodiff  works  on  an  even  simpler  function,\n",
            "g(x, y) = 5 + xy. The graph for that function is represented on the left. After forward-\n",
            "mode autodiff, we get the graph on the right, which represents the partial derivative\n",
            "∂g/∂x = 0 + (0 × x + y × 1) = y (we could similarly obtain the partial derivative with\n",
            "regard to y).\n",
            "\n",
            "The algorithm will go through the computation graph from the inputs to the outputs\n",
            "(hence  the  name  “forward  mode”).  It  starts  by  getting  the  partial  derivatives  of  the\n",
            "leaf  nodes.  The  constant  node  (5)  returns  the  constant  0,  since  the  derivative  of  a\n",
            "constant  is  always  0.  The  variable  x  returns  the  constant  1  since  ∂x/∂x  =  1,  and  the\n",
            "variable  y  returns  the  constant  0  since  ∂y/∂x  =  0  (if  we  were  looking  for  the  partial\n",
            "derivative with regard to y, it would be the reverse).\n",
            "\n",
            "Now we have all we need to move up the graph to the multiplication node in function\n",
            "g.  Calculus  tells  us  that  the  derivative  of  the  product  of  two  functions  u  and  v  is\n",
            "∂(u × v)/∂x = ∂v/∂x × u + v × ∂u/∂x. We can therefore construct a large part of the\n",
            "graph on the right, representing 0 × x + y × 1.\n",
            "\n",
            "Finally, we can go up to the addition node in function g. As mentioned, the derivative\n",
            "of  a  sum  of  functions  is  the  sum  of  these  functions’  derivatives,  so  we  just  need  to\n",
            "create  an  addition  node  and  connect  it  to  the  parts  of  the  graph  we  have  already\n",
            "computed. We get the correct partial derivative: ∂g/∂x = 0 + (0 × x + y × 1).\n",
            "\n",
            "Autodiff \n",
            "\n",
            "| \n",
            "\n",
            "787\n",
            "\n",
            "\fFigure B-1. Forward-mode autodiff\n",
            "\n",
            "However, this equation can be simplified (a lot). By applying a few pruning steps to\n",
            "the  computation  graph  to  get  rid  of  all  the  unnecessary  operations,  we  get  a  much\n",
            "smaller graph with just one node: ∂g/∂x = y. In this case simplification is fairly easy,\n",
            "but  for  a  more  complex  function  forward-mode  autodiff  can  produce  a  huge  graph\n",
            "that may be tough to simplify and lead to suboptimal performance.\n",
            "\n",
            "Note that we started with a computation graph, and forward-mode autodiff produced\n",
            "another computation graph. This is called symbolic differentiation, and it has two nice\n",
            "features:  first,  once  the  computation  graph  of  the  derivative  has  been  produced,  we\n",
            "can use it as many times as we want to compute the derivatives of the given function\n",
            "for  any  value  of  x  and  y;  second,  we  can  run  forward-mode  autodiff  again  on  the\n",
            "resulting graph to get second-order derivatives if we ever need to (i.e., derivatives of\n",
            "derivatives). We could even compute third-order derivatives, and so on.\n",
            "\n",
            "But  it  is  also  possible  to  run  forward-mode  autodiff  without  constructing  a  graph\n",
            "(i.e.,  numerically,  not  symbolically),  just  by  computing  intermediate  results  on  the\n",
            "fly.  One  way  to  do  this  is  to  use  dual  numbers,  which  are  weird  but  fascinating\n",
            "numbers of the form a + bε, where a and b are real numbers and ε is an infinitesimal\n",
            "number such that ε2 = 0 (but ε ≠ 0). You can think of the dual number 42 + 24ε as\n",
            "something akin to 42.0000⋯000024 with an infinite number of 0s (but of course this\n",
            "\n",
            "788 \n",
            "\n",
            "|  Appendix B: Autodiff\n",
            "\n",
            "\fis simplified just to give you some idea of what dual numbers are). A dual number is\n",
            "represented in memory as a pair of floats. For example, 42 + 24ε is represented by the\n",
            "pair (42.0, 24.0).\n",
            "\n",
            "Dual numbers can be added, multiplied, and so on, as shown in Equation B-3.\n",
            "\n",
            "Equation B-3. A few operations with dual numbers\n",
            "\n",
            "λ a + bε = λa + λbε\n",
            "a + bε + c + dε = a + c + b + d ε\n",
            "a + bε × c + dε = ac + ad + bc ε + bd ε2 = ac + ad + bc ε\n",
            "\n",
            "Most importantly, it can be shown that h(a + bε) = h(a) + b × h′(a)ε, so computing\n",
            "h(a  +  ε)  gives  you  both  h(a)  and  the  derivative  h′(a)  in  just  one  shot.  Figure  B-2\n",
            "shows that the partial derivative of f(x, y) with regard to x at x = 3 and y = 4 (which\n",
            "I will write ∂f/∂x (3, 4)) can be computed using dual numbers. All we need to do is\n",
            "compute f(3 + ε, 4); this will output a dual number whose first component is equal to\n",
            "f(3, 4) and whose second component is equal to ∂f/∂x (3, 4).\n",
            "\n",
            "Figure B-2. Forward-mode autodiff using dual numbers\n",
            "\n",
            "Autodiff \n",
            "\n",
            "| \n",
            "\n",
            "789\n",
            "\n",
            "\fTo compute ∂f/∂y (3, 4) we would have to go through the graph again, but this time\n",
            "with x = 3 and y = 4 + ε.\n",
            "\n",
            "So, forward-mode autodiff is much more accurate than finite difference approxima‐\n",
            "tion, but it suffers from the same major flaw, at least when there are many inputs and\n",
            "few  outputs  (as  is  the  case  when  dealing  with  neural  networks):  if  there  were  1,000\n",
            "parameters, it would require 1,000 passes through the graph to compute all the partial\n",
            "derivatives. This is where reverse-mode autodiff shines: it can compute all of them in\n",
            "just two passes through the graph. Let’s see how.\n",
            "\n",
            "Reverse-Mode Autodiff\n",
            "Reverse-mode  autodiff  is  the  solution  implemented  by  TensorFlow.  It  first  goes\n",
            "through  the  graph  in  the  forward  direction  (i.e.,  from  the  inputs  to  the  output)  to\n",
            "compute the value of each node. Then it does a second pass, this time in the reverse\n",
            "direction (i.e., from the output to the inputs), to compute all the partial derivatives.\n",
            "The  name  “reverse  mode”  comes  from  this  second  pass  through  the  graph,  where\n",
            "gradients flow in the reverse direction. Figure B-3 represents the second pass. During\n",
            "the first pass, all the node values were computed, starting from x = 3 and y = 4. You\n",
            "can see those values at the bottom right of each node (e.g., x × x = 9). The nodes are\n",
            "labeled n1 to n7 for clarity. The output node is n7: f(3, 4) = n7 = 42.\n",
            "\n",
            "Figure B-3. Reverse-mode autodiff\n",
            "\n",
            "790 \n",
            "\n",
            "|  Appendix B: Autodiff\n",
            "\n",
            "\fThe idea is to gradually go down the graph, computing the partial derivative of f(x, y)\n",
            "with  regard  to  each  consecutive  node,  until  we  reach  the  variable  nodes.  For  this,\n",
            "reverse-mode autodiff relies heavily on the chain rule, shown in Equation B-4.\n",
            "\n",
            "Equation B-4. Chain rule\n",
            "\n",
            "∂ f\n",
            "∂x =\n",
            "\n",
            "∂ f\n",
            "∂ni\n",
            "\n",
            "×\n",
            "\n",
            "∂ni\n",
            "∂x\n",
            "\n",
            "Since n7 is the output node, f = n7 so ∂f / ∂n7 = 1.\n",
            "\n",
            "Let’s  continue  down  the  graph  to  n5:  how  much  does  f  vary  when  n5  varies?  The\n",
            "answer is ∂f / ∂n5 = ∂f / ∂n7 × ∂n7 / ∂n5. We already know that ∂f / ∂n7 = 1, so all we\n",
            "need is ∂n7 / ∂n5. Since n7 simply performs the sum n5 + n6, we find that ∂n7 / ∂n5 = 1,\n",
            "so ∂f / ∂n5 = 1 × 1 = 1.\n",
            "\n",
            "Now we can proceed to node n4: how much does f vary when n4 varies? The answer is\n",
            "∂f / ∂n4 = ∂f / ∂n5 × ∂n5 / ∂n4. Since n5 = n4 × n2, we find that ∂n5 / ∂n4 = n2, so ∂f / ∂n4\n",
            "= 1 × n2 = 4.\n",
            "\n",
            "The process continues until we reach the bottom of the graph. At that point we will\n",
            "have calculated all the partial derivatives of f(x, y) at the point x = 3 and y = 4. In this\n",
            "example, we find ∂f / ∂x = 24 and ∂f / ∂y = 10. Sounds about right!\n",
            "\n",
            "Reverse-mode  autodiff  is  a  very  powerful  and  accurate  technique,  especially  when\n",
            "there are many inputs and few outputs, since it requires only one forward pass plus\n",
            "one  reverse  pass  per  output  to  compute  all  the  partial  derivatives  for  all  outputs\n",
            "with  regard  to  all  the  inputs.  When  training  neural  networks,  we  generally  want  to\n",
            "minimize  the  loss,  so  there  is  a  single  output  (the  loss),  and  hence  only  two  passes\n",
            "through the graph are needed to compute the gradients. Reverse-mode autodiff can\n",
            "also  handle  functions  that  are  not  entirely  differentiable,  as  long  as  you  ask  it  to\n",
            "compute the partial derivatives at points that are differentiable.\n",
            "\n",
            "In Figure B-3, the numerical results are computed on the fly, at each node. However,\n",
            "that’s not exactly what TensorFlow does: instead, it creates a new computation graph.\n",
            "In other words, it implements symbolic reverse-mode autodiff. This way, the compu‐\n",
            "tation graph to compute the gradients of the loss with regard to all the parameters in\n",
            "the neural network only needs to be generated once, and then it can be executed over\n",
            "and  over  again,  whenever  the  optimizer  needs  to  compute  the  gradients.  Moreover,\n",
            "this makes it possible to compute higher-order derivatives if needed.\n",
            "\n",
            "Autodiff \n",
            "\n",
            "| \n",
            "\n",
            "791\n",
            "\n",
            "\fIf you ever want to implement a new type of low-level TensorFlow\n",
            "operation in C++, and you want to make it compatible with auto‐\n",
            "diff, then you will need to provide a function that returns the par‐\n",
            "tial  derivatives  of  the  function’s  outputs  with  regard  to  its  inputs.\n",
            "For example, suppose you implement a function that computes the\n",
            "square of its input: f(x) = x2. In that case you would need to provide\n",
            "the corresponding derivative function: f′(x) = 2x.\n",
            "\n",
            "792 \n",
            "\n",
            "|  Appendix B: Autodiff\n",
            "\n",
            "\fAPPENDIX C\n",
            "Special Data Structures\n",
            "\n",
            "In  this  appendix  we  will  take  a  very  quick  look  at  the  data  structures  supported  by\n",
            "TensorFlow,  beyond  regular  float  or  integer  tensors.  This  includes  strings,  ragged\n",
            "tensors, sparse tensors, tensor arrays, sets, and queues.\n",
            "\n",
            "Strings\n",
            "Tensors  can  hold  byte  strings,  which  is  useful  in  particular  for  natural  language\n",
            "processing (see Chapter 16):\n",
            "\n",
            ">>> tf.constant(b\"hello world\")\n",
            "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>\n",
            "\n",
            "If you try to build a tensor with a Unicode string, TensorFlow automatically encodes\n",
            "it to UTF-8:\n",
            "\n",
            ">>> tf.constant(\"café\")\n",
            "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>\n",
            "\n",
            "It is also possible to create tensors representing Unicode strings. Just create an array\n",
            "of 32-bit integers, each representing a single Unicode code point:1\n",
            "\n",
            ">>> u = tf.constant([ord(c) for c in \"café\"])\n",
            ">>> u\n",
            "<tf.Tensor: shape=(4,), [...], numpy=array([ 99,  97, 102, 233], dtype=int32)>\n",
            "\n",
            "1 If you are not familiar with Unicode code points, please check out https://homl.info/unicode.\n",
            "\n",
            "793\n",
            "\n",
            "\fIn  tensors  of  type  tf.string,  the  string  length  is  not  part  of  the\n",
            "tensor’s  shape.  In  other  words,  strings  are  considered  as  atomic\n",
            "values. However, in a Unicode string tensor (i.e., an int32 tensor),\n",
            "the length of the string is part of the tensor’s shape.\n",
            "\n",
            "The  tf.strings  package  contains  several  functions  to  manipulate  string  tensors,\n",
            "such  as  length()  to  count  the  number  of  bytes  in  a  byte  string  (or  the  number  of\n",
            "code  points  if  you  set  unit=\"UTF8_CHAR\"),  unicode_encode()  to  convert  a  Unicode\n",
            "string tensor (i.e., int32 tensor) to a byte string tensor, and unicode_decode() to do\n",
            "the reverse:\n",
            "\n",
            ">>> b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
            ">>> b\n",
            "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>\n",
            ">>> tf.strings.length(b, unit=\"UTF8_CHAR\")\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=4>\n",
            ">>> tf.strings.unicode_decode(b, \"UTF-8\")\n",
            "<tf.Tensor: shape=(4,), [...], numpy=array([ 99,  97, 102, 233], dtype=int32)>\n",
            "\n",
            "You can also manipulate tensors containing multiple strings:\n",
            "\n",
            ">>> p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])\n",
            ">>> tf.strings.length(p, unit=\"UTF8_CHAR\")\n",
            "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>\n",
            ">>> r = tf.strings.unicode_decode(p, \"UTF8\")\n",
            ">>> r\n",
            "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97,\n",
            "102, 102, 232], [21654, 21857]]>\n",
            "\n",
            "Notice that the decoded strings are stored in a RaggedTensor. What is that?\n",
            "\n",
            "Ragged Tensors\n",
            "A ragged tensor is a special kind of tensor that represents a list of arrays of different\n",
            "sizes.  More  generally,  it  is  a  tensor  with  one  or  more  ragged  dimensions,  meaning\n",
            "dimensions  whose  slices  may  have  different  lengths.  In  the  ragged  tensor  r,  the\n",
            "second dimension is a ragged dimension. In all ragged tensors, the first dimension is\n",
            "always a regular dimension (also called a uniform dimension).\n",
            "\n",
            "All the elements of the ragged tensor r are regular tensors. For example, let’s look at\n",
            "the second element of the ragged tensor:\n",
            "\n",
            ">>> r[1]\n",
            "<tf.Tensor: [...], numpy=array([ 67, 111, 102, 102, 101, 101], dtype=int32)>\n",
            "\n",
            "The  tf.ragged  package  contains  several  functions  to  create  and  manipulate  ragged\n",
            "tensors. Let’s create a second ragged tensor using tf.ragged.constant() and concat‐\n",
            "enate it with the first ragged tensor, along axis 0:\n",
            "\n",
            "794 \n",
            "\n",
            "|  Appendix C: Special Data Structures\n",
            "\n",
            "\f>>> r2 = tf.ragged.constant([[65, 66], [], [67]])\n",
            ">>> tf.concat([r, r2], axis=0)\n",
            "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97,\n",
            "102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\n",
            "\n",
            "The result is not too surprising: the tensors in r2 were appended after the tensors in r\n",
            "along axis 0. But what if we concatenate r and another ragged tensor along axis 1?\n",
            "\n",
            ">>> r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n",
            ">>> print(tf.concat([r, r3], axis=1))\n",
            "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101,\n",
            "71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n",
            "\n",
            "This  time,  notice  that  the  ith  tensor  in  r  and  the  ith  tensor  in  r3  were  concatenated.\n",
            "Now that’s more unusual, since all of these tensors can have different lengths.\n",
            "\n",
            "If  you  call  the  to_tensor()  method,  the  ragged  tensor  gets  converted  to  a  regular\n",
            "tensor,  padding  shorter  tensors  with  zeros  to  get  tensors  of  equal  lengths  (you  can\n",
            "change the default value by setting the default_value argument):\n",
            "\n",
            ">>> r.to_tensor()\n",
            "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
            "array([[   67,    97,   102,   233,     0,     0],\n",
            "       [   67,   111,   102,   102,   101,   101],\n",
            "       [   99,    97,   102,   102,   232,     0],\n",
            "       [21654, 21857,     0,     0,     0,     0]], dtype=int32)>\n",
            "\n",
            "Many TF operations support ragged tensors. For the full list, see the documentation\n",
            "of the tf.RaggedTensor class.\n",
            "\n",
            "Sparse Tensors\n",
            "TensorFlow  can  also  efficiently  represent  sparse  tensors  (i.e.,  tensors  containing\n",
            "mostly  zeros).  Just  create  a  tf.SparseTensor,  specifying  the  indices  and  values  of\n",
            "the  nonzero  elements  and  the  tensor’s  shape.  The  indices  must  be  listed  in  “read‐\n",
            "ing  order”  (from  left  to  right,  and  top  to  bottom).  If  you  are  unsure,  just  use\n",
            "tf.sparse.reorder().  You  can  convert  a  sparse  tensor  to  a  dense  tensor  (i.e.,  a\n",
            "regular tensor) using tf.sparse.to_dense():\n",
            "\n",
            ">>> s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
            "...                     values=[1., 2., 3.],\n",
            "...                     dense_shape=[3, 4])\n",
            "...\n",
            ">>> tf.sparse.to_dense(s)\n",
            "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
            "array([[0., 1., 0., 0.],\n",
            "       [2., 0., 0., 0.],\n",
            "       [0., 0., 0., 3.]], dtype=float32)>\n",
            "\n",
            "Special Data Structures \n",
            "\n",
            "| \n",
            "\n",
            "795\n",
            "\n",
            "\fNote  that  sparse  tensors  do  not  support  as  many  operations  as  dense  tensors.  For\n",
            "example,  you  can  multiply  a  sparse  tensor  by  any  scalar  value,  and  you  get  a  new\n",
            "sparse tensor, but you cannot add a scalar value to a sparse tensor, as this would not\n",
            "return a sparse tensor:\n",
            "\n",
            ">>> s * 42.0\n",
            "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f84a6749f10>\n",
            ">>> s + 42.0\n",
            "[...] TypeError: unsupported operand type(s) for +: 'SparseTensor' and 'float'\n",
            "\n",
            "Tensor Arrays\n",
            "A tf.TensorArray represents a list of tensors. This can be handy in dynamic models\n",
            "containing  loops,  to  accumulate  results  and  later  compute  some  statistics.  You  can\n",
            "read or write tensors at any location in the array:\n",
            "\n",
            "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
            "array = array.write(0, tf.constant([1., 2.]))\n",
            "array = array.write(1, tf.constant([3., 10.]))\n",
            "array = array.write(2, tf.constant([5., 7.]))\n",
            "tensor1 = array.read(1)  # => returns (and zeros out!) tf.constant([3., 10.])\n",
            "\n",
            "By default, reading an item also replaces it with a tensor of the same shape but full of\n",
            "zeros. You can set clear_after_read to False if you don’t want this.\n",
            "\n",
            "When  you  write  to  the  array,  you  must  assign  the  output  back  to\n",
            "the  array,  as  shown  in  this  code  example.  If  you  don’t,  although\n",
            "your code will work fine in eager mode, it will break in graph mode\n",
            "(these modes are discussed in Chapter 12).\n",
            "\n",
            "By  default,  a  TensorArray  has  a  fixed  size  that  is  set  upon  creation.  Alternatively,\n",
            "you can set size=0 and dynamic_size=True to let the array grow automatically when\n",
            "needed. However, this will hinder performance, so if you know the size in advance,\n",
            "it’s better to use a fixed-size array. You must also specify the dtype, and all elements\n",
            "must have the same shape as the first one written to the array.\n",
            "\n",
            "You can stack all the items into a regular tensor by calling the stack() method:\n",
            "\n",
            ">>> array.stack()\n",
            "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
            "array([[1., 2.],\n",
            "       [0., 0.],\n",
            "       [5., 7.]], dtype=float32)>\n",
            "\n",
            "796 \n",
            "\n",
            "|  Appendix C: Special Data Structures\n",
            "\n",
            "\fSets\n",
            "TensorFlow  supports  sets  of  integers  or  strings  (but  not  floats).  It  represents  sets\n",
            "using regular tensors. For example, the set {1, 5, 9} is just represented as the tensor\n",
            "[[1,  5,  9]].  Note  that  the  tensor  must  have  at  least  two  dimensions,  and  the  sets\n",
            "must be in the last dimension. For example, [[1, 5, 9], [2, 5, 11]] is a tensor\n",
            "holding two independent sets: {1, 5, 9} and {2, 5, 11}.\n",
            "\n",
            "The tf.sets package contains several functions to manipulate sets. For example, let’s\n",
            "create  two  sets  and  compute  their  union  (the  result  is  a  sparse  tensor,  so  we  call\n",
            "to_dense() to display it):\n",
            "\n",
            ">>> a = tf.constant([[1, 5, 9]])\n",
            ">>> b = tf.constant([[5, 6, 9, 11]])\n",
            ">>> u = tf.sets.union(a, b)\n",
            ">>> u\n",
            "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x132b60d30>\n",
            ">>> tf.sparse.to_dense(u)\n",
            "<tf.Tensor: [...], numpy=array([[ 1,  5,  6,  9, 11]], dtype=int32)>\n",
            "\n",
            "You can also compute the union of multiple pairs of sets simultaneously. If some sets\n",
            "are shorter than others, you must pad them with a padding value, such as 0:\n",
            "\n",
            ">>> a = tf.constant([[1, 5, 9], [10, 0, 0]])\n",
            ">>> b = tf.constant([[5, 6, 9, 11], [13, 0, 0, 0]])\n",
            ">>> u = tf.sets.union(a, b)\n",
            ">>> tf.sparse.to_dense(u)\n",
            "<tf.Tensor: [...] numpy=array([[ 1,  5,  6,  9, 11],\n",
            "                               [ 0, 10, 13,  0,  0]], dtype=int32)>\n",
            "\n",
            "If  you  prefer  to  use  a  different  padding  value,  such  as  –1,  then  you  must  set\n",
            "default_value=-1 (or your preferred value) when calling to_dense().\n",
            "\n",
            "The  default  default_value  is  0,  so  when  dealing  with  string  sets,\n",
            "you must set this parameter (e.g., to an empty string).\n",
            "\n",
            "Other  functions  available  in  tf.sets  include  difference(),  intersection(),  and\n",
            "size(), which are self-explanatory. If you want to check whether or not a set contains\n",
            "some given values, you can compute the intersection of that set and the values. If you\n",
            "want to add some values to a set, you can compute the union of the set and the values.\n",
            "\n",
            "Special Data Structures \n",
            "\n",
            "| \n",
            "\n",
            "797\n",
            "\n",
            "\fQueues\n",
            "A  queue  is  a  data  structure  to  which  you  can  push  data  records,  and  later  pull\n",
            "them out. TensorFlow implements several types of queues in the tf.queue package.\n",
            "They used to be very important when implementing efficient data loading and pre‐\n",
            "processing pipelines, but the tf.data API has essentially rendered them useless (except\n",
            "perhaps  in  some  rare  cases)  because  it  is  much  simpler  to  use  and  provides  all  the\n",
            "tools you need to build efficient pipelines. For the sake of completeness, though, let’s\n",
            "take a quick look at them.\n",
            "\n",
            "The  simplest  kind  of  queue  is  the  first-in,  first-out  (FIFO)  queue.  To  build  it,  you\n",
            "need  to  specify  the  maximum  number  of  records  it  can  contain.  Moreover,  each\n",
            "record is a tuple of tensors, so you must specify the type of each tensor, and option‐\n",
            "ally their shapes. For example, the following code example creates a FIFO queue with\n",
            "a  maximum  of  three  records,  each  containing  a  tuple  with  a  32-bit  integer  and  a\n",
            "string. Then it pushes two records to it, looks at the size (which is 2 at this point), and\n",
            "pulls a record out:\n",
            "\n",
            ">>> q = tf.queue.FIFOQueue(3, [tf.int32, tf.string], shapes=[(), ()])\n",
            ">>> q.enqueue([10, b\"windy\"])\n",
            ">>> q.enqueue([15, b\"sunny\"])\n",
            ">>> q.size()\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=2>\n",
            ">>> q.dequeue()\n",
            "[<tf.Tensor: shape=(), dtype=int32, numpy=10>,\n",
            " <tf.Tensor: shape=(), dtype=string, numpy=b'windy'>]\n",
            "\n",
            "It  is  also  possible  to  enqueue  and  dequeue  multiple  records  at  once  using\n",
            "enqueue_many() and dequeue_many() (to use dequeue_many(), you must specify the\n",
            "shapes argument when you create the queue, as we did previously):\n",
            "\n",
            ">>> q.enqueue_many([[13, 16], [b'cloudy', b'rainy']])\n",
            ">>> q.dequeue_many(3)\n",
            "[<tf.Tensor: [...], numpy=array([15, 13, 16], dtype=int32)>,\n",
            " <tf.Tensor: [...], numpy=array([b'sunny', b'cloudy', b'rainy'], dtype=object)>]\n",
            "\n",
            "Other queue types include:\n",
            "\n",
            "PaddingFIFOQueue\n",
            "\n",
            "Same  as  FIFOQueue,  but  its  dequeue_many()  method  supports  dequeueing  mul‐\n",
            "tiple  records  of  different  shapes.  It  automatically  pads  the  shortest  records  to\n",
            "ensure all the records in the batch have the same shape.\n",
            "\n",
            "PriorityQueue\n",
            "\n",
            "A  queue  that  dequeues  records  in  a  prioritized  order.  The  priority  must  be  a\n",
            "64-bit integer included as the first element of each record. Surprisingly, records\n",
            "with a lower priority will be dequeued first. Records with the same priority will\n",
            "be dequeued in FIFO order.\n",
            "\n",
            "798 \n",
            "\n",
            "|  Appendix C: Special Data Structures\n",
            "\n",
            "\fRandomShuffleQueue\n",
            "\n",
            "A  queue  whose  records  are  dequeued  in  random  order.  This  was  useful  to\n",
            "implement a shuffle buffer before tf.data existed.\n",
            "\n",
            "If  a  queue  is  already  full  and  you  try  to  enqueue  another  record,  the  enqueue*()\n",
            "method will freeze until a record is dequeued by another thread. Similarly, if a queue\n",
            "is  empty  and  you  try  to  dequeue  a  record,  the  dequeue*()  method  will  freeze  until\n",
            "records are pushed to the queue by another thread.\n",
            "\n",
            "Special Data Structures \n",
            "\n",
            "| \n",
            "\n",
            "799\n",
            "\n",
            "\f\fAPPENDIX D\n",
            "TensorFlow Graphs\n",
            "\n",
            "In  this  appendix,  we  will  explore  the  graphs  generated  by  TF  functions  (see\n",
            "Chapter 12).\n",
            "\n",
            "TF Functions and Concrete Functions\n",
            "TF  functions  are  polymorphic,  meaning  they  support  inputs  of  different  types  (and\n",
            "shapes). For example, consider the following tf_cube() function:\n",
            "\n",
            "@tf.function\n",
            "def tf_cube(x):\n",
            "    return x ** 3\n",
            "\n",
            "Every time you call a TF function with a new combination of input types or shapes,\n",
            "it  generates  a  new  concrete  function,  with  its  own  graph  specialized  for  this  partic‐\n",
            "ular  combination.  Such  a  combination  of  argument  types  and  shapes  is  called  an\n",
            "input  signature.  If  you  call  the  TF  function  with  an  input  signature  it  has  already\n",
            "seen  before,  it  will  reuse  the  concrete  function  it  generated  earlier.  For  example,  if\n",
            "you call tf_cube(tf.constant(3.0)), the TF function will reuse the same concrete\n",
            "function  it  used  for  tf_cube(tf.constant(2.0))  (for  float32  scalar  tensors).  But  it\n",
            "will  generate  a  new  concrete  function  if  you  call  tf_cube(tf.constant([2.0]))  or\n",
            "tf_cube(tf.constant([3.0])) (for float32 tensors of shape [1]), and yet another for\n",
            "tf_cube(tf.constant([[1.0,  2.0],  [3.0,  4.0]]))  (for  float32  tensors  of  shape\n",
            "[2,  2]).  You  can  get  the  concrete  function  for  a  particular  combination  of  inputs  by\n",
            "calling  the  TF  function’s  get_concrete_function()  method.  It  can  then  be  called\n",
            "like a regular function, but it will only support one input signature (in this example,\n",
            "float32 scalar tensors):\n",
            "\n",
            "801\n",
            "\n",
            "\f>>> concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n",
            ">>> concrete_function\n",
            "<ConcreteFunction tf_cube(x) at 0x7F84411F4250>\n",
            ">>> concrete_function(tf.constant(2.0))\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n",
            "\n",
            "Figure  D-1  shows  the  tf_cube()  TF  function,  after  we  called  tf_cube(2)  and\n",
            "tf_cube(tf.constant(2.0)):  two  concrete  functions  were  generated,  one  for  each\n",
            "signature,  each  with  its  own  optimized  function  graph  (FuncGraph)  and  its  own\n",
            "function  definition  (FunctionDef).  A  function  definition  points  to  the  parts  of  the\n",
            "graph  that  correspond  to  the  function’s  inputs  and  outputs.  In  each  FuncGraph,  the\n",
            "nodes  (ovals)  represent  operations  (e.g.,  power,  constants,  or  placeholders  for  argu‐\n",
            "ments  like  x),  while  the  edges  (the  solid  arrows  between  the  operations)  represent\n",
            "the  tensors  that  will  flow  through  the  graph.  The  concrete  function  on  the  left  is\n",
            "specialized  for  x=2,  so  TensorFlow  managed  to  simplify  it  to  just  output  8  all  the\n",
            "time  (note  that  the  function  definition  does  not  even  have  an  input).  The  concrete\n",
            "function  on  the  right  is  specialized  for  float32  scalar  tensors,  and  it  could  not  be\n",
            "simplified. If we call tf_cube(tf.constant(5.0)), the second concrete function will\n",
            "be  called,  the  placeholder  operation  for  x  will  output  5.0,  then  the  power  operation\n",
            "will compute 5.0 ** 3, so the output will be 125.0.\n",
            "\n",
            "Figure D-1. The tf_cube() TF function, with its ConcreteFunctions and their\n",
            "FuncGraphs\n",
            "\n",
            "The tensors in these graphs are symbolic tensors, meaning they don’t have an actual\n",
            "value,  just  a  data  type,  a  shape,  and  a  name.  They  represent  the  future  tensors  that\n",
            "will flow through the graph once an actual value is fed to the placeholder x and the\n",
            "graph is executed. Symbolic tensors make it possible to specify ahead of time how to\n",
            "connect operations, and they also allow TensorFlow to recursively infer the data types\n",
            "and shapes of all tensors, given the data types and shapes of their inputs.\n",
            "\n",
            "802 \n",
            "\n",
            "|  Appendix D: TensorFlow Graphs\n",
            "\n",
            "\fNow let’s continue to peek under the hood, and see how to access function definitions\n",
            "and function graphs and how to explore a graph’s operations and tensors.\n",
            "\n",
            "Exploring Function Definitions and Graphs\n",
            "You  can  access  a  concrete  function’s  computation  graph  using  the  graph  attribute,\n",
            "and get the list of its operations by calling the graph’s get_operations() method:\n",
            "\n",
            ">>> concrete_function.graph\n",
            "<tensorflow.python.framework.func_graph.FuncGraph at 0x7f84411f4790>\n",
            ">>> ops = concrete_function.graph.get_operations()\n",
            ">>> ops\n",
            "[<tf.Operation 'x' type=Placeholder>,\n",
            " <tf.Operation 'pow/y' type=Const>,\n",
            " <tf.Operation 'pow' type=Pow>,\n",
            " <tf.Operation 'Identity' type=Identity>]\n",
            "\n",
            "In  this  example,  the  first  operation  represents  the  input  argument  x  (it  is  called  a\n",
            "placeholder),  the  second  “operation”  represents  the  constant  3,  the  third  operation\n",
            "represents the power operation (**), and the final operation represents the output of\n",
            "this function (it is an identity operation, meaning it will do nothing more than copy\n",
            "the  output  of  the  power  operation1).  Each  operation  has  a  list  of  input  and  output\n",
            "tensors that you can easily access using the operation’s inputs and outputs attributes.\n",
            "For example, let’s get the list of inputs and outputs of the power operation:\n",
            "\n",
            ">>> pow_op = ops[2]\n",
            ">>> list(pow_op.inputs)\n",
            "[<tf.Tensor 'x:0' shape=() dtype=float32>,\n",
            " <tf.Tensor 'pow/y:0' shape=() dtype=float32>]\n",
            ">>> pow_op.outputs\n",
            "[<tf.Tensor 'pow:0' shape=() dtype=float32>]\n",
            "\n",
            "This computation graph is represented in Figure D-2.\n",
            "\n",
            "1 You can safely ignore it—it is only here for technical reasons, to ensure that TF functions don’t leak internal\n",
            "\n",
            "structures.\n",
            "\n",
            "TensorFlow Graphs \n",
            "\n",
            "| \n",
            "\n",
            "803\n",
            "\n",
            "\fFigure D-2. Example of a computation graph\n",
            "\n",
            "Note  that  each  operation  has  a  name.  It  defaults  to  the  name  of  the  operation  (e.g.,\n",
            "\"pow\"),  but  you  can  define  it  manually  when  calling  the  operation  (e.g.,  tf.pow(x,\n",
            "3,  name=\"other_name\")).  If  a  name  already  exists,  TensorFlow  automatically  adds\n",
            "a  unique  index  (e.g.,  \"pow_1\",  \"pow_2\",  etc.).  Each  tensor  also  has  a  unique  name:\n",
            "it  is  always  the  name  of  the  operation  that  outputs  this  tensor,  plus  :0  if  it  is  the\n",
            "operation’s  first  output,  or  :1  if  it  is  the  second  output,  and  so  on.  You  can  fetch\n",
            "an  operation  or  a  tensor  by  name  using  the  graph’s  get_operation_by_name()  or\n",
            "get_tensor_by_name() methods:\n",
            "\n",
            ">>> concrete_function.graph.get_operation_by_name('x')\n",
            "<tf.Operation 'x' type=Placeholder>\n",
            ">>> concrete_function.graph.get_tensor_by_name('Identity:0')\n",
            "<tf.Tensor 'Identity:0' shape=() dtype=float32>\n",
            "\n",
            "The concrete function also contains the function definition (represented as a protocol\n",
            "buffer2),  which  includes  the  function’s  signature.  This  signature  allows  the  concrete\n",
            "function to know which placeholders to feed with the input values, and which tensors\n",
            "to return:\n",
            "\n",
            ">>> concrete_function.function_def.signature\n",
            "name: \"__inference_tf_cube_3515903\"\n",
            "input_arg {\n",
            "  name: \"x\"\n",
            "  type: DT_FLOAT\n",
            "}\n",
            "output_arg {\n",
            "  name: \"identity\"\n",
            "  type: DT_FLOAT\n",
            "}\n",
            "\n",
            "2 A popular binary format discussed in Chapter 13.\n",
            "\n",
            "804 \n",
            "\n",
            "|  Appendix D: TensorFlow Graphs\n",
            "\n",
            "\fNow let’s look more closely at tracing.\n",
            "\n",
            "A Closer Look at Tracing\n",
            "Let’s tweak the tf_cube() function to print its input:\n",
            "\n",
            "@tf.function\n",
            "def tf_cube(x):\n",
            "    print(f\"x = {x}\")\n",
            "    return x ** 3\n",
            "\n",
            "Now let’s call it:\n",
            "\n",
            ">>> result = tf_cube(tf.constant(2.0))\n",
            "x = Tensor(\"x:0\", shape=(), dtype=float32)\n",
            ">>> result\n",
            "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n",
            "\n",
            "The result looks good, but look at what was printed: x is a symbolic tensor! It has a\n",
            "shape and a data type, but no value. Plus it has a name (\"x:0\"). This is because the\n",
            "print() function is not a TensorFlow operation, so it will only run when the Python\n",
            "function  is  traced,  which  happens  in  graph  mode,  with  arguments  replaced  with\n",
            "symbolic  tensors  (same  type  and  shape,  but  no  value).  Since  the  print()  function\n",
            "was not captured into the graph, the next times we call tf_cube() with float32 scalar\n",
            "tensors, nothing is printed:\n",
            "\n",
            ">>> result = tf_cube(tf.constant(3.0))\n",
            ">>> result = tf_cube(tf.constant(4.0))\n",
            "\n",
            "But  if  we  call  tf_cube()  with  a  tensor  of  a  different  type  or  shape,  or  with  a  new\n",
            "Python value, the function will be traced again, so the print() function will be called:\n",
            "\n",
            ">>> result = tf_cube(2)  # new Python value: trace!\n",
            "x = 2\n",
            ">>> result = tf_cube(3)  # new Python value: trace!\n",
            "x = 3\n",
            ">>> result = tf_cube(tf.constant([[1., 2.]]))  # new shape: trace!\n",
            "x = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\n",
            ">>> result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))  # new shape: trace!\n",
            "x = Tensor(\"x:0\", shape=(None, 2), dtype=float32)\n",
            ">>> result = tf_cube(tf.constant([[7., 8.], [9., 10.]]))  # same shape: no trace\n",
            "\n",
            "If your function has Python side effects (e.g., it saves some logs to\n",
            "disk),  be  aware  that  this  code  will  only  run  when  the  function  is\n",
            "traced  (i.e.,  every  time  the  TF  function  is  called  with  a  new  input\n",
            "signature).  It’s  best  to  assume  that  the  function  may  be  traced  (or\n",
            "not) any time the TF function is called.\n",
            "\n",
            "TensorFlow Graphs \n",
            "\n",
            "| \n",
            "\n",
            "805\n",
            "\n",
            "\fIn some cases, you may want to restrict a TF function to a specific input signature.\n",
            "For  example,  suppose  you  know  that  you  will  only  ever  call  a  TF  function  with\n",
            "batches  of  28  ×  28–pixel  images,  but  the  batches  will  have  very  different  sizes.  You\n",
            "may  not  want  TensorFlow  to  generate  a  different  concrete  function  for  each  batch\n",
            "size, or count on it to figure out on its own when to use None. In this case, you can\n",
            "specify the input signature like this:\n",
            "\n",
            "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
            "def shrink(images):\n",
            "    return images[:, ::2, ::2]  # drop half the rows and columns\n",
            "\n",
            "This TF function will accept any float32 tensor of shape [*, 28, 28], and it will reuse\n",
            "the same concrete function every time:\n",
            "\n",
            "img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\n",
            "img_batch_2 = tf.random.uniform(shape=[50, 28, 28])\n",
            "preprocessed_images = shrink(img_batch_1)  # works fine, traces the function\n",
            "preprocessed_images = shrink(img_batch_2)  # works fine, same concrete function\n",
            "\n",
            "However,  if  you  try  to  call  this  TF  function  with  a  Python  value,  or  a  tensor  of  an\n",
            "unexpected data type or shape, you will get an exception:\n",
            "\n",
            "img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\n",
            "preprocessed_images = shrink(img_batch_3)  # ValueError! Incompatible inputs\n",
            "\n",
            "Using AutoGraph to Capture Control Flow\n",
            "If  your  function  contains  a  simple  for  loop,  what  do  you  expect  will  happen?  For\n",
            "example, let’s write a function that will add 10 to its input, by just adding 1 10 times:\n",
            "\n",
            "@tf.function\n",
            "def add_10(x):\n",
            "    for i in range(10):\n",
            "        x += 1\n",
            "    return x\n",
            "\n",
            "It works fine, but when we look at its graph, we find that it does not contain a loop: it\n",
            "just contains 10 addition operations!\n",
            "\n",
            ">>> add_10(tf.constant(0))\n",
            "<tf.Tensor: shape=(), dtype=int32, numpy=15>\n",
            ">>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n",
            "[<tf.Operation 'x' type=Placeholder>, [...],\n",
            " <tf.Operation 'add' type=AddV2>, [...],\n",
            " <tf.Operation 'add_1' type=AddV2>, [...],\n",
            " <tf.Operation 'add_2' type=AddV2>, [...],\n",
            " [...]\n",
            " <tf.Operation 'add_9' type=AddV2>, [...],\n",
            " <tf.Operation 'Identity' type=Identity>]\n",
            "\n",
            "806 \n",
            "\n",
            "|  Appendix D: TensorFlow Graphs\n",
            "\n",
            "\fThis actually makes sense: when the function got traced, the loop ran 10 times, so the\n",
            "x += 1 operation was run 10 times, and since it was in graph mode, it recorded this\n",
            "operation 10 times in the graph. You can think of this for loop as a “static” loop that\n",
            "gets unrolled when the graph is created.\n",
            "\n",
            "If you want the graph to contain a “dynamic” loop instead (i.e., one that runs when\n",
            "the graph is executed), you can create one manually using the tf.while_loop() oper‐\n",
            "ation, but it is not very intuitive (see the “Using AutoGraph to Capture Control Flow”\n",
            "section  of  the  Chapter  12  notebook  for  an  example).  Instead,  it  is  much  simpler  to\n",
            "use TensorFlow’s AutoGraph feature, discussed in Chapter 12. AutoGraph is actually\n",
            "activated  by  default  (if  you  ever  need  to  turn  it  off,  you  can  pass  autograph=False\n",
            "to tf.function()). So if it is on, why didn’t it capture the for loop in the add_10()\n",
            "function?  It  only  captures  for  loops  that  iterate  over  tensors  of  tf.data.Dataset\n",
            "objects, so you should use tf.range(), not range(). This is to give you the choice:\n",
            "\n",
            "• If you use range(), the for loop will be static, meaning it will only be executed\n",
            "•\n",
            "when the function is traced. The loop will be “unrolled” into a set of operations\n",
            "for each iteration, as we saw.\n",
            "\n",
            "• If you use tf.range(), the loop will be dynamic, meaning that it will be included\n",
            "•\n",
            "\n",
            "in the graph itself (but it will not run during tracing).\n",
            "\n",
            "Let’s look at the graph that gets generated if we just replace range() with tf.range()\n",
            "in the add_10() function:\n",
            "\n",
            ">>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n",
            "[<tf.Operation 'x' type=Placeholder>, [...],\n",
            " <tf.Operation 'while' type=StatelessWhile>, [...]]\n",
            "\n",
            "As you can see, the graph now contains a While loop operation, as if we had called the\n",
            "tf.while_loop() function.\n",
            "\n",
            "Handling Variables and Other Resources in TF Functions\n",
            "In  TensorFlow,  variables  and  other  stateful  objects,  such  as  queues  or  datasets,  are\n",
            "called resources. TF functions treat them with special care: any operation that reads or\n",
            "updates a resource is considered stateful, and TF functions ensure that stateful opera‐\n",
            "tions are executed in the order they appear (as opposed to stateless operations, which\n",
            "may be run in parallel, so their order of execution is not guaranteed). Moreover, when\n",
            "you pass a resource as an argument to a TF function, it gets passed by reference, so\n",
            "the function may modify it. For example:\n",
            "\n",
            "TensorFlow Graphs \n",
            "\n",
            "| \n",
            "\n",
            "807\n",
            "\n",
            "\fcounter = tf.Variable(0)\n",
            "\n",
            "@tf.function\n",
            "def increment(counter, c=1):\n",
            "    return counter.assign_add(c)\n",
            "\n",
            "increment(counter)  # counter is now equal to 1\n",
            "increment(counter)  # counter is now equal to 2\n",
            "\n",
            "If you peek at the function definition, the first argument is marked as a resource:\n",
            "\n",
            ">>> function_def = increment.get_concrete_function(counter).function_def\n",
            ">>> function_def.signature.input_arg[0]\n",
            "name: \"counter\"\n",
            "type: DT_RESOURCE\n",
            "\n",
            "It  is  also  possible  to  use  a  tf.Variable  defined  outside  of  the  function,  without\n",
            "explicitly passing it as an argument:\n",
            "\n",
            "counter = tf.Variable(0)\n",
            "\n",
            "@tf.function\n",
            "def increment(c=1):\n",
            "    return counter.assign_add(c)\n",
            "\n",
            "The  TF  function  will  treat  this  as  an  implicit  first  argument,  so  it  will  actually  end\n",
            "up  with  the  same  signature  (except  for  the  name  of  the  argument).  However,  using\n",
            "global  variables  can  quickly  become  messy,  so  you  should  generally  wrap  variables\n",
            "(and other resources) inside classes. The good news is @tf.function works fine with\n",
            "methods too:\n",
            "\n",
            "class Counter:\n",
            "    def __init__(self):\n",
            "        self.counter = tf.Variable(0)\n",
            "\n",
            "    @tf.function\n",
            "    def increment(self, c=1):\n",
            "        return self.counter.assign_add(c)\n",
            "\n",
            "Do not use =, +=, -=, or any other Python assignment operator with\n",
            "TF  variables.  Instead,  you  must  use  the  assign(),  assign_add(),\n",
            "or  assign_sub()  methods.  If  you  try  to  use  a  Python  assignment\n",
            "operator, you will get an exception when you call the method.\n",
            "\n",
            "A good example of this object-oriented approach is, of course, Keras. Let’s see how to\n",
            "use TF functions with Keras.\n",
            "\n",
            "808 \n",
            "\n",
            "|  Appendix D: TensorFlow Graphs\n",
            "\n",
            "\fUsing TF Functions with Keras (or Not)\n",
            "By default, any custom function, layer, or model you use with Keras will automatically\n",
            "be  converted  to  a  TF  function;  you  do  not  need  to  do  anything  at  all!  However,  in\n",
            "some  cases  you  may  want  to  deactivate  this  automatic  conversion—for  example,  if\n",
            "your custom code cannot be turned into a TF function, or if you just want to debug\n",
            "your  code  (which  is  much  easier  in  eager  mode).  To  do  this,  you  can  simply  pass\n",
            "dynamic=True when creating the model or any of its layers:\n",
            "\n",
            "model = MyModel(dynamic=True)\n",
            "\n",
            "If your custom model or layer will always be dynamic, you can instead call the base\n",
            "class’s constructor with dynamic=True:\n",
            "\n",
            "class MyDense(tf.keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(dynamic=True, **kwargs)\n",
            "        [...]\n",
            "\n",
            "Alternatively, you can pass run_eagerly=True when calling the compile() method:\n",
            "\n",
            "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae],\n",
            "              run_eagerly=True)\n",
            "\n",
            "Now  you  know  how  TF  functions  handle  polymorphism  (with  multiple  concrete\n",
            "functions),  how  graphs  are  automatically  generated  using  AutoGraph  and  tracing,\n",
            "what graphs look like, how to explore their symbolic operations and tensors, how to\n",
            "handle variables and resources, and how to use TF functions with Keras.\n",
            "\n",
            "TensorFlow Graphs \n",
            "\n",
            "| \n",
            "\n",
            "809\n",
            "\n",
            "\f\fIndex\n",
            "\n",
            "Symbols\n",
            "@ operator (matrix multiplication), 135\n",
            "β (momentum), 380\n",
            "γ (gamma) value, 182\n",
            "ε (tolerance), 144, 183\n",
            "ε greedy policy, 706\n",
            "ε neighborhood, 279\n",
            "ε sensitive, 184\n",
            "χ² test, 202\n",
            "ℓ₀ norm, 45\n",
            "ℓ₁ norm, 45\n",
            "ℓ₂ norm, 45\n",
            "ℓₖ norm, 45\n",
            "\n",
            "A\n",
            "A/B experiments, 721\n",
            "accelerated k-means, 268\n",
            "accelerated linear algebra (XLA), 434\n",
            "accuracy performance measure, 4, 107\n",
            "ACF (autocorrelation function), 551\n",
            "action advantage, reinforcement learning, 694\n",
            "action potentials (APs), 301\n",
            "actions, in reinforcement learning, 684,\n",
            "\n",
            "693-694\n",
            "\n",
            "activation functions, 306, 312-313\n",
            "\n",
            "for Conv2D layer, 490\n",
            "custom models, 415\n",
            "ELU, 363-366\n",
            "GELU, 365-367\n",
            "hyperbolic tangent (htan), 312, 565\n",
            "hyperparameters, 352\n",
            "initialization parameters, 360\n",
            "leakyReLU, 361-363\n",
            "Mish, 366\n",
            "\n",
            "PReLU, 362\n",
            "ReLU (see ReLU)\n",
            "RReLU, 362\n",
            "SELU, 364, 397\n",
            "sigmoid, 164, 312, 358, 651\n",
            "SiLU, 366\n",
            "softmax, 170, 315, 320, 600\n",
            "softplus, 314\n",
            "Swish, 366\n",
            "active learning, 278\n",
            "actor-critic, 717\n",
            "actual class, 109\n",
            "actual versus estimated probabilities, 118\n",
            "AdaBoost, 222-226\n",
            "AdaGrad, 382\n",
            "Adam optimization, 384, 394\n",
            "AdaMax, 385\n",
            "AdamW, 386, 394\n",
            "adaptive boosting (AdaBoost), 222-226\n",
            "adaptive instance normalization (AdaIN), 671\n",
            "adaptive learning rate algorithms, 382-387\n",
            "adaptive moment estimation (Adam), 384\n",
            "additive attention, 606\n",
            "advantage actor-critic (A2C), 717\n",
            "adversarial learning, 534, 636\n",
            "affine transformations, 671\n",
            "affinity function, 261\n",
            "affinity propagation, 283\n",
            "agents, reinforcement learning, 16, 684, 699,\n",
            "\n",
            "705\n",
            "\n",
            "agglomerative clustering, 282\n",
            "Akaike information criterion (AIC), 289\n",
            "AlexNet, 499\n",
            "algorithms, preparing data for, 67-88\n",
            "\n",
            "811\n",
            "\n",
            "\falignment model, 606\n",
            "AllReduce algorithm, 760\n",
            "alpha dropout, 397\n",
            "AlphaGo, 17, 683, 716\n",
            "anchor priors, 528\n",
            "ANNs (see artificial neural networks)\n",
            "anomaly detection, 8, 13\n",
            "clustering for, 262\n",
            "GMM, 288-289\n",
            "isolation forest, 293\n",
            "AP (average precision), 529\n",
            "APs (action potentials), 301\n",
            "area under the curve (AUC), 116\n",
            "argmax(), 171\n",
            "ARIMA model, 550-551\n",
            "ARMA model family, 549-551\n",
            "artificial neural networks (ANNs), 299-353\n",
            "\n",
            "autoencoders (see autoencoders)\n",
            "backpropagation, 309-312\n",
            "biological neurons as background to,\n",
            "\n",
            "301-302\n",
            "\n",
            "evolution of, 300-301\n",
            "hyperparameter fine-tuning, 344-353\n",
            "implementing MLPs with Keras, 317-344\n",
            "logical computations with neurons, 303-303\n",
            "perceptrons (see multilayer perceptrons)\n",
            "reinforcement learning policies, 691\n",
            "\n",
            "artificial neuron, 303\n",
            "association rule learning, 14\n",
            "assumptions, checking in model building, 37,\n",
            "\n",
            "46\n",
            "\n",
            "asynchronous advantage actor-critic (A3C),\n",
            "\n",
            "717\n",
            "\n",
            "asynchronous gang scheduling, 764\n",
            "asynchronous updates, with centralized param‐\n",
            "\n",
            "eters, 761\n",
            "\n",
            "à-trous convolutional layer, 533\n",
            "attention mechanisms, 578, 604-619, 625\n",
            "\n",
            "(see also transformer models)\n",
            "\n",
            "attributes, 52-55\n",
            "\n",
            "categorical, 71, 74\n",
            "combinations of, 66-67\n",
            "preprocessed, 55\n",
            "target, 55\n",
            "unsupervised learning, 11\n",
            "AUC (area under the curve), 116\n",
            "autocorrelated time series, 545\n",
            "autocorrelation function (ACF), 551\n",
            "autodiff (automatic differentiation), 785-791\n",
            "\n",
            "812 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "for computing gradients, 426-430\n",
            "finite difference approximation, 786\n",
            "forward-mode, 787-790\n",
            "manual differentiation, 785\n",
            "reverse-mode, 790-791\n",
            "\n",
            "autoencoders, 635-659\n",
            "\n",
            "convolutional, 648-649\n",
            "denoising, 649-650\n",
            "efficient data representations, 637-638\n",
            "overcomplete, 649\n",
            "PCA with undercomplete linear autoen‐\n",
            "\n",
            "coder, 639-640\n",
            "\n",
            "sparse, 651-654\n",
            "stacked, 640-648\n",
            "training one at a time, 646-648\n",
            "undercomplete, 638\n",
            "variational, 654-658\n",
            "\n",
            "Autograph, 435\n",
            "AutoGraph, 806\n",
            "AutoML service, 349, 775\n",
            "autoregressive integrated moving average\n",
            "\n",
            "(ARIMA) model, 550-551\n",
            "\n",
            "autoregressive model, 549\n",
            "autoregressive moving average (ARMA) model\n",
            "\n",
            "family, 549-551\n",
            "\n",
            "auxiliary task, pretraining on, 378\n",
            "average absolute deviation, 45\n",
            "average pooling layer, 493\n",
            "average precision (AP), 529\n",
            "\n",
            "B\n",
            "backbone, model, 504\n",
            "backpropagation, 309-312, 358, 429, 660\n",
            "backpropagation through time (BPTT), 542\n",
            "bagging (bootstrap aggregating), 215-219\n",
            "Bahdanau attention, 606\n",
            "balanced iterative reducing and clustering using\n",
            "\n",
            "hierarchies (BIRCH), 282\n",
            "bandwidth saturation, 763-765\n",
            "BaseEstimator, 80\n",
            "batch gradient descent, 142-144, 156\n",
            "batch learning, 18-19\n",
            "batch normalization (BN), 367-372, 565\n",
            "batch predictions, 731, 732, 739\n",
            "Bayesian Gaussian mixtures, 292\n",
            "Bayesian information criterion (BIC), 289\n",
            "beam search, 603-604\n",
            "Bellman optimality equation, 701\n",
            "bias, 155\n",
            "\n",
            "\fbias and fairness, NLP transformers, 630\n",
            "bias term constant, 132, 137\n",
            "bias/variance trade-off, 155\n",
            "BIC (Bayesian information criterion), 289\n",
            "bidirectional recurrent layer, 601\n",
            "binary classifiers, 106, 164\n",
            "binary logarithm, 200\n",
            "binary trees, 198, 282\n",
            "biological neural networks (BNNs), 301-302\n",
            "BIRCH (balanced iterative reducing and clus‐\n",
            "\n",
            "tering using hierarchies), 282\n",
            "\n",
            "black box models, 199\n",
            "blending, in stacking, 232\n",
            "BN (batch normalization), 367, 565\n",
            "BNNs (biological neural networks), 301-302\n",
            "boosting, 222-231\n",
            "bootstrap aggregating (bagging), 215-219\n",
            "bootstrapping, 215\n",
            "bottleneck layers, 503\n",
            "bounding boxes, image identification, 521-530\n",
            "BPTT (backpropagation through time), 542\n",
            "bucketizing a feature, 77\n",
            "byte pair encoding (BPE), 588\n",
            "\n",
            "C\n",
            "California Housing Prices dataset, 40-46\n",
            "callbacks, 338-340\n",
            "CART (Classification and Regression Tree)\n",
            "\n",
            "algorithm, 198, 200, 205\n",
            "catastrophic forgetting, 712\n",
            "categorical attributes, 71, 74\n",
            "categorical features, encoding, 466-471\n",
            "CategoryEncoding layer, 463\n",
            "causal model, 563, 601\n",
            "centralized parameters, 760-762\n",
            "centroid, cluster, 262, 264, 265-268\n",
            "chain rule, 311\n",
            "chain-of-thought prompting, 623\n",
            "ChainClassifier, 126\n",
            "chaining transformations, 443-445\n",
            "char-RNN model, 578-586\n",
            "chatbot or personal assistant, 8\n",
            "check_estimator(), 82\n",
            "chi-squared (χ²) test, 202\n",
            "classification, 103-128\n",
            "\n",
            "application examples of, 8-9\n",
            "binary classifier, 106, 164\n",
            "CNNs, 521-530\n",
            "error analysis, 122-125\n",
            "\n",
            "hard margin, 176, 187\n",
            "hard voting classifiers, 212\n",
            "image (see images)\n",
            "logistic regression (see logistic regression)\n",
            "MLPs for, 315-316\n",
            "MNIST dataset, 103-105\n",
            "multiclass, 119-121, 315-316\n",
            "multilabel, 125-127\n",
            "multioutput, 127-128\n",
            "performance measures, 107-119\n",
            "and regression, 10, 127\n",
            "soft margin, 177-178\n",
            "softmax regression, 170-173\n",
            "SVMs (see support vector machines)\n",
            "text, 587-595\n",
            "voting classifiers, 212-215\n",
            "\n",
            "Classification and Regression Tree (CART)\n",
            "\n",
            "algorithm, 198, 200, 205\n",
            "\n",
            "clone(), 163\n",
            "closed-form equation/solution, 131, 134, 157,\n",
            "\n",
            "166\n",
            "\n",
            "cloud platform deployment with Vertex AI,\n",
            "\n",
            "732-739\n",
            "\n",
            "clustering algorithms, 9, 11, 260-283\n",
            "\n",
            "affinity propagation, 283\n",
            "agglomerative clustering, 282\n",
            "applications for, 261-262\n",
            "BIRCH, 282\n",
            "DBSCAN, 279-281\n",
            "GMM, 283-294\n",
            "image segmentation, 262, 273-275\n",
            "k-means (see k-means algorithm)\n",
            "mean-shift, 282\n",
            "responsibilities of clusters for instances, 285\n",
            "semi-supervised learning with, 275-278\n",
            "spectral clustering, 283\n",
            "\n",
            "CNNs (see convolutional neural networks)\n",
            "Colab, 46-48\n",
            "color channels, 486\n",
            "color segmentation, images, 273\n",
            "column vectors, 133\n",
            "ColumnTransformer, 85\n",
            "complex models with functional API, 329-335\n",
            "compound scaling, 513\n",
            "compressed TFRecord files, 454\n",
            "compression and decompression, PCA,\n",
            "\n",
            "249-250\n",
            "\n",
            "computation graphs, 404\n",
            "computational complexity\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "813\n",
            "\n",
            "\fDBSCAN, 281\n",
            "decision trees, 200\n",
            "Gaussian mixture model, 288\n",
            "histogram-based gradient boosting, 230\n",
            "k-means algorithm, 265\n",
            "Normal equation, 138\n",
            "and SVM classes, 183\n",
            "concatenative attention, 606\n",
            "concrete function, 801-805\n",
            "conditional GAN, 668\n",
            "conditional probability, 603\n",
            "confidence interval, 96\n",
            "confusion matrix (CM), 108-110, 122-125\n",
            "ConfusionMatrixDisplay, 122\n",
            "connectionism, 300\n",
            "constrained optimization, 187\n",
            "constraints, custom models, 415\n",
            "convergence rate, 144\n",
            "convex function, 141\n",
            "convex quadratic optimization, 188\n",
            "convolution kernels (kernels), 484, 496\n",
            "convolutional neural networks (CNNs),\n",
            "\n",
            "479-535\n",
            "architectures, 495-515\n",
            "autoencoders, 648-649\n",
            "classification and localization, 521-530\n",
            "convolutional layers, 481-491, 531-535, 573,\n",
            "\n",
            "574-575, 591\n",
            "evolution of, 479\n",
            "GANs, 665-667\n",
            "object detection, 523-530\n",
            "object tracking, 530\n",
            "pooling layers, 491-495\n",
            "pretrained models from Keras, 516-518\n",
            "ResNet-34 CNN using Keras, 515\n",
            "semantic segmentation, 8, 273, 531-535\n",
            "splitting across devices, 757\n",
            "transfer learning pretrained models,\n",
            "\n",
            "518-521\n",
            "U-Net, 678\n",
            "and vision transformers, 624\n",
            "visual cortex architecture, 480\n",
            "WaveNet, 574-575\n",
            "copy.deepcopy(), 163\n",
            "core instance, 279\n",
            "correlation coefficient, 63-66\n",
            "cost function, 24, 45\n",
            "in AdaBoost, 223\n",
            "in autoencoders, 639\n",
            "\n",
            "814 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "in bounding box prediction model, 522\n",
            "in CART training algorithm, 200, 205\n",
            "in elastic net, 161\n",
            "in gradient descent, 132, 138-142, 145-148,\n",
            "\n",
            "358\n",
            "\n",
            "in lasso regression, 158-161\n",
            "in Nesterov accelerated gradient, 381\n",
            "in linear regression, 134\n",
            "in logistic regression, 165-166\n",
            "in momentum optimization, 379\n",
            "in ridge regression, 156\n",
            "in variational autoencoders, 656\n",
            "credit assignment problem, 693-694\n",
            "cross entropy, 171\n",
            "cross-validation, 35, 89-91, 97, 107-108,\n",
            "\n",
            "152-155\n",
            "\n",
            "cross_val_predict(), 108, 113, 117, 122, 232\n",
            "cross_val_score(), 90, 107\n",
            "CUDA library, 747\n",
            "curiosity-based exploration, 718\n",
            "curriculum learning, 719\n",
            "custom models and training algorithms,\n",
            "\n",
            "412-433\n",
            "activation functions, 415\n",
            "autodiff for computing gradients, 426-430\n",
            "constraints, 415\n",
            "initializers, 415\n",
            "layers, 419-422\n",
            "loss functions, 412, 424-426\n",
            "metrics, 416-419, 424-426\n",
            "models, 422-424\n",
            "regularizers, 415\n",
            "saving and loading models, 413-415\n",
            "training loops, 430-433\n",
            "custom transformers, 79-83\n",
            "customer segmentation, 261\n",
            "\n",
            "D\n",
            "DALL-E, 628\n",
            "data\n",
            "\n",
            "downloading, 48-49\n",
            "efficient data representations, 637-638\n",
            "enqueuing and dequeuing, 798\n",
            "finding correlations in, 63-66\n",
            "making assumptions about, 37\n",
            "overfitting (see overfitting of data)\n",
            "preparing for ML algorithms, 67-88\n",
            "preparing for ML models, 552-555\n",
            "\n",
            "\fpreprocessing (see loading and preprocess‐\n",
            "\n",
            "ing data)\n",
            "shuffling of, 445\n",
            "test data (see test set)\n",
            "time series (see time series data)\n",
            "training data (see training set)\n",
            "underfitting of, 89, 151-155, 180\n",
            "unreasonable effectiveness, 27\n",
            "visualizing (see visualization)\n",
            "working with real data, 39-40\n",
            "data analysis, clustering for, 261\n",
            "data augmentation, 129, 500, 519\n",
            "data cleaning, 68-71\n",
            "data drift, 18\n",
            "data mining, 7\n",
            "data mismatch, 35\n",
            "data parallelism, 759-765\n",
            "data pipeline, 42\n",
            "data snooping bias, 56\n",
            "data structure, 51-55, 793-799\n",
            "data-efficient image transformers (DeiT), 627\n",
            "DataFrame, 68, 74, 78\n",
            "Dataquest, xxi\n",
            "Datasets library, 475\n",
            "DBSCAN, 279-281\n",
            "DCGANs (deep convolutional GANS), 665-667\n",
            "DDPM (denoising diffusion probabilistic\n",
            "\n",
            "model), 636, 674-681\n",
            "DDQN (dueling DQN), 715\n",
            "decision boundaries, 167-169, 172, 198, 207,\n",
            "\n",
            "264\n",
            "\n",
            "decision function, 111, 186-189\n",
            "decision stumps, 226\n",
            "decision threshold, 111-115\n",
            "decision trees, 195-208, 211\n",
            "\n",
            "(see also ensemble learning)\n",
            "bagging and pasting, 217\n",
            "CART training algorithm, 200\n",
            "class probability estimates, 199\n",
            "computational complexity, 200\n",
            "and decision boundaries, 198\n",
            "GINI impurity or entropy measures, 201\n",
            "high variance with, 207\n",
            "predictions, 197-200\n",
            "regression tasks, 204-205\n",
            "regularization hyperparameters, 201-203\n",
            "sensitivity to axis orientation, 206\n",
            "training and visualizing, 195-196\n",
            "in training the model, 89-91\n",
            "\n",
            "DecisionTreeClassifier, 195, 201, 202, 207, 220\n",
            "DecisionTreeRegressor, 89, 195, 204, 226\n",
            "decision_function(), 112\n",
            "deconvolution layer, 532\n",
            "deep autoencoders (see stacked autoencoders)\n",
            "deep convolutional GANS (DCGANs), 665-667\n",
            "deep Gaussian process, 397\n",
            "deep learning, xv\n",
            "\n",
            "(see also deep neural networks; reinforce‐\n",
            "\n",
            "ment learning)\n",
            "\n",
            "deep neural networks (DNNs), 357-401\n",
            "\n",
            "CNNs (see convolutional neural networks)\n",
            "default configuration, 400\n",
            "faster optimizers for, 379-387\n",
            "learning rate scheduling, 388-392\n",
            "MLPs (see multilayer perceptrons)\n",
            "regularization, 392-400\n",
            "reusing pretrained layers, 373-379\n",
            "RNNs (see recurrent neural networks)\n",
            "and transfer learning, 16\n",
            "unstable gradients, 358\n",
            "vanishing and exploding gradients, 358-373\n",
            "\n",
            "deep neuroevolution, 349\n",
            "deep Q-learning, 707-716\n",
            "deep Q-networks (DQNs) (see Q-learning algo‐\n",
            "\n",
            "rithm)\n",
            "\n",
            "deepcopy(), 163\n",
            "DeepMind, 17\n",
            "degrees of freedom, 32, 155\n",
            "DeiT (data-efficient image transformers), 627\n",
            "denoising autoencoders, 649-650\n",
            "denoising diffusion probabilistic model\n",
            "\n",
            "(DDPM), 636, 674-681\n",
            "Dense layer, 305, 320, 321, 332\n",
            "dense matrix, 76, 86\n",
            "density estimation, 260, 279-281, 286\n",
            "density threshold, 288\n",
            "depth concatenation layer, 502\n",
            "depthwise separable convolution layer, 509\n",
            "deque, 709\n",
            "describe(), 53\n",
            "development set (dev set), 35\n",
            "differencing, time series forecasting, 545, 548,\n",
            "\n",
            "549\n",
            "\n",
            "diffusion models, 635, 673-681\n",
            "dilated filter, 533\n",
            "dilation rate, 533\n",
            "dimensionality reduction, 12, 237-257\n",
            "\n",
            "approaches to, 239-242\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "815\n",
            "\n",
            "\fautoencoders, 635, 638-640\n",
            "choosing the right number of dimensions,\n",
            "\n",
            "EllipticEnvelope, 293\n",
            "ELMo (Embeddings from Language Models),\n",
            "\n",
            "247\n",
            "\n",
            "593\n",
            "\n",
            "clustering, 261\n",
            "curse of dimensionality, 238-239\n",
            "for data visualization, 238\n",
            "information loss from, 237\n",
            "Isomap, 256\n",
            "linear discriminant analysis, 257\n",
            "LLE, 254-256\n",
            "multidimensional scaling, 256\n",
            "PCA (see principal component analysis)\n",
            "random projection algorithm, 252-254\n",
            "t-distributed stochastic neighbor embed‐\n",
            "\n",
            "ding, 256, 643\n",
            "\n",
            "discount factor γ in reward system, 693\n",
            "discounted rewards, 693, 696\n",
            "Discretization layer, 463\n",
            "discriminator, GAN, 636, 660-665\n",
            "DistilBERT model, 622, 631-633\n",
            "distillation, 622\n",
            "distribution strategies API, 765\n",
            "Docker container, 726\n",
            "dot product, 607\n",
            "Dota 2, 718\n",
            "Double DQN, 714\n",
            "downloading data, 48-49\n",
            "DQNs (deep Q-networks) (see Q-learning algo‐\n",
            "\n",
            "rithm)\n",
            "drop(), 68\n",
            "dropna(), 68\n",
            "Dropout, 649\n",
            "dropout rate, 394\n",
            "dropout regularization, 394-397\n",
            "dual numbers, 788\n",
            "dual problem, 189-193\n",
            "dueling DQN (DDQN), 715\n",
            "dummy attributes, 72\n",
            "dying ReLU problem, 361\n",
            "dynamic models with subclassing API, 336-337\n",
            "dynamic programming, 701\n",
            "\n",
            "E\n",
            "eager execution (eager mode), 436\n",
            "early stopping regularization, 162-163, 229, 555\n",
            "edge computing, 741\n",
            "efficient data representations, autoencoders,\n",
            "\n",
            "637-638\n",
            "elastic net, 161\n",
            "\n",
            "816 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "ELU (exponential linear unit), 363-366\n",
            "EM (expectation-maximization), 284\n",
            "embedded device, deploying model to, 741-744\n",
            "embedded Reber grammars, 633\n",
            "embedding matrix, 468, 600\n",
            "embedding size, 599, 613\n",
            "embeddings, 74\n",
            "\n",
            "encoding categorical features using, 466-471\n",
            "reusing pretrained, 593-595\n",
            "sentiment analysis, 589\n",
            "\n",
            "Embeddings from Language Models (ELMo),\n",
            "\n",
            "593\n",
            "encoder, 637\n",
            "\n",
            "(see also autoencoders)\n",
            "\n",
            "encoder–decoder models, 541, 578, 595-604\n",
            "\n",
            "(see also attention mechanisms)\n",
            "end-to-end ML project exercise, 39-100\n",
            "\n",
            "building the model, 41-46\n",
            "discovering and visualizing data, 60-67\n",
            "fine-tuning your model, 91-97\n",
            "getting the data, 46-60\n",
            "preparing data for ML algorithms, 67-88\n",
            "real data, advantages of working with, 39-40\n",
            "selecting and training a model, 88-89\n",
            "endpoint, deploying model on GCP, 738\n",
            "ensemble learning, 211-235\n",
            "\n",
            "bagging and pasting, 215-219\n",
            "boosting, 222-231\n",
            "cross-validation, 90\n",
            "fine-tuning the system, 95\n",
            "random forests (see random forests)\n",
            "stacking, 232-235\n",
            "voting classifiers, 212-215\n",
            "\n",
            "entailment, 620\n",
            "entropy impurity measure, 201\n",
            "environments, reinforcement learning, 684,\n",
            "\n",
            "687-691\n",
            "epochs, 143\n",
            "equalized learning rate, GAN, 670\n",
            "equivariance, 493\n",
            "error analysis, classification, 122-125\n",
            "estimated versus actual probabilities, 118\n",
            "estimators, 70, 84, 214\n",
            "Euclidean norm, 45\n",
            "event files, TensorBoard, 340\n",
            "Example protobuf, 456\n",
            "\n",
            "\fexclusive or (XOR) problem, 307\n",
            "exemplars, affinity propagation, 283\n",
            "expectation-maximization (EM), 284\n",
            "experience replay, 664\n",
            "explainability, attention mechanisms, 625\n",
            "explained variance ratio, 246\n",
            "explained variance, plotting, 247-249\n",
            "exploding gradients, 358\n",
            "\n",
            "(see also vanishing and exploding gradients)\n",
            "\n",
            "exploration policies, 703, 706\n",
            "exploration/exploitation dilemma, reinforce‐\n",
            "\n",
            "ment learning, 692\n",
            "\n",
            "exponential linear unit (ELU), 363-366\n",
            "exponential scheduling, 389, 392\n",
            "export_graphviz(), 196\n",
            "extra-trees, random forest, 220\n",
            "extremely randomized trees ensemble (extra-\n",
            "\n",
            "trees), 221\n",
            "\n",
            "F\n",
            "face-recognition classifier, 125\n",
            "false negatives, confusion matrix, 109\n",
            "false positive rate (FPR) or fall-out, 115\n",
            "false positives, confusion matrix, 109\n",
            "fan-in/fan-out, 359\n",
            "fast-MCD, 293\n",
            "FCNs (fully convolutional networks), 525-526,\n",
            "\n",
            "532\n",
            "\n",
            "feature engineering, 30, 262\n",
            "feature extraction, 12, 30\n",
            "feature maps, 485-487, 488, 508, 511\n",
            "feature scaling, 75-79, 141, 176\n",
            "feature selection, 30, 95, 159, 221\n",
            "feature vectors, 45, 133, 186\n",
            "features, 11\n",
            "federated learning, 746\n",
            "feedforward neural network (FNN), 309, 538\n",
            "fetch_openml(), 104\n",
            "fillna(), 68\n",
            "filters, convolutional layers, 484, 487, 497, 509\n",
            "first moment (mean of gradient), 384\n",
            "first-order partial derivatives (Jacobians), 386\n",
            "fit()\n",
            "\n",
            "and custom transformers, 80, 84\n",
            "data cleaning, 69\n",
            "versus partial_fit(), 148\n",
            "using only with training set, 75\n",
            "\n",
            "fitness function, 24\n",
            "fit_transform(), 70, 75, 80, 84\n",
            "\n",
            "fixed Q-value targets, 713\n",
            "flat dataset, 553\n",
            "flowers dataset, 518-519\n",
            "FNN (feedforward neural network), 309, 538\n",
            "folds, 90, 105, 107, 108\n",
            "forecasting time series (see time series data)\n",
            "forget gate, LSTM, 569\n",
            "forward pass, in backpropagation, 311\n",
            "forward process, diffusion model, 674-676\n",
            "FPR (false positive rate) or fall-out, 115\n",
            "from_predictions(), 122\n",
            "full gradient descent, 143\n",
            "fully connected layer, 305, 481, 491\n",
            "fully convolutional networks (FCNs), 525-526,\n",
            "\n",
            "532\n",
            "\n",
            "function definition (FuncDef), 802\n",
            "function graph (FuncGraph), 802\n",
            "functional API, complex models with, 329-335\n",
            "FunctionTransformer, 79\n",
            "F₁ score, 111\n",
            "\n",
            "G\n",
            "game play (see reinforcement learning)\n",
            "gamma (γ) value, 182\n",
            "GANs (see generative adversarial networks)\n",
            "gate controllers, LSTM, 570\n",
            "gated activation units, 574\n",
            "gated recurrent unit (GRU) cell, 571-572, 591\n",
            "Gaussian distribution, 76, 166, 656-657\n",
            "Gaussian mixture model (GMM), 283-294\n",
            "\n",
            "anomaly detection, 288-289\n",
            "Bayesian Gaussian mixtures, 292\n",
            "fast-MCD, 293\n",
            "inverse_transform() with PCA, 294\n",
            "isolation forest, 293\n",
            "and k-means limitations, 272\n",
            "local outlier factor, 293\n",
            "one-class SVM, 294\n",
            "selecting number of clusters, 289-291\n",
            "\n",
            "Gaussian process, 348\n",
            "Gaussian RBF kernel, 80, 181-184, 192\n",
            "GBRT (gradient boosted regression trees), 226,\n",
            "\n",
            "227-231\n",
            "\n",
            "GCP (Google Cloud Platform), 732-737\n",
            "GCS (Google Cloud Storage), 736\n",
            "GD (see gradient descent)\n",
            "GELU, 365-367\n",
            "generalization error, 34-35\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "817\n",
            "\n",
            "\fgenerative adversarial networks (GANs),\n",
            "\n",
            "stochastic gradient descent, 145-148\n",
            "\n",
            "659-673\n",
            "deep convolutional, 665-667\n",
            "progressive growing of, 668-671\n",
            "StyleGANs, 671-673\n",
            "training difficulties, 663-665\n",
            "\n",
            "generative autoencoders, 654\n",
            "generative models, 635\n",
            "\n",
            "gradient tree boosting, 226\n",
            "gradients\n",
            "\n",
            "autodiff for computing, 426-430\n",
            "bandwidth saturation issue, 763\n",
            "PG algorithm, 687, 694-698\n",
            "stale, 761\n",
            "unstable (see vanishing and exploding gra‐\n",
            "\n",
            "(see also Gaussian mixture model)\n",
            "\n",
            "dients)\n",
            "\n",
            "generator, GAN, 636, 659-665\n",
            "genetic algorithm, 686\n",
            "geodesic distance, 256\n",
            "geographic data, visualizing, 61-62\n",
            "get_dummies(), 73\n",
            "get_feature_names_out(), 81\n",
            "get_params(), 80\n",
            "GINI impurity, 197, 201\n",
            "global average pooling layer, 495\n",
            "global versus local minimum, gradient descent,\n",
            "\n",
            "140\n",
            "\n",
            "Glorot initialization, 359-361\n",
            "GMM (see Gaussian mixture model)\n",
            "Google Cloud Platform (GCP), 732-737\n",
            "Google Cloud Storage (GCS), 736\n",
            "Google Colab, 46-48\n",
            "Google Vertex AI (see Vertex AI)\n",
            "GoogLeNet, 502-505, 509\n",
            "GPU implementation, 148, 746-756, 765\n",
            "getting your own GPU, 747-749\n",
            "managing RAM, 749-752\n",
            "operations handling, 754\n",
            "parallel execution across multiple devices,\n",
            "\n",
            "753-756\n",
            "\n",
            "placing operations and variables on devices,\n",
            "\n",
            "752-753\n",
            "gradient ascent, 687\n",
            "gradient boosted regression trees (GBRT), 226,\n",
            "\n",
            "227-231\n",
            "\n",
            "gradient boosting, 226-230\n",
            "gradient clipping, 372\n",
            "gradient descent (GD), 132, 138-149\n",
            "algorithm comparisons, 148\n",
            "batch gradient descent, 142-144, 156\n",
            "local versus global minimum, 140\n",
            "mini-batch gradient descent, 148-149\n",
            "minimizing hinge loss, 188\n",
            "versus momentum optimization, 380\n",
            "with optimizers, 379-387\n",
            "shuffling data, 445\n",
            "\n",
            "818 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "graph mode, 436\n",
            "graphical processing units (see GPU implemen‐\n",
            "\n",
            "tation)\n",
            "\n",
            "graphs and functions, TensorFlow, 404,\n",
            "\n",
            "433-438, 801-809\n",
            "\n",
            "Graphviz, 196\n",
            "greedy algorithm, CART as, 200\n",
            "greedy decoding, 582\n",
            "greedy layer-wise pretraining, 377, 647, 668\n",
            "grid search, 91-93\n",
            "GridSearchCV, 91-93\n",
            "gRPC API, querying through, 729\n",
            "GRU (gated recurrent unit) cell, 571-572, 591\n",
            "\n",
            "H\n",
            "hard clustering, 264\n",
            "hard margin classification, 176, 187\n",
            "hard voting classifiers, 212\n",
            "harmonic mean, 111\n",
            "hashing collision, 466\n",
            "Hashing layer, 466\n",
            "hashing trick, 466\n",
            "HDBSCAN (hierarchical DBSCAN), 281\n",
            "He initialization, 359-361\n",
            "Heaviside step function, 304\n",
            "heavy tail, feature distribution, 76\n",
            "Hebb's rule, 306\n",
            "Hebbian learning, 306\n",
            "Hessians, 386\n",
            "hidden layers\n",
            "\n",
            "neurons per layer, 350\n",
            "number of, 349\n",
            "stacked autoencoders, 640-648\n",
            "\n",
            "hierarchical clustering, 11\n",
            "hierarchical DBSCAN (HDBSCAN), 281\n",
            "high variance, with decision trees, 207\n",
            "hinge loss function, 188\n",
            "histogram-based gradient boosting (HGB),\n",
            "\n",
            "230-231\n",
            "histograms, 54\n",
            "\n",
            "\fhold-out sets, 34\n",
            "holdout validation, 34\n",
            "housing dataset, 40-46\n",
            "Huber loss, 314, 412, 416, 555\n",
            "Hugging Face, 629-633\n",
            "Hungarian algorithm, 531\n",
            "Hyperband tuner, 347\n",
            "hyperbolic tangent (htan), 312, 565\n",
            "hyperparameters, 32, 344-353\n",
            "activation function, 352\n",
            "batch size, 352\n",
            "CART algorithm, 200\n",
            "convolutional layers, 490\n",
            "in custom transformations, 80\n",
            "decision tree, 228\n",
            "dimensionality reduction, 248\n",
            "gamma (γ) value, 182\n",
            "GAN challenges, 664\n",
            "Keras Tuner, 775\n",
            "learning rate, 139, 351\n",
            "momentum β, 380\n",
            "Monte Carlo samples, 399\n",
            "neurons per hidden layer, 350\n",
            "and normalization, 76\n",
            "number of hidden layers, 349\n",
            "number of iterations, 352\n",
            "optimizer, 352\n",
            "PG algorithms, 697\n",
            "preprocessor and model interaction, 92\n",
            "randomized search, 344-346\n",
            "saving along with model, 416\n",
            "SGDClassifier, 183\n",
            "subsample, 230\n",
            "SVM classifiers with polynomial kernel, 180\n",
            "tolerance (ε), 183\n",
            "tuning of, 34-35, 91-93, 97, 327, 772-776\n",
            "\n",
            "hypothesis, 45\n",
            "hypothesis boosting (see boosting)\n",
            "hypothesis function, 133\n",
            "\n",
            "I\n",
            "identity matrix, 157\n",
            "IID (see independent and identically dis‐\n",
            "\n",
            "tributed)\n",
            "\n",
            "image generation, 534, 671, 673\n",
            "image segmentation, 262, 273-275\n",
            "images, classifying and generating, 8\n",
            "autoencoders (see autoencoders)\n",
            "CNNs (see convolutional neural networks)\n",
            "\n",
            "diffusion models, 673-681\n",
            "generating with GANs, 659-673\n",
            "implementing MLPs, 318-328\n",
            "labels, 521\n",
            "loading and preprocessing data, 474\n",
            "representative images, 276\n",
            "semantic segmentation, 273\n",
            "tuning hyperparameters, 344\n",
            "\n",
            "importance sampling (IS), 714\n",
            "impurity measures, 197, 201\n",
            "imputation, 68\n",
            "incremental learning, 20\n",
            "incremental PCA (IPCA), 250-251\n",
            "independent and identically distributed (IID),\n",
            "\n",
            "training instances as, 147\n",
            "\n",
            "inductive bias, 626\n",
            "inertia, model, 267, 269\n",
            "inference, 26\n",
            "info(), 52\n",
            "information theory, 171, 201\n",
            "inliers, 260\n",
            "input and output sequences, RNNs, 541-542\n",
            "input gate, LSTM, 569\n",
            "input layer, neural network, 305, 320, 331\n",
            "\n",
            "(see also hidden layers)\n",
            "\n",
            "input signature, 801\n",
            "instance segmentation, 273, 534\n",
            "instance-based learning, 21, 26\n",
            "inter-op thread pool, 754\n",
            "intercept term constant, 132\n",
            "interleaving lines from multiple files, 446-447\n",
            "interpretable ML, 199\n",
            "invariance, max pooling layer, 492\n",
            "inverse_transform(), 78, 81, 249, 294\n",
            "IPCA (incremental PCA), 250-251\n",
            "iris dataset, 167\n",
            "irreducible error, 155\n",
            "IS (importance sampling), 714\n",
            "isolation forest, 293\n",
            "Isomap, 256\n",
            "isotropic noise, 674\n",
            "IterativeImputer, 69\n",
            "\n",
            "J\n",
            "Jacobians, 386\n",
            "joblib library, 97-98\n",
            "JSON Lines, 739, 741\n",
            "Jupyter, 46\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "819\n",
            "\n",
            "\fK\n",
            "k-fold cross-validation, 90, 107, 108\n",
            "k-means algorithm, 82, 263-273\n",
            "accelerated k-means, 268\n",
            "centroid initialization methods, 267-268\n",
            "finding optimal number of clusters, 269-271\n",
            "limitations of, 272\n",
            "mini-batch k-means, 268\n",
            "workings of, 265-266\n",
            "\n",
            "k-means++, 267\n",
            "k-nearest neighbors regression, 26\n",
            "Kaiming initialization, 360\n",
            "Kalman Filters, 530\n",
            "Keras API, xvi, 405\n",
            "\n",
            "(see also artificial neural networks)\n",
            "and accessing TensorFlow API directly, 475\n",
            "activation function support, 362, 363, 367\n",
            "convolutional layer implementation,\n",
            "\n",
            "487-490\n",
            "\n",
            "custom functions in, 435\n",
            "gradient clipping, 373\n",
            "image preprocessing layers, 474\n",
            "implementing MLPs with, 317-344\n",
            "initialization handling, 360\n",
            "initializers, 322\n",
            "layer preprocessing, 459-474\n",
            "learning rate scheduling, 390-392\n",
            "loading a dataset, 318\n",
            "PG algorithm, 694-698\n",
            "pool layer implementation, 493-495\n",
            "pretrained CNN models, 516-518\n",
            "ResNet-34 CNN with, 515\n",
            "saving models, 724, 742\n",
            "stacked encoder implementation, 641\n",
            "tf.data API dataset, 452-453\n",
            "tf.keras library, 318\n",
            "tf.keras.activations.get(), 420\n",
            "tf.keras.activations.relu(), 320\n",
            "tf.keras.applications module, 516\n",
            "tf.keras.applications.xception.prepro‐\n",
            "\n",
            "cess_input(), 519\n",
            "\n",
            "tf.keras.backend module, 408\n",
            "tf.keras.callbacks.EarlyStopping, 339\n",
            "tf.keras.callbacks.LearningRateScheduler,\n",
            "\n",
            "390\n",
            "\n",
            "tf.keras.callbacks.ModelCheckpoint, 338\n",
            "tf.keras.callbacks.TensorBoard, 341, 592\n",
            "tf.keras.datasets.imdb.load_data(), 587\n",
            "tf.keras.initializers.VarianceScaling, 360\n",
            "\n",
            "820 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "tf.keras.layers.ActivityRegularization, 652\n",
            "tf.keras.layers.AdditiveAttention, 608\n",
            "tf.keras.layers.Attention, 608\n",
            "tf.keras.layers.AvgPool2D, 495\n",
            "tf.keras.layers.BatchNormalization, 369,\n",
            "\n",
            "370-372, 566\n",
            "\n",
            "tf.keras.layers.Bidirectional, 602\n",
            "tf.keras.layers.CategoryEncoding, 463\n",
            "tf.keras.layers.CenterCrop, 474\n",
            "tf.keras.layers.Concatenate, 331, 332, 502\n",
            "tf.keras.layers.Conv1D, 533, 592\n",
            "tf.keras.layers.Conv2D, 487\n",
            "tf.keras.layers.Conv2DTranspose, 532\n",
            "tf.keras.layers.Conv3D, 533\n",
            "tf.keras.layers.Dense, 331, 332, 420-421, 471,\n",
            "\n",
            "646, 657\n",
            "\n",
            "tf.keras.layers.Discretization, 463\n",
            "tf.keras.layers.Dropout, 396\n",
            "tf.keras.layers.Embedding, 468, 581, 589,\n",
            "\n",
            "613\n",
            "\n",
            "tf.keras.layers.GlobalAvgPool2D, 495\n",
            "tf.keras.layers.GRU, 572\n",
            "tf.keras.layers.GRUCell, 572\n",
            "tf.keras.layers.Hashing, 466\n",
            "tf.keras.layers.Input, 331\n",
            "tf.keras.layers.Lambda, 419, 582\n",
            "tf.keras.layers.LayerNormalization, 566\n",
            "tf.keras.layers.LeakyReLU, 362\n",
            "tf.keras.layers.LSTM, 568\n",
            "tf.keras.layers.Masking, 591\n",
            "tf.keras.layers.MaxPool2D, 493\n",
            "tf.keras.layers.MultiHeadAttention, 617\n",
            "tf.keras.layers.Normalization, 329, 331,\n",
            "\n",
            "460-462\n",
            "\n",
            "tf.keras.layers.PReLU, 362\n",
            "tf.keras.layers.Rescaling, 474\n",
            "tf.keras.layers.Resizing, 474, 517\n",
            "tf.keras.layers.RNN, 568\n",
            "tf.keras.layers.SeparableConv2D, 510\n",
            "tf.keras.layers.StringLookup, 465\n",
            "tf.keras.layers.TextVectorization, 471-473,\n",
            "\n",
            "579, 582, 588, 589-599\n",
            "\n",
            "tf.keras.layers.TimeDistributed, 564\n",
            "tf.keras.losses.Huber, 412\n",
            "tf.keras.losses.kullback_leibler_diver‐\n",
            "\n",
            "gence(), 653\n",
            "\n",
            "tf.keras.losses.Loss, 413\n",
            "tf.keras.losses.sparse_categorical_crossen‐\n",
            "\n",
            "tropy(), 323, 497, 582, 596, 632\n",
            "\n",
            "\ftf.keras.metrics.MeanIoU, 522\n",
            "tf.keras.metrics.Metric, 418\n",
            "tf.keras.metrics.Precision, 417\n",
            "tf.keras.Model, 331\n",
            "tf.keras.models.clone_model(), 337, 375\n",
            "tf.keras.models.load_model(), 338, 413-415,\n",
            "\n",
            "large margin classification, 175\n",
            "Lasso, 161\n",
            "lasso regression, 158-161\n",
            "latent diffusion models, 680\n",
            "latent loss, 656\n",
            "latent representation of inputs, 627, 635, 637,\n",
            "\n",
            "423, 766\n",
            "\n",
            "667\n",
            "\n",
            "tf.keras.optimizers.Adam, 329, 386\n",
            "tf.keras.optimizers.Adamax, 386\n",
            "tf.keras.optimizers.experimental.AdamW,\n",
            "\n",
            "386\n",
            "\n",
            "tf.keras.optimizers.Nadam, 386\n",
            "tf.keras.optimizers.schedules, 391\n",
            "tf.keras.optimizers.SGD, 380\n",
            "tf.keras.preprocessing.image.ImageData‐\n",
            "\n",
            "Generator, 519\n",
            "\n",
            "tf.keras.regularizers.l1_l2(), 393\n",
            "tf.keras.Sequential, 319, 329, 497\n",
            "tf.keras.utils.get_file(), 579\n",
            "tf.keras.utils.set_random_seed(), 319\n",
            "tf.keras.utils.timeseries_data‐\n",
            "set_from_array(), 552, 555\n",
            "tf.keras.utils.to_categorical(), 324\n",
            "time series forecasting for RNN, 556-558\n",
            "transfer learning with, 375-377\n",
            "using TF functions (or not), 809\n",
            "\n",
            "Keras session, 322\n",
            "Keras Tuner, 775\n",
            "kernel trick, 180-184, 190-193\n",
            "kernelized SVMs, 190-193\n",
            "kernels (convolution kernels), 484, 496\n",
            "kernels (runtimes), 48\n",
            "KL (Kullback-Leibler) divergence, 172, 653\n",
            "KLDivergenceRegularizer, 653\n",
            "KMeans, 81\n",
            "KNeighborsClassifier, 125, 128\n",
            "KNNImputer, 69\n",
            "Kullback-Leibler (KL) divergence, 172, 652\n",
            "\n",
            "L\n",
            "label propagation, 277-278\n",
            "labels, 43\n",
            "\n",
            "in clustering, 263\n",
            "image classification, 521\n",
            "supervised learning, 11\n",
            "unlabeled data issue, 645\n",
            "\n",
            "landmarks, 181\n",
            "language models, 578\n",
            "\n",
            "(see also natural language processing)\n",
            "\n",
            "latent space, 656\n",
            "law of large numbers, 213\n",
            "layer normalization, 439, 537, 566\n",
            "LDA (linear discriminant analysis), 257\n",
            "leaf node, decision tree, 197, 199, 202\n",
            "leakyReLU, 361-363\n",
            "learning curves, overfit or underfit analysis,\n",
            "\n",
            "152-155\n",
            "\n",
            "learning rate, 20, 139, 143, 145, 351, 705\n",
            "learning rate schedules, 388-392\n",
            "learning schedules, 145\n",
            "LeCun initialization, 360\n",
            "LeNet-5, 481, 498\n",
            "Levenshtein distance, 182\n",
            "liblinear library, 183\n",
            "libsvm library, 183\n",
            "life satisfaction dataset, 22\n",
            "likelihood function, 290-291\n",
            "linear discriminant analysis (LDA), 257\n",
            "linear models\n",
            "\n",
            "forecasting time series, 555\n",
            "linear regression (see linear regression)\n",
            "regularized, 156-163\n",
            "SVM, 175-178\n",
            "training and running example, 23\n",
            "\n",
            "linear regression, 24-26, 132-149\n",
            "\n",
            "comparison of algorithms, 149\n",
            "computational complexity, 138\n",
            "gradient descent in, 138-149\n",
            "learning curves in, 151-155\n",
            "Normal equation, 134-137\n",
            "regularizing models (see regularization)\n",
            "ridge regression, 156-158, 161\n",
            "training set evaluation, 88\n",
            "using stochastic gradient descent, 147\n",
            "linear SVM classification, 175-178, 186-193\n",
            "linear threshold units (LTUs), 304\n",
            "linearly separable, SVM classes, 175-176, 181\n",
            "LinearRegression, 71, 79, 88, 137, 138, 150\n",
            "LinearSVC, 177, 183, 188\n",
            "LinearSVR, 184-186\n",
            "Lipschitz continuous, derivative as, 141\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "821\n",
            "\n",
            "\fLLE (locally linear embedding), 254-256\n",
            "loading and preprocessing data, 441-477\n",
            "image preprocessing layers, 474\n",
            "layer preprocessing in Keras, 442, 459-474\n",
            "tf.data API, 441-453\n",
            "TFDS project, 475-477\n",
            "TFRecord format, 441, 453-459\n",
            "\n",
            "local outlier factor (LOF), 293\n",
            "local receptive field, 480\n",
            "local response normalization (LRN), 501\n",
            "local versus global minimum, gradient descent,\n",
            "\n",
            "140\n",
            "\n",
            "locality sensitive hashing (LSH), 254\n",
            "localization, CNNs, 521-525\n",
            "locally linear embedding (LLE), 254-256\n",
            "LOF (local outlier factor), 293\n",
            "log loss, 166\n",
            "log-odds function, 165\n",
            "log-transformer, 79\n",
            "logical GPU device, 750\n",
            "logistic function, 164\n",
            "logistic regression, 10, 78, 164-173\n",
            "\n",
            "decision boundaries illustration, 167-169\n",
            "estimating probabilities, 164-165\n",
            "softmax regression model, 170-173\n",
            "training and cost function, 165-166\n",
            "\n",
            "LogisticRegression, 168, 172\n",
            "logit function, 165\n",
            "long sequences, training RNN on, 565-575\n",
            "short-term memory problem, 568-575\n",
            "unstable gradients problem, 565-568\n",
            "\n",
            "long short-term memory (LSTM) cell, 568-571,\n",
            "\n",
            "591, 599, 602\n",
            "\n",
            "loss functions\n",
            "\n",
            "based on model internals, 424-426\n",
            "custom, 412\n",
            "versus metrics, 416\n",
            "output, 334\n",
            "\n",
            "LRN (local response normalization), 501\n",
            "LSH (locality sensitive hashing), 254\n",
            "LSTM (long short-term memory) cell, 568-571,\n",
            "\n",
            "591, 599, 602\n",
            "\n",
            "LTUs (linear threshold units), 304\n",
            "Luong attention, 607\n",
            "\n",
            "M\n",
            "machine learning (ML), 4\n",
            "\n",
            "application/technique examples, 8-9\n",
            "challenges of, 27-33\n",
            "\n",
            "822 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "notations, 44-45\n",
            "project checklist, 41\n",
            "\n",
            "(see also end-to-end ML project exer‐\n",
            "\n",
            "cise)\n",
            "\n",
            "reasons for using, 5-7\n",
            "resources on, xxi-xxii\n",
            "spam filter example, 3-7\n",
            "testing and validating, 34-37\n",
            "types of systems, 9-26\n",
            "\n",
            "MAE (mean absolute error), 45\n",
            "majority-vote predictions, 209\n",
            "make_column_selector(), 86\n",
            "make_column_transformer(), 86\n",
            "make_pipeline(), 84, 154\n",
            "Manhattan norm, 45\n",
            "manifold hypothesis, 242\n",
            "manifold learning, dimension reduction,\n",
            "\n",
            "241-242\n",
            "\n",
            "MAP (maximum a-posteriori) estimation, 291\n",
            "mAP (mean average precision), 528\n",
            "MAPE (mean absolute percentage error), 546\n",
            "mapping network, StyleGANs, 671\n",
            "MapReduce, 43\n",
            "margin violations, 177, 187\n",
            "Markov chains, 699\n",
            "Markov decision processes (MDPs), 699-703,\n",
            "\n",
            "706\n",
            "\n",
            "mask R-CNN, 534\n",
            "mask tensor, 590\n",
            "masked language model (MLM), 620\n",
            "masked multi-head attention layer, 612\n",
            "masking, 590-593, 617\n",
            "Matching Engine service, Vertex AI, 732\n",
            "Matplotlib, 46\n",
            "max pooling layer, 491, 497\n",
            "max-norm regularization, 399\n",
            "maximization step, Gaussian mixtures, 284\n",
            "maximum a-posteriori (MAP) estimation, 291\n",
            "maximum likelihood estimate (MLE), 291\n",
            "MC (Monte Carlo) dropout regularization,\n",
            "\n",
            "397-399\n",
            "\n",
            "MCTS (Monte Carlo tree search), 716\n",
            "MDPs (Markov decision processes), 699-703,\n",
            "\n",
            "706\n",
            "\n",
            "MDS (multidimensional scaling), 256\n",
            "mean absolute error (MAE), 45\n",
            "mean absolute percentage error (MAPE), 546\n",
            "mean average precision (mAP), 528\n",
            "mean squared error (MSE), 133, 656\n",
            "\n",
            "\fmean-shift, clustering algorithms, 282\n",
            "mean_squared_error(), 88\n",
            "measure of similarity, 21\n",
            "memory bandwidth, GPU card, 451\n",
            "memory cells (cells), RNNs, 540, 568-575\n",
            "memory requirements, convolutional layers,\n",
            "\n",
            "490\n",
            "\n",
            "Mercer's theorem, 192\n",
            "meta learner, 232\n",
            "metagraphs, SavedModel, 724\n",
            "min-max scaling, 75\n",
            "mini-batch discrimination, 664\n",
            "mini-batch gradient descent, 148-149, 163\n",
            "mini-batch k-means, 268\n",
            "mini-batches, 19, 669\n",
            "MinMaxScaler, 76\n",
            "mirrored strategy, data parallelism, 759, 765,\n",
            "\n",
            "768\n",
            "\n",
            "Mish activation function, 366\n",
            "mixing regularization, StyleGAN, 673\n",
            "ML (see machine learning)\n",
            "ML Operations (MLOps), 100\n",
            "MLE (maximum likelihood estimate), 291\n",
            "MLM (masked language model), 620\n",
            "MLPs (see multilayer perceptrons)\n",
            "MNIST dataset, 103-105\n",
            "mobile device, deploying model to, 741-744\n",
            "mode collapse, 628, 664\n",
            "model parallelism, 756-758\n",
            "model parameters, 23\n",
            "\n",
            "early stopping regularization, 166\n",
            "in gradient descent, 142, 144\n",
            "linear SVM classifier mechanics, 186\n",
            "and variable updating, 410\n",
            "weight matrix shape, 323\n",
            "\n",
            "model rot, 18\n",
            "model selection, 23, 34-35\n",
            "model server (see TensorFlow Serving)\n",
            "model warmup, 730\n",
            "model-based learning, 22-26\n",
            "modes, 77\n",
            "momentum optimization, 379\n",
            "momentum β, 380\n",
            "Monte Carlo (MC) dropout, 397-399\n",
            "Monte Carlo tree search (MCTS), 716\n",
            "MSE (mean squared error), 133, 656\n",
            "multi-head attention layer, 611, 615-619\n",
            "multi-hot encoding, 464\n",
            "\n",
            "multiclass (multinomial) classification,\n",
            "\n",
            "119-121, 315-316\n",
            "\n",
            "multidimensional scaling (MDS), 256\n",
            "multilabel classifiers, 125-127\n",
            "multilayer perceptrons (MLPs), 299, 308-344\n",
            "\n",
            "and autoencoders, 638, 639\n",
            "and backpropagation, 309-312\n",
            "callbacks, 338-340\n",
            "classification MLPs, 315-316\n",
            "complex models, 329-335\n",
            "dynamic models, 336-337\n",
            "image classifier, 318-328\n",
            "regression MLPs, 313-314, 328\n",
            "saving and restoring a model, 337-338\n",
            "visualization with TensorBoard, 340-344\n",
            "\n",
            "multimodal distribution, 77\n",
            "multimodal transformers, 627\n",
            "multinomial logistic regression, 170-173\n",
            "multioutput classifiers, 127-128\n",
            "multiple regression, 43\n",
            "multiplicative attention, 607\n",
            "multitask classification, 333\n",
            "multivariate regression, 43\n",
            "multivariate time series, 545, 559-560\n",
            "\n",
            "N\n",
            "Nadam, 386\n",
            "NAG (Nesterov accelerated gradient), 381, 386\n",
            "naive forecasting, 545\n",
            "Nash equilibrium, 663\n",
            "natural language processing (NLP), 577-633\n",
            "\n",
            "char-RNN model to generate text, 578-586\n",
            "encoder–decoder network for machine\n",
            "\n",
            "translation, 595-601\n",
            "\n",
            "machine learning examples, 8\n",
            "sentiment analysis, 587-595\n",
            "text classification, 587-595\n",
            "text encoding, 471-474\n",
            "text summarization, 8\n",
            "transformer models (see transformer mod‐\n",
            "\n",
            "els)\n",
            "\n",
            "word embeddings, 467\n",
            "\n",
            "natural language understanding (NLU), 8\n",
            "NCCL (Nvidia collective communications\n",
            "\n",
            "library), 766\n",
            "\n",
            "NEAT (neuroevolution of augmenting topolo‐\n",
            "\n",
            "gies), 686\n",
            "\n",
            "negative class, 109, 164\n",
            "nested dataset, 553\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "823\n",
            "\n",
            "\fNesterov accelerated gradient (NAG), 381, 386\n",
            "Nesterov momentum optimization, 381, 386\n",
            "neural machine translation (NMT), 595-619\n",
            "and attention mechanisms, 604-619\n",
            "with transformers, 620-624\n",
            "\n",
            "neural networks (see artificial neural networks)\n",
            "neuroevolution of augmenting topologies\n",
            "\n",
            "(NEAT), 686\n",
            "\n",
            "next sentence prediction (NSP), 621\n",
            "NLU (natural language understanding), 8\n",
            "NMT (see neural machine translation)\n",
            "No Free Lunch theorem, 37\n",
            "non-max suppression, bounding boxes, 524\n",
            "nonlinear dimensionality reduction (NLDR),\n",
            "\n",
            "254-256\n",
            "\n",
            "nonlinear SVM classifiers, 178-184\n",
            "nonparametric models, 201\n",
            "nonrepresentative training data, 28\n",
            "nonresponse bias, 29\n",
            "normal distribution, 358, 360\n",
            "Normal equation, 134-137\n",
            "normalization, 76, 367-372, 565\n",
            "Normalization layer, 460-462\n",
            "normalized exponential (softmax function),\n",
            "\n",
            "170\n",
            "\n",
            "notations, 44-45, 134\n",
            "novelty detection, 13, 288\n",
            "NP-Complete problem, CART as, 200\n",
            "NSP (next sentence prediction), 621\n",
            "nucleus sampling, 584\n",
            "null hypothesis, 202\n",
            "number of inputs, 323, 359\n",
            "number of neurons per hidden layer, 350\n",
            "NumPy, 46\n",
            "NumPy arrays, 71, 79, 407-411\n",
            "Nvidia collective communications library\n",
            "\n",
            "(NCCL), 766\n",
            "\n",
            "Nvidia GPU card, 748-749\n",
            "\n",
            "O\n",
            "OAuth 2.0, 734\n",
            "object detection, CNNs, 523-530\n",
            "object tracking, CNNs, 530\n",
            "objectness score, 523\n",
            "observation space, reinforcement learning, 684,\n",
            "\n",
            "693\n",
            "\n",
            "observations, 689-690\n",
            "OCR (optical character recognition), 3\n",
            "OEL (open-ended learning), 719\n",
            "\n",
            "824 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "off-policy algorithm, 706\n",
            "offline learning, 18\n",
            "on-policy algorithm, 706\n",
            "one-class SVM, 294\n",
            "one-hot encoding, 72-74, 464, 471\n",
            "one-versus-all (OvA) strategy, 119-121\n",
            "one-versus-one (OvO) strategy, 119-121\n",
            "one-versus-the-rest (OvR) strategy, 119-121\n",
            "1cycle scheduling, 389, 392\n",
            "1D convolutional layers, 533, 573\n",
            "OneHotEncoder, 72-74, 77, 86\n",
            "online kernelized SVMs, 193\n",
            "online learning, 19-20\n",
            "online model, DQN, 713\n",
            "OOB (out-of-bag) evaluation, 218-219\n",
            "open-ended learning (OEL), 719\n",
            "OpenAI Gym, 687-691\n",
            "operations (ops) and tensors, 404, 407-408\n",
            "optical character recognition (OCR), 3\n",
            "optimal state value, MDP, 701\n",
            "optimizers, 379-387\n",
            "AdaGrad, 382\n",
            "Adam optimization, 384\n",
            "AdaMax, 385\n",
            "AdamW, 386\n",
            "hyperparameters, 352\n",
            "momentum optimization, 379\n",
            "Nadam, 386\n",
            "Nesterov accelerated gradient, 381\n",
            "output layer, 600\n",
            "RMSProp, 383\n",
            "\n",
            "oracle, 346\n",
            "order of integration (d) hyperparameter, 549\n",
            "OrdinalEncoder, 71\n",
            "orthogonal matrix, 600\n",
            "out-of-bag (OOB) evaluation, 218-219\n",
            "out-of-core learning, 19\n",
            "out-of-sample error, 34\n",
            "outlier detection (see anomaly detection)\n",
            "outliers, 260\n",
            "output gate, LSTM, 569\n",
            "output layer, neural network, 600\n",
            "OvA (one-versus-all) strategy, 119-121\n",
            "overcomplete autoencoder, 649\n",
            "overfitting of data, 30-33, 55\n",
            "\n",
            "avoiding through regularization, 392-400\n",
            "and decision trees, 201, 205\n",
            "and dropout regularization, 396\n",
            "gamma (γ) hyperparameter to adjust, 182\n",
            "\n",
            "\fimage classification, 324\n",
            "learning curves to assess, 152-155\n",
            "number of neurons per hidden layer, 351\n",
            "polynomial regression, 132\n",
            "SVM model, 177\n",
            "\n",
            "OvO (one-versus-one) strategy, 119-121\n",
            "OvR (one-versus-the-rest) strategy, 119-121\n",
            "\n",
            "P\n",
            "p-value, 202\n",
            "PACF (partial autocorrelation function), 551\n",
            "padding options, convolutional layer, 488-489\n",
            "PaLM (Pathways language model), 623\n",
            "Pandas, 46, 64-66, 73, 78\n",
            "parallelism, training models across devices,\n",
            "\n",
            "756-776\n",
            "data parallelism, 759-765\n",
            "distribution strategies API, 765\n",
            "with GPU, 753-756\n",
            "hyperparameter tuning, 772-776\n",
            "model parallelism, 756-758\n",
            "on TensorFlow cluster, 767-770\n",
            "Vertex AI for running large jobs, 770-772\n",
            "\n",
            "parameter efficiency, 349\n",
            "parameter matrix, 170\n",
            "parameter servers, 760\n",
            "parameter space, 142\n",
            "parameter vector, 133, 138, 165, 170\n",
            "parametric leaky ReLU (PReLU), 362\n",
            "parametric models, 202\n",
            "partial autocorrelation function (PACF), 551\n",
            "partial derivative, 142\n",
            "partial_fit(), 148\n",
            "Pathways language model (PaLM), 623\n",
            "PCA (see principal component analysis)\n",
            "PDF (probability density function), 260, 291\n",
            "Pearson's r, 63\n",
            "penalties, reinforcement learning, 16\n",
            "PER (prioritized experience replay), 714\n",
            "Perceiver, 627\n",
            "percentiles, 54\n",
            "perceptrons, 299, 304-316\n",
            "\n",
            "(see also multilayer perceptrons)\n",
            "\n",
            "performance measures, 107-119\n",
            "confusion matrix, 108-110\n",
            "cross-validation to measure accuracy,\n",
            "\n",
            "107-108\n",
            "\n",
            "precision and recall, 110-115\n",
            "ROC curve, 115-119\n",
            "\n",
            "selecting, 43-45\n",
            "\n",
            "performance scheduling, 389, 392\n",
            "permutation(), 56\n",
            "PG (policy gradients) algorithm, 687, 694-698\n",
            "piecewise constant scheduling, 389, 391\n",
            "PipeDream, 763\n",
            "Pipeline class, 83\n",
            "Pipeline constructor, 84-87\n",
            "pipelines, 42, 83-88, 154, 177\n",
            "pixelwise normalization layer, 670\n",
            "placeholders, function definitions, 803\n",
            "POET algorithm, 719\n",
            "policy gradients (PG) algorithm, 687, 694-698\n",
            "policy parameters, 686\n",
            "policy space, 686\n",
            "policy, reinforcement learning, 16, 685, 691\n",
            "polynomial features, SVM classifiers, 180\n",
            "polynomial kernel, 180, 190\n",
            "polynomial regression, 132, 149-151\n",
            "polynomial time, 200\n",
            "PolynomialFeatures, 150, 179\n",
            "pooling kernel, 491\n",
            "pooling layers, 491-495\n",
            "positional encodings, 612-614\n",
            "positive class, 109, 164\n",
            "post-training quantization, 743\n",
            "posterior distribution, 654\n",
            "power law distribution, 76\n",
            "power scheduling, 389\n",
            "PPO (proximal policy optimization), 718\n",
            "precision and recall, classifier metrics, 110-115\n",
            "precision/recall curve (PR), 113, 117\n",
            "precision/recall trade-off, 111-115\n",
            "predict(), 71, 81, 84\n",
            "predicted class, 109\n",
            "prediction service, on Vertex AI, 732-741\n",
            "predictions\n",
            "\n",
            "backpropagation, 312\n",
            "confusion matrix, 108-110\n",
            "cross-validation to measure accuracy,\n",
            "\n",
            "107-108\n",
            "\n",
            "decision trees, 197-200\n",
            "with linear SVM classifier, 186\n",
            "\n",
            "predictors, 11\n",
            "\n",
            "(see also ensemble learning)\n",
            "\n",
            "predict_log_proba(), 178\n",
            "predict_proba(), 178\n",
            "prefetching of data, 450-452\n",
            "PReLU (parametric leaky ReLU), 362\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "825\n",
            "\n",
            "\fpreprocessed attributes, 55\n",
            "preprocessing data (see loading and prepro‐\n",
            "\n",
            "cessing data)\n",
            "\n",
            "preprocessing mismatch, 460\n",
            "pretraining and pretrained layers\n",
            "\n",
            "on auxiliary task, 378\n",
            "CNNs, 516-521\n",
            "greedy layer-wise pretraining, 377, 647, 668\n",
            "language model components, 473\n",
            "reusing embeddings, 593-595\n",
            "reusing layers, 373-379\n",
            "in unsupervised learning, 377, 593, 620, 644\n",
            "\n",
            "primal problem, 189\n",
            "principal component (PC), 244-245\n",
            "principal component analysis (PCA), 243-251\n",
            "choosing number of dimensions, 247-249\n",
            "for compression, 249-250\n",
            "explained variance ratio, 246\n",
            "finding principal components, 245\n",
            "incremental PCA, 250-251\n",
            "preserving variance, 243\n",
            "projecting down to d dimensions, 245\n",
            "randomized PCA, 250\n",
            "for scaling data in decision trees, 206\n",
            "with undercomplete linear autoencoder,\n",
            "\n",
            "639-640\n",
            "\n",
            "using Scikit_Learn for, 246\n",
            "\n",
            "prior distribution, 654\n",
            "prioritized experience replay (PER), 714\n",
            "probabilistic autoencoders, 654\n",
            "probabilities, estimating, 164-165, 199, 215\n",
            "probability density function (PDF), 260, 291\n",
            "probability versus likelihood, 290-291\n",
            "profiling the network, with TensorBoard, 340\n",
            "progressive web app (PWA), 745\n",
            "projection, dimensionality reduction, 239-240\n",
            "propositional logic, 300\n",
            "protocol buffers (protobuf), 454-457, 459, 729\n",
            "proximal policy optimization (PPO), 718\n",
            "pruning of decision tree nodes, 202\n",
            "pseudoinverse, 137\n",
            "PWA (progressive web app), 745\n",
            "Python API, 46\n",
            "\n",
            "Q\n",
            "Q-learning algorithm, 704-716\n",
            "\n",
            "approximate Q-learning, 707\n",
            "exploration policies, 706\n",
            "implementing deep Q-learning, 708-712\n",
            "\n",
            "826 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "variants in deep Q-learning, 713-716\n",
            "\n",
            "Q-value iteration algorithm, 702-703\n",
            "Q-values, 702-703\n",
            "quadratic equation, 150\n",
            "quadratic programming (QP) problems, 188\n",
            "quantization-aware training, 744\n",
            "quartiles, 54\n",
            "queries per second (QPS), 721, 738\n",
            "question-answering modules, 8\n",
            "queues, 411, 798\n",
            "\n",
            "R\n",
            "radial basis function (RBF), 77\n",
            "ragged dimensions, 411\n",
            "Rainbow agent, 716\n",
            "random forests, 90, 211, 220-222\n",
            "\n",
            "analysis of models and their errors, 95\n",
            "decision trees (see decision trees)\n",
            "extra-trees, 221\n",
            "feature importance measurement, 221\n",
            "\n",
            "random initialization, 138, 140\n",
            "random patches, 219\n",
            "random projection algorithm, 252-254\n",
            "random subspaces, 219\n",
            "RandomForestClassifier, 117-119\n",
            "RandomForestRegressor, 90\n",
            "randomized leaky ReLU (RReLU), 362\n",
            "randomized PCA, 250\n",
            "randomized search, 93-94, 344-346\n",
            "RBF (radial basis function), 77\n",
            "rbf_kernel(), 77, 80\n",
            "recall metric, 110\n",
            "receiver operating characteristic (ROC) curve,\n",
            "\n",
            "115-119\n",
            "\n",
            "recognition network, 637\n",
            "\n",
            "(see also autoencoders)\n",
            "\n",
            "recommender system, 9\n",
            "reconstruction error, 249, 646\n",
            "reconstruction loss, 424, 638\n",
            "rectified linear units (ReLU) (see ReLU)\n",
            "recurrent dropout, 537\n",
            "recurrent layer normalization, 537, 566\n",
            "recurrent neural networks (RNNs), 537-604\n",
            "\n",
            "bidirectional, 601\n",
            "deep RNN, 557\n",
            "forecasting time series (see time series data)\n",
            "gradient clipping, 372\n",
            "handling long sequences, 565-575\n",
            "input and output sequences, 541-542\n",
            "\n",
            "\fmemory cells, 540, 568-575\n",
            "NLP (see natural language processing)\n",
            "and Perceiver, 627\n",
            "splitting across devices, 758\n",
            "stateful, 578, 584-586\n",
            "stateless, 578, 586\n",
            "training, 542\n",
            "and vision transformers, 624\n",
            "\n",
            "recurrent neurons, 538\n",
            "reduce operation, 760\n",
            "region proposal network (RPN), 530\n",
            "regression MLPs, 313-314\n",
            "regression models\n",
            "\n",
            "and classification, 10, 127\n",
            "decision tree tasks, 204-205\n",
            "forecasting example, 8\n",
            "lasso regression, 158-161\n",
            "linear regression (see linear regression)\n",
            "logistic regression (see logistic regression)\n",
            "multiple regression, 43\n",
            "multivariate regression, 43\n",
            "polynomial regression, 132, 149-151\n",
            "regression MLPs, 328\n",
            "ridge regression, 156-158, 161\n",
            "softmax regression, 170-173\n",
            "SVM, 184-186\n",
            "univariate regression, 43\n",
            "\n",
            "regression to the mean, 10\n",
            "regularization, 32, 392-400\n",
            "custom regularizers, 415\n",
            "decision trees, 203\n",
            "dropout, 394-397\n",
            "early stopping, 162-163, 229, 555\n",
            "elastic net, 161\n",
            "hyperparameters, 201-203\n",
            "lasso regression, 158-161\n",
            "linear models, 156-163\n",
            "max-norm, 399\n",
            "MC dropout, 397-399\n",
            "ridge, 157\n",
            "shrinkage, 228\n",
            "subword, 588\n",
            "Tikhonov, 156-158\n",
            "weight decay, 386\n",
            "ℓ₁ and ℓ₂ regularization, 393\n",
            "\n",
            "REINFORCE algorithms, 694\n",
            "reinforcement learning (RL), 16, 683-719\n",
            "\n",
            "actions, 693-694\n",
            "credit assignment problem, 693-694\n",
            "\n",
            "examples of, 9, 684\n",
            "learning in order to optimizing rewards, 684\n",
            "Markov decision processes, 699-703\n",
            "neural network policies, 691\n",
            "OpenAI Gym, 687-691\n",
            "PG algorithms, 694-698\n",
            "policy search, 685\n",
            "Q-learning, 704-716\n",
            "TD learning, 703\n",
            "\n",
            "ReLU (rectified linear units)\n",
            "and backpropagation, 312\n",
            "in CNN architectures, 495\n",
            "as default for simple tasks, 366\n",
            "leakyReLU, 361-363\n",
            "and MLPS, 314\n",
            "RNN unstable gradients problem, 565\n",
            "RReLU, 362\n",
            "tuning hyperparameters, 345\n",
            "\n",
            "replay buffer, 709\n",
            "representation learning, 74\n",
            "(see also autoencoders)\n",
            "\n",
            "residual block, 423-424\n",
            "residual errors, 226-230\n",
            "residual learning, 505\n",
            "residual network (ResNet), 505-509\n",
            "residual units, 506\n",
            "ResNet-152, 508\n",
            "ResNet-34, 508, 515\n",
            "ResNet-50, 517\n",
            "ResNeXt, 512\n",
            "REST API, querying through, 727\n",
            "return, in reinforcement learning, 696\n",
            "reverse process, diffusion model, 675-677\n",
            "reverse-mode autodiff, 310, 428\n",
            "rewards, reinforcement learning, 16, 684\n",
            "Ridge, 157\n",
            "ridge regression, 156-158, 161\n",
            "ridge regularization, 157\n",
            "RidgeCV, 158\n",
            "RL (see reinforcement learning)\n",
            "RMSProp, 383\n",
            "ROC (receiver operating characteristic) curve,\n",
            "\n",
            "115-119\n",
            "\n",
            "root mean square error (RMSE), 43-45, 88, 133,\n",
            "\n",
            "162\n",
            "\n",
            "root node, decision tree, 197, 198\n",
            "RPN (region proposal network), 530\n",
            "RReLU (randomized leaky ReLU), 362\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "827\n",
            "\n",
            "\fS\n",
            "SAC (soft actor-critic), 717\n",
            "“same” padding, computer vision, 488\n",
            "SAMME, 225\n",
            "sample inefficiency, 698\n",
            "sampled softmax, 600\n",
            "sampling bias, 29, 57\n",
            "sampling noise, 29\n",
            "SARIMA model, 550-551\n",
            "SavedModel, 723-725\n",
            "saving, loading, and restoring models, 337-338,\n",
            "\n",
            "413-415\n",
            "\n",
            "scaled dot-product attention layer, 616\n",
            "scatter matrix, 64-65\n",
            "Scikit-Learn, xvi\n",
            "\n",
            "bagging and pasting in, 217\n",
            "CART algorithm, 199, 205\n",
            "cross-validation, 89-91\n",
            "design principles, 70-71\n",
            "PCA implementation, 246\n",
            "Pipeline constructor, 84-87\n",
            "sklearn.base.BaseEstimator, 80\n",
            "sklearn.base.clone(), 163\n",
            "sklearn.base.TransformerMixin, 80\n",
            "sklearn.cluster.DBSCAN, 279\n",
            "sklearn.cluster.KMeans, 81, 263\n",
            "sklearn.cluster.MiniBatchKMeans, 268\n",
            "sklearn.compose.ColumnTransformer, 85\n",
            "sklearn.compose.TransformedTargetRegres‐\n",
            "\n",
            "sor, 79\n",
            "\n",
            "sklearn.datasets.load_iris(), 167\n",
            "sklearn.datasets.make_moons(), 179\n",
            "sklearn.decomposition.IncrementalPCA,\n",
            "\n",
            "251\n",
            "\n",
            "sklearn.decomposition.PCA, 246\n",
            "sklearn.ensemble.AdaBoostClassifier, 226\n",
            "sklearn.ensemble.BaggingClassifier, 217\n",
            "sklearn.ensemble.GradientBoostingRegres‐\n",
            "\n",
            "sor, 227-230\n",
            "\n",
            "sklearn.ensemble.HistGradientBoosting‐\n",
            "\n",
            "Classifier, 230\n",
            "\n",
            "sklearn.ensemble.HistGradientBoostingRe‐\n",
            "\n",
            "gressor, 230\n",
            "\n",
            "sklearn.ensemble.RandomForestClassifier,\n",
            "\n",
            "117-119, 214, 220, 221, 248\n",
            "\n",
            "sklearn.ensemble.RandomForestRegressor,\n",
            "\n",
            "90, 220\n",
            "\n",
            "sklearn.ensemble.StackingClassifier, 234\n",
            "sklearn.ensemble.StackingRegressor, 234\n",
            "\n",
            "828 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "sklearn.ensemble.VotingClassifier, 214\n",
            "sklearn.externals.joblib, 97-98\n",
            "sklearn.feature_selection.SelectFromModel,\n",
            "\n",
            "95\n",
            "\n",
            "sklearn.impute.IterativeImputer, 69\n",
            "sklearn.impute.KNNImputer, 69\n",
            "sklearn.impute.SimpleImputer, 68\n",
            "sklearn.linear_model.ElasticNet, 162\n",
            "sklearn.linear_model.Lasso, 161\n",
            "sklearn.linear_model.LinearRegression, 25,\n",
            "\n",
            "71, 79, 137, 138, 150\n",
            "\n",
            "sklearn.linear_model.LogisticRegression,\n",
            "\n",
            "168, 172, 275\n",
            "\n",
            "sklearn.linear_model.Perceptron, 307\n",
            "sklearn.linear_model.Ridge, 157\n",
            "sklearn.linear_model.RidgeCV, 158\n",
            "sklearn.linear_model.SGDClassifier, 106,\n",
            "111, 112, 117-119, 121, 183, 188, 307\n",
            "sklearn.linear_model.SGDRegressor, 147,\n",
            "\n",
            "163\n",
            "\n",
            "sklearn.manifold.LocallyLinearEmbedding,\n",
            "\n",
            "254\n",
            "\n",
            "sklearn.metrics.ConfusionMatrixDisplay,\n",
            "\n",
            "122\n",
            "\n",
            "sklearn.metrics.confusion_matrix(), 109,\n",
            "\n",
            "122\n",
            "\n",
            "sklearn.metrics.f1_score(), 111, 126\n",
            "sklearn.metrics.mean_squared_error(), 88\n",
            "sklearn.metrics.precision_recall_curve(),\n",
            "\n",
            "113, 117\n",
            "\n",
            "sklearn.metrics.precision_score(), 110\n",
            "sklearn.metrics.recall_score(), 110\n",
            "sklearn.metrics.roc_auc_score(), 116\n",
            "sklearn.metrics.roc_curve(), 115\n",
            "sklearn.metrics.silhouette_score(), 271\n",
            "sklearn.mixture.BayesianGaussianMixture,\n",
            "\n",
            "292\n",
            "\n",
            "sklearn.mixture.GaussianMixture, 283\n",
            "sklearn.model_selection.cross_val_pre‐\n",
            "\n",
            "dict(), 108, 113, 117, 122, 232\n",
            "\n",
            "sklearn.model_selection.cross_val_score(),\n",
            "\n",
            "90, 107\n",
            "\n",
            "sklearn.model_selection.GridSearchCV,\n",
            "\n",
            "91-93\n",
            "\n",
            "sklearn.model_selection.learning_curve(),\n",
            "\n",
            "152\n",
            "\n",
            "sklearn.model_selection.Randomized‐\n",
            "\n",
            "SearchCV, 248\n",
            "\n",
            "\fsklearn.model_selection.StratifiedKFold,\n",
            "\n",
            "108\n",
            "\n",
            "sklearn.model_selection.StratifiedShuffleS‐\n",
            "\n",
            "plit, 59\n",
            "\n",
            "sklearn.model_selection.train_test_split(),\n",
            "\n",
            "57, 59, 89\n",
            "\n",
            "sklearn.multiclass.OneVsOneClassifier, 120\n",
            "sklearn.multiclass.OneVsRestClassifier, 120\n",
            "sklearn.multioutput.ChainClassifier, 126\n",
            "sklearn.neighbors.KNeighborsClassifier,\n",
            "\n",
            "125, 128, 280\n",
            "\n",
            "sklearn.neighbors.KNeighborsRegressor, 26\n",
            "sklearn.neural_network.MLPClassifier, 316\n",
            "sklearn.neural_network.MLPRegressor, 313\n",
            "sklearn.pipeline.make_pipeline(), 154\n",
            "sklearn.pipeline.Pipeline, 83\n",
            "sklearn.preprocessing.FunctionTransformer,\n",
            "\n",
            "79\n",
            "\n",
            "sklearn.preprocessing.MinMaxScaler, 76\n",
            "sklearn.preprocessing.OneHotEncoder,\n",
            "\n",
            "72-74, 77, 86\n",
            "\n",
            "sklearn.preprocessing.OrdinalEncoder, 71,\n",
            "\n",
            "231\n",
            "\n",
            "sklearn.preprocessing.PolynomialFeatures,\n",
            "\n",
            "150, 179\n",
            "\n",
            "sklearn.preprocessing.StandardScaler, 76,\n",
            "\n",
            "141, 156, 179\n",
            "\n",
            "sklearn.random_projection.GaussianRan‐\n",
            "\n",
            "score(), 71\n",
            "search engines, clustering for, 262\n",
            "search space, 93\n",
            "seasonality, time series modeling, 545\n",
            "second moment (variance of gradient), 384\n",
            "second-order partial derivatives (Hessians), 386\n",
            "segment embedding, 621\n",
            "SelectFromModel, 95\n",
            "self-attention layers, 612, 618, 627\n",
            "self-distillation, 627\n",
            "self-normalization, 364, 397, 401\n",
            "self-supervised learning, 15-16\n",
            "SELU (scaled ELU) activation function, 364,\n",
            "\n",
            "397\n",
            "\n",
            "semantic interpolation, 659\n",
            "semantic segmentation, 8, 273, 531-535\n",
            "semi-supervised learning, 14, 262, 275-278\n",
            "SENet, 510-512\n",
            "sensitivity (recall), ROC curve, 115\n",
            "sensitivity metric, 110\n",
            "sensors, 684\n",
            "sentence encoder, 474, 594\n",
            "SentencePiece project, 588\n",
            "sentiment analysis, 587-595\n",
            "sentiment neuron, 586\n",
            "separable convolution layer, 509\n",
            "sequence length, 573, 612\n",
            "sequence-to-sequence (seq2seq) network, 541,\n",
            "\n",
            "domProjection, 253\n",
            "\n",
            "562-564\n",
            "\n",
            "sklearn.random_projection.SparseRandom‐\n",
            "\n",
            "Projection, 253\n",
            "\n",
            "sklearn.semi_supervised.LabelPropagation,\n",
            "\n",
            "278\n",
            "\n",
            "sklearn.semi_supervised.LabelSpreading,\n",
            "\n",
            "278\n",
            "\n",
            "sklearn.semi_supervised.SelfTrainingClassi‐\n",
            "\n",
            "fier, 278\n",
            "\n",
            "sklearn.svm.LinearSVC, 177, 183, 188\n",
            "sklearn.svm.SVC, 120, 180, 183, 188\n",
            "sklearn.svm.SVR, 185\n",
            "sklearn.tree.DecisionTreeClassifier, 195,\n",
            "\n",
            "201, 202, 207, 220\n",
            "\n",
            "sklearn.tree.DecisionTreeRegressor, 89, 195,\n",
            "\n",
            "204, 226\n",
            "\n",
            "sklearn.tree.export_graphviz(), 196\n",
            "sklearn.tree.ExtraTreesClassifier, 221\n",
            "sklearn.utils.estimator_checks, 82\n",
            "sklearn.utils.validation module, 80\n",
            "SVM classification classes, 183\n",
            "\n",
            "sequence-to-vector network, 541\n",
            "SequenceExample protobuf, 459\n",
            "sequential API, image classifier with, 318-328\n",
            "service account, GCP, 735\n",
            "service worker, 745\n",
            "sets, 797\n",
            "set_params(), 80\n",
            "SGD (see stochastic gradient descent)\n",
            "SGDClassifier, 106, 111, 112, 117-119, 121, 183,\n",
            "\n",
            "188, 188\n",
            "\n",
            "SGDRegressor, 147, 163\n",
            "sharpening, NLP transformers, 628\n",
            "shrinkage, 228\n",
            "shuffle_and_split_data(), 57, 57\n",
            "shuffling data, 146, 445-446\n",
            "Siamese neural network, 756\n",
            "sigmoid activation function, 164, 312, 358, 651\n",
            "signals, 301\n",
            "silhouette coefficient, 270\n",
            "silhouette diagram, 271\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "829\n",
            "\n",
            "\fSiLU activation function, 366\n",
            "similarity features, SVM, 181\n",
            "SimpleImputer, 68\n",
            "simulated annealing, 145\n",
            "simulated environments, 687-691\n",
            "single program, multiple data (SPMD), 759-765\n",
            "single-shot learning, 534\n",
            "singular value decomposition (SVD), 137, 245,\n",
            "\n",
            "250\n",
            "\n",
            "skewed datasets, 108\n",
            "skewed left/right, 55\n",
            "skip connections, 364, 505\n",
            "slack variable, 188\n",
            "smoothing terms, 368, 382\n",
            "SMOTE (synthetic minority oversampling\n",
            "\n",
            "technique), 500\n",
            "\n",
            "soft actor-critic (SAC), 717\n",
            "soft clustering, 264\n",
            "soft margin classification, 177-178, 188\n",
            "soft voting, 215\n",
            "softmax activation function, 170, 315, 320, 600\n",
            "softmax regression, 170-173\n",
            "softplus activation function, 314\n",
            "spam filters, 3-7, 9-10\n",
            "sparse autoencoders, 651-654\n",
            "sparse features, SVC class, 183\n",
            "sparse matrix, 73, 76, 86\n",
            "sparse models, 159, 387\n",
            "sparse tensors, 795\n",
            "sparsity loss, 652\n",
            "specificity, ROC curve, 115\n",
            "spectral clustering, 283\n",
            "speech recognition, 6, 8\n",
            "split node, decision tree, 197\n",
            "SPMD (single program, multiple data), 759-765\n",
            "squared hinge loss, 188\n",
            "Stable Diffusion, 680\n",
            "stacked autoencoders, 640-648\n",
            "stacking (stacked generalization), 232-235\n",
            "stale gradients, 761\n",
            "standard correlation coefficient, 63\n",
            "standard deviation, 53\n",
            "standardization, 76\n",
            "StandardScaler, 76, 141, 156, 179\n",
            "state-action values (Q-Values), 702-703\n",
            "stateful metric, 417\n",
            "stateful RNN, 578, 584-586\n",
            "stateless RNN, 578, 584, 586\n",
            "stationary time series, 548\n",
            "\n",
            "830 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "statistical mode, 216\n",
            "statistical significance, 202\n",
            "statsmodels library, 550\n",
            "stemming, 130\n",
            "step functions, TLU, 304\n",
            "stochastic gradient boosting, 230\n",
            "stochastic gradient descent (SGD), 145-148, 183\n",
            "\n",
            "early stopping, 163\n",
            "image classification, 324\n",
            "and perceptron learning algorithm, 307\n",
            "ridge regularization, 158\n",
            "and TD learning, 704\n",
            "training binary classifier, 106\n",
            "\n",
            "stratified sampling, 58-60, 108\n",
            "strat_train_set, 68\n",
            "streaming metric, 417\n",
            "strides, 482, 531, 573\n",
            "string kernels, 182\n",
            "string tensors, 411\n",
            "StringLookup layer, 465\n",
            "strings, 793-794\n",
            "strong learners, 213\n",
            "style mixing, StyleGAN, 673\n",
            "style transfer, GANs, 671\n",
            "StyleGANs, 671-673\n",
            "subclassing API, 336-337\n",
            "subgradient vector, 161\n",
            "subsampling, pooling layer, 491\n",
            "subword regularization, 588\n",
            "super-convergence, 390\n",
            "super-resolution, 534\n",
            "supervised learning, 10\n",
            "support vector machines (SVMs), 175-193\n",
            "\n",
            "decision function, 186-189\n",
            "dual problem, 189-193\n",
            "kernelized SVMs, 190-193\n",
            "linear classification, 175-178\n",
            "mechanics of, 186-193\n",
            "in multiclass classification, 120\n",
            "nonlinear classifiers, 178\n",
            "one-class SVM, 294\n",
            "SVM regression, 184-186\n",
            "\n",
            "support vectors, 176\n",
            "SVC class, 180, 183\n",
            "SVD (singular value decomposition), 137, 245,\n",
            "\n",
            "250\n",
            "\n",
            "SVMs (see support vector machines)\n",
            "SVR class, 185\n",
            "Swish activation function, 366\n",
            "\n",
            "\fSwiss roll dataset, 241-242\n",
            "symbolic differentiation, 788\n",
            "symbolic tensors, 436, 802\n",
            "synchronous updates, with centralized parame‐\n",
            "\n",
            "ters, 761\n",
            "\n",
            "synthetic minority oversampling technique\n",
            "\n",
            "(SMOTE), 500\n",
            "\n",
            "T\n",
            "t-distributed stochastic neighbor embedding (t-\n",
            "\n",
            "SNE), 256, 643\n",
            "target attributes, 55\n",
            "target distribution, transforming, 78\n",
            "target model, DQN, 713\n",
            "TD error, 704\n",
            "TD target, 704\n",
            "TD-Gammon, 683\n",
            "teacher forcing, 595\n",
            "temperature, Char-RNN model, 583\n",
            "temporal difference (TD) learning, 703, 715\n",
            "tensor arrays, 796\n",
            "tensor processing units (TPUs), 404, 731, 744,\n",
            "\n",
            "770\n",
            "\n",
            "TensorBoard, 340-344, 406, 592, 770\n",
            "TensorFlow, xvi, xvi, 403-477, 721-776\n",
            "\n",
            "(see also Keras API)\n",
            "architecture, 405\n",
            "creating training function, 452\n",
            "custom models (see custom models and\n",
            "\n",
            "training algorithms)\n",
            "\n",
            "deploying model to a mobile device,\n",
            "\n",
            "741-744\n",
            "\n",
            "functions and graphs, 801-809\n",
            "GPU management with, 749-752, 753-756\n",
            "graphs and functions, 404, 433-438, 801-809\n",
            "hub.KerasLayer, 474\n",
            "math operations, 408\n",
            "with NumPy, 407-411\n",
            "operations (ops) and tensors, 404, 407-409\n",
            "parallelism to train models (see parallelism)\n",
            "platforms and APIs available, 405\n",
            "serving a model (see TensorFlow Serving)\n",
            "special data structures, 793-799\n",
            "tf.add(), 408\n",
            "tf.autograph.to_code(), 437\n",
            "tf.cast(), 409\n",
            "tf.config.set_soft_device_placement, 753\n",
            "tf.config.threading.set_inter_op_parallel‐\n",
            "\n",
            "ism_threads(), 755\n",
            "\n",
            "tf.config.threading.set_intra_op_parallel‐\n",
            "\n",
            "ism_threads(), 755\n",
            "\n",
            "tf.constant(), 407\n",
            "tf.data API (see tf.data API)\n",
            "tf.device(), 753\n",
            "tf.distribute.experimental.CentralStorageS‐\n",
            "\n",
            "trategy, 766\n",
            "\n",
            "tf.distribute.experimental.TPUStrategy, 770\n",
            "tf.distribute.MirroredStrategy, 766, 773\n",
            "tf.distribute.MultiWorkerMirroredStrategy,\n",
            "\n",
            "768\n",
            "\n",
            "tf.float32, 409\n",
            "tf.float32 data type, 418, 448\n",
            "tf.function(), 433, 437\n",
            "tf.int32 data type, 411\n",
            "tf.io.decode_base64(), 741\n",
            "tf.io.decode_csv(), 449\n",
            "tf.io.decode_image(), 741\n",
            "tf.io.decode_png(), 741\n",
            "tf.io.decode_proto(), 456\n",
            "tf.io.FixedLenFeature, 457\n",
            "tf.io.parse_example(), 458\n",
            "tf.io.parse_sequence_example(), 459\n",
            "tf.io.parse_single_example(), 457\n",
            "tf.io.parse_single_sequence_example(), 459\n",
            "tf.io.parse_tensor(), 458\n",
            "tf.io.serialize_tensor(), 458\n",
            "tf.io.TFRecordOptions, 454\n",
            "tf.io.TFRecordWriter, 453\n",
            "tf.io.VarLenFeature, 457\n",
            "tf.linalg.band_part(), 618\n",
            "tf.lite.TFLiteConverter.from_keras_model(),\n",
            "\n",
            "742\n",
            "\n",
            "tf.make_tensor_proto(), 729\n",
            "tf.matmul(), 408, 616\n",
            "tf.nn.conv2d(), 487\n",
            "tf.nn.embedding_lookup(), 475\n",
            "tf.nn.local_response_normalization(), 501\n",
            "tf.nn.moments(), 475\n",
            "tf.nn.sampled_softmax_loss(), 600\n",
            "tf.py_function(), 437, 724\n",
            "tf.queue module, 411\n",
            "tf.queue.FIFOQueue, 798\n",
            "tf.RaggedTensor, 411\n",
            "tf.random.categorical(), 583\n",
            "tf.reduce_max(), 495\n",
            "tf.reduce_mean(), 432\n",
            "tf.reduce_sum(), 434\n",
            "tf.saved_model_cli command, 724\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "831\n",
            "\n",
            "\ftf.sets module, 411\n",
            "tf.sort(), 437\n",
            "tf.SparseTensor, 411\n",
            "tf.stack(), 449, 585\n",
            "tf.string data type, 411\n",
            "tf.strings module, 411\n",
            "tf.Tensor, 407, 410\n",
            "tf.TensorArray, 411\n",
            "tf.transpose(), 408\n",
            "tf.Variable, 410\n",
            "tf.Variable.assign(), 410\n",
            "type conversions, 409\n",
            "variables, 410\n",
            "web page, running a model in, 745\n",
            "\n",
            "TensorFlow cluster, 767-770\n",
            "TensorFlow Datasets (TFDS) project, 475-477\n",
            "TensorFlow Extended (TFX), 406\n",
            "TensorFlow Hub, 406, 473, 594\n",
            "TensorFlow Lite, 406\n",
            "TensorFlow playground, 316\n",
            "TensorFlow Serving (TF Serving), 722-741\n",
            "batch prediction jobs on Vertex AI, 739\n",
            "creating prediction service, 732-739\n",
            "deploying new model version, 730-731\n",
            "Docker container, 726\n",
            "exporting SavedModels, 723-725\n",
            "gRPC API, querying through, 729\n",
            "installing and starting up, 725-727\n",
            "REST API, querying through, 727\n",
            "\n",
            "TensorFlow Text, 473, 589\n",
            "TensorFlow.js (TFJS) JavaScript library, 745\n",
            "tensors, 407-409\n",
            "term-frequency x inverse-document-frequency\n",
            "\n",
            "(TF-IDF), 472\n",
            "\n",
            "terminal state, Markov chain, 699\n",
            "test set, 34, 55-60\n",
            "text attributes, 71\n",
            "text processing (see natural language process‐\n",
            "\n",
            "ing)\n",
            "\n",
            "TF Serving (see TensorFlow Serving)\n",
            "tf.data API, 442-453\n",
            "\n",
            "chaining transformations, 443-445\n",
            "interleaving lines from multiple files,\n",
            "\n",
            "446-447\n",
            "\n",
            "and Keras preprocessing layers, 462\n",
            "prefetching, 450-452\n",
            "preprocessing the data, 448-449\n",
            "shuffling data, 445-446\n",
            "tf.data.AUTOTUNE, 444\n",
            "\n",
            "832 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "tf.data.Dataset.from_tensor_slices(),\n",
            "\n",
            "442-444\n",
            "\n",
            "tf.data.TFRecordDataset, 454, 454, 457\n",
            "using dataset with Keras, 452-453\n",
            "\n",
            "TFDS (TensorFlow Datasets) project, 475-477\n",
            "TFJS (TensorFlow.js) JavaScript library, 745\n",
            "TFLite, 742-744\n",
            "TFRecord format, 441, 453-459\n",
            "TFX (TensorFlow Extended), 406\n",
            "theoretical information criterion, 289\n",
            "3D convolutional layers, 533\n",
            "threshold logic units (TLUs), 304, 309\n",
            "Tikhonov regularization, 156-158, 161\n",
            "time series data, forecasting, 537, 543-564\n",
            "\n",
            "ARMA model family, 549-551\n",
            "data preparation for ML models, 552-555\n",
            "with deep RNN, 557\n",
            "with linear model, 555\n",
            "multivariate time series, 545, 559-560\n",
            "with sequence-to-sequence model, 541,\n",
            "\n",
            "562-564\n",
            "\n",
            "several time steps ahead, 560-562\n",
            "with simple RNN, 556-557\n",
            "\n",
            "TLUs (threshold logic units), 304, 309\n",
            "TNR (true negative rate), 115\n",
            "tokenizers library, 589\n",
            "tolerance (ε), 144, 183\n",
            "TPR (true positive rate), 110, 115\n",
            "TPUs (tensor processing units), 404, 731, 744,\n",
            "\n",
            "770\n",
            "\n",
            "train-dev set, 36\n",
            "training, 24\n",
            "training instance, 4\n",
            "training loops, 430-433, 452\n",
            "training models, 131-173\n",
            "\n",
            "learning curves in, 151-155\n",
            "linear regression, 131, 132-149\n",
            "logistic regression, 164-173\n",
            "perceptrons, 304-308\n",
            "polynomial regression, 132, 149-151\n",
            "\n",
            "training set, 4, 34\n",
            "\n",
            "cost function of, 165-166\n",
            "insufficient quantities, 27\n",
            "irrelevant features, 30\n",
            "min-max scaling, 75\n",
            "nonrepresentative, 28\n",
            "overfitting, 30-33\n",
            "preparing for ML algorithms, 68\n",
            "training and evaluating on, 88-89\n",
            "\n",
            "\ftransforming data, 79\n",
            "underfitting, 33\n",
            "visualizing data, 60\n",
            "\n",
            "training set expansion, 129, 500, 519\n",
            "training/serving skew, 442\n",
            "train_test_split(), 59, 89\n",
            "transfer learning, 16, 373, 375-377, 518-521\n",
            "transform(), 71, 73, 75, 80\n",
            "transformation of data\n",
            "\n",
            "custom transformers, 79-83\n",
            "estimator transformers, 70\n",
            "and feature scaling, 75-79\n",
            "transformer models (see transformer mod‐\n",
            "\n",
            "els)\n",
            "\n",
            "transformation pipelines, 83-88\n",
            "TransformedTargetRegressor, 79\n",
            "transformer, 609\n",
            "transformer models\n",
            "\n",
            "attention mechanisms, 609-619, 625\n",
            "BERT, 620\n",
            "DistilBERT, 622, 631-633\n",
            "Hugging Face library, 629-633\n",
            "Pathways language model, 623\n",
            "vision transformers, 624-629\n",
            "\n",
            "TransformerMixin, 80\n",
            "transformers library, 629-632\n",
            "translation, with RNNs, 578, 595-604\n",
            "(see also transformer models)\n",
            "\n",
            "transpose operator, 44\n",
            "transposed convolutional layer, 532-534\n",
            "true negative rate (TNR), 115\n",
            "true negatives, confusion matrix, 109\n",
            "true positive rate (TPR), 110, 115\n",
            "true positives, confusion matrix, 109\n",
            "trust region policy optimization (TRPO), 718\n",
            "2D convolutional layers, 487\n",
            "tying weights, 645\n",
            "type I errors, confusion matrix, 109\n",
            "type II errors, confusion matrix, 109\n",
            "\n",
            "U\n",
            "uncertainty sampling, 278\n",
            "undercomplete, autoencoder as, 638-640\n",
            "underfitting of data, 33, 89, 151-155, 182\n",
            "univariate regression, 43\n",
            "univariate time series, 545\n",
            "Universal Sentence Encoder, 594-595\n",
            "unreasonable effectiveness of data, 27\n",
            "unrolling the network through time, 538\n",
            "\n",
            "unstable gradients problem, 358, 565\n",
            "\n",
            "(see also vanishing and exploding gradients)\n",
            "\n",
            "unsupervised learning, 11-14, 259-294\n",
            "\n",
            "anomaly detection, 13\n",
            "association rule learning, 14\n",
            "autoencoders (see autoencoders)\n",
            "clustering (see clustering algorithms)\n",
            "density estimation, 260\n",
            "diffusion models, 673-681\n",
            "dimensionality reduction (see dimensional‐\n",
            "\n",
            "ity reduction)\n",
            "\n",
            "GANs (see generative adversarial networks)\n",
            "GMM, 283-294\n",
            "k-means (see k-means algorithm)\n",
            "novelty detection, 13\n",
            "pretraining, 377, 593, 620, 644\n",
            "stacked autoencoders, 644\n",
            "transformer models, 621\n",
            "visualization algorithms, 12-13\n",
            "\n",
            "upsampling layer, 532\n",
            "utility function, 24\n",
            "\n",
            "V\n",
            "VAEs (variational autoencoders), 654-658\n",
            "“valid” padding, computer vision, 488\n",
            "validation set, 35, 325-327\n",
            "value_counts(), 53\n",
            "vanishing and exploding gradients, 358-373\n",
            "\n",
            "activation function improvements, 361-366\n",
            "batch normalization, 367-372\n",
            "Glorot and He initialization, 359-361\n",
            "gradient clipping, 372\n",
            "unstable gradients problem, 565-568\n",
            "\n",
            "variables\n",
            "\n",
            "handling in TF functions, 807-808\n",
            "persistence of, 419\n",
            "placing on GPUs, 752\n",
            "in TensorFlow, 410\n",
            "\n",
            "variance\n",
            "\n",
            "bias/variance trade-off, 155\n",
            "explained, 246-249\n",
            "high variance with decision trees, 207\n",
            "preserving, 243\n",
            "\n",
            "variational autoencoders (VAEs), 654-658\n",
            "vector-to-sequence network, 541\n",
            "vectors, norms for measuring distance, 45\n",
            "Vertex AI, 98, 732-739, 770-772\n",
            "VGGNet, 505\n",
            "virtual GPU device, 750\n",
            "\n",
            "Index \n",
            "\n",
            "| \n",
            "\n",
            "833\n",
            "\n",
            "\fvision transformers (ViTs), 624-629\n",
            "visual cortex architecture, 480\n",
            "visualization of data, 9, 12-13, 60-67\n",
            "\n",
            "decision trees, 195-196\n",
            "dimensionality reduction, 238, 247\n",
            "end-to-end exercise, 60-67\n",
            "MLPs with TensorBoard, 340-344\n",
            "stacked autoencoders, 642-643\n",
            "t-SNE, 256\n",
            "\n",
            "in prioritized experience replay, 715\n",
            "saving instead of whole model, 338\n",
            "\n",
            "white box models, 199\n",
            "Wide & Deep neural network, 329-333\n",
            "window length, 581\n",
            "wisdom of the crowd, 211\n",
            "word embeddings, 467\n",
            "\n",
            "neural machine translation, 595-604\n",
            "sentiment analysis, 587-595\n",
            "\n",
            "ViTs (vision transformers), 624-629\n",
            "voting classifiers, 212-215\n",
            "\n",
            "worker task type, 766\n",
            "workers, 760\n",
            "\n",
            "W\n",
            "wall time, 370\n",
            "warmup phase, asynchronous model updates,\n",
            "\n",
            "X\n",
            "Xavier initialization, 359\n",
            "Xception (Extreme Inception), 509-510,\n",
            "\n",
            "762\n",
            "\n",
            "WaveNet, 538, 574-575\n",
            "weak learners, 213\n",
            "web page, running a model in, 744\n",
            "weight decay, 386\n",
            "weight stashing, 764\n",
            "weight-tying, 645\n",
            "weights\n",
            "\n",
            "boosting, 223-225\n",
            "convolutional layers, 490\n",
            "freezing reused layers, 374\n",
            "of hidden layers, 319\n",
            "\n",
            "518-521\n",
            "\n",
            "XLA (accelerated linear algebra), 434\n",
            "XOR (exclusive or) problem, 307\n",
            "\n",
            "Y\n",
            "You Only Look Once (YOLO), 527-530\n",
            "\n",
            "Z\n",
            "zero padding, 482, 488\n",
            "zero-shot learning (ZSL), 622, 628\n",
            "ZFNet, 501\n",
            "\n",
            "834 \n",
            "\n",
            "| \n",
            "\n",
            "Index\n",
            "\n",
            "\fAbout the Author\n",
            "\n",
            "Aurélien Géron is a machine learning consultant and lecturer. A former Googler, he\n",
            "led  YouTube’s  video  classification  team  from  2013  to  2016.  He’s  been  a  founder  of\n",
            "and CTO at a few different companies: Wifirst, a leading wireless ISP in France; Poly‐\n",
            "conseil, a consulting firm focused on telecoms, media, and strategy; and Kiwisoft, a\n",
            "consulting firm focused on machine learning and data privacy.\n",
            "\n",
            "Before  all  that  Aurélien  worked  as  an  engineer  in  a  variety  of  domains:  finance\n",
            "(JP  Morgan  and  Société  Générale),  defense  (Canada’s  DOD),  and  healthcare  (blood\n",
            "transfusion).  He  also  published  a  few  technical  books  (on  C++,  WiFi,  and  internet\n",
            "architectures) and lectured about computer science at a French engineering school.\n",
            "\n",
            "A  few  fun  facts:  he  taught  his  three  children  to  count  in  binary  with  their  fingers\n",
            "(up  to  1,023),  he  studied  microbiology  and  evolutionary  genetics  before  going  into\n",
            "software engineering, and his parachute didn’t open on the second jump.\n",
            "\n",
            "Colophon\n",
            "\n",
            "The  animal  on  the  cover  of  Hands-On  Machine  Learning  with  Scikit-Learn,  Keras,\n",
            "and TensorFlow is the fire salamander (Salamandra salamandra), an amphibian found\n",
            "across most of Europe. Its black, glossy skin features large yellow spots on the head\n",
            "and  back,  signaling  the  presence  of  alkaloid  toxins.  This  is  a  possible  source  of  this\n",
            "amphibian’s  common  name:  contact  with  these  toxins  (which  they  can  also  spray\n",
            "short distances) causes convulsions and hyperventilation. Either the painful poisons\n",
            "or the moistness of the salamander’s skin (or both) led to a misguided belief that these\n",
            "creatures not only could survive being placed in fire but could extinguish it as well.\n",
            "\n",
            "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
            "the pools or other freshwater bodies that facilitate their breeding. Though they spend\n",
            "most  of  their  lives  on  land,  they  give  birth  to  their  young  in  water.  They  subsist\n",
            "mostly on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up\n",
            "to a foot in length, and in captivity may live as long as 50 years.\n",
            "\n",
            "The fire salamander’s numbers have been reduced by destruction of their forest habi‐\n",
            "tat and capture for the pet trade, but the greatest threat they face is the susceptibility\n",
            "of  their  moisture-permeable  skin  to  pollutants  and  microbes.  Since  2014,  they  have\n",
            "become extinct in parts of the Netherlands and Belgium due to an introduced fungus.\n",
            "\n",
            "Many  of  the  animals  on  O’Reilly  covers  are  endangered;  all  of  them  are  important\n",
            "to the world. The cover illustration is by Karen Montgomery, based on an engraving\n",
            "from  Wood’s  Illustrated  Natural  History.  The  cover  fonts  are  URW  Typewriter  and\n",
            "Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\n",
            "Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
            "\n",
            "\fLearn from experts.  \n",
            "Become one yourself.\n",
            "\n",
            "Books | Live online courses   \n",
            "Instant Answers | Virtual events \n",
            "Videos | Interactive learning\n",
            "\n",
            "Get started at oreilly.com. \n",
            "\n",
            "5\n",
            "7\n",
            "1\n",
            "\n",
            "|\n",
            "\n",
            ".\n",
            "\n",
            "c\n",
            "n\n",
            "\n",
            "I\n",
            "\n",
            ",\n",
            "\n",
            "i\n",
            "\n",
            "a\n",
            "d\n",
            "e\n",
            "M\n",
            "y\n",
            "\n",
            "l\n",
            "l\n",
            "i\n",
            "\n",
            "e\n",
            "R\n",
            "O\n",
            "\n",
            "’\n",
            "\n",
            "f\n",
            "o\n",
            "k\n",
            "r\n",
            "a\n",
            "m\n",
            "e\n",
            "d\n",
            "a\n",
            "r\n",
            "t\n",
            "d\n",
            "e\n",
            "r\n",
            "e\n",
            "t\n",
            "s\n",
            "g\n",
            "e\n",
            "r\n",
            "\n",
            "i\n",
            "\n",
            "a\n",
            "\n",
            "s\n",
            "\n",
            "i\n",
            "\n",
            "y\n",
            "\n",
            "l\n",
            "l\n",
            "i\n",
            "\n",
            "e\n",
            "R\n",
            "O\n",
            "\n",
            "’\n",
            "\n",
            ".\n",
            "\n",
            "c\n",
            "n\n",
            "\n",
            "I\n",
            "\n",
            ",\n",
            "\n",
            "i\n",
            "\n",
            "a\n",
            "d\n",
            "e\n",
            "M\n",
            "y\n",
            "\n",
            "l\n",
            "l\n",
            "i\n",
            "\n",
            "’\n",
            "\n",
            "e\n",
            "R\n",
            "O\n",
            "2\n",
            "2\n",
            "0\n",
            "2\n",
            "©\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "text = extract_text(\"sample.pdf\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFJ9VrXklBS8"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r\"[a-zA-Z]+,{1}\\S{1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foz3xZCPl5AP"
      },
      "outputs": [],
      "source": [
        "matches = pattern.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn-x_Ujwl-JM",
        "outputId": "93035f8e-33b0-4b77-9e5b-07872e58353d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['time,2', 'paper,9', 'X,h', 'array,1', 'matrix,9', 'p,q', 'conditions,5', 'pi,k', 'replacement,1', 'wi,j', 'wi,j', 'wi,j', 'layers,9', 'layers,1', 'paper,2', 'inputs,2', 'Classification,”', 'Network,”', 'monotonic,9', 'paper,1', 'Adam,2', 'technique,2', 'inference,2', 'Keras,2', 'method,1', 'MedInc,H', 'ouseAge,A', 'veRooms,A', 'veBedrms,P', 'AveOccup,L', 'neocognitron,4', 'kernel,9', 'Research,1', 'modules,1', 'VGGNet,1', 'identifier,2', 'SSD,3', 'CNN,3', 'decoder,2', 'Sak,1', 'paper,1', 'epochs,3', 'paper,5', 'paper,1', 't,i', 't,i', 'paper,2', 'layers,2', 'Pp,i', 'masking,2', 'paper,3', 'model,3', 'techniques,3', 'prompting,3', 'DINO,4', 'paper,4', 'E,4', 'paper,5', 'GATO,5', 'paper,4', 'paper,5', 'paper,1', 'architecture,2', 'years,1', 'scratch,2', 'control,b', 'd,a', 'tari,a', 'paper,1', 'paper,2', 'paper,2', 'image,4', 'outputs,1']\n"
          ]
        }
      ],
      "source": [
        "print(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ykQFJq5l_Qf",
        "outputId": "5e1399da-1ea5-49b1-9a37-d1a4a12113eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['time', 'paper', 'X', 'array', 'matrix', 'p', 'conditions', 'pi', 'replacement', 'wi', 'wi', 'wi', 'layers', 'layers', 'paper', 'inputs', 'Classification', 'Network', 'monotonic', 'paper', 'Adam', 'technique', 'inference', 'Keras', 'method', 'MedInc', 'ouseAge', 'veRooms', 'veBedrms', 'AveOccup', 'neocognitron', 'kernel', 'Research', 'modules', 'VGGNet', 'identifier', 'SSD', 'CNN', 'decoder', 'Sak', 'paper', 'epochs', 'paper', 'paper', 't', 't', 'paper', 'layers', 'Pp', 'masking', 'paper', 'model', 'techniques', 'prompting', 'DINO', 'paper', 'E', 'paper', 'GATO', 'paper', 'paper', 'paper', 'architecture', 'years', 'scratch', 'control', 'd', 'tari', 'paper', 'paper', 'paper', 'image', 'outputs']\n"
          ]
        }
      ],
      "source": [
        "names = [n[:-2] for n in matches]\n",
        "print(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwkSBLOvmWfV"
      },
      "source": [
        "# images from PDf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOuXItF-mdPm",
        "outputId": "bc986e8d-ad72-4f91-d096-287c390a2311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.5 PyMuPDFb-1.24.3\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcV9Qu_3mnlF"
      },
      "outputs": [],
      "source": [
        "import fitz #PyMuPDF\n",
        "import PIL.Image #Pillow\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D74fzbycm0ni"
      },
      "outputs": [],
      "source": [
        "pdf = fitz.open(\"sample.pdf\")\n",
        "counter  = 1\n",
        "\n",
        "for i in range(len(pdf)):\n",
        "  page = pdf[i]\n",
        "  images = page.get_images()\n",
        "  for image in images:\n",
        "    base_img = pdf.extract_image(image[0])\n",
        "    image_data = base_img[\"image\"]\n",
        "    img = PIL.Image.open(io.BytesIO(image_data))\n",
        "    extension = base_img[\"ext\"]\n",
        "    img.save(open(f\"image{counter}.{extension}\",\"wb\"))\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rs4V1EO0nbdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5506f85-6725-4e17-888d-b8ccac2e12fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.9.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.25.2)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from tabula-py) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Installing collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.9.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tabula-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "\n",
        "tables = tabula.read_pdf(\"sample.pdf\",pages='all')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lJtyXoiQpN63",
        "outputId": "94885e9b-9b8a-43d2-97f1-d02151e943c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2518296b7a1d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabula/io.py\u001b[0m in \u001b[0;36mread_pdf\u001b[0;34m(input_path, output_format, encoding, java_options, pandas_options, multiple_tables, user_agent, use_raw_url, pages, guess, area, relative_area, lattice, stream, password, silent, columns, relative_columns, format, batch, output_path, force_subprocess, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         output = _run(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mtabula_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mjava_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabula/io.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(options, java_options, path, encoding, force_subprocess)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java_options is ignored until rebooting the Python process.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tabula_vm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_tabula_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabula/backend.py\u001b[0m in \u001b[0;36mcall_tabula_java\u001b[0;34m(self, options, path)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             result = subprocess.run(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = tables[0]"
      ],
      "metadata": {
        "id": "v2h7-55_pj9k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j3VnyhGrISm",
        "outputId": "cb75caff-835a-4bd4-8bc7-6eeeec71b9d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ing unusual credit card transactions to prevent fraud, catching manufacturing defects,\n",
            "0  or automatically removing outliers from a data...                                    \n",
            "1  ing algorithm. The system is shown mostly norm...                                    \n",
            "2  learns to recognize them; then, when it sees a...                                    \n",
            "3  like a normal one or whether it is likely an a...                                    \n",
            "4  task is novelty detection: it aims to detect n...                                    \n",
            "5  instances in the training set. This requires h...                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4WnTwz4rJXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJKJT+uXDAIPvX0xxiMhUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}